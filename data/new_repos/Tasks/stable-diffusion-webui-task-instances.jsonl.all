{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 16667, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-16667", "issue_numbers": ["16650"], "base_commit": "7799859f9fb8a0428d33e50a867e2407be4b8620", "patch": "diff --git a/requirements_versions.txt b/requirements_versions.txt\nindex 0306ce94fda..389e2af33b0 100644\n--- a/requirements_versions.txt\n+++ b/requirements_versions.txt\n@@ -22,7 +22,7 @@ protobuf==3.20.0\n psutil==5.9.5\r\n pytorch_lightning==1.9.4\r\n resize-right==0.0.2\r\n-safetensors==0.4.2\r\n+safetensors==0.4.5\r\n scikit-image==0.21.0\r\n spandrel==0.3.4\r\n spandrel-extra-arches==0.1.1\r\n", "test_patch": "", "problem_statement": "[Bug]: Error when loading v-pred model on dev branch\n### Checklist\r\n\r\n- [ ] The issue exists after disabling all extensions\r\n- [ ] The issue exists on a clean installation of webui\r\n- [ ] The issue is caused by an extension, but I believe it is caused by a bug in the webui\r\n- [ ] The issue exists in the current version of the webui\r\n- [X] The issue has not been reported before recently\r\n- [ ] The issue has been reported before but has not been fixed yet\r\n\r\n### What happened?\r\n\r\nAn error is thrown when trying to load a v-pred model using the dev branch.\r\n\r\n### Steps to reproduce the problem\r\n\r\n1. Pull dev branch\r\n2. Start WebUI\r\n3. Try to load v-pred model\r\n\r\n### What should have happened?\r\n\r\nWebUI should successfully load the model.\r\n\r\n### What browsers do you use to access the UI ?\r\n\r\nMozilla Firefox\r\n\r\n### Sysinfo\r\n\r\n\u2022\u2000 version: [v1.10.1-42-g7799859f]\r\n\u2022\u2000 python: 3.10.6 \u2000\r\n\u2022\u2000 torch: 2.3.0+cu121 \u2000\r\n\u2022\u2000 xformers: 0.0.26.post1 \u2000\r\n\u2022\u2000 gradio: 3.41.2 \u2000\r\n\r\n### Console logs\r\n\r\n```Shell\r\nchanging setting sd_model_checkpoint to noobaiXLNAIXL_vPred05Version.safetensors [78748f163e]: ValueError\r\nTraceback (most recent call last):\r\n  File \"A:\\AI\\stable-diffusion-webui\\modules\\options.py\", line 165, in set\r\n    option.onchange()\r\n  File \"A:\\AI\\stable-diffusion-webui\\modules\\call_queue.py\", line 14, in f\r\n    res = func(*args, **kwargs)\r\n  File \"A:\\AI\\stable-diffusion-webui\\modules\\initialize_util.py\", line 181, in <lambda>\r\n    shared.opts.onchange(\"sd_model_checkpoint\", wrap_queued_call(lambda: sd_models.reload_model_weights()), call=False)\r\n  File \"A:\\AI\\stable-diffusion-webui\\modules\\sd_models.py\", line 972, in reload_model_weights\r\n    state_dict = get_checkpoint_state_dict(checkpoint_info, timer)\r\n  File \"A:\\AI\\stable-diffusion-webui\\modules\\sd_models.py\", line 344, in get_checkpoint_state_dict\r\n    res = read_state_dict(checkpoint_info.filename)\r\n  File \"A:\\AI\\stable-diffusion-webui\\modules\\sd_models.py\", line 320, in read_state_dict\r\n    pl_sd = safetensors.torch.load(open(checkpoint_file, 'rb').read())\r\n  File \"A:\\AI\\stable-diffusion-webui\\venv\\lib\\site-packages\\safetensors\\torch.py\", line 338, in load\r\n    return _view2torch(flat)\r\n  File \"A:\\AI\\stable-diffusion-webui\\venv\\lib\\site-packages\\safetensors\\torch.py\", line 386, in _view2torch\r\n    arr = torch.frombuffer(v[\"data\"], dtype=dtype).reshape(v[\"shape\"])\r\nValueError: both buffer length (0) and count (-1) must not be 0\r\n```\r\n\r\n\r\n### Additional information\r\n\r\nI have tried manually loading the checkpoint using the WebUI env using Diffusers via the following snippet and it successfully loads the model and generates an image: https://huggingface.co/Laxhar/noobai-XL-Vpred-0.5#method-iv-diffusers\r\n\r\nI've also tried loading it via the safetensors module (the thing throwing the error above) and it loads fine that way too:\r\n```\r\nimport safetensors.torch\r\nimport torch\r\n\r\nmodel_path = \"./models/Stable-diffusion/noobaiXLNAIXL_vPred05Version.safetensors\"\r\n\r\ntry:\r\n    model = safetensors.torch.load_file(model_path, device='cuda' if torch.cuda.is_available() else 'cpu')\r\n    print(\"V-pred model loaded successfully.\")\r\nexcept Exception as e:\r\n    print(f\"Failed to load V-pred model: {e}\")\r\n```\r\n\r\nThis would seem to indicate that the issue is within WebUI itself.\n", "hints_text": "> I have tried manually loading the checkpoint using the WebUI env using Diffusers via the following snippet and it successfully\r\n\r\nare you using the same exact file?\r\nI'm suspecting that your model is just corrupted somehow, reason is that in your logs\r\n```\r\nchanging setting sd_model_checkpoint to noobaiXLNAIXL_vPred05Version.safetensors [78748f163e]: ValueError\r\n```\r\n`78748f163e` is the first 10 characters of the sha256 hash of the file\r\nwhich does not match sha256 https://huggingface.co/Laxhar/noobai-XL-Vpred-0.5/blob/main/noobai-xl-vpred-v0.5.safetensors\r\nwhitch is `f63416db812dbbdfeeb37715ca34ceed293b49bc17d3b99ec8c79364419e5eb1`\r\n\r\n\nI redownloaded it from [Civit](https://civitai.com/models/833294) twice when checking it, and the shasum matches what's listed there for Auto V2. Guess they uploaded slightly different files to Civit and HF or something.\r\nAlso, as noted it generates fine when using diffusers rather than WebUI.\r\n\r\nEither way, I've tried with the HF one and same result:\r\n```\r\nLoading weights [f63416db81] from A:\\AI\\stable-diffusion-webui\\models\\Stable-diffusion\\noobai-xl-vpred-v0.5.safetensors\r\nchanging setting sd_model_checkpoint to noobai-xl-vpred-v0.5.safetensors: ValueError\r\nTraceback (most recent call last):\r\n  File \"A:\\AI\\stable-diffusion-webui\\modules\\options.py\", line 165, in set\r\n    option.onchange()\r\n  File \"A:\\AI\\stable-diffusion-webui\\modules\\call_queue.py\", line 14, in f\r\n    res = func(*args, **kwargs)\r\n  File \"A:\\AI\\stable-diffusion-webui\\modules\\initialize_util.py\", line 181, in <lambda>\r\n    shared.opts.onchange(\"sd_model_checkpoint\", wrap_queued_call(lambda: sd_models.reload_model_weights()), call=False)\r\n  File \"A:\\AI\\stable-diffusion-webui\\modules\\sd_models.py\", line 972, in reload_model_weights\r\n    state_dict = get_checkpoint_state_dict(checkpoint_info, timer)\r\n  File \"A:\\AI\\stable-diffusion-webui\\modules\\sd_models.py\", line 344, in get_checkpoint_state_dict\r\n    res = read_state_dict(checkpoint_info.filename)\r\n  File \"A:\\AI\\stable-diffusion-webui\\modules\\sd_models.py\", line 320, in read_state_dict\r\n    pl_sd = safetensors.torch.load(open(checkpoint_file, 'rb').read())\r\n  File \"A:\\AI\\stable-diffusion-webui\\venv\\lib\\site-packages\\safetensors\\torch.py\", line 338, in load\r\n    return _view2torch(flat)\r\n  File \"A:\\AI\\stable-diffusion-webui\\venv\\lib\\site-packages\\safetensors\\torch.py\", line 386, in _view2torch\r\n    arr = torch.frombuffer(v[\"data\"], dtype=dtype).reshape(v[\"shape\"])\r\nValueError: both buffer length (0) and count (-1) must not be 0\r\n```\ni used same model and had same problem, then fixed this by, Settings \u2192 System, Disable\r\n![image](https://github.com/user-attachments/assets/ca518aaf-2d78-4435-9c3a-ecf665324c33)\r\n![image](https://github.com/user-attachments/assets/990fee06-d29e-42e1-a38d-6c7a6de0c09d)\r\n\nSo unchecking that box does indeed let the model load and work. FYI in English it reads:\r\n```\r\nDisable memmapping for loading .safetensors files. (fixes very slow loading speed in some cases)\r\n```\r\n\r\nBeing that I had checked this box to indeed fix slow loading speeds, I now wonder whether this is still a bug, or if memmapping is simply necessary for v-pred models to work.", "created_at": "2024-11-19T01:49:20Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 16460, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-16460", "issue_numbers": ["16459"], "base_commit": "48239090f18d07acd8070a7af9da38b0021fdea3", "patch": "diff --git a/modules/sd_models.py b/modules/sd_models.py\nindex 55bd9ca5e43..f9f3f07310b 100644\n--- a/modules/sd_models.py\n+++ b/modules/sd_models.py\n@@ -159,7 +159,7 @@ def list_models():\n         model_url = None\r\n         expected_sha256 = None\r\n     else:\r\n-        model_url = f\"{shared.hf_endpoint}/runwayml/stable-diffusion-v1-5/resolve/main/v1-5-pruned-emaonly.safetensors\"\r\n+        model_url = f\"{shared.hf_endpoint}/stable-diffusion-v1-5/stable-diffusion-v1-5/resolve/main/v1-5-pruned-emaonly.safetensors\"\r\n         expected_sha256 = '6ce0161689b3853acaa03779ec93eafe75a02f4ced659bee03f50797806fa2fa'\r\n \r\n     model_list = modelloader.load_models(model_path=model_path, model_url=model_url, command_path=shared.cmd_opts.ckpt_dir, ext_filter=[\".ckpt\", \".safetensors\"], download_name=\"v1-5-pruned-emaonly.safetensors\", ext_blacklist=[\".vae.ckpt\", \".vae.safetensors\"], hash_prefix=expected_sha256)\r\n", "test_patch": "", "problem_statement": "[Bug]: runwayml removed SD1.5 repo\n### Checklist\r\n\r\n- [ ] The issue exists after disabling all extensions\r\n- [X] The issue exists on a clean installation of webui\r\n- [ ] The issue is caused by an extension, but I believe it is caused by a bug in the webui\r\n- [X] The issue exists in the current version of the webui\r\n- [ ] The issue has not been reported before recently\r\n- [ ] The issue has been reported before but has not been fixed yet\r\n\r\n### What happened?\r\n\r\nwhen no checkpoint/safetensor download, model will pull the default sd1.5 safetensor from runwayml HF repo. recently they removed it \r\n\r\n### Steps to reproduce the problem\r\n\r\nrun the webUI script as usual\r\n\r\n### What should have happened?\r\n\r\nif no safetensor/checkpoint is download, it shall pull the sd1.5 safetensor from HF\r\n\r\n### What browsers do you use to access the UI ?\r\n\r\nGoogle Chrome\r\n\r\n### Sysinfo\r\n\r\n[sysinfo-2024-09-03-19-04.json](https://github.com/user-attachments/files/16853635/sysinfo-2024-09-03-19-04.json)\r\n\r\n\r\n### Console logs\r\n\r\n```Shell\r\nPython 3.10.12 (main, Jul 29 2024, 16:56:48) [GCC 11.4.0]\r\nVersion: v1.10.1\r\nCommit hash: 82a973c04367123ae98bd9abdf80d9eda9b910e2\r\nInstalling requirements\r\nLaunching Web UI with arguments: --listen\r\nno module 'xformers'. Processing without...\r\nno module 'xformers'. Processing without...\r\nNo module 'xformers'. Proceeding without it.\r\nDownloading: \"https://huggingface.co/runwayml/stable-diffusion-v1-5/resolve/main/v1-5-pruned-emaonly.safetensors\" to /home/user1/Downloads/play_ground/drawing/stable-diffusion-webui/models/Stable-diffusion/v1-5-pruned-emaonly.safetensors\r\n\r\nloading stable diffusion model: FileNotFoundError\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.10/threading.py\", line 973, in _bootstrap\r\n    self._bootstrap_inner()\r\n  File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\r\n    self.run()\r\n  File \"/usr/lib/python3.10/threading.py\", line 953, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"/home/user1/Downloads/play_ground/drawing/stable-diffusion-webui/modules/initialize.py\", line 149, in load_model\r\n    shared.sd_model  # noqa: B018\r\n  File \"/home/user1/Downloads/play_ground/drawing/stable-diffusion-webui/modules/shared_items.py\", line 175, in sd_model\r\n    return modules.sd_models.model_data.get_sd_model()\r\n  File \"/home/user1/Downloads/play_ground/drawing/stable-diffusion-webui/modules/sd_models.py\", line 693, in get_sd_model\r\n    load_model()\r\n  File \"/home/user1/Downloads/play_ground/drawing/stable-diffusion-webui/modules/sd_models.py\", line 788, in load_model\r\n    checkpoint_info = checkpoint_info or select_checkpoint()\r\n  File \"/home/user1/Downloads/play_ground/drawing/stable-diffusion-webui/modules/sd_models.py\", line 234, in select_checkpoint\r\n    raise FileNotFoundError(error_message)\r\nFileNotFoundError: No checkpoints found. When searching for checkpoints, looked at:\r\n - file /home/user1/Downloads/play_ground/drawing/stable-diffusion-webui/model.ckpt\r\n - directory /home/user1/Downloads/play_ground/drawing/stable-diffusion-webui/models/Stable-diffusionCan't run without a checkpoint. Find and place a .ckpt or .safetensors file into any of those locations.\r\n\r\n\r\nStable diffusion model failed to load\r\nApplying attention optimization: Doggettx... done.\r\nRunning on local URL:  http://0.0.0.0:7860\r\n\r\nTo create a public link, set `share=True` in `launch()`.\r\nStartup time: 11.8s (prepare environment: 4.6s, import torch: 2.6s, import gradio: 0.6s, setup paths: 2.6s, other imports: 0.6s, list SD models: 0.1s, load scripts: 0.2s, create ui: 0.3s).\r\nloading stable diffusion model: FileNotFoundError\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.10/threading.py\", line 973, in _bootstrap\r\n    self._bootstrap_inner()\r\n  File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\r\n    self.run()\r\n  File \"/home/user1/Downloads/play_ground/test/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 807, in run\r\n    result = context.run(func, *args)\r\n  File \"/home/user1/Downloads/play_ground/test/lib/python3.10/site-packages/gradio/utils.py\", line 707, in wrapper\r\n    response = f(*args, **kwargs)\r\n  File \"/home/user1/Downloads/play_ground/drawing/stable-diffusion-webui/modules/ui.py\", line 1165, in <lambda>\r\n    update_image_cfg_scale_visibility = lambda: gr.update(visible=shared.sd_model and shared.sd_model.cond_stage_key == \"edit\")\r\n  File \"/home/user1/Downloads/play_ground/drawing/stable-diffusion-webui/modules/shared_items.py\", line 175, in sd_model\r\n    return modules.sd_models.model_data.get_sd_model()\r\n  File \"/home/user1/Downloads/play_ground/drawing/stable-diffusion-webui/modules/sd_models.py\", line 693, in get_sd_model\r\n    load_model()\r\n  File \"/home/user1/Downloads/play_ground/drawing/stable-diffusion-webui/modules/sd_models.py\", line 788, in load_model\r\n    checkpoint_info = checkpoint_info or select_checkpoint()\r\n  File \"/home/user1/Downloads/play_ground/drawing/stable-diffusion-webui/modules/sd_models.py\", line 234, in select_checkpoint\r\n    raise FileNotFoundError(error_message)\r\nFileNotFoundError: No checkpoints found. When searching for checkpoints, looked at:\r\n - file /home/user1/Downloads/play_ground/drawing/stable-diffusion-webui/model.ckpt\r\n - directory /home/user1/Downloads/play_ground/drawing/stable-diffusion-webui/models/Stable-diffusionCan't run without a checkpoint. Find and place a .ckpt or .safetensors file into any of those locations.\r\n\r\n\r\nStable diffusion model failed to load\r\n```\r\n\r\n\r\n### Additional information\r\n\r\nthis repo has original sd1.5 archive saved \r\nhttps://huggingface.co/botp/stable-diffusion-v1-5\n", "hints_text": "", "created_at": "2024-09-04T02:09:17Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 16287, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-16287", "issue_numbers": ["16205"], "base_commit": "48239090f18d07acd8070a7af9da38b0021fdea3", "patch": "diff --git a/webui.bat b/webui.bat\nindex 7b162ce27cc..f29a40b36e9 100644\n--- a/webui.bat\n+++ b/webui.bat\n@@ -4,7 +4,16 @@ if exist webui.settings.bat (\n     call webui.settings.bat\r\n )\r\n \r\n-if not defined PYTHON (set PYTHON=python)\r\n+if not defined PYTHON (\r\n+  for /f \"delims=\" %%A in ('where python ^| findstr /n . ^| findstr ^^1:') do (\r\n+    if /i \"%%~xA\" == \".exe\" (\r\n+      set PYTHON=python\r\n+    ) else (\r\n+      set PYTHON=call python\r\n+    )\r\n+  )\r\n+)\r\n+\r\n if defined GIT (set \"GIT_PYTHON_GIT_EXECUTABLE=%GIT%\")\r\n if not defined VENV_DIR (set \"VENV_DIR=%~dp0%venv\")\r\n \r\n", "test_patch": "", "problem_statement": "[Bug]: Calling webui-user.bat hangs because python commands are not called with 'call' statement\n### Checklist\n\n- [X] The issue exists after disabling all extensions\n- [X] The issue exists on a clean installation of webui\n- [X] The issue is caused by an extension, but I believe it is caused by a bug in the webui\n- [X] The issue exists in the current version of the webui\n- [X] The issue has not been reported before recently\n- [X] The issue has been reported before but has not been fixed yet\n\n### What happened?\n\nCalling webui-user.bat in a fresh installations hangs forever. \r\n\r\nThis is due to how python behaves on Windows if python calls are not called with 'call'. The solution is to change all lines where a python command is invoked in webui.bat.\r\n\r\n```\r\ncall %PYTHON% -c \"\" >tmp/stdout.txt 2>tmp/stderr.txt\r\ncall %PYTHON% -mpip --help >tmp/stdout.txt 2>tmp/stderr.txt\r\ncall %PYTHON% \"%PIP_INSTALLER_LOCATION%\" >tmp/stdout.txt 2>tmp/stderr.txt\r\ncall %PYTHON_FULLNAME% -m venv \"%VENV_DIR%\" >tmp/stdout.txt 2>tmp/stderr.txt\r\n```\n\n### Steps to reproduce the problem\n\n1. >webui-user.bat\n\n### What should have happened?\n\nWebui launch script should make progress and start somewhen\n\n### What browsers do you use to access the UI ?\n\nMozilla Firefox\n\n### Sysinfo\n\nSince WebUI doesn't start without help this is not appropriate ;)\r\n\r\nI am using python 3.10.5 via pyenv-win on Windows 11 latest.\n\n### Console logs\n\n```Shell\nNo logs generated in tmp/*, stderr and stdout are empty.\n```\n\n\n### Additional information\n\nnothing to add here.\n", "hints_text": "`set PYTHON=call python` in `webui-user.bat` is enough. I just tested before I updated my https://github.com/AUTOMATIC1111/stable-diffusion-webui/issues/16261#issuecomment-2250806110\nlatest dev branch will stuck on startup after the line \"Commit hash\", pressing ctrl z after a while show call stacks with bottom lines with \"... _wait_for_tstate_lock\r\nif lock.acquire(block, timeout): ....\"\r\n\r\nswitching back to master will not stuck", "created_at": "2024-07-28T22:07:53Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 16192, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-16192", "issue_numbers": ["16169"], "base_commit": "93c00b2af7f8df84b0e22deea19beea056a22025", "patch": "diff --git a/scripts/xyz_grid.py b/scripts/xyz_grid.py\nindex b702c74d821..5664932669c 100644\n--- a/scripts/xyz_grid.py\n+++ b/scripts/xyz_grid.py\n@@ -118,7 +118,7 @@ def apply_size(p, x: str, xs) -> None:\n \r\n \r\n def find_vae(name: str):\r\n-    if name := name.strip().lower() in ('auto', 'automatic'):\r\n+    if (name := name.strip().lower()) in ('auto', 'automatic'):\r\n         return 'Automatic'\r\n     elif name == 'none':\r\n         return 'None'\r\n", "test_patch": "", "problem_statement": "Py 3.9 compatibility\n## Description\r\nsince we are trying to keey compatibility with python 3.9 some changes need to be made\r\n\r\nmatch case syntax in `xyz.find_vae()`\r\n- same as https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16088\r\n\r\n`from __future__ import annotations` for `extensions-builtin/Lora/networks.py`\r\nhttps://github.com/AUTOMATIC1111/stable-diffusion-webui/blob/c3d8b78b47dddb03bb4558d62c6eaffc167cc51b/extensions-builtin/Lora/networks.py#L596\r\n\r\n---\r\n\r\n- https://github.com/AUTOMATIC1111/stable-diffusion-webui/issues/16168\r\n\r\nthis person was using python 3.9\r\n\r\n---\r\n\r\npush to RC\r\n\r\n## Checklist:\r\n\r\n- [x] I have read [contributing wiki page](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Contributing)\r\n- [x] I have performed a self-review of my own code\r\n- [x] My code follows the [style guidelines](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Contributing#code-style)\r\n- [x] My code passes [tests](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Tests)\r\n\n", "hints_text": "Is there some build server CI/CD magic to enforce this?\r\nI've had version issues before and those are not always easy to find out. In this case the syntax error made it _easy_ to discover as a python feature that was newer then the python i had.\r\n\r\nWhat also doesn't help is the mention spread on this site in various install documents (ex. [here](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Install-and-Run-on-AMD-GPUs) and [here](https://github.com/AUTOMATIC1111/stable-diffusion-webui?tab=readme-ov-file#automatic-installation-on-windows)) that `3.10.6` is the minimal python requirement. I'd say that either all those docs would have to reflect the actual minimal requirement (3.9.something) or that 3.10.6 should in fact be the minimal requirement but then things like this docker image should be updated to reflect that.\nfrom the start the intended version is always being 3.10\r\nbut it used to be that Google Colab is on an old version of python\r\nso an effort was made to maintaining compatibility with old versions\r\nbut since then Colab has upgreaded so most devs just don't care any more\r\n\r\n---\r\n\r\nif you use other versions python from you will get big warning\r\nhttps://github.com/AUTOMATIC1111/stable-diffusion-webui/blob/feee37d75f1b168768014e4634dcb156ee649c05/modules/launch_utils.py#L47-L61\r\nbut most people just ignore the warning\r\n\r\nupdate,\r\nahh I see, the warning is basically disabled on linux\r\n\r\n---\r\n\r\nI only made this PR because someone made a [prior PR](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16088) that change the `match case` (whitch I myself introduced)\r\nand since somemade that PR and was merged, I made this since it's a trivial change to follow suit\r\n\r\nif in the future that due to reason such as package incompatibility or if 3.9 is preventing us development we will probably just don't care about 3.9 and continue on\r\n\r\nquoting someone other dev\r\n> I would put zero effort into trying to make 3.9 work, but if people are contributing things that make it work, I would take them\r\n\r\npersonally if it's up to me I would have deliberately broken compatibility for python 3.9\r\nforcing people to upgrade and reducing the chance of potential odd issues\r\n> I would even upgrade to 3.11, I know for a fact that a couple of devs are using 3.11 for a long time now\r\n\r\n---\r\n\r\nas far as I'm where we are did not made any official docker container image\r\nif you're using an image that use 3.9 then whoever made that image didn't configure the image correctly\r\n\nHi @w-e-w,\r\n\r\nThank you for your elaborate response! That's nice and helps :)\r\nI only have one remark left, that's for docker.\r\nWhile this repo might not _host_ a container, the instructions to get one up and running are here: https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Install-and-Run-on-AMD-GPUs And that container by default has that python 3.9.x version.\r\n\r\nIt's simple to fix the instructions:\r\n- `conda init bash`\r\n- and then a line to setup conda with the right python version\r\n- plus a line to use that environment\r\n- and the container works (probably.. i did have to install the `timm` so there might be something wrong with the requirements too somewhere..\r\n\r\nBut that's about it and gives a \"hey, it works with instructions\" experience again :)\noh my it worte 3.9...", "created_at": "2024-07-11T09:48:47Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 15657, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-15657", "issue_numbers": ["15645"], "base_commit": "ddb28b33a3561a360b429c76f28f7ff1ffe282a0", "patch": "diff --git a/javascript/dragdrop.js b/javascript/dragdrop.js\nindex 0c018356419..882562d7367 100644\n--- a/javascript/dragdrop.js\n+++ b/javascript/dragdrop.js\n@@ -56,6 +56,15 @@ function eventHasFiles(e) {\n     return false;\n }\n \n+function isURL(url) {\n+    try {\n+        const _ = new URL(url);\n+        return true;\n+    } catch {\n+        return false;\n+    }\n+}\n+\n function dragDropTargetIsPrompt(target) {\n     if (target?.placeholder && target?.placeholder.indexOf(\"Prompt\") >= 0) return true;\n     if (target?.parentNode?.parentNode?.className?.indexOf(\"prompt\") > 0) return true;\n@@ -77,7 +86,7 @@ window.document.addEventListener('dragover', e => {\n window.document.addEventListener('drop', async e => {\n     const target = e.composedPath()[0];\n     const url = e.dataTransfer.getData('text/uri-list') || e.dataTransfer.getData('text/plain');\n-    if (!eventHasFiles(e) && !url) return;\n+    if (!eventHasFiles(e) && !isURL(url)) return;\n \n     if (dragDropTargetIsPrompt(target)) {\n         e.stopPropagation();\n", "test_patch": "", "problem_statement": "[Bug]: Prompt in textarea cannot be drag and drop since 1.9.0\n### Checklist\n\n- [X] The issue exists after disabling all extensions\n- [X] The issue exists on a clean installation of webui\n- [ ] The issue is caused by an extension, but I believe it is caused by a bug in the webui\n- [X] The issue exists in the current version of the webui\n- [X] The issue has not been reported before recently\n- [ ] The issue has been reported before but has not been fixed yet\n\n### What happened?\n\ndrag and drop prompt text in textarea is not working since 1.9.0rc, it's work at 1.8.0\n\n### Steps to reproduce the problem\n\n1. Type \"masterpiece, highly details, ultra quality\" in prompt\r\n2. Select masterpiece and drag it between \"highly details\" and \"ultra quality\"\r\n3. Nothing happened\n\n### What should have happened?\n\n\"masterpiece\" between \"highly details\" and \"ultra quality\"\n\n### What browsers do you use to access the UI ?\n\nGoogle Chrome\n\n### Sysinfo\n\n[sysinfo-2024-04-27-20-12.json](https://github.com/AUTOMATIC1111/stable-diffusion-webui/files/15139759/sysinfo-2024-04-27-20-12.json)\r\n\n\n### Console logs\n\n```Shell\ndragdrop.js?1713798036.0:97 \r\nGET https://127.0.0.1:8080/masterpiece 404 (Not Found)\r\n(anonymous) @ dragdrop.js?1713798036.0:97\r\ndragdrop.js?1713798036.0:99 Error fetching URL: masterpiece 404\r\n(anonymous) @ dragdrop.js?1713798036.0:99\n```\n\n\n### Additional information\n\n_No response_\n", "hints_text": "Prompts can also be reordered with Alt+Left/Right.\nmy guess this is the cause\r\n- https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15262\r\n\r\nirrc this was to support drag drop image as input in some browsers like firefox\n> Prompts can also be reordered with Alt+Left/Right.\r\n\r\nTrue but using mouse more accuracy and quicker for long prompt.\r\n\r\n\r\n> my guess this is the cause\r\n> \r\n> * [Support dragdrop for URLs to read infotext\u00a0#15262](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15262)\r\n> \r\n> irrc this was to support drag drop image as input in some browsers like firefox\r\n\r\nCorrect, the console logs shown it caused by dragdrop.js", "created_at": "2024-04-29T04:49:03Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 15656, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-15656", "issue_numbers": ["15603"], "base_commit": "ddb28b33a3561a360b429c76f28f7ff1ffe282a0", "patch": "diff --git a/modules/api/api.py b/modules/api/api.py\nindex f468c385275..d8e54529b40 100644\n--- a/modules/api/api.py\n+++ b/modules/api/api.py\n@@ -438,15 +438,19 @@ def text2imgapi(self, txt2imgreq: models.StableDiffusionTxt2ImgProcessingAPI):\n         self.apply_infotext(txt2imgreq, \"txt2img\", script_runner=script_runner, mentioned_script_args=infotext_script_args)\n \n         selectable_scripts, selectable_script_idx = self.get_selectable_script(txt2imgreq.script_name, script_runner)\n+        sampler, scheduler = sd_samplers.get_sampler_and_scheduler(txt2imgreq.sampler_name or txt2imgreq.sampler_index, txt2imgreq.scheduler)\n \n         populate = txt2imgreq.copy(update={  # Override __init__ params\n-            \"sampler_name\": validate_sampler_name(txt2imgreq.sampler_name or txt2imgreq.sampler_index),\n+            \"sampler_name\": validate_sampler_name(sampler),\n             \"do_not_save_samples\": not txt2imgreq.save_images,\n             \"do_not_save_grid\": not txt2imgreq.save_images,\n         })\n         if populate.sampler_name:\n             populate.sampler_index = None  # prevent a warning later on\n \n+        if not populate.scheduler and scheduler != \"Automatic\":\n+            populate.scheduler = scheduler\n+\n         args = vars(populate)\n         args.pop('script_name', None)\n         args.pop('script_args', None) # will refeed them to the pipeline directly after initializing them\n@@ -502,9 +506,10 @@ def img2imgapi(self, img2imgreq: models.StableDiffusionImg2ImgProcessingAPI):\n         self.apply_infotext(img2imgreq, \"img2img\", script_runner=script_runner, mentioned_script_args=infotext_script_args)\n \n         selectable_scripts, selectable_script_idx = self.get_selectable_script(img2imgreq.script_name, script_runner)\n+        sampler, scheduler = sd_samplers.get_sampler_and_scheduler(img2imgreq.sampler_name or img2imgreq.sampler_index, img2imgreq.scheduler)\n \n         populate = img2imgreq.copy(update={  # Override __init__ params\n-            \"sampler_name\": validate_sampler_name(img2imgreq.sampler_name or img2imgreq.sampler_index),\n+            \"sampler_name\": validate_sampler_name(sampler),\n             \"do_not_save_samples\": not img2imgreq.save_images,\n             \"do_not_save_grid\": not img2imgreq.save_images,\n             \"mask\": mask,\n@@ -512,6 +517,9 @@ def img2imgapi(self, img2imgreq: models.StableDiffusionImg2ImgProcessingAPI):\n         if populate.sampler_name:\n             populate.sampler_index = None  # prevent a warning later on\n \n+        if not populate.scheduler and scheduler != \"Automatic\":\n+            populate.scheduler = scheduler\n+\n         args = vars(populate)\n         args.pop('include_init_images', None)  # this is meant to be done by \"exclude\": True in model, but it's for a reason that I cannot determine.\n         args.pop('script_name', None)\n", "test_patch": "", "problem_statement": "[Feature Request]: Backwards compatibility for txt2img and img2img API calls in 1.9.x versions\n### Is there an existing issue for this?\n\n- [X] I have searched the existing issues and checked the recent builds/commits\n\n### What would your feature do ?\n\nProvide backwards compatibility for the txt2img and img2img APIs\n\n### Proposed workflow\n\nMake the API backwards compatible for people who are already using the API in their applications by making the `txt2img` and `img2img` API calls accept the previous sampler names and automatically split the legacy sampler names into their new `sampler` and `scheduler` settings instead of failing with 404 sampler not found.\r\n\r\nThis will help people who are using the API to seamlessly upgrade to 1.9.x versions without having to rewrite any of their existing code for the API calls to function correctly.\r\n\r\n\n\n### Additional information\n\n_No response_\n", "hints_text": "", "created_at": "2024-04-29T04:43:32Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 15567, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-15567", "issue_numbers": ["15546"], "base_commit": "ff6f4680c40cd033697af1f87e704f74af0977c7", "patch": "diff --git a/modules/textual_inversion/image_embedding.py b/modules/textual_inversion/image_embedding.py\nindex ea4b88333ac..d6a6a260b56 100644\n--- a/modules/textual_inversion/image_embedding.py\n+++ b/modules/textual_inversion/image_embedding.py\n@@ -1,12 +1,15 @@\n import base64\r\n import json\r\n import warnings\r\n+import logging\r\n \r\n import numpy as np\r\n import zlib\r\n from PIL import Image, ImageDraw\r\n import torch\r\n \r\n+logger = logging.getLogger(__name__)\r\n+\r\n \r\n class EmbeddingEncoder(json.JSONEncoder):\r\n     def default(self, obj):\r\n@@ -114,7 +117,7 @@ def extract_image_data_embed(image):\n     outarr = crop_black(np.array(image.convert('RGB').getdata()).reshape(image.size[1], image.size[0], d).astype(np.uint8)) & 0x0F\r\n     black_cols = np.where(np.sum(outarr, axis=(0, 2)) == 0)\r\n     if black_cols[0].shape[0] < 2:\r\n-        print('No Image data blocks found.')\r\n+        logger.debug('No Image data blocks found.')\r\n         return None\r\n \r\n     data_block_lower = outarr[:, :black_cols[0].min(), :].astype(np.uint8)\r\n", "test_patch": "", "problem_statement": "[Feature Request]: No Image data blocks found\n### What would your feature do ?\r\n\r\nOn launch the web verify things and say this cryptic message: No Image data blocks found\r\nI think it's because one extension has a missing image.preview or another thing, but it is possible to be more precise on this error message? Like also giving the extension name, the file path or any other info why this message show? \r\nThank\r\n\n", "hints_text": "", "created_at": "2024-04-19T03:02:28Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 15316, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-15316", "issue_numbers": ["15309"], "base_commit": "7ac7600dc34040e4b14910a39cd4850a264ca8be", "patch": "diff --git a/modules/ui_extra_networks.py b/modules/ui_extra_networks.py\nindex 34c46ed403a..bfb66a70920 100644\n--- a/modules/ui_extra_networks.py\n+++ b/modules/ui_extra_networks.py\n@@ -10,6 +10,7 @@\n import gradio as gr\r\n import json\r\n import html\r\n+import re\r\n from fastapi.exceptions import HTTPException\r\n \r\n from modules.infotext_utils import image_from_url_text\r\n@@ -236,7 +237,7 @@ def create_item_html(\n             )\r\n             onclick = html.escape(onclick)\r\n \r\n-        btn_copy_path = self.btn_copy_path_tpl.format(**{\"filename\": item[\"filename\"]})\r\n+        btn_copy_path = self.btn_copy_path_tpl.format(**{\"filename\": re.sub(r\"[\\\\\\\"']\", r\"\\\\\\g<0>\", item[\"filename\"])})\r\n         btn_metadata = \"\"\r\n         metadata = item.get(\"metadata\")\r\n         if metadata:\r\n", "test_patch": "", "problem_statement": "[Bug]: \"Copy Path To Clipboard\" function - No directory separators on Windows, loses subdirectories\n### Checklist\r\n\r\n- [X] The issue exists after disabling all extensions\r\n- [X] The issue exists on a clean installation of webui\r\n- [ ] The issue is caused by an extension, but I believe it is caused by a bug in the webui\r\n- [X] The issue exists in the current version of the webui\r\n- [X] The issue has not been reported before recently\r\n- [ ] The issue has been reported before but has not been fixed yet\r\n\r\n### What happened?\r\n\r\nWhen attempting to copy the path of a checkpoint or Lora to clipboard using the \"Copy Path To Clipboard\" icon (either on the card, or on the tree view), the resulting data in the clipboard lacks any directory separators.  \r\n\r\ne.g. `E:A1111 Web UIStabilityMatrixDataPackagesStable Diffusion WebUImodelsStable-diffusion`\r\n\r\nI suspect this an escaping issue, since Windows uses the backslash as a directory separator, while python uses it for character escaping.\r\n\r\n![Capture](https://github.com/AUTOMATIC1111/stable-diffusion-webui/assets/11566412/1b94a544-6e67-4959-93a3-9826fd9a6414)\r\n\r\n\r\n\r\n### Steps to reproduce the problem\r\n\r\n1. Load WebUI on Windows \r\n2. Go to LORA or Checkpoints tab\r\n3. Click on the \"Copy To Clipboard\" icon on any resource\r\n4. Paste the clipboard contents into a text editor\r\n\r\n### What should have happened?\r\n\r\nThe clipboard contents should contain a proper windows path to the location of the resournce.  e.g `E:\\A1111 Web UI\\StabilityMatrix\\Data\\Packages\\Stable Diffusion WebUI\\models\\Stable-diffusion\\00. Favorites\\Illustration`\r\n\r\n### What browsers do you use to access the UI ?\r\n\r\nMozilla Firefox, Google Chrome, Other\r\n\r\n### Sysinfo\r\n\r\n[sysinfo-2024-03-18-18-41.json](https://github.com/AUTOMATIC1111/stable-diffusion-webui/files/14641037/sysinfo-2024-03-18-18-41.json)\r\n\r\n\r\n### Console logs\r\n\r\n```Shell\r\nPython 3.10.11 (tags/v3.10.11:7d4cc5a, Apr  5 2023, 00:38:17) [MSC v.1929 64 bit (AMD64)]\r\nVersion: v1.8.0\r\nCommit hash: bef51aed032c0aaa5cfd80445bc4cf0d85b408b5\r\nLaunching Web UI with arguments: --xformers --api --skip-python-version-check --no-download-sd-model --no-half-vae --listen --theme dark \r\nLoading weights [27a4ac756c] from E:\\A1111 Web UI\\StabilityMatrixData\\Packages\\Stable Diffusion WebUI\\models\\Stable-diffusion\\SD1.5-Vanilla.ckpt\r\nRunning on local URL:  http://0.0.0.0:7860\r\nCreating model from config: E:\\A1111 Web UI\\StabilityMatrixData\\Packages\\Stable Diffusion WebUI\\configs\\v1-inference.yaml\r\nApplying attention optimization: xformers... done.\r\nModel loaded in 2.2s (load weights from disk: 1.1s, create model: 0.2s, apply weights to model: 0.4s, load textual inversion embeddings: 0.2s, calculate empty prompt: 0.1s).\r\n\r\nTo create a public link, set `share=True` in `launch()`.\r\nStartup time: 12.4s (prepare environment: 1.7s, import torch: 2.8s, import gradio: 0.8s, setup paths: 0.8s, initialize shared: 0.2s, other imports: 0.5s, list SD models: 0.1s, load scripts: 0.6s, create ui: 0.6s, gradio launch: 4.3s, add APIs: 0.1s).\r\n```\r\n\r\n\r\n### Additional information\r\n\r\n_No response_\n", "hints_text": "", "created_at": "2024-03-19T03:33:45Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 15263, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-15263", "issue_numbers": ["15258"], "base_commit": "3e0146f9bdd79ed13d1fed729c76b97f7ab91587", "patch": "diff --git a/modules/processing_scripts/comments.py b/modules/processing_scripts/comments.py\nindex 638e39f2989..cf81dfd8b49 100644\n--- a/modules/processing_scripts/comments.py\n+++ b/modules/processing_scripts/comments.py\n@@ -26,6 +26,13 @@ def process(self, p, *args):\n         p.main_prompt = strip_comments(p.main_prompt)\r\n         p.main_negative_prompt = strip_comments(p.main_negative_prompt)\r\n \r\n+        if getattr(p, 'enable_hr', False):\r\n+            p.all_hr_prompts = [strip_comments(x) for x in p.all_hr_prompts]\r\n+            p.all_hr_negative_prompts = [strip_comments(x) for x in p.all_hr_negative_prompts]\r\n+\r\n+            p.hr_prompt = strip_comments(p.hr_prompt)\r\n+            p.hr_negative_prompt = strip_comments(p.hr_negative_prompt)\r\n+\r\n \r\n def before_token_counter(params: script_callbacks.BeforeTokenCounterParams):\r\n     if not shared.opts.enable_prompt_comments:\r\n", "test_patch": "", "problem_statement": "[Bug]: Hires Fix ignores prompt comments\n### Checklist\n\n- [X] The issue exists after disabling all extensions\n- [X] The issue exists on a clean installation of webui\n- [ ] The issue is caused by an extension, but I believe it is caused by a bug in the webui\n- [X] The issue exists in the current version of the webui\n- [X] The issue has not been reported before recently\n- [ ] The issue has been reported before but has not been fixed yet\n\n### What happened?\n\nHighres fix ignores the prompt comment featured added in 1.8.0.\r\n\r\nPrompt example:\r\n># emma watson, red head #\r\n>woman\r\n\r\n![image](https://github.com/AUTOMATIC1111/stable-diffusion-webui/assets/136133907/0b083ad6-9686-4463-93d8-5a5a1f87a551)\r\n\r\nIt works fine before the highres.\n\n### Steps to reproduce the problem\n\n1. Make sure prompt comment is enabled.\r\n2. Write a commented prompt in txt2img.\r\n3. Generate an image with highres fix enabled.\n\n### What should have happened?\n\nHighres shouldn't be able to use commented text in the generation.\n\n### What browsers do you use to access the UI ?\n\n_No response_\n\n### Sysinfo\n\n{\r\n    \"Platform\": \"Windows-10-10.0.19045-SP0\",\r\n    \"Python\": \"3.10.6\",\r\n    \"Version\": \"v1.8.0\",\r\n    \"Commit\": \"bef51aed032c0aaa5cfd80445bc4cf0d85b408b5\",\r\n    \"Script path\": \"H:\\\\SD\\\\stable-diffusion-webui\",\r\n    \"Data path\": \"H:\\\\SD\\\\stable-diffusion-webui\",\r\n    \"Extensions dir\": \"H:\\\\SD\\\\stable-diffusion-webui\\\\extensions\",\r\n    \"Checksum\": \"c891551968bcf79a877607b8b1547f2053f0d6936bd8f40db8dfdf2cdf3f030d\",\r\n    \"Commandline\": [\r\n        \"launch.py\",\r\n        \"--ckpt-dir\",\r\n        \"E:/Git/stable-diffusion-webui/models/Stable-diffusion\",\r\n        \"--hypernetwork-dir\",\r\n        \"E:/Git/stable-diffusion-webui/models/hypernetworks\",\r\n        \"--embeddings-dir\",\r\n        \"E:/Git/stable-diffusion-webui/embeddings\",\r\n        \"--vae-dir\",\r\n        \"E:/Git/stable-diffusion-webui/models/VAE\",\r\n        \"--lora-dir\",\r\n        \"E:/Git/stable-diffusion-webui/models/Lora\"\r\n    ],\r\n    \"Torch env info\": {\r\n        \"torch_version\": \"2.1.2+cu121\",\r\n        \"is_debug_build\": \"False\",\r\n        \"cuda_compiled_version\": \"12.1\",\r\n        \"gcc_version\": null,\r\n        \"clang_version\": null,\r\n        \"cmake_version\": \"version 3.25.1\",\r\n        \"os\": \"Microsoft Windows 10 Pro\",\r\n        \"libc_version\": \"N/A\",\r\n        \"python_version\": \"3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)] (64-bit runtime)\",\r\n        \"python_platform\": \"Windows-10-10.0.19045-SP0\",\r\n        \"is_cuda_available\": \"True\",\r\n        \"cuda_runtime_version\": null,\r\n        \"cuda_module_loading\": \"LAZY\",\r\n        \"nvidia_driver_version\": \"551.61\",\r\n        \"nvidia_gpu_models\": \"GPU 0: NVIDIA GeForce RTX 3060\",\r\n        \"cudnn_version\": null,\r\n        \"pip_version\": \"pip3\",\r\n        \"pip_packages\": [\r\n            \"numpy==1.26.2\",\r\n            \"open-clip-torch==2.20.0\",\r\n            \"pytorch-lightning==1.9.4\",\r\n            \"torch==2.1.2+cu121\",\r\n            \"torchdiffeq==0.2.3\",\r\n            \"torchmetrics==1.3.1\",\r\n            \"torchsde==0.2.6\",\r\n            \"torchvision==0.16.2+cu121\"\r\n        ],\r\n        \"conda_packages\": null,\r\n        \"hip_compiled_version\": \"N/A\",\r\n        \"hip_runtime_version\": \"N/A\",\r\n        \"miopen_runtime_version\": \"N/A\",\r\n        \"caching_allocator_config\": \"\",\r\n        \"is_xnnpack_available\": \"True\",\r\n        \"cpu_info\": [\r\n            \"Architecture=9\",\r\n            \"CurrentClockSpeed=3400\",\r\n            \"DeviceID=CPU0\",\r\n            \"Family=107\",\r\n            \"L2CacheSize=3072\",\r\n            \"L2CacheSpeed=\",\r\n            \"Manufacturer=AuthenticAMD\",\r\n            \"MaxClockSpeed=3400\",\r\n            \"Name=AMD Ryzen 5 2600 Six-Core Processor            \",\r\n            \"ProcessorType=3\",\r\n            \"Revision=2050\"\r\n        ]\r\n    },\r\n    \"Exceptions\": [],\r\n    \"CPU\": {\r\n        \"model\": \"AMD64 Family 23 Model 8 Stepping 2, AuthenticAMD\",\r\n        \"count logical\": 12,\r\n        \"count physical\": 6\r\n    },\r\n    \"RAM\": {\r\n        \"total\": \"32GB\",\r\n        \"used\": \"18GB\",\r\n        \"free\": \"14GB\"\r\n    },\r\n    \"Extensions\": [],\r\n    \"Inactive extensions\": [],\r\n    \"Environment\": {\r\n        \"COMMANDLINE_ARGS\": \" --ckpt-dir E:/Git/stable-diffusion-webui/models/Stable-diffusion --hypernetwork-dir E:/Git/stable-diffusion-webui/models/hypernetworks --embeddings-dir E:/Git/stable-diffusion-webui/embeddings --vae-dir E:/Git/stable-diffusion-webui/models/VAE --lora-dir E:/Git/stable-diffusion-webui/models/Lora\",\r\n        \"GRADIO_ANALYTICS_ENABLED\": \"False\"\r\n    },\r\n    \"Config\": {\r\n        \"ldsr_steps\": 100,\r\n        \"ldsr_cached\": false,\r\n        \"SCUNET_tile\": 256,\r\n        \"SCUNET_tile_overlap\": 8,\r\n        \"SWIN_tile\": 192,\r\n        \"SWIN_tile_overlap\": 8,\r\n        \"SWIN_torch_compile\": false,\r\n        \"hypertile_enable_unet\": false,\r\n        \"hypertile_enable_unet_secondpass\": false,\r\n        \"hypertile_max_depth_unet\": 3,\r\n        \"hypertile_max_tile_unet\": 256,\r\n        \"hypertile_swap_size_unet\": 3,\r\n        \"hypertile_enable_vae\": false,\r\n        \"hypertile_max_depth_vae\": 3,\r\n        \"hypertile_max_tile_vae\": 128,\r\n        \"hypertile_swap_size_vae\": 3,\r\n        \"sd_model_checkpoint\": \"epicrealism_pureEvolutionV5.safetensors [76be5be1b2]\",\r\n        \"sd_checkpoint_hash\": \"76be5be1b2a317e38dbe80a9b33d6a35db5a27883ba0569e7a1624281f2a6c95\",\r\n        \"outdir_samples\": \"\",\r\n        \"outdir_txt2img_samples\": \"output\\\\txt2img-images\",\r\n        \"outdir_img2img_samples\": \"output\\\\img2img-images\",\r\n        \"outdir_extras_samples\": \"output\\\\extras-images\",\r\n        \"outdir_grids\": \"\",\r\n        \"outdir_txt2img_grids\": \"output\\\\txt2img-grids\",\r\n        \"outdir_img2img_grids\": \"output\\\\img2img-grids\",\r\n        \"outdir_save\": \"log\\\\images\",\r\n        \"outdir_init_images\": \"output\\\\init-images\",\r\n        \"samples_save\": true,\r\n        \"samples_format\": \"png\",\r\n        \"samples_filename_pattern\": \"\",\r\n        \"save_images_add_number\": true,\r\n        \"save_images_replace_action\": \"Replace\",\r\n        \"grid_save\": true,\r\n        \"grid_format\": \"png\",\r\n        \"grid_extended_filename\": false,\r\n        \"grid_only_if_multiple\": true,\r\n        \"grid_prevent_empty_spots\": false,\r\n        \"grid_zip_filename_pattern\": \"\",\r\n        \"n_rows\": -1,\r\n        \"font\": \"\",\r\n        \"grid_text_active_color\": \"#000000\",\r\n        \"grid_text_inactive_color\": \"#999999\",\r\n        \"grid_background_color\": \"#ffffff\",\r\n        \"save_images_before_face_restoration\": false,\r\n        \"save_images_before_highres_fix\": true,\r\n        \"save_images_before_color_correction\": false,\r\n        \"save_mask\": false,\r\n        \"save_mask_composite\": false,\r\n        \"jpeg_quality\": 80,\r\n        \"webp_lossless\": false,\r\n        \"export_for_4chan\": true,\r\n        \"img_downscale_threshold\": 4.0,\r\n        \"target_side_length\": 4000.0,\r\n        \"img_max_size_mp\": 200.0,\r\n        \"use_original_name_batch\": true,\r\n        \"use_upscaler_name_as_suffix\": false,\r\n        \"save_selected_only\": true,\r\n        \"save_init_img\": false,\r\n        \"temp_dir\": \"\",\r\n        \"clean_temp_dir_at_start\": false,\r\n        \"save_incomplete_images\": false,\r\n        \"notification_audio\": true,\r\n        \"notification_volume\": 100,\r\n        \"save_to_dirs\": true,\r\n        \"grid_save_to_dirs\": true,\r\n        \"use_save_to_dirs_for_ui\": false,\r\n        \"directories_filename_pattern\": \"[date]\",\r\n        \"directories_max_prompt_words\": 8,\r\n        \"auto_backcompat\": true,\r\n        \"use_old_emphasis_implementation\": false,\r\n        \"use_old_karras_scheduler_sigmas\": false,\r\n        \"no_dpmpp_sde_batch_determinism\": false,\r\n        \"use_old_hires_fix_width_height\": false,\r\n        \"dont_fix_second_order_samplers_schedule\": false,\r\n        \"hires_fix_use_firstpass_conds\": false,\r\n        \"use_old_scheduling\": false,\r\n        \"use_downcasted_alpha_bar\": false,\r\n        \"lora_functional\": false,\r\n        \"extra_networks_show_hidden_directories\": true,\r\n        \"extra_networks_dir_button_function\": false,\r\n        \"extra_networks_hidden_models\": \"When searched\",\r\n        \"extra_networks_default_multiplier\": 1,\r\n        \"extra_networks_card_width\": 0.0,\r\n        \"extra_networks_card_height\": 0.0,\r\n        \"extra_networks_card_text_scale\": 1,\r\n        \"extra_networks_card_show_desc\": true,\r\n        \"extra_networks_card_description_is_html\": false,\r\n        \"extra_networks_card_order_field\": \"Path\",\r\n        \"extra_networks_card_order\": \"Ascending\",\r\n        \"extra_networks_tree_view_default_enabled\": false,\r\n        \"extra_networks_add_text_separator\": \" \",\r\n        \"ui_extra_networks_tab_reorder\": \"\",\r\n        \"textual_inversion_print_at_load\": false,\r\n        \"textual_inversion_add_hashes_to_infotext\": true,\r\n        \"sd_hypernetwork\": \"None\",\r\n        \"sd_lora\": \"None\",\r\n        \"lora_preferred_name\": \"Alias from file\",\r\n        \"lora_add_hashes_to_infotext\": true,\r\n        \"lora_show_all\": false,\r\n        \"lora_hide_unknown_for_versions\": [],\r\n        \"lora_in_memory_limit\": 0,\r\n        \"lora_not_found_warning_console\": false,\r\n        \"lora_not_found_gradio_warning\": false,\r\n        \"cross_attention_optimization\": \"Automatic\",\r\n        \"s_min_uncond\": 0,\r\n        \"token_merging_ratio\": 0,\r\n        \"token_merging_ratio_img2img\": 0,\r\n        \"token_merging_ratio_hr\": 0,\r\n        \"pad_cond_uncond\": false,\r\n        \"pad_cond_uncond_v0\": false,\r\n        \"persistent_cond_cache\": true,\r\n        \"batch_cond_uncond\": true,\r\n        \"fp8_storage\": \"Disable\",\r\n        \"cache_fp16_weight\": false,\r\n        \"hide_samplers\": [],\r\n        \"eta_ddim\": 0,\r\n        \"eta_ancestral\": 1,\r\n        \"ddim_discretize\": \"uniform\",\r\n        \"s_churn\": 0,\r\n        \"s_tmin\": 0,\r\n        \"s_tmax\": 0,\r\n        \"s_noise\": 1,\r\n        \"k_sched_type\": \"Automatic\",\r\n        \"sigma_min\": 0.0,\r\n        \"sigma_max\": 0.0,\r\n        \"rho\": 0.0,\r\n        \"eta_noise_seed_delta\": 0,\r\n        \"always_discard_next_to_last_sigma\": false,\r\n        \"sgm_noise_multiplier\": false,\r\n        \"uni_pc_variant\": \"bh1\",\r\n        \"uni_pc_skip_type\": \"time_uniform\",\r\n        \"uni_pc_order\": 3,\r\n        \"uni_pc_lower_order_final\": true,\r\n        \"sd_noise_schedule\": \"Default\",\r\n        \"sd_checkpoints_limit\": 1,\r\n        \"sd_checkpoints_keep_in_cpu\": true,\r\n        \"sd_checkpoint_cache\": 0,\r\n        \"sd_unet\": \"Automatic\",\r\n        \"enable_quantization\": false,\r\n        \"emphasis\": \"Original\",\r\n        \"enable_batch_seeds\": true,\r\n        \"comma_padding_backtrack\": 20,\r\n        \"CLIP_stop_at_last_layers\": 1,\r\n        \"upcast_attn\": false,\r\n        \"randn_source\": \"GPU\",\r\n        \"tiling\": false,\r\n        \"hires_fix_refiner_pass\": \"second pass\",\r\n        \"enable_prompt_comments\": true,\r\n        \"sdxl_crop_top\": 0.0,\r\n        \"sdxl_crop_left\": 0.0,\r\n        \"sdxl_refiner_low_aesthetic_score\": 2.5,\r\n        \"sdxl_refiner_high_aesthetic_score\": 6.0,\r\n        \"sd_vae_checkpoint_cache\": 0,\r\n        \"sd_vae\": \"Automatic\",\r\n        \"sd_vae_overrides_per_model_preferences\": true,\r\n        \"auto_vae_precision_bfloat16\": false,\r\n        \"auto_vae_precision\": true,\r\n        \"sd_vae_encode_method\": \"Full\",\r\n        \"sd_vae_decode_method\": \"Full\",\r\n        \"inpainting_mask_weight\": 1,\r\n        \"initial_noise_multiplier\": 1,\r\n        \"img2img_extra_noise\": 0,\r\n        \"img2img_color_correction\": false,\r\n        \"img2img_fix_steps\": false,\r\n        \"img2img_background_color\": \"#ffffff\",\r\n        \"img2img_editor_height\": 720,\r\n        \"img2img_sketch_default_brush_color\": \"#ffffff\",\r\n        \"img2img_inpaint_mask_brush_color\": \"#ffffff\",\r\n        \"img2img_inpaint_sketch_default_brush_color\": \"#ffffff\",\r\n        \"return_mask\": false,\r\n        \"return_mask_composite\": false,\r\n        \"img2img_batch_show_results_limit\": 32,\r\n        \"overlay_inpaint\": true,\r\n        \"return_grid\": true,\r\n        \"do_not_show_images\": false,\r\n        \"js_modal_lightbox\": true,\r\n        \"js_modal_lightbox_initially_zoomed\": true,\r\n        \"js_modal_lightbox_gamepad\": false,\r\n        \"js_modal_lightbox_gamepad_repeat\": 250.0,\r\n        \"sd_webui_modal_lightbox_icon_opacity\": 1,\r\n        \"sd_webui_modal_lightbox_toolbar_opacity\": 0.9,\r\n        \"gallery_height\": \"\",\r\n        \"open_dir_button_choice\": \"Subdirectory\",\r\n        \"enable_pnginfo\": true,\r\n        \"save_txt\": false,\r\n        \"add_model_name_to_info\": true,\r\n        \"add_model_hash_to_info\": true,\r\n        \"add_vae_name_to_info\": true,\r\n        \"add_vae_hash_to_info\": true,\r\n        \"add_user_name_to_info\": false,\r\n        \"add_version_to_infotext\": true,\r\n        \"disable_weights_auto_swap\": true,\r\n        \"infotext_skip_pasting\": [],\r\n        \"infotext_styles\": \"Apply if any\",\r\n        \"show_progressbar\": true,\r\n        \"live_previews_enable\": true,\r\n        \"live_previews_image_format\": \"png\",\r\n        \"show_progress_grid\": true,\r\n        \"show_progress_every_n_steps\": 10,\r\n        \"show_progress_type\": \"Approx NN\",\r\n        \"live_preview_allow_lowvram_full\": false,\r\n        \"live_preview_content\": \"Prompt\",\r\n        \"live_preview_refresh_period\": 1000.0,\r\n        \"live_preview_fast_interrupt\": false,\r\n        \"js_live_preview_in_modal_lightbox\": false,\r\n        \"keyedit_precision_attention\": 0.1,\r\n        \"keyedit_precision_extra\": 0.05,\r\n        \"keyedit_delimiters\": \".,\\\\/!?%^*;:{}=`~() \",\r\n        \"keyedit_delimiters_whitespace\": [\r\n            \"Tab\",\r\n            \"Carriage Return\",\r\n            \"Line Feed\"\r\n        ],\r\n        \"keyedit_move\": true,\r\n        \"disable_token_counters\": false,\r\n        \"include_styles_into_token_counters\": true,\r\n        \"extra_options_txt2img\": [],\r\n        \"extra_options_img2img\": [],\r\n        \"extra_options_cols\": 1,\r\n        \"extra_options_accordion\": false,\r\n        \"compact_prompt_box\": false,\r\n        \"samplers_in_dropdown\": true,\r\n        \"dimensions_and_batch_together\": true,\r\n        \"sd_checkpoint_dropdown_use_short\": false,\r\n        \"hires_fix_show_sampler\": false,\r\n        \"hires_fix_show_prompts\": false,\r\n        \"txt2img_settings_accordion\": false,\r\n        \"img2img_settings_accordion\": false,\r\n        \"interrupt_after_current\": true,\r\n        \"localization\": \"None\",\r\n        \"quicksettings_list\": [\r\n            \"sd_model_checkpoint\"\r\n        ],\r\n        \"ui_tab_order\": [],\r\n        \"hidden_tabs\": [],\r\n        \"ui_reorder_list\": [],\r\n        \"gradio_theme\": \"Default\",\r\n        \"gradio_themes_cache\": true,\r\n        \"show_progress_in_title\": true,\r\n        \"send_seed\": true,\r\n        \"send_size\": true,\r\n        \"api_enable_requests\": true,\r\n        \"api_forbid_local_requests\": true,\r\n        \"api_useragent\": \"\",\r\n        \"auto_launch_browser\": \"Local\",\r\n        \"enable_console_prompts\": false,\r\n        \"show_warnings\": false,\r\n        \"show_gradio_deprecation_warnings\": true,\r\n        \"memmon_poll_rate\": 8,\r\n        \"samples_log_stdout\": false,\r\n        \"multiple_tqdm\": true,\r\n        \"enable_upscale_progressbar\": true,\r\n        \"print_hypernet_extra\": false,\r\n        \"list_hidden_files\": true,\r\n        \"disable_mmap_load_safetensors\": false,\r\n        \"hide_ldm_prints\": true,\r\n        \"dump_stacks_on_signal\": false,\r\n        \"face_restoration\": false,\r\n        \"face_restoration_model\": \"CodeFormer\",\r\n        \"code_former_weight\": 0.5,\r\n        \"face_restoration_unload\": false,\r\n        \"postprocessing_enable_in_main_ui\": [],\r\n        \"postprocessing_operation_order\": [],\r\n        \"upscaling_max_images_in_cache\": 5,\r\n        \"postprocessing_existing_caption_action\": \"Ignore\",\r\n        \"ESRGAN_tile\": 192,\r\n        \"ESRGAN_tile_overlap\": 8,\r\n        \"realesrgan_enabled_models\": [\r\n            \"R-ESRGAN 4x+\",\r\n            \"R-ESRGAN 4x+ Anime6B\"\r\n        ],\r\n        \"dat_enabled_models\": [\r\n            \"DAT x2\",\r\n            \"DAT x3\",\r\n            \"DAT x4\"\r\n        ],\r\n        \"DAT_tile\": 192,\r\n        \"DAT_tile_overlap\": 8,\r\n        \"unload_models_when_training\": false,\r\n        \"pin_memory\": false,\r\n        \"save_optimizer_state\": false,\r\n        \"save_training_settings_to_txt\": true,\r\n        \"dataset_filename_word_regex\": \"\",\r\n        \"dataset_filename_join_string\": \" \",\r\n        \"training_image_repeats_per_epoch\": 1,\r\n        \"training_write_csv_every\": 500.0,\r\n        \"training_xattention_optimizations\": false,\r\n        \"training_enable_tensorboard\": false,\r\n        \"training_tensorboard_save_images\": false,\r\n        \"training_tensorboard_flush_every\": 120.0,\r\n        \"canvas_hotkey_zoom\": \"Alt\",\r\n        \"canvas_hotkey_adjust\": \"Ctrl\",\r\n        \"canvas_hotkey_shrink_brush\": \"Q\",\r\n        \"canvas_hotkey_grow_brush\": \"W\",\r\n        \"canvas_hotkey_move\": \"F\",\r\n        \"canvas_hotkey_fullscreen\": \"S\",\r\n        \"canvas_hotkey_reset\": \"R\",\r\n        \"canvas_hotkey_overlap\": \"O\",\r\n        \"canvas_show_tooltip\": true,\r\n        \"canvas_auto_expand\": true,\r\n        \"canvas_blur_prompt\": false,\r\n        \"canvas_disabled_functions\": [\r\n            \"Overlap\"\r\n        ],\r\n        \"interrogate_keep_models_in_memory\": false,\r\n        \"interrogate_return_ranks\": false,\r\n        \"interrogate_clip_num_beams\": 1,\r\n        \"interrogate_clip_min_length\": 24,\r\n        \"interrogate_clip_max_length\": 48,\r\n        \"interrogate_clip_dict_limit\": 1500.0,\r\n        \"interrogate_clip_skip_categories\": [],\r\n        \"interrogate_deepbooru_score_threshold\": 0.5,\r\n        \"deepbooru_sort_alpha\": true,\r\n        \"deepbooru_use_spaces\": true,\r\n        \"deepbooru_escape\": true,\r\n        \"deepbooru_filter_tags\": \"\"\r\n    },\r\n    \"Startup\": {\r\n        \"total\": 32.69029521942139,\r\n        \"records\": {\r\n            \"initial startup\": 0.07414388656616211,\r\n            \"prepare environment/checks\": 0.01100301742553711,\r\n            \"prepare environment/git version info\": 0.12100005149841309,\r\n            \"prepare environment/torch GPU test\": 6.296570062637329,\r\n            \"prepare environment/clone repositores\": 0.25691962242126465,\r\n            \"prepare environment/run extensions installers\": 0.014172077178955078,\r\n            \"prepare environment\": 6.7564380168914795,\r\n            \"launcher\": 0.012732982635498047,\r\n            \"import torch\": 9.162429094314575,\r\n            \"import gradio\": 3.5922298431396484,\r\n            \"setup paths\": 3.3042116165161133,\r\n            \"import ldm\": 0.011002063751220703,\r\n            \"import sgm\": 0.0,\r\n            \"initialize shared\": 0.4220004081726074,\r\n            \"other imports\": 2.022930145263672,\r\n            \"opts onchange\": 0.0010001659393310547,\r\n            \"setup SD model\": 0.0,\r\n            \"setup codeformer\": 0.0020003318786621094,\r\n            \"setup gfpgan\": 0.056000709533691406,\r\n            \"set samplers\": 0.0,\r\n            \"list extensions\": 0.0019991397857666016,\r\n            \"restore config state file\": 0.0,\r\n            \"list SD models\": 0.3970005512237549,\r\n            \"list localizations\": 0.0009999275207519531,\r\n            \"load scripts/custom_code.py\": 0.014999866485595703,\r\n            \"load scripts/img2imgalt.py\": 0.0009996891021728516,\r\n            \"load scripts/loopback.py\": 0.0010013580322265625,\r\n            \"load scripts/outpainting_mk_2.py\": 0.0009987354278564453,\r\n            \"load scripts/poor_mans_outpainting.py\": 0.0010008811950683594,\r\n            \"load scripts/postprocessing_caption.py\": 0.0009989738464355469,\r\n            \"load scripts/postprocessing_codeformer.py\": 0.0,\r\n            \"load scripts/postprocessing_create_flipped_copies.py\": 0.001001119613647461,\r\n            \"load scripts/postprocessing_focal_crop.py\": 0.002000570297241211,\r\n            \"load scripts/postprocessing_gfpgan.py\": 0.0009992122650146484,\r\n            \"load scripts/postprocessing_split_oversized.py\": 0.0,\r\n            \"load scripts/postprocessing_upscale.py\": 0.0009996891021728516,\r\n            \"load scripts/processing_autosized_crop.py\": 0.0010006427764892578,\r\n            \"load scripts/prompt_matrix.py\": 0.0009992122650146484,\r\n            \"load scripts/prompts_from_file.py\": 0.0010001659393310547,\r\n            \"load scripts/sd_upscale.py\": 0.0009996891021728516,\r\n            \"load scripts/xyz_grid.py\": 0.002003192901611328,\r\n            \"load scripts/ldsr_model.py\": 1.0569970607757568,\r\n            \"load scripts/lora_script.py\": 0.7220323085784912,\r\n            \"load scripts/scunet_model.py\": 0.034002065658569336,\r\n            \"load scripts/swinir_model.py\": 0.0319974422454834,\r\n            \"load scripts/hotkey_config.py\": 0.0010025501251220703,\r\n            \"load scripts/extra_options_section.py\": 0.0,\r\n            \"load scripts/hypertile_script.py\": 0.06599783897399902,\r\n            \"load scripts/hypertile_xyz.py\": 0.0,\r\n            \"load scripts/soft_inpainting.py\": 0.001001596450805664,\r\n            \"load scripts/comments.py\": 0.030997514724731445,\r\n            \"load scripts/refiner.py\": 0.0,\r\n            \"load scripts/seed.py\": 0.0010006427764892578,\r\n            \"load scripts\": 1.976032018661499,\r\n            \"load upscalers\": 0.005002021789550781,\r\n            \"refresh VAE\": 0.015000581741333008,\r\n            \"refresh textual inversion templates\": 0.0,\r\n            \"scripts list_optimizers\": 0.0019991397857666016,\r\n            \"scripts list_unets\": 0.0,\r\n            \"reload hypernetworks\": 0.0009999275207519531,\r\n            \"initialize extra networks\": 0.016000032424926758,\r\n            \"scripts before_ui_callback\": 0.0030002593994140625,\r\n            \"create ui\": 4.436000347137451,\r\n            \"gradio launch\": 0.4740011692047119,\r\n            \"add APIs\": 0.009000062942504883,\r\n            \"app_started_callback/lora_script.py\": 0.0,\r\n            \"app_started_callback\": 0.0\r\n        }\r\n    },\r\n    \"Packages\": [\r\n        \"accelerate==0.21.0\",\r\n        \"aenum==3.1.15\",\r\n        \"aiofiles==23.2.1\",\r\n        \"aiohttp==3.9.3\",\r\n        \"aiosignal==1.3.1\",\r\n        \"altair==5.2.0\",\r\n        \"antlr4-python3-runtime==4.9.3\",\r\n        \"anyio==3.7.1\",\r\n        \"async-timeout==4.0.3\",\r\n        \"attrs==23.2.0\",\r\n        \"blendmodes==2022\",\r\n        \"certifi==2024.2.2\",\r\n        \"charset-normalizer==3.3.2\",\r\n        \"clean-fid==0.1.35\",\r\n        \"click==8.1.7\",\r\n        \"clip==1.0\",\r\n        \"colorama==0.4.6\",\r\n        \"contourpy==1.2.0\",\r\n        \"cycler==0.12.1\",\r\n        \"deprecation==2.1.0\",\r\n        \"einops==0.4.1\",\r\n        \"exceptiongroup==1.2.0\",\r\n        \"facexlib==0.3.0\",\r\n        \"fastapi==0.94.0\",\r\n        \"ffmpy==0.3.2\",\r\n        \"filelock==3.13.1\",\r\n        \"filterpy==1.4.5\",\r\n        \"fonttools==4.49.0\",\r\n        \"frozenlist==1.4.1\",\r\n        \"fsspec==2024.2.0\",\r\n        \"ftfy==6.1.3\",\r\n        \"gitdb==4.0.11\",\r\n        \"gitpython==3.1.32\",\r\n        \"gradio-client==0.5.0\",\r\n        \"gradio==3.41.2\",\r\n        \"h11==0.12.0\",\r\n        \"httpcore==0.15.0\",\r\n        \"httpx==0.24.1\",\r\n        \"huggingface-hub==0.21.4\",\r\n        \"idna==3.6\",\r\n        \"imageio==2.34.0\",\r\n        \"importlib-resources==6.3.0\",\r\n        \"inflection==0.5.1\",\r\n        \"jinja2==3.1.3\",\r\n        \"jsonmerge==1.8.0\",\r\n        \"jsonschema-specifications==2023.12.1\",\r\n        \"jsonschema==4.21.1\",\r\n        \"kiwisolver==1.4.5\",\r\n        \"kornia==0.6.7\",\r\n        \"lark==1.1.2\",\r\n        \"lazy-loader==0.3\",\r\n        \"lightning-utilities==0.10.1\",\r\n        \"llvmlite==0.42.0\",\r\n        \"markupsafe==2.1.5\",\r\n        \"matplotlib==3.8.3\",\r\n        \"mpmath==1.3.0\",\r\n        \"multidict==6.0.5\",\r\n        \"networkx==3.2.1\",\r\n        \"numba==0.59.0\",\r\n        \"numpy==1.26.2\",\r\n        \"omegaconf==2.2.3\",\r\n        \"open-clip-torch==2.20.0\",\r\n        \"opencv-python==4.9.0.80\",\r\n        \"orjson==3.9.15\",\r\n        \"packaging==24.0\",\r\n        \"pandas==2.2.1\",\r\n        \"piexif==1.1.3\",\r\n        \"pillow==9.5.0\",\r\n        \"pip==22.2.1\",\r\n        \"protobuf==3.20.0\",\r\n        \"psutil==5.9.5\",\r\n        \"pydantic==1.10.14\",\r\n        \"pydub==0.25.1\",\r\n        \"pyparsing==3.1.2\",\r\n        \"python-dateutil==2.9.0.post0\",\r\n        \"python-multipart==0.0.9\",\r\n        \"pytorch-lightning==1.9.4\",\r\n        \"pytz==2024.1\",\r\n        \"pywavelets==1.5.0\",\r\n        \"pyyaml==6.0.1\",\r\n        \"referencing==0.33.0\",\r\n        \"regex==2023.12.25\",\r\n        \"requests==2.31.0\",\r\n        \"resize-right==0.0.2\",\r\n        \"rpds-py==0.18.0\",\r\n        \"safetensors==0.4.2\",\r\n        \"scikit-image==0.21.0\",\r\n        \"scipy==1.12.0\",\r\n        \"semantic-version==2.10.0\",\r\n        \"sentencepiece==0.2.0\",\r\n        \"setuptools==63.2.0\",\r\n        \"six==1.16.0\",\r\n        \"smmap==5.0.1\",\r\n        \"sniffio==1.3.1\",\r\n        \"spandrel==0.1.6\",\r\n        \"starlette==0.26.1\",\r\n        \"sympy==1.12\",\r\n        \"tifffile==2024.2.12\",\r\n        \"timm==0.9.16\",\r\n        \"tokenizers==0.13.3\",\r\n        \"tomesd==0.1.3\",\r\n        \"toolz==0.12.1\",\r\n        \"torch==2.1.2+cu121\",\r\n        \"torchdiffeq==0.2.3\",\r\n        \"torchmetrics==1.3.1\",\r\n        \"torchsde==0.2.6\",\r\n        \"torchvision==0.16.2+cu121\",\r\n        \"tqdm==4.66.2\",\r\n        \"trampoline==0.1.2\",\r\n        \"transformers==4.30.2\",\r\n        \"typing-extensions==4.10.0\",\r\n        \"tzdata==2024.1\",\r\n        \"urllib3==2.2.1\",\r\n        \"uvicorn==0.28.0\",\r\n        \"wcwidth==0.2.13\",\r\n        \"websockets==11.0.3\",\r\n        \"yarl==1.9.4\"\r\n    ]\r\n}\n\n### Console logs\n\n```Shell\nvenv \"H:\\SD\\stable-diffusion-webui\\venv\\Scripts\\Python.exe\"\r\nPython 3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]\r\nVersion: v1.8.0\r\nCommit hash: bef51aed032c0aaa5cfd80445bc4cf0d85b408b5\r\nLaunching Web UI with arguments: --ckpt-dir E:/Git/stable-diffusion-webui/models/Stable-diffusion --hypernetwork-dir E:/Git/stable-diffusion-webui/models/hypernetworks --embeddings-dir E:/Git/stable-diffusion-webui/embeddings --vae-dir E:/Git/stable-diffusion-webui/models/VAE --lora-dir E:/Git/stable-diffusion-webui/models/Lora\r\nno module 'xformers'. Processing without...\r\nno module 'xformers'. Processing without...\r\nNo module 'xformers'. Proceeding without it.\r\nLoading weights [76be5be1b2] from E:\\Git\\stable-diffusion-webui\\models\\Stable-diffusion\\epicrealism_pureEvolutionV5.safetensors\r\nCreating model from config: H:\\SD\\stable-diffusion-webui\\configs\\v1-inference.yaml\r\nApplying attention optimization: Doggettx... done.\r\nNo Image data blocks found.\r\nModel loaded in 3.2s (load weights from disk: 0.2s, create model: 0.6s, apply weights to model: 1.1s, load textual inversion embeddings: 0.9s, calculate empty prompt: 0.2s).\r\nRunning on local URL:  http://127.0.0.1:7860\r\n\r\nTo create a public link, set `share=True` in `launch()`.\r\nStartup time: 32.7s (prepare environment: 6.8s, import torch: 9.2s, import gradio: 3.6s, setup paths: 3.3s, initialize shared: 0.4s, other imports: 2.0s, list SD models: 0.4s, load scripts: 2.0s, create ui: 4.4s, gradio launch: 0.5s).\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 30/30 [00:05<00:00,  5.02it/s]\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 30/30 [00:39<00:00,  1.31s/it]\r\nTotal progress: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 60/60 [00:47<00:00,  1.27it/s]\r\nApplying attention optimization: Doggettx... done.\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 60/60 [00:47<00:00,  1.30s/it]\r\nUsing already loaded model epicrealism_pureEvolutionV5.safetensors [76be5be1b2]: done in 0.0s\r\nLoading weights [76be5be1b2] from E:\\Git\\stable-diffusion-webui\\models\\Stable-diffusion\\epicrealism_pureEvolutionV5.safetensors\r\nApplying attention optimization: Doggettx... done.\r\nWeights loaded in 2.0s (calculate hash: 0.9s, apply weights to model: 0.5s, move model to device: 0.4s).\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 30/30 [00:05<00:00,  5.37it/s]\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 30/30 [00:39<00:00,  1.31s/it]\r\nTotal progress: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 60/60 [00:46<00:00,  1.29it/s]\r\nTotal progress: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 60/60 [00:46<00:00,  1.30s/it]\n```\n\n\n### Additional information\n\n_No response_\n", "hints_text": "Just to be sure, the prompt used in the image above is:\r\n\r\n![image](https://github.com/AUTOMATIC1111/stable-diffusion-webui/assets/136133907/cd321187-416d-4dac-b14e-329b3c2560a6)\r\n", "created_at": "2024-03-15T04:27:42Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 15199, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-15199", "issue_numbers": ["15184"], "base_commit": "6136db1409b1d9d4a01358a558602fec40562488", "patch": "diff --git a/modules/ui_extra_networks_user_metadata.py b/modules/ui_extra_networks_user_metadata.py\nindex 2ca937fd117..6bc25a4d2c2 100644\n--- a/modules/ui_extra_networks_user_metadata.py\n+++ b/modules/ui_extra_networks_user_metadata.py\n@@ -133,8 +133,10 @@ def write_user_metadata(self, name, metadata):\n         filename = item.get(\"filename\", None)\r\n         basename, ext = os.path.splitext(filename)\r\n \r\n-        with open(basename + '.json', \"w\", encoding=\"utf8\") as file:\r\n+        metadata_path = basename + '.json'\r\n+        with open(metadata_path, \"w\", encoding=\"utf8\") as file:\r\n             json.dump(metadata, file, indent=4, ensure_ascii=False)\r\n+        self.page.lister.update_file_entry(metadata_path)\r\n \r\n     def save_user_metadata(self, name, desc, notes):\r\n         user_metadata = self.get_user_metadata(name)\r\n@@ -200,6 +202,3 @@ def setup_ui(self, gallery):\n             inputs=[self.edit_name_input],\r\n             outputs=[]\r\n         )\r\n-\r\n-\r\n-\r\ndiff --git a/modules/util.py b/modules/util.py\nindex 8d1aea44f5c..cb690e734f8 100644\n--- a/modules/util.py\n+++ b/modules/util.py\n@@ -81,6 +81,17 @@ def __init__(self, dirname):\n         self.files = {x[0].lower(): x for x in files}\r\n         self.files_cased = {x[0]: x for x in files}\r\n \r\n+    def update_entry(self, filename):\r\n+        \"\"\"Add a file to the cache\"\"\"\r\n+        file_path = os.path.join(self.dirname, filename)\r\n+        try:\r\n+            stat = os.stat(file_path)\r\n+            entry = (filename, stat.st_mtime, stat.st_ctime)\r\n+            self.files[filename.lower()] = entry\r\n+            self.files_cased[filename] = entry\r\n+        except FileNotFoundError as e:\r\n+            print(f'MassFileListerCachedDir.add_entry: \"{file_path}\" {e}')\r\n+\r\n \r\n class MassFileLister:\r\n     \"\"\"A class that provides a way to check for the existence and mtime/ctile of files without doing more than one stat call per file.\"\"\"\r\n@@ -136,3 +147,9 @@ def mctime(self, path):\n     def reset(self):\r\n         \"\"\"Clear the cache of all directories.\"\"\"\r\n         self.cached_dirs.clear()\r\n+\r\n+    def update_file_entry(self, path):\r\n+        \"\"\"Update the cache for a specific directory.\"\"\"\r\n+        dirname, filename = os.path.split(path)\r\n+        if cached_dir := self.cached_dirs.get(dirname):\r\n+            cached_dir.update_entry(filename)\r\n", "test_patch": "", "problem_statement": "[Bug] Lora activation string is not applied to the prompt\n### Checklist\r\n\r\n- [ ] The issue exists after disabling all extensions\r\n- [ ] The issue exists on a clean installation of webui\r\n- [ ] The issue is caused by an extension, but I believe it is caused by a bug in the webui\r\n- [X] The issue exists in the current version of the webui\r\n- [X] The issue has not been reported before recently\r\n- [ ] The issue has been reported before but has not been fixed yet\r\n\r\n### What happened?\r\n\r\nAfter upgrading to v1.8.0 when I add a Lora activation string in the Lora details window that string is not applied to the prompt when I click the Lora in the Lora selection tab. The Lora string ( \\<some-kind-of-lora:1.0\\> ) is added but no activation string is added after that.\r\n\r\nThis happens only with newly added Loras (and also on old Loras where I didn't edited the activation string). If I had already defined activation strings on older Loras I can edit them and the recent changes are reflected on the prompt when I select the Lora. \r\n\r\nwindows11\r\nversion: [v1.8.0]\u2000\u2022\u2000 python: 3.10.6 \u2000\u2022\u2000 torch: 2.0.1+cu118 \u2000\u2022\u2000 xformers: N/A \u2000\u2022\u2000 gradio: 3.41.2\r\n\r\n### Steps to reproduce the problem\r\n\r\n1. go to lora selection tab\r\n2. click edit details on a Lora (with no activation string defined)\r\n3. add an activation string\r\n4. click on the lora and check the string added to the prompt\r\n\r\n### What should have happened?\r\n\r\nthe new activation string should appear after the lora reference: \"\\<a-lora:1.0\\> some-activation-string\"\r\n\r\n### What browsers do you use to access the UI ?\r\n\r\nMozilla Firefox, Microsoft Edge\r\n\r\n### Sysinfo\r\n\r\n-\r\n\r\n### Console logs\r\n\r\n```Shell\r\n-\r\n```\r\n\r\n\r\n### Additional information\r\n\r\n_No response_\n", "hints_text": "", "created_at": "2024-03-09T21:22:00Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 15183, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-15183", "issue_numbers": ["15182"], "base_commit": "7d1368c51ca39c23f65a3a7431211cd0235bddbe", "patch": "diff --git a/style.css b/style.css\nindex 49978a771a5..fe74ec41fc0 100644\n--- a/style.css\n+++ b/style.css\n@@ -1,6 +1,6 @@\n /* temporary fix to load default gradio font in frontend instead of backend */\r\n \r\n-@import url('webui-assets/css/sourcesanspro.css');\r\n+@import url('/webui-assets/css/sourcesanspro.css');\r\n \r\n \r\n /* temporary fix to hide gradio crop tool until it's fixed https://github.com/gradio-app/gradio/issues/3810 */\r\n", "test_patch": "", "problem_statement": "[Bug]: Font not loaded in 1.8.0 (sourcesanspro.css)\n### Checklist\n\n- [ ] The issue exists after disabling all extensions\n- [X] The issue exists on a clean installation of webui\n- [ ] The issue is caused by an extension, but I believe it is caused by a bug in the webui\n- [X] The issue exists in the current version of the webui\n- [X] The issue has not been reported before recently\n- [ ] The issue has been reported before but has not been fixed yet\n\n### What happened?\n\nThe font Source Sans Pro is not loaded.\n\n### Steps to reproduce the problem\n\n1. Clean install v1.8.0\n\n### What should have happened?\n\nSource Sans Pro should be loaded.\n\n### What browsers do you use to access the UI ?\n\nBrave\n\n### Sysinfo\n\n{\r\n    \"Platform\": \"Linux-5.15.146.1-microsoft-standard-WSL2-x86_64-with-glibc2.36\",\r\n    \"Python\": \"3.10.13\",\r\n    \"Version\": \"1.8.0-RC\",\r\n    \"Commit\": \"<none>\",\r\n    \"Script path\": \"/app\",\r\n    \"Data path\": \"/data\",\r\n    \"Extensions dir\": \"/data/extensions\",\r\n    \"Checksum\": \"be195380b92da9844e31ff7c86cda7e09abbecc487350d432fcb668c84628d47\",\r\n    \"Commandline\": [\r\n        \"/app/launch.py\",\r\n        \"--listen\",\r\n        \"--port\",\r\n        \"7860\",\r\n        \"--data-dir\",\r\n        \"/data\",\r\n        \"--gradio-allowed-path\",\r\n        \"/app\",\r\n        \"--allow-code\",\r\n        \"--medvram\",\r\n        \"--xformers\",\r\n        \"--enable-insecure-extension-access\",\r\n        \"--api\"\r\n    ],\r\n    \"Torch env info\": {\r\n        \"torch_version\": \"2.1.2+cu121\",\r\n        \"is_debug_build\": \"False\",\r\n        \"cuda_compiled_version\": \"12.1\",\r\n        \"gcc_version\": \"(Debian 12.2.0-14) 12.2.0\",\r\n        \"clang_version\": null,\r\n        \"cmake_version\": null,\r\n        \"os\": \"Debian GNU/Linux 12 (bookworm) (x86_64)\",\r\n        \"libc_version\": \"glibc-2.36\",\r\n        \"python_version\": \"3.10.13 (main, Feb 13 2024, 10:39:58) [GCC 12.2.0] (64-bit runtime)\",\r\n        \"python_platform\": \"Linux-5.15.146.1-microsoft-standard-WSL2-x86_64-with-glibc2.36\",\r\n        \"is_cuda_available\": \"True\",\r\n        \"cuda_runtime_version\": null,\r\n        \"cuda_module_loading\": \"LAZY\",\r\n        \"nvidia_driver_version\": \"546.33\",\r\n        \"nvidia_gpu_models\": \"GPU 0: NVIDIA GeForce RTX 3060\",\r\n        \"cudnn_version\": null,\r\n        \"pip_version\": \"pip3\",\r\n        \"pip_packages\": [\r\n            \"numpy==1.26.2\",\r\n            \"open-clip-torch==2.20.0\",\r\n            \"pytorch-lightning==1.9.4\",\r\n            \"torch==2.1.2+cu121\",\r\n            \"torchdiffeq==0.2.3\",\r\n            \"torchmetrics==1.3.1\",\r\n            \"torchsde==0.2.6\",\r\n            \"torchvision==0.16.2+cu121\",\r\n            \"triton==2.1.0\"\r\n        ],\r\n        \"conda_packages\": null,\r\n        \"hip_compiled_version\": \"N/A\",\r\n        \"hip_runtime_version\": \"N/A\",\r\n        \"miopen_runtime_version\": \"N/A\",\r\n        \"caching_allocator_config\": \"\",\r\n        \"is_xnnpack_available\": \"True\",\r\n        \"cpu_info\": [\r\n            \"Architecture:                       x86_64\",\r\n            \"CPU op-mode(s):                     32-bit, 64-bit\",\r\n            \"Address sizes:                      46 bits physical, 48 bits virtual\",\r\n            \"Byte Order:                         Little Endian\",\r\n            \"CPU(s):                             12\",\r\n            \"On-line CPU(s) list:                0-11\",\r\n            \"Vendor ID:                          GenuineIntel\",\r\n            \"Model name:                         13th Gen Intel(R) Core(TM) i7-13700\",\r\n            \"CPU family:                         6\",\r\n            \"Model:                              183\",\r\n            \"Thread(s) per core:                 2\",\r\n            \"Core(s) per socket:                 6\",\r\n            \"Socket(s):                          1\",\r\n            \"Stepping:                           1\",\r\n            \"BogoMIPS:                           4224.00\",\r\n            \"Flags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology tsc_reliable nonstop_tsc cpuid pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch ssbd ibrs ibpb stibp ibrs_enhanced fsgsbase bmi1 avx2 smep bmi2 erms invpcid rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves umip gfni vaes vpclmulqdq rdpid fsrm flush_l1d arch_capabilities\",\r\n            \"Hypervisor vendor:                  Microsoft\",\r\n            \"Virtualization type:                full\",\r\n            \"L1d cache:                          288 KiB (6 instances)\",\r\n            \"L1i cache:                          192 KiB (6 instances)\",\r\n            \"L2 cache:                           12 MiB (6 instances)\",\r\n            \"L3 cache:                           30 MiB (1 instance)\",\r\n            \"Vulnerability Gather data sampling: Not affected\",\r\n            \"Vulnerability Itlb multihit:        Not affected\",\r\n            \"Vulnerability L1tf:                 Not affected\",\r\n            \"Vulnerability Mds:                  Not affected\",\r\n            \"Vulnerability Meltdown:             Not affected\",\r\n            \"Vulnerability Mmio stale data:      Unknown: No mitigations\",\r\n            \"Vulnerability Retbleed:             Mitigation; Enhanced IBRS\",\r\n            \"Vulnerability Spec rstack overflow: Not affected\",\r\n            \"Vulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\",\r\n            \"Vulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\",\r\n            \"Vulnerability Spectre v2:           Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\",\r\n            \"Vulnerability Srbds:                Not affected\",\r\n            \"Vulnerability Tsx async abort:      Not affected\"\r\n        ]\r\n    },\r\n    \"Exceptions\": [\r\n        {\r\n            \"exception\": \"list index out of range\",\r\n            \"traceback\": [\r\n                [\r\n                    \"/app/modules/scripts.py, line 527, load_scripts\",\r\n                    \"script_module = script_loading.load_module(scriptfile.path)\"\r\n                ],\r\n                [\r\n                    \"/app/modules/script_loading.py, line 10, load_module\",\r\n                    \"module_spec.loader.exec_module(module)\"\r\n                ],\r\n                [\r\n                    \"<frozen importlib._bootstrap_external>, line 883, exec_module\",\r\n                    \"\"\r\n                ],\r\n                [\r\n                    \"<frozen importlib._bootstrap>, line 241, _call_with_frames_removed\",\r\n                    \"\"\r\n                ],\r\n                [\r\n                    \"/app/extensions-builtin/hypertile/scripts/hypertile_xyz.py, line 4, <module>\",\r\n                    \"xyz_grid = [x for x in scripts.scripts_data if x.script_class.__module__ == \\\"xyz_grid.py\\\"][0].module\"\r\n                ]\r\n            ]\r\n        },\r\n        {\r\n            \"exception\": \"list index out of range\",\r\n            \"traceback\": [\r\n                [\r\n                    \"/app/modules/scripts.py, line 527, load_scripts\",\r\n                    \"script_module = script_loading.load_module(scriptfile.path)\"\r\n                ],\r\n                [\r\n                    \"/app/modules/script_loading.py, line 10, load_module\",\r\n                    \"module_spec.loader.exec_module(module)\"\r\n                ],\r\n                [\r\n                    \"<frozen importlib._bootstrap_external>, line 883, exec_module\",\r\n                    \"\"\r\n                ],\r\n                [\r\n                    \"<frozen importlib._bootstrap>, line 241, _call_with_frames_removed\",\r\n                    \"\"\r\n                ],\r\n                [\r\n                    \"/app/extensions-builtin/hypertile/scripts/hypertile_script.py, line 3, <module>\",\r\n                    \"from scripts.hypertile_xyz import add_axis_options\"\r\n                ],\r\n                [\r\n                    \"/app/extensions-builtin/hypertile/scripts/hypertile_xyz.py, line 4, <module>\",\r\n                    \"xyz_grid = [x for x in scripts.scripts_data if x.script_class.__module__ == \\\"xyz_grid.py\\\"][0].module\"\r\n                ]\r\n            ]\r\n        }\r\n    ],\r\n    \"CPU\": {\r\n        \"model\": \"\",\r\n        \"count logical\": 12,\r\n        \"count physical\": 6\r\n    },\r\n    \"RAM\": {\r\n        \"total\": \"23GB\",\r\n        \"used\": \"6GB\",\r\n        \"free\": \"1GB\",\r\n        \"active\": \"7GB\",\r\n        \"inactive\": \"13GB\",\r\n        \"buffers\": \"416MB\",\r\n        \"cached\": \"16GB\",\r\n        \"shared\": \"79MB\"\r\n    },\r\n    \"Extensions\": [],\r\n    \"Inactive extensions\": [],\r\n    \"Environment\": {\r\n        \"GRADIO_ANALYTICS_ENABLED\": \"False\",\r\n        \"PYTHONPATH\": \":/home/1001/.local/lib/python3.10/site-packages\"\r\n    },\r\n    \"Config\": {\r\n        \"ldsr_steps\": 100,\r\n        \"ldsr_cached\": false,\r\n        \"SCUNET_tile\": 256,\r\n        \"SCUNET_tile_overlap\": 8,\r\n        \"SWIN_tile\": 192,\r\n        \"SWIN_tile_overlap\": 8,\r\n        \"SWIN_torch_compile\": false,\r\n        \"sd_model_checkpoint\": \"v1-5-pruned-emaonly.safetensors [6ce0161689]\"\r\n    },\r\n    \"Startup\": {\r\n        \"total\": 21.453456163406372,\r\n        \"records\": {\r\n            \"initial startup\": 0.025907039642333984,\r\n            \"prepare environment/checks\": 4.863739013671875e-05,\r\n            \"prepare environment/git version info\": 0.005784034729003906,\r\n            \"prepare environment/torch GPU test\": 1.602529764175415,\r\n            \"prepare environment/install clip\": 3.7782399654388428,\r\n            \"prepare environment/clone repositores\": 0.029155492782592773,\r\n            \"prepare environment/install requirements\": 10.270246744155884,\r\n            \"prepare environment/run extensions installers\": 0.00315093994140625,\r\n            \"prepare environment\": 15.688153743743896,\r\n            \"launcher\": 0.002853870391845703,\r\n            \"import torch\": 2.024134635925293,\r\n            \"import gradio\": 0.6038839817047119,\r\n            \"setup paths\": 0.9850642681121826,\r\n            \"import ldm\": 0.0020847320556640625,\r\n            \"import sgm\": 2.6226043701171875e-06,\r\n            \"initialize shared\": 0.2091052532196045,\r\n            \"other imports\": 0.47343921661376953,\r\n            \"opts onchange\": 0.00031113624572753906,\r\n            \"setup SD model\": 0.0012323856353759766,\r\n            \"setup codeformer\": 0.004063844680786133,\r\n            \"setup gfpgan\": 0.004363536834716797,\r\n            \"set samplers\": 3.7670135498046875e-05,\r\n            \"list extensions\": 0.0019216537475585938,\r\n            \"restore config state file\": 1.1920928955078125e-05,\r\n            \"list SD models\": 0.006326436996459961,\r\n            \"list localizations\": 0.00011610984802246094,\r\n            \"load scripts/ldsr_model.py\": 0.14216876029968262,\r\n            \"load scripts/lora_script.py\": 0.11543917655944824,\r\n            \"load scripts/scunet_model.py\": 0.019077539443969727,\r\n            \"load scripts/swinir_model.py\": 0.016368389129638672,\r\n            \"load scripts/hotkey_config.py\": 0.0004935264587402344,\r\n            \"load scripts/extra_options_section.py\": 0.0006854534149169922,\r\n            \"load scripts/hypertile_script.py\": 0.004834413528442383,\r\n            \"load scripts/hypertile_xyz.py\": 0.00016427040100097656,\r\n            \"load scripts/soft_inpainting.py\": 0.0019490718841552734,\r\n            \"load scripts/comments.py\": 0.017961502075195312,\r\n            \"load scripts/refiner.py\": 0.0006122589111328125,\r\n            \"load scripts/seed.py\": 0.0008003711700439453,\r\n            \"load scripts\": 0.3205845355987549,\r\n            \"load upscalers\": 0.003819704055786133,\r\n            \"refresh VAE\": 0.008000373840332031,\r\n            \"refresh textual inversion templates\": 4.839897155761719e-05,\r\n            \"scripts list_optimizers\": 0.00022935867309570312,\r\n            \"scripts list_unets\": 5.245208740234375e-06,\r\n            \"reload hypernetworks\": 0.017555713653564453,\r\n            \"initialize extra networks\": 0.05547022819519043,\r\n            \"scripts before_ui_callback\": 0.005842685699462891,\r\n            \"create ui\": 0.4803781509399414,\r\n            \"gradio launch\": 0.22127676010131836,\r\n            \"add APIs\": 0.3075387477874756,\r\n            \"app_started_callback/lora_script.py\": 0.00026035308837890625,\r\n            \"app_started_callback\": 0.00026345252990722656\r\n        }\r\n    },\r\n    \"Packages\": [\r\n        \"accelerate==0.21.0\",\r\n        \"aenum==3.1.15\",\r\n        \"aiofiles==23.2.1\",\r\n        \"aiohttp==3.9.3\",\r\n        \"aiosignal==1.3.1\",\r\n        \"altair==5.2.0\",\r\n        \"annotated-types==0.6.0\",\r\n        \"antlr4-python3-runtime==4.9.3\",\r\n        \"anyio==3.7.1\",\r\n        \"async-timeout==4.0.3\",\r\n        \"attrs==23.2.0\",\r\n        \"blendmodes==2022\",\r\n        \"certifi==2024.2.2\",\r\n        \"charset-normalizer==3.3.2\",\r\n        \"clean-fid==0.1.35\",\r\n        \"click==8.1.7\",\r\n        \"clip==1.0\",\r\n        \"contourpy==1.2.0\",\r\n        \"cycler==0.12.1\",\r\n        \"deprecation==2.1.0\",\r\n        \"einops==0.4.1\",\r\n        \"exceptiongroup==1.2.0\",\r\n        \"facexlib==0.3.0\",\r\n        \"fastapi==0.94.0\",\r\n        \"ffmpy==0.3.2\",\r\n        \"filelock==3.13.1\",\r\n        \"filterpy==1.4.5\",\r\n        \"fonttools==4.49.0\",\r\n        \"frozenlist==1.4.1\",\r\n        \"fsspec==2024.2.0\",\r\n        \"ftfy==6.1.3\",\r\n        \"gitdb==4.0.11\",\r\n        \"gitpython==3.1.32\",\r\n        \"gradio-client==0.5.0\",\r\n        \"gradio==3.41.2\",\r\n        \"h11==0.12.0\",\r\n        \"httpcore==0.15.0\",\r\n        \"httpx==0.24.1\",\r\n        \"huggingface-hub==0.21.4\",\r\n        \"idna==3.6\",\r\n        \"imageio==2.34.0\",\r\n        \"importlib-resources==6.1.3\",\r\n        \"inflection==0.5.1\",\r\n        \"jinja2==3.1.3\",\r\n        \"jsonmerge==1.8.0\",\r\n        \"jsonschema-specifications==2023.12.1\",\r\n        \"jsonschema==4.21.1\",\r\n        \"kiwisolver==1.4.5\",\r\n        \"kornia==0.6.7\",\r\n        \"lark==1.1.2\",\r\n        \"lazy-loader==0.3\",\r\n        \"lightning-utilities==0.10.1\",\r\n        \"llvmlite==0.42.0\",\r\n        \"markupsafe==2.1.5\",\r\n        \"matplotlib==3.8.3\",\r\n        \"mpmath==1.3.0\",\r\n        \"multidict==6.0.5\",\r\n        \"networkx==3.2.1\",\r\n        \"numba==0.59.0\",\r\n        \"numpy==1.26.2\",\r\n        \"omegaconf==2.2.3\",\r\n        \"open-clip-torch==2.20.0\",\r\n        \"opencv-python==4.9.0.80\",\r\n        \"orjson==3.9.15\",\r\n        \"packaging==23.2\",\r\n        \"pandas==2.2.1\",\r\n        \"piexif==1.1.3\",\r\n        \"pillow-simd==9.0.0.post1\",\r\n        \"pillow==9.5.0\",\r\n        \"pip==24.0\",\r\n        \"protobuf==3.20.3\",\r\n        \"psutil==5.9.5\",\r\n        \"pydantic-core==2.16.3\",\r\n        \"pydantic==1.10.14\",\r\n        \"pydub==0.25.1\",\r\n        \"pyparsing==3.1.2\",\r\n        \"python-dateutil==2.9.0.post0\",\r\n        \"python-multipart==0.0.9\",\r\n        \"pytorch-lightning==1.9.4\",\r\n        \"pytz==2024.1\",\r\n        \"pywavelets==1.5.0\",\r\n        \"pyyaml==6.0.1\",\r\n        \"referencing==0.33.0\",\r\n        \"regex==2023.12.25\",\r\n        \"requests==2.31.0\",\r\n        \"resize-right==0.0.2\",\r\n        \"rpds-py==0.18.0\",\r\n        \"safetensors==0.4.2\",\r\n        \"scikit-image==0.21.0\",\r\n        \"scipy==1.12.0\",\r\n        \"semantic-version==2.10.0\",\r\n        \"sentencepiece==0.2.0\",\r\n        \"setuptools==69.1.1\",\r\n        \"six==1.16.0\",\r\n        \"smmap==5.0.1\",\r\n        \"sniffio==1.3.1\",\r\n        \"spandrel==0.1.6\",\r\n        \"starlette==0.26.1\",\r\n        \"sympy==1.12\",\r\n        \"tifffile==2024.2.12\",\r\n        \"timm==0.9.16\",\r\n        \"tokenizers==0.13.3\",\r\n        \"tomesd==0.1.3\",\r\n        \"toolz==0.12.1\",\r\n        \"torch==2.1.2+cu121\",\r\n        \"torchdiffeq==0.2.3\",\r\n        \"torchmetrics==1.3.1\",\r\n        \"torchsde==0.2.6\",\r\n        \"torchvision==0.16.2+cu121\",\r\n        \"tqdm==4.66.2\",\r\n        \"trampoline==0.1.2\",\r\n        \"transformers==4.30.2\",\r\n        \"triton==2.1.0\",\r\n        \"typing-extensions==4.10.0\",\r\n        \"tzdata==2024.1\",\r\n        \"urllib3==2.2.1\",\r\n        \"uvicorn==0.27.1\",\r\n        \"wcwidth==0.2.13\",\r\n        \"websockets==11.0.3\",\r\n        \"wheel==0.42.0\",\r\n        \"xformers==0.0.23.post1\",\r\n        \"yarl==1.9.4\"\r\n    ]\r\n}\n\n### Console logs\n\n```Shell\n2024-03-09 01:56:06 fatal: not a git repository: /app/../.git/modules/stable-diffusion-webui\r\n2024-03-09 01:56:06 fatal: not a git repository: /app/../.git/modules/stable-diffusion-webui\r\n2024-03-09 01:56:26 *** Error loading script: hypertile_script.py\r\n2024-03-09 01:56:26     Traceback (most recent call last):\r\n2024-03-09 01:56:26       File \"/app/modules/scripts.py\", line 527, in load_scripts\r\n2024-03-09 01:56:26         script_module = script_loading.load_module(scriptfile.path)\r\n2024-03-09 01:56:26       File \"/app/modules/script_loading.py\", line 10, in load_module\r\n2024-03-09 01:56:26         module_spec.loader.exec_module(module)\r\n2024-03-09 01:56:26       File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\r\n2024-03-09 01:56:26       File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\r\n2024-03-09 01:56:26       File \"/app/extensions-builtin/hypertile/scripts/hypertile_script.py\", line 3, in <module>\r\n2024-03-09 01:56:26         from scripts.hypertile_xyz import add_axis_options\r\n2024-03-09 01:56:26       File \"/app/extensions-builtin/hypertile/scripts/hypertile_xyz.py\", line 4, in <module>\r\n2024-03-09 01:56:26         xyz_grid = [x for x in scripts.scripts_data if x.script_class.__module__ == \"xyz_grid.py\"][0].module\r\n2024-03-09 01:56:26     IndexError: list index out of range\r\n2024-03-09 01:56:26 \r\n2024-03-09 01:56:26 ---\r\n2024-03-09 01:56:26 *** Error loading script: hypertile_xyz.py\r\n2024-03-09 01:56:26     Traceback (most recent call last):\r\n2024-03-09 01:56:26       File \"/app/modules/scripts.py\", line 527, in load_scripts\r\n2024-03-09 01:56:26         script_module = script_loading.load_module(scriptfile.path)\r\n2024-03-09 01:56:26       File \"/app/modules/script_loading.py\", line 10, in load_module\r\n2024-03-09 01:56:26         module_spec.loader.exec_module(module)\r\n2024-03-09 01:56:26       File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\r\n2024-03-09 01:56:26       File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\r\n2024-03-09 01:56:26       File \"/app/extensions-builtin/hypertile/scripts/hypertile_xyz.py\", line 4, in <module>\r\n2024-03-09 01:56:26         xyz_grid = [x for x in scripts.scripts_data if x.script_class.__module__ == \"xyz_grid.py\"][0].module\r\n2024-03-09 01:56:26     IndexError: list index out of range\r\n2024-03-09 01:56:26 \r\n2024-03-09 01:56:26 ---\r\n2024-03-09 01:56:27 Python 3.10.13 (main, Feb 13 2024, 10:39:58) [GCC 12.2.0]\r\n2024-03-09 01:56:27 Version: 1.8.0-RC\r\n2024-03-09 01:56:27 Commit hash: <none>\r\n2024-03-09 01:56:27 Installing clip\r\n2024-03-09 01:56:27 Installing requirements\r\n2024-03-09 01:56:27 Launching Web UI with arguments: --listen --port 7860 --data-dir /data --gradio-allowed-path /app --allow-code --medvram --xformers --enable-insecure-extension-access --api\r\n2024-03-09 01:56:27 Loading weights [6ce0161689] from /data/models/Stable-diffusion/v1-5-pruned-emaonly.safetensors\n```\n\n\n### Additional information\n\nIt's a bug relative to 2f98a35fc4508494355c01ec45f5bec725f570a6\r\n\r\nUI is looking for `http://localhost:7863/file=/app/webui-assets/css/sourcesanspro.css` but the assets repo is mounted at `http://localhost:7863/webui-assets`. It is because `style.css` is loaded at `http://localhost:7863/file=/app/style.css` and `@import url()` with relative path.\r\n\r\nI will submit a PR.\r\n\r\n![image](https://github.com/AUTOMATIC1111/stable-diffusion-webui/assets/16995691/0989a2a6-9ff4-42c3-88e9-3218581f889d)\r\n\n", "hints_text": "", "created_at": "2024-03-08T18:11:11Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 15135, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-15135", "issue_numbers": ["15132"], "base_commit": "eee46a5094f748ebe9fe4978b0d440a8ebbab4a6", "patch": "diff --git a/modules/styles.py b/modules/styles.py\nindex 60bd8a7fb01..a9d8636a98d 100644\n--- a/modules/styles.py\n+++ b/modules/styles.py\n@@ -42,7 +42,7 @@ def extract_style_text_from_prompt(style_text, prompt):\n     stripped_style_text = style_text.strip()\r\n \r\n     if \"{prompt}\" in stripped_style_text:\r\n-        left, right = stripped_style_text.split(\"{prompt}\", 2)\r\n+        left, _, right = stripped_style_text.partition(\"{prompt}\")\r\n         if stripped_prompt.startswith(left) and stripped_prompt.endswith(right):\r\n             prompt = stripped_prompt[len(left):len(stripped_prompt)-len(right)]\r\n             return True, prompt\r\n", "test_patch": "", "problem_statement": "[Bug]: Error when hitting \u267b\ufe0f recycle button\n### Checklist\r\n\r\n- [ ] The issue exists after disabling all extensions\r\n- [ ] The issue exists on a clean installation of webui\r\n- [ ] The issue is caused by an extension, but I believe it is caused by a bug in the webui\r\n- [ ] The issue exists in the current version of the webui\r\n- [ ] The issue has not been reported before recently\r\n- [ ] The issue has been reported before but has not been fixed yet\r\n\r\n### What happened?\r\n\r\nWhen I click reuse seed button nothing happens in the UI.\r\nIt still happens when I disabled all extentions.\r\n\r\n### Steps to reproduce the problem\r\n\r\n1. Generate an image\r\n2. Click reuse see button\r\n\r\n### What should have happened?\r\n\r\nThe see field should populate with the seed from the previous generation.\r\n\r\n### What browsers do you use to access the UI ?\r\n\r\nChrome\r\n\r\n### Sysinfo\r\n\r\n```\r\n{\r\n    \"Platform\": \"Windows-10-10.0.22631-SP0\",\r\n    \"Python\": \"3.10.6\",\r\n    \"Version\": \"v1.8.0\",\r\n    \"Commit\": \"bef51aed032c0aaa5cfd80445bc4cf0d85b408b5\",\r\n    \"Script path\": \"E:\\\\AI\\\\A1111\",\r\n    \"Data path\": \"E:\\\\AI\\\\A1111\",\r\n    \"Extensions dir\": \"E:\\\\AI\\\\A1111\\\\extensions\",\r\n    \"Checksum\": \"87a450ec99b96a8109df61d99e5ea59b78d567b91848137f0fcd476b5f87864a\",\r\n    \"Commandline\": [\r\n        \"launch.py\",\r\n        \"--listen\",\r\n        \"--port\",\r\n        \"7799\",\r\n        \"--api\",\r\n        \"--update-check\",\r\n        \"--xformers\",\r\n        \"--cors-allow-origins=https://www.painthua.com\",\r\n        \"--medvram-sdxl\",\r\n        \"--enable-insecure-extension-access\",\r\n        \"--styles-file\",\r\n        \"E:\\\\AI\\\\MODELS\\\\styles-full.csv\",\r\n        \"--ckpt-dir\",\r\n        \"E:\\\\AI\\\\MODELS\\\\Stable-diffusion\",\r\n        \"--lora-dir\",\r\n        \"E:\\\\AI\\\\MODELS\\\\LoRA\",\r\n        \"--embeddings-dir\",\r\n        \"E:\\\\AI\\\\MODELS\\\\Embeddings\"\r\n    ],\r\n    \"Torch env info\": {\r\n        \"torch_version\": \"2.1.2+cu121\",\r\n        \"is_debug_build\": \"False\",\r\n        \"cuda_compiled_version\": \"12.1\",\r\n        \"gcc_version\": null,\r\n        \"clang_version\": null,\r\n        \"cmake_version\": \"version 3.28.3\",\r\n        \"os\": \"Microsoft Windows 11 Pro\",\r\n        \"libc_version\": \"N/A\",\r\n        \"python_version\": \"3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)] (64-bit runtime)\",\r\n        \"python_platform\": \"Windows-10-10.0.22631-SP0\",\r\n        \"is_cuda_available\": \"True\",\r\n        \"cuda_runtime_version\": \"11.7.99\\r\",\r\n        \"cuda_module_loading\": \"LAZY\",\r\n        \"nvidia_driver_version\": \"551.52\",\r\n        \"nvidia_gpu_models\": \"GPU 0: NVIDIA GeForce RTX 3080\",\r\n        \"cudnn_version\": null,\r\n        \"pip_version\": \"pip3\",\r\n        \"pip_packages\": [\r\n            \"numpy==1.26.2\",\r\n            \"open-clip-torch==2.20.0\",\r\n            \"pytorch-lightning==1.9.4\",\r\n            \"torch==2.1.2+cu121\",\r\n            \"torchdiffeq==0.2.3\",\r\n            \"torchmetrics==0.11.0\",\r\n            \"torchsde==0.2.6\",\r\n            \"torchvision==0.16.2+cu121\"\r\n        ],\r\n        \"conda_packages\": \"\",\r\n        \"hip_compiled_version\": \"N/A\",\r\n        \"hip_runtime_version\": \"N/A\",\r\n        \"miopen_runtime_version\": \"N/A\",\r\n        \"caching_allocator_config\": \"\",\r\n        \"is_xnnpack_available\": \"True\",\r\n        \"cpu_info\": [\r\n            \"Architecture=9\",\r\n            \"CurrentClockSpeed=3901\",\r\n            \"DeviceID=CPU0\",\r\n            \"Family=107\",\r\n            \"L2CacheSize=4096\",\r\n            \"L2CacheSpeed=\",\r\n            \"Manufacturer=AuthenticAMD\",\r\n            \"MaxClockSpeed=3901\",\r\n            \"Name=AMD Ryzen 7 3800X 8-Core Processor             \",\r\n            \"ProcessorType=3\",\r\n            \"Revision=28928\"\r\n        ]\r\n    },\r\n    \"Exceptions\": [],\r\n    \"CPU\": {\r\n        \"model\": \"AMD64 Family 23 Model 113 Stepping 0, AuthenticAMD\",\r\n        \"count logical\": 16,\r\n        \"count physical\": 8\r\n    },\r\n    \"RAM\": {\r\n        \"total\": \"64GB\",\r\n        \"used\": \"26GB\",\r\n        \"free\": \"38GB\"\r\n    },\r\n```\r\n    \r\n\r\n### Console logs\r\n\r\n```Shell\r\n*** Error retrieving seed from generation info: {\"prompt\": \"tree\", \"all_prompts\": [\"tree\"], \"negative_prompt\": \"\", \"all_negative_prompts\": [\"\"], \"seed\": 2117452332, \"all_seeds\": [2117452332], \"subseed\": 1569315522, \"all_subseeds\": [1569315522], \"subseed_strength\": 0, \"width\": 512, \"height\": 704, \"sampler_name\": \"Euler a\", \"cfg_scale\": 7, \"steps\": 20, \"batch_size\": 1, \"restore_faces\": false, \"face_restoration_model\": null, \"sd_model_name\": \"Soft Realistic 155 Xxx\", \"sd_model_hash\": \"48de823bb4\", \"sd_vae_name\": null, \"sd_vae_hash\": null, \"seed_resize_from_w\": -1, \"seed_resize_from_h\": -1, \"denoising_strength\": 0.35, \"extra_generation_params\": {}, \"index_of_first_image\": 0, \"infotexts\": [\"tree\\nSteps: 20, Sampler: Euler a, CFG scale: 7, Seed: 2117452332, Size: 512x704, Model: Soft Realistic 155 Xxx, ENSD: 31337, Version: v1.8.0\"], \"styles\": [], \"job_timestamp\": \"20240304154958\", \"clip_skip\": 1, \"is_using_inpainting_conditioning\": false, \"version\": \"v1.8.0\"}\r\n    Traceback (most recent call last):\r\n      File \"E:\\AI\\A1111\\modules/processing_scripts\\seed.py\", line 91, in copy_seed\r\n        gen_parameters = infotext_utils.parse_generation_parameters(infotext, [])\r\n      File \"E:\\AI\\A1111\\modules\\infotext_utils.py\", line 269, in parse_generation_parameters\r\n        found_styles, prompt, negative_prompt = shared.prompt_styles.extract_styles_from_prompt(prompt, negative_prompt)\r\n      File \"E:\\AI\\A1111\\modules\\styles.py\", line 218, in extract_styles_from_prompt\r\n        is_match, new_prompt, new_neg_prompt = extract_original_prompts(\r\n      File \"E:\\AI\\A1111\\modules\\styles.py\", line 70, in extract_original_prompts\r\n        match_positive, extracted_positive = extract_style_text_from_prompt(style.prompt, prompt)\r\n      File \"E:\\AI\\A1111\\modules\\styles.py\", line 45, in extract_style_text_from_prompt\r\n        left, right = stripped_style_text.split(\"{prompt}\", 2)\r\n    ValueError: too many values to unpack (expected 2)\r\n\r\n---\r\nPython 3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]\r\nVersion: v1.8.0\r\n```\r\n\r\n\r\n### Additional information\r\n\r\n_No response_\n", "hints_text": "", "created_at": "2024-03-05T03:42:29Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 14995, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-14995", "issue_numbers": ["14591", "14591"], "base_commit": "0a271938d897fe73924388295e53a72347c75029", "patch": "diff --git a/javascript/settings.js b/javascript/settings.js\nindex e6009290ab3..b2d981c2144 100644\n--- a/javascript/settings.js\n+++ b/javascript/settings.js\n@@ -55,8 +55,8 @@ onOptionsChanged(function() {\n     });\n \n     opts._categories.forEach(function(x) {\n-        var section = x[0];\n-        var category = x[1];\n+        var section = localization[x[0]] ?? x[0];\n+        var category = localization[x[1]] ?? x[1];\n \n         var span = document.createElement('SPAN');\n         span.textContent = category;\n", "test_patch": "", "problem_statement": "[Bug]: The categories layout is different when localization is on\n### Checklist\n\n- [X] The issue exists after disabling all extensions\n- [X] The issue exists on a clean installation of webui\n- [ ] The issue is caused by an extension, but I believe it is caused by a bug in the webui\n- [X] The issue exists in the current version of the webui\n- [X] The issue has not been reported before recently\n- [ ] The issue has been reported before but has not been fixed yet\n\n### What happened?\n\n![image](https://github.com/AUTOMATIC1111/stable-diffusion-webui/assets/2220320/250bd342-0cfc-445b-80ca-22a88bc7fcae)\r\n\r\n\r\n![image](https://github.com/AUTOMATIC1111/stable-diffusion-webui/assets/2220320/65ddcc3a-33b1-4dec-9e09-625a5ba2564a)\n\n### Steps to reproduce the problem\n\n1. Load a localization file\n\n### What should have happened?\n\nThe categories should be consistent across localizations.\n\n### What browsers do you use to access the UI ?\n\nGoogle Chrome\n\n### Sysinfo\n\n[sysinfo-2024-01-09-03-00.json](https://github.com/AUTOMATIC1111/stable-diffusion-webui/files/13868318/sysinfo-2024-01-09-03-00.json)\r\n\n\n### Console logs\n\n```Shell\nPython 3.10.12 | packaged by conda-forge | (main, Jun 23 2023, 22:34:57) [MSC v.1936 64 bit (AMD64)]\r\nVersion: v1.7.0-311-g6869d958\r\nCommit hash: 6869d95890849c9b209bb66774539bfdf870df2c\r\nLaunching Web UI with arguments: --medvram --theme dark --xformers --xformers-flash-attention --api --autolaunch --allow-code\r\nStyle database not found: D:\\stable-diffusion-webui\\styles.csv\r\nhf_hub_download Bingsu/adetailer face_yolov8n.pt None None None None None None None auto None False None None 10 False None False False\r\nhf_hub_download Bingsu/adetailer face_yolov8s.pt None None None None None None None auto None False None None 10 False None False False\r\nhf_hub_download Bingsu/adetailer hand_yolov8n.pt None None None None None None None auto None False None None 10 False None False False\r\nhf_hub_download Bingsu/adetailer person_yolov8n-seg.pt None None None None None None None auto None False None None 10 False None False False\r\nhf_hub_download Bingsu/adetailer person_yolov8s-seg.pt None None None None None None None auto None False None None 10 False None False False\r\n[-] ADetailer initialized. version: 24.1.0, num models: 9\r\nControlNet preprocessor location: D:\\stable-diffusion-webui\\extensions\\sd-webui-controlnet\\annotator\\downloads\r\n2024-01-08 20:59:43,456 - ControlNet - INFO - ControlNet v1.1.427\r\n2024-01-08 20:59:43,713 - ControlNet - INFO - ControlNet v1.1.427\r\n[sd-webui-freeu] Controlnet support: *enabled*\r\n[Vec. CC] Style Sheet Loaded...\r\nLoading weights [c936130834] from D:\\stable-diffusion-webui\\models\\Stable-diffusion\\Nordrin_little-v3.0.safetensors\r\n2024-01-08 20:59:45,251 - AnimateDiff - INFO - Injecting LCM to UI.\r\nCreating model from config: D:\\stable-diffusion-webui\\configs\\v1-inference.yaml\r\nhf_hub_download openai/clip-vit-large-patch14 vocab.json None None None None None D:\\stable-diffusion-webui\\.cache\\huggingface\\hub None auto transformers/4.30.2; python/3.10.12; session_id/cd3bee2c76a146e38ef7ed0e5b90b0d5; torch/2.1.2; tensorflow/2.15.0; file_type/tokenizer; from_auto_class/False; is_fast/False False None None 10 False None True False\r\nhf_hub_download openai/clip-vit-large-patch14 config.json None None None None None D:\\stable-diffusion-webui\\.cache\\huggingface\\hub None auto transformers/4.30.2; python/3.10.12; session_id/cd3bee2c76a146e38ef7ed0e5b90b0d5; torch/2.1.2; tensorflow/2.15.0; file_type/config; from_auto_class/False False None None 10 False None True False\r\nApplying attention optimization: xformers... done.\r\nModel loaded in 2.4s (load weights from disk: 0.5s, create model: 0.5s, apply weights to model: 0.7s, calculate empty prompt: 0.7s).\r\n2024-01-08 20:59:48,041 - AnimateDiff - INFO - Hacking i2i-batch.\r\n*** Error executing callback ui_tabs_callback for D:\\stable-diffusion-webui\\extensions\\sd-webui-supermerger\\scripts\\supermerger.py\r\n    Traceback (most recent call last):\r\n      File \"D:\\stable-diffusion-webui\\modules\\script_callbacks.py\", line 169, in ui_tabs_callback\r\n        res += c.callback() or []\r\n      File \"D:\\stable-diffusion-webui\\extensions\\sd-webui-supermerger\\scripts\\supermerger.py\", line 406, in on_ui_tabs\r\n        mgallery, mgeninfo, mhtmlinfo, mhtmllog = create_output_panel(\"txt2img\", opts.outdir_txt2img_samples)\r\n    TypeError: cannot unpack non-iterable OutputPanel object\r\n\r\n---\r\nRunning on local URL:  http://127.0.0.1:7860\r\n\r\nTo create a public link, set `share=True` in `launch()`.\r\n2024-01-08 20:59:52,275 - AnimateDiff - INFO - Hacking i2i-batch is already done.\r\n2024-01-08 20:59:52,281 - AnimateDiff - INFO - Hacking i2i-batch is already done.\r\n*** Error executing callback app_started_callback for D:\\stable-diffusion-webui\\extensions\\sd-webui-supermerger\\scripts\\GenParamGetter.py\n```\n\n\n### Additional information\n\n_No response_\n[Bug]: The categories layout is different when localization is on\n### Checklist\n\n- [X] The issue exists after disabling all extensions\n- [X] The issue exists on a clean installation of webui\n- [ ] The issue is caused by an extension, but I believe it is caused by a bug in the webui\n- [X] The issue exists in the current version of the webui\n- [X] The issue has not been reported before recently\n- [ ] The issue has been reported before but has not been fixed yet\n\n### What happened?\n\n![image](https://github.com/AUTOMATIC1111/stable-diffusion-webui/assets/2220320/250bd342-0cfc-445b-80ca-22a88bc7fcae)\r\n\r\n\r\n![image](https://github.com/AUTOMATIC1111/stable-diffusion-webui/assets/2220320/65ddcc3a-33b1-4dec-9e09-625a5ba2564a)\n\n### Steps to reproduce the problem\n\n1. Load a localization file\n\n### What should have happened?\n\nThe categories should be consistent across localizations.\n\n### What browsers do you use to access the UI ?\n\nGoogle Chrome\n\n### Sysinfo\n\n[sysinfo-2024-01-09-03-00.json](https://github.com/AUTOMATIC1111/stable-diffusion-webui/files/13868318/sysinfo-2024-01-09-03-00.json)\r\n\n\n### Console logs\n\n```Shell\nPython 3.10.12 | packaged by conda-forge | (main, Jun 23 2023, 22:34:57) [MSC v.1936 64 bit (AMD64)]\r\nVersion: v1.7.0-311-g6869d958\r\nCommit hash: 6869d95890849c9b209bb66774539bfdf870df2c\r\nLaunching Web UI with arguments: --medvram --theme dark --xformers --xformers-flash-attention --api --autolaunch --allow-code\r\nStyle database not found: D:\\stable-diffusion-webui\\styles.csv\r\nhf_hub_download Bingsu/adetailer face_yolov8n.pt None None None None None None None auto None False None None 10 False None False False\r\nhf_hub_download Bingsu/adetailer face_yolov8s.pt None None None None None None None auto None False None None 10 False None False False\r\nhf_hub_download Bingsu/adetailer hand_yolov8n.pt None None None None None None None auto None False None None 10 False None False False\r\nhf_hub_download Bingsu/adetailer person_yolov8n-seg.pt None None None None None None None auto None False None None 10 False None False False\r\nhf_hub_download Bingsu/adetailer person_yolov8s-seg.pt None None None None None None None auto None False None None 10 False None False False\r\n[-] ADetailer initialized. version: 24.1.0, num models: 9\r\nControlNet preprocessor location: D:\\stable-diffusion-webui\\extensions\\sd-webui-controlnet\\annotator\\downloads\r\n2024-01-08 20:59:43,456 - ControlNet - INFO - ControlNet v1.1.427\r\n2024-01-08 20:59:43,713 - ControlNet - INFO - ControlNet v1.1.427\r\n[sd-webui-freeu] Controlnet support: *enabled*\r\n[Vec. CC] Style Sheet Loaded...\r\nLoading weights [c936130834] from D:\\stable-diffusion-webui\\models\\Stable-diffusion\\Nordrin_little-v3.0.safetensors\r\n2024-01-08 20:59:45,251 - AnimateDiff - INFO - Injecting LCM to UI.\r\nCreating model from config: D:\\stable-diffusion-webui\\configs\\v1-inference.yaml\r\nhf_hub_download openai/clip-vit-large-patch14 vocab.json None None None None None D:\\stable-diffusion-webui\\.cache\\huggingface\\hub None auto transformers/4.30.2; python/3.10.12; session_id/cd3bee2c76a146e38ef7ed0e5b90b0d5; torch/2.1.2; tensorflow/2.15.0; file_type/tokenizer; from_auto_class/False; is_fast/False False None None 10 False None True False\r\nhf_hub_download openai/clip-vit-large-patch14 config.json None None None None None D:\\stable-diffusion-webui\\.cache\\huggingface\\hub None auto transformers/4.30.2; python/3.10.12; session_id/cd3bee2c76a146e38ef7ed0e5b90b0d5; torch/2.1.2; tensorflow/2.15.0; file_type/config; from_auto_class/False False None None 10 False None True False\r\nApplying attention optimization: xformers... done.\r\nModel loaded in 2.4s (load weights from disk: 0.5s, create model: 0.5s, apply weights to model: 0.7s, calculate empty prompt: 0.7s).\r\n2024-01-08 20:59:48,041 - AnimateDiff - INFO - Hacking i2i-batch.\r\n*** Error executing callback ui_tabs_callback for D:\\stable-diffusion-webui\\extensions\\sd-webui-supermerger\\scripts\\supermerger.py\r\n    Traceback (most recent call last):\r\n      File \"D:\\stable-diffusion-webui\\modules\\script_callbacks.py\", line 169, in ui_tabs_callback\r\n        res += c.callback() or []\r\n      File \"D:\\stable-diffusion-webui\\extensions\\sd-webui-supermerger\\scripts\\supermerger.py\", line 406, in on_ui_tabs\r\n        mgallery, mgeninfo, mhtmlinfo, mhtmllog = create_output_panel(\"txt2img\", opts.outdir_txt2img_samples)\r\n    TypeError: cannot unpack non-iterable OutputPanel object\r\n\r\n---\r\nRunning on local URL:  http://127.0.0.1:7860\r\n\r\nTo create a public link, set `share=True` in `launch()`.\r\n2024-01-08 20:59:52,275 - AnimateDiff - INFO - Hacking i2i-batch is already done.\r\n2024-01-08 20:59:52,281 - AnimateDiff - INFO - Hacking i2i-batch is already done.\r\n*** Error executing callback app_started_callback for D:\\stable-diffusion-webui\\extensions\\sd-webui-supermerger\\scripts\\GenParamGetter.py\n```\n\n\n### Additional information\n\n_No response_\n", "hints_text": "\n", "created_at": "2024-02-22T03:44:11Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 14715, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-14715", "issue_numbers": ["13994"], "base_commit": "8a6a4ad894c2e28bcb6924490396997bfed7f07f", "patch": "diff --git a/modules/api/api.py b/modules/api/api.py\nindex b3d74e513a3..b6bb9d06ab1 100644\n--- a/modules/api/api.py\n+++ b/modules/api/api.py\n@@ -230,6 +230,7 @@ def __init__(self, app: FastAPI, queue_lock: Lock):\n         self.add_api_route(\"/sdapi/v1/realesrgan-models\", self.get_realesrgan_models, methods=[\"GET\"], response_model=list[models.RealesrganItem])\n         self.add_api_route(\"/sdapi/v1/prompt-styles\", self.get_prompt_styles, methods=[\"GET\"], response_model=list[models.PromptStyleItem])\n         self.add_api_route(\"/sdapi/v1/embeddings\", self.get_embeddings, methods=[\"GET\"], response_model=models.EmbeddingsResponse)\n+        self.add_api_route(\"/sdapi/v1/refresh-embeddings\", self.refresh_embeddings, methods=[\"POST\"])\n         self.add_api_route(\"/sdapi/v1/refresh-checkpoints\", self.refresh_checkpoints, methods=[\"POST\"])\n         self.add_api_route(\"/sdapi/v1/refresh-vae\", self.refresh_vae, methods=[\"POST\"])\n         self.add_api_route(\"/sdapi/v1/create/embedding\", self.create_embedding, methods=[\"POST\"], response_model=models.CreateResponse)\n@@ -643,6 +644,10 @@ def convert_embeddings(embeddings):\n             \"skipped\": convert_embeddings(db.skipped_embeddings),\n         }\n \n+    def refresh_embeddings(self):\n+        with self.queue_lock:\n+            sd_hijack.model_hijack.embedding_db.load_textual_inversion_embeddings(force_reload=True)\n+\n     def refresh_checkpoints(self):\n         with self.queue_lock:\n             shared.refresh_checkpoints()\n", "test_patch": "", "problem_statement": "[Feature Request]: Add an API endpoint for refreshing Embeddings\n### Is there an existing issue for this?\n\n- [x] I have searched the existing issues and checked the recent builds/commits\n\n### What would your feature do ?\n\nI am currently running 2 instances of the WebUI locally, and that (for performance reasons) both use different hardware for different tasks. I interact with those via a backend communicating with the API. When I create an embedding on one instance it needs a manual refresh (e.g. refreshing the browser window, using the button in the UI etc) before the other instance can use it. \r\n\r\nAs far as I know its currently possible to do something like refreshing via the API only for Checkpoints and VAE's, not for embeddings. \r\n\r\nAn API endpoint for refreshing the embeddings would be a big problem solver for not only my use-case but lots of others in terms of automation. \n\n### Proposed workflow\n\nIn my case:\r\n1. Perform an operation on an embedding that needs a refresh to be accessed somewhere else (e.g. another instance)\r\n2. Make a request to the corresponding URL for refreshing embeddings as shown in the API docs \r\n3. Use the newly available embedding any way you want in your preferred location\r\n\r\n\n\n### Additional information\n\n_No response_\n", "hints_text": "**Bad practice temporary solution I used for this:**\r\n- Everytime my application creates an embedding on one instance of the server, it also creates one on the other, which refreshes the embeddings list on that other instance.\r\n- I gave this embedding a placeholder name like 'refresh' (or anything that is noticably different to your other \"real\" embeddings) with the property 'overwrite_old' set to _true_ so that my embeddings list doesnt get cluttered with these refreshers.\r\n- When showing the embeddings on my frontend I just filtered out the refresh embedding. \nHere's how I built it\uff08sdapi/v1/refresh-embeddings\uff09in api.py, after refreshing the query in the api updated\uff08sdapi/v1/embeddings\uff09, but the ui content is not updated.All this is for reference only\uff1a\r\n1. \r\n`self.add_api_route(\"/sdapi/v1/refresh-embeddings\", self.refresh_embeddings, methods=[\"POST\"])`\r\n2.\r\n`def refresh_embeddings(self):\r\n        sd_hijack.model_hijack.embedding_db.load_textual_inversion_embeddings(force_reload=True) `\r\n\r\n![image](https://github.com/AUTOMATIC1111/stable-diffusion-webui/assets/33999813/8d15e01b-4d31-4920-8426-2d268f08a0cb)\r\n\r\n\r\n![image](https://github.com/AUTOMATIC1111/stable-diffusion-webui/assets/33999813/ed105f21-6f1b-4a58-b4dc-139f50f08872)\r\n\r\n![image](https://github.com/AUTOMATIC1111/stable-diffusion-webui/assets/33999813/71837179-51a6-4bf8-9978-a459d75e7d42)\r\n", "created_at": "2024-01-21T13:13:12Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 14690, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-14690", "issue_numbers": ["13367"], "base_commit": "cb5b335acddd126d4f6c990982816c06beb0d6ae", "patch": "diff --git a/modules/dat_model.py b/modules/dat_model.py\nnew file mode 100644\nindex 00000000000..495d5f4937d\n--- /dev/null\n+++ b/modules/dat_model.py\n@@ -0,0 +1,79 @@\n+import os\n+\n+from modules import modelloader, errors\n+from modules.shared import cmd_opts, opts\n+from modules.upscaler import Upscaler, UpscalerData\n+from modules.upscaler_utils import upscale_with_model\n+\n+\n+class UpscalerDAT(Upscaler):\n+    def __init__(self, user_path):\n+        self.name = \"DAT\"\n+        self.user_path = user_path\n+        self.scalers = []\n+        super().__init__()\n+\n+        for file in self.find_models(ext_filter=[\".pt\", \".pth\"]):\n+            name = modelloader.friendly_name(file)\n+            scaler_data = UpscalerData(name, file, upscaler=self, scale=None)\n+            self.scalers.append(scaler_data)\n+\n+        for model in get_dat_models(self):\n+            if model.name in opts.dat_enabled_models:\n+                self.scalers.append(model)\n+\n+    def do_upscale(self, img, path):\n+        try:\n+            info = self.load_model(path)\n+        except Exception:\n+            errors.report(f\"Unable to load DAT model {path}\", exc_info=True)\n+            return img\n+\n+        model_descriptor = modelloader.load_spandrel_model(\n+            info.local_data_path,\n+            device=self.device,\n+            prefer_half=(not cmd_opts.no_half and not cmd_opts.upcast_sampling),\n+            expected_architecture=\"DAT\",\n+        )\n+        return upscale_with_model(\n+            model_descriptor,\n+            img,\n+            tile_size=opts.DAT_tile,\n+            tile_overlap=opts.DAT_tile_overlap,\n+        )\n+\n+    def load_model(self, path):\n+        for scaler in self.scalers:\n+            if scaler.data_path == path:\n+                if scaler.local_data_path.startswith(\"http\"):\n+                    scaler.local_data_path = modelloader.load_file_from_url(\n+                        scaler.data_path,\n+                        model_dir=self.model_download_path,\n+                    )\n+                if not os.path.exists(scaler.local_data_path):\n+                    raise FileNotFoundError(f\"DAT data missing: {scaler.local_data_path}\")\n+                return scaler\n+        raise ValueError(f\"Unable to find model info: {path}\")\n+\n+\n+def get_dat_models(scaler):\n+    return [\n+        UpscalerData(\n+            name=\"DAT x2\",\n+            path=\"https://github.com/n0kovo/dat_upscaler_models/raw/main/DAT/DAT_x2.pth\",\n+            scale=2,\n+            upscaler=scaler,\n+        ),\n+        UpscalerData(\n+            name=\"DAT x3\",\n+            path=\"https://github.com/n0kovo/dat_upscaler_models/raw/main/DAT/DAT_x3.pth\",\n+            scale=3,\n+            upscaler=scaler,\n+        ),\n+        UpscalerData(\n+            name=\"DAT x4\",\n+            path=\"https://github.com/n0kovo/dat_upscaler_models/raw/main/DAT/DAT_x4.pth\",\n+            scale=4,\n+            upscaler=scaler,\n+        ),\n+    ]\ndiff --git a/modules/shared_items.py b/modules/shared_items.py\nindex 13fb2814f2b..88f636452c7 100644\n--- a/modules/shared_items.py\n+++ b/modules/shared_items.py\n@@ -8,6 +8,11 @@ def realesrgan_models_names():\n     return [x.name for x in modules.realesrgan_model.get_realesrgan_models(None)]\r\n \r\n \r\n+def dat_models_names():\r\n+    import modules.dat_model\r\n+    return [x.name for x in modules.dat_model.get_dat_models(None)]\r\n+\r\n+\r\n def postprocessing_scripts():\r\n     import modules.scripts\r\n \r\ndiff --git a/modules/shared_options.py b/modules/shared_options.py\nindex 63488f4e700..74a2a67f933 100644\n--- a/modules/shared_options.py\n+++ b/modules/shared_options.py\n@@ -97,6 +97,9 @@\n     \"ESRGAN_tile\": OptionInfo(192, \"Tile size for ESRGAN upscalers.\", gr.Slider, {\"minimum\": 0, \"maximum\": 512, \"step\": 16}).info(\"0 = no tiling\"),\r\n     \"ESRGAN_tile_overlap\": OptionInfo(8, \"Tile overlap for ESRGAN upscalers.\", gr.Slider, {\"minimum\": 0, \"maximum\": 48, \"step\": 1}).info(\"Low values = visible seam\"),\r\n     \"realesrgan_enabled_models\": OptionInfo([\"R-ESRGAN 4x+\", \"R-ESRGAN 4x+ Anime6B\"], \"Select which Real-ESRGAN models to show in the web UI.\", gr.CheckboxGroup, lambda: {\"choices\": shared_items.realesrgan_models_names()}),\r\n+    \"dat_enabled_models\": OptionInfo([\"DAT x2\", \"DAT x3\", \"DAT x4\"], \"Select which DAT models to show in the web UI.\", gr.CheckboxGroup, lambda: {\"choices\": shared_items.dat_models_names()}),\r\n+    \"DAT_tile\": OptionInfo(192, \"Tile size for DAT upscalers.\", gr.Slider, {\"minimum\": 0, \"maximum\": 512, \"step\": 16}).info(\"0 = no tiling\"),\r\n+    \"DAT_tile_overlap\": OptionInfo(8, \"Tile overlap for DAT upscalers.\", gr.Slider, {\"minimum\": 0, \"maximum\": 48, \"step\": 1}).info(\"Low values = visible seam\"),\r\n     \"upscaler_for_img2img\": OptionInfo(None, \"Upscaler for img2img\", gr.Dropdown, lambda: {\"choices\": [x.name for x in shared.sd_upscalers]}),\r\n }))\r\n \r\n", "test_patch": "", "problem_statement": "[Feature Request]: DAT (Dual Aggregation Transformer) for Image Super-Resolution\n### Is there an existing issue for this?\n\n- [X] I have searched the existing issues and checked the recent builds/commits\n\n### What would your feature do ?\n\nThis look to be very promising hi-res upscale for image. Please integrate it to A1111. ComfyUI already support this DAT\r\nThank you so much\n\n### Proposed workflow\n\n(https://github.com/zhengchen1999/DAT)\n\n### Additional information\n\n_No response_\n", "hints_text": "Yeah missing DAT support is only reason I have to use ComfyUI when dealing with faces. I hope it will be available in next update on automatic1111\nI have a few DAT upscalers that I can't use. :(", "created_at": "2024-01-18T23:42:37Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 14583, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-14583", "issue_numbers": ["13952"], "base_commit": "6869d95890849c9b209bb66774539bfdf870df2c", "patch": "diff --git a/modules/sd_samplers.py b/modules/sd_samplers.py\nindex 45faae62821..a58528a0b52 100644\n--- a/modules/sd_samplers.py\n+++ b/modules/sd_samplers.py\n@@ -1,4 +1,4 @@\n-from modules import sd_samplers_kdiffusion, sd_samplers_timesteps, shared\r\n+from modules import sd_samplers_kdiffusion, sd_samplers_timesteps, sd_samplers_lcm, shared\r\n \r\n # imports for functions that previously were here and are used by other modules\r\n from modules.sd_samplers_common import samples_to_image_grid, sample_to_image  # noqa: F401\r\n@@ -6,6 +6,7 @@\n all_samplers = [\r\n     *sd_samplers_kdiffusion.samplers_data_k_diffusion,\r\n     *sd_samplers_timesteps.samplers_data_timesteps,\r\n+    *sd_samplers_lcm.samplers_data_lcm,\r\n ]\r\n all_samplers_map = {x.name: x for x in all_samplers}\r\n \r\ndiff --git a/modules/sd_samplers_lcm.py b/modules/sd_samplers_lcm.py\nnew file mode 100644\nindex 00000000000..59839b720dd\n--- /dev/null\n+++ b/modules/sd_samplers_lcm.py\n@@ -0,0 +1,104 @@\n+import torch\n+\n+from k_diffusion import utils, sampling\n+from k_diffusion.external import DiscreteEpsDDPMDenoiser\n+from k_diffusion.sampling import default_noise_sampler, trange\n+\n+from modules import shared, sd_samplers_cfg_denoiser, sd_samplers_kdiffusion, sd_samplers_common\n+\n+\n+class LCMCompVisDenoiser(DiscreteEpsDDPMDenoiser):\n+    def __init__(self, model):\n+        timesteps = 1000\n+        original_timesteps = 50     # LCM Original Timesteps (default=50, for current version of LCM)\n+        self.skip_steps = timesteps // original_timesteps\n+\n+        alphas_cumprod_valid = torch.zeros((original_timesteps), dtype=torch.float32, device=model.device)\n+        for x in range(original_timesteps):\n+            alphas_cumprod_valid[original_timesteps - 1 - x] = model.alphas_cumprod[timesteps - 1 - x * self.skip_steps]\n+\n+        super().__init__(model, alphas_cumprod_valid, quantize=None)\n+\n+\n+    def get_sigmas(self, n=None,):\n+        if n is None:\n+            return sampling.append_zero(self.sigmas.flip(0))\n+\n+        start = self.sigma_to_t(self.sigma_max)\n+        end = self.sigma_to_t(self.sigma_min)\n+\n+        t = torch.linspace(start, end, n, device=shared.sd_model.device)\n+\n+        return sampling.append_zero(self.t_to_sigma(t))\n+\n+\n+    def sigma_to_t(self, sigma, quantize=None):\n+        log_sigma = sigma.log()\n+        dists = log_sigma - self.log_sigmas[:, None]\n+        return dists.abs().argmin(dim=0).view(sigma.shape) * self.skip_steps + (self.skip_steps - 1)\n+\n+\n+    def t_to_sigma(self, timestep):\n+        t = torch.clamp(((timestep - (self.skip_steps - 1)) / self.skip_steps).float(), min=0, max=(len(self.sigmas) - 1))\n+        return super().t_to_sigma(t)\n+\n+\n+    def get_eps(self, *args, **kwargs):\n+        return self.inner_model.apply_model(*args, **kwargs)\n+\n+\n+    def get_scaled_out(self, sigma, output, input):\n+        sigma_data = 0.5\n+        scaled_timestep = utils.append_dims(self.sigma_to_t(sigma), output.ndim) * 10.0\n+\n+        c_skip = sigma_data**2 / (scaled_timestep**2 + sigma_data**2)\n+        c_out = scaled_timestep / (scaled_timestep**2 + sigma_data**2) ** 0.5\n+\n+        return c_out * output + c_skip * input\n+\n+\n+    def forward(self, input, sigma, **kwargs):\n+        c_out, c_in = [utils.append_dims(x, input.ndim) for x in self.get_scalings(sigma)]\n+        eps = self.get_eps(input * c_in, self.sigma_to_t(sigma), **kwargs)\n+        return self.get_scaled_out(sigma, input + eps * c_out, input)\n+\n+\n+def sample_lcm(model, x, sigmas, extra_args=None, callback=None, disable=None, noise_sampler=None):\n+    extra_args = {} if extra_args is None else extra_args\n+    noise_sampler = default_noise_sampler(x) if noise_sampler is None else noise_sampler\n+    s_in = x.new_ones([x.shape[0]])\n+\n+    for i in trange(len(sigmas) - 1, disable=disable):\n+        denoised = model(x, sigmas[i] * s_in, **extra_args)\n+\n+        if callback is not None:\n+            callback({'x': x, 'i': i, 'sigma': sigmas[i], 'sigma_hat': sigmas[i], 'denoised': denoised})\n+\n+        x = denoised\n+        if sigmas[i + 1] > 0:\n+            x += sigmas[i + 1] * noise_sampler(sigmas[i], sigmas[i + 1])\n+    return x\n+\n+\n+class CFGDenoiserLCM(sd_samplers_cfg_denoiser.CFGDenoiser):\n+    @property\n+    def inner_model(self):\n+        if self.model_wrap is None:\n+            denoiser = LCMCompVisDenoiser\n+            self.model_wrap = denoiser(shared.sd_model)\n+\n+        return self.model_wrap\n+\n+\n+class LCMSampler(sd_samplers_kdiffusion.KDiffusionSampler):\n+    def __init__(self, funcname, sd_model, options=None):\n+        super().__init__(funcname, sd_model, options)\n+        self.model_wrap_cfg = CFGDenoiserLCM(self)\n+        self.model_wrap = self.model_wrap_cfg.inner_model\n+\n+\n+samplers_lcm = [('LCM', sample_lcm, ['k_lcm'], {})]\n+samplers_data_lcm = [\n+    sd_samplers_common.SamplerData(label, lambda model, funcname=funcname: LCMSampler(funcname, model), aliases, options)\n+    for label, funcname, aliases, options in samplers_lcm\n+]\n", "test_patch": "", "problem_statement": "[Feature Request]: new sampler \"lcm\" for lcm-loras\n### Is there an existing issue for this?\r\n\r\n- [X] I have searched the existing issues and checked the recent builds/commits\r\n\r\n### What would your feature do ?\r\n\r\n_This feature requires not so much: no a new model class support, only a new sampler_\r\n\r\nLCM had major update and now we can use it like a regular lora:\r\nhttps://huggingface.co/latent-consistency/lcm-lora-sdv1-5\r\nhttps://huggingface.co/latent-consistency/lcm-lora-ssd-1b\r\nhttps://huggingface.co/latent-consistency/lcm-lora-sdxl\r\n\r\nWe only need to rename them and put into lora models directory. Set sampling steps to 4, CFG Scale to 1.0. Sampling method to \"DPM2\" or \"Euler a\". It gives decent results, but for better work it requires a special sampler, with is similar to others but with little change. You can look how it works in ComfyUI: https://github.com/comfyanonymous/ComfyUI/commit/002aefa382585d171aef13c7bd21f64b8664fe28\r\n\r\n### Proposed workflow\r\n\r\n1. Go to Sampling method \r\n2. Select LCM sampler\r\n\r\n\r\n### Additional information\r\n\r\nHow it works now\r\n![Screenshot 2023-11-12 at 00-13-13 Stable Diffusion](https://github.com/AUTOMATIC1111/stable-diffusion-webui/assets/33491867/db8309e4-b2b7-4a8d-93d6-97a5fd003021)\r\n\r\nCompyUI's recent update:\r\n![Screenshot_20231112_001837](https://github.com/AUTOMATIC1111/stable-diffusion-webui/assets/33491867/271f0277-d9be-485f-82f9-439ada5a65f7)\r\n\n", "hints_text": "I'm awaiting the integration of the LCM sampler into AUTOMATIC1111, While AUTOMATIC1111 is an excellent program, the implementation of new features, such as the LCM sampler and consistency VAE, appears to be sluggish. The consistency VAE is currently only accessible in beta, while comfyui already offers both consistency VAE and LCM sampler support at no time :(\n> I'm awaiting the integration of the LCM sampler into AUTOMATIC1111, While AUTOMATIC1111 is an excellent program, the implementation of new features, such as the LCM sampler and consistency VAE, appears to be sluggish. The consistency VAE is currently only accessible in beta, while comfyui already offers both consistency VAE and LCM sampler support at no time :(\r\n\r\nLet's not forget that TensorRT for SDXL is sitting on the Dev branch, too. I desperately need all three of these moved up into the Main branch.\n> > I'm awaiting the integration of the LCM sampler into AUTOMATIC1111, While AUTOMATIC1111 is an excellent program, the implementation of new features, such as the LCM sampler and consistency VAE, appears to be sluggish. The consistency VAE is currently only accessible in beta, while comfyui already offers both consistency VAE and LCM sampler support at no time :(\r\n> \r\n> Let's not forget that TensorRT for SDXL is sitting on the Dev branch, too. I desperately need all three of these moved up into the Main branch.\r\n\r\nlet's hope this will be soon \ud83e\udd1e\ud83e\udd1e\ud83e\udd1e\ud83e\udd1e\nSomeone reported nearly 5 gens per second on a 3090 with both TensorRT and LCM Lora. We're living in a pretty good timeline over here. Can't wait to see all this stuff get in so we can use it all together. Never ever thought I'd see a factor of 15+ speed up this quick. \n> Someone reported nearly 5 gens per second on a 3090 with both TensorRT and LCM Lora. We're living in a pretty good timeline over here. Can't wait to see all this stuff get in so we can use it all together. Never ever thought I'd see a factor of 15+ speed up this quick.\r\n\r\nMakes me feel happy with my 3090 Ti!\r\nCan you elaborate more on TensorRT? is that something that can be enabled on automatic1111? arent sdp and xformers the only options?\nI could make tensorrt works with lcm only after merging lcm lora into model (and training difference in supermerger)\nI\u2019m looking forward to this feature too. \r\nI wanna use contrlnets with it.\n> Someone reported nearly 5 gens per second on a 3090 with both TensorRT and LCM Lora. We're living in a pretty good timeline over here. Can't wait to see all this stuff get in so we can use it all together. Never ever thought I'd see a factor of 15+ speed up this quick.\r\n\r\nI just installed TensorRT and yeah its amazing (3090Ti)\r\nSDXL 1024x1024 from 11 seconds to 3.5 seconds an image! its a must!\n@iChristGit I assume you ran this on the Dev branch?\n> @iChristGit I assume you ran this on the Dev branch?\r\n\r\nYeah git checkout dev \r\n\nSomeone from Reddit implemented it 4 day ago\r\nhttps://www.reddit.com/r/StableDiffusion/comments/17ti2zo/you_can_add_the_lcm_sampler_to_a1111_with_a/\r\n\r\n<blockquote>\r\nYou can add the LCM sampler to A1111 with a little trick\r\nTutorial | Guide\r\n\r\nSo I was trying out the new LCM LoRA and found out the sampler is missing in A1111. As as long shot I just copied the code from Comfy, and to my surprise it seems to work. I think.\r\n\r\nYou have to make two small edits with a text editor. Here's how you do it:\r\n\r\nEdit the file `sampling.py` found at this path:\r\n\r\n`...\\stable-diffusion-webui\\repositories\\k-diffusion\\k_diffusion\\sampling.py`\r\n\r\nAdd the following at the end of the file:\r\n\r\n```python\r\n@torch.no_grad()\r\ndef sample_lcm(model, x, sigmas, extra_args=None, callback=None, disable=None, noise_sampler=None):\r\n    extra_args = {} if extra_args is None else extra_args\r\n    noise_sampler = default_noise_sampler(x) if noise_sampler is None else noise_sampler\r\n    s_in = x.new_ones([x.shape[0]])\r\n    for i in trange(len(sigmas) - 1, disable=disable):\r\n        denoised = model(x, sigmas[i] * s_in, **extra_args)\r\n        if callback is not None:\r\n            callback({'x': x, 'i': i, 'sigma': sigmas[i], 'sigma_hat': sigmas[i], 'denoised': denoised})\r\n\r\n        x = denoised\r\n        if sigmas[i + 1] > 0:\r\n            x += sigmas[i + 1] * noise_sampler(sigmas[i], sigmas[i + 1])\r\n    return x\r\n```\r\n\r\nThe second change is done in the file `sd_samplers_kdiffusion.py` found here:\r\n\r\n`...\\stable-diffusion-webui-new\\modules\\sd_samplers_kdiffusion.py`\r\n\r\nOn line 39 add this:\r\n\r\n```python\r\n('LCM Test', 'sample_lcm', ['lcm'], {}),\r\n```\r\n\r\nThat should give you a new sampler option called `'LCM Test'`.\r\n\r\n![\u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0435](https://github.com/AUTOMATIC1111/stable-diffusion-webui/assets/33491867/ba903067-b037-48ad-9ad1-a37cdfc62976)\r\n\r\n\r\n</blockquote>\nBut also there was found a bug in LCM Scheduler, and the algorithm will be updated \r\nhttps://github.com/huggingface/diffusers/issues/5815\r\n\nThere is almost no difference between \"Euler a\" and \"LCM Test\" for SD1. But for SDXL it solves all the problems\r\n\r\n![xl](https://github.com/AUTOMATIC1111/stable-diffusion-webui/assets/33491867/b5e8a01a-4c43-42e9-8536-371d6a90c7ab)\r\n![1 5](https://github.com/AUTOMATIC1111/stable-diffusion-webui/assets/33491867/ee25f0bb-b356-495c-9fc9-78e3433aeaa8)\r\n\nI've modified patch from reddit to not edit external repository:\r\n\r\n```diff\r\ndiff --git a/modules/sd_samplers_extra.py b/modules/sd_samplers_extra.py\r\nindex 1b981ca8..d154a2b6 100644\r\n--- a/modules/sd_samplers_extra.py\r\n+++ b/modules/sd_samplers_extra.py\r\n@@ -72,3 +72,20 @@ def restart_sampler(model, x, sigmas, extra_args=None, callback=None, disable=No\r\n         last_sigma = new_sigma\r\n\r\n     return x\r\n+\r\n+\r\n+@torch.no_grad()\r\n+def sample_lcm(model, x, sigmas, extra_args=None, callback=None, disable=None, noise_sampler=None):\r\n+    extra_args = {} if extra_args is None else extra_args\r\n+    noise_sampler = k_diffusion.sampling.default_noise_sampler(x) if noise_sampler is None else noise_sampler\r\n+    s_in = x.new_ones([x.shape[0]])\r\n+    for i in tqdm.auto.trange(len(sigmas) - 1, disable=disable):\r\n+        denoised = model(x, sigmas[i] * s_in, **extra_args)\r\n+        if callback is not None:\r\n+            callback({'x': x, 'i': i, 'sigma': sigmas[i], 'sigma_hat': sigmas[i], 'denoised': denoised})\r\n+\r\n+        x = denoised\r\n+        if sigmas[i + 1] > 0:\r\n+            x += sigmas[i + 1] * noise_sampler(sigmas[i], sigmas[i + 1])\r\n+    return x\r\n+\r\ndiff --git a/modules/sd_samplers_kdiffusion.py b/modules/sd_samplers_kdiffusion.py\r\nindex 8a8c87e0..b6c3dc44 100644\r\n--- a/modules/sd_samplers_kdiffusion.py\r\n+++ b/modules/sd_samplers_kdiffusion.py\r\n@@ -36,6 +36,7 @@ samplers_k_diffusion = [\r\n     ('DPM2 a Karras', 'sample_dpm_2_ancestral', ['k_dpm_2_a_ka'], {'scheduler': 'karras', 'discard_next_to_last_sigma': True, \"uses_ensd\": True, \"second_order\": True}),\r\n     ('DPM++ 2S a Karras', 'sample_dpmpp_2s_ancestral', ['k_dpmpp_2s_a_ka'], {'scheduler': 'karras', \"uses_ensd\": True, \"second_order\": True}),\r\n     ('Restart', sd_samplers_extra.restart_sampler, ['restart'], {'scheduler': 'karras', \"second_order\": True}),\r\n+    ('LCM Test', sd_samplers_extra.sample_lcm, ['lcm'], {}),\r\n ]\r\n\r\n```\nI have wrapped this patch in extention! \ud83c\udf89 \r\nhttps://github.com/light-and-ray/sd-webui-lcm-sampler\nWe also need Skipping-Step from paper. Without it we still have incomplete implementation.\nWe hope @AUTOMATIC1111 will add this sampler natively and correctly, because I know nothing about defussion theory\nThere's this: https://github.com/0xbitches/sd-webui-lcm\n> There's this: https://github.com/0xbitches/sd-webui-lcm\n\nNooo. It is converted gradio demo of lcm model before the loras were released\n\nIt is separated tab. It's no longer relevant\n> > There's this: https://github.com/0xbitches/sd-webui-lcm\r\n> \r\n> Nooo. It is converted gradio demo of lcm model before the loras were released\r\n> \r\n> It is separated tab. It's no longer relevant\r\n\r\nThanks. So is the light-and-ray thing an actual solution? I'm hoping for a version that will work in Deform.\n> Thanks. So is the light-and-ray thing an actual solution? I'm hoping for a version that will work in Deform.\n\nYes, you're right\nuse my extension to use LCM sampler in exaly the way you describe. https://github.com/continue-revolution/sd-webui-animatediff#lcm\n> But for SDXL it solves all the problems\r\n\r\nThanks for your research! Any ideas why in sdxl other samplers not work?\n> We hope @AUTOMATIC1111 will add this sampler natively and correctly, because I know nothing about defussion theory\r\n\r\nDo @AUTOMATIC1111  have time to support the LCM sampler?\nIf you install the latest update of the Animatediff extension, that installs the LCM sampler for you. No need for this workaround, now.\r\nLink: https://github.com/continue-revolution/sd-webui-animatediff\r\n\n> use my extension to use LCM sampler in exaly the way you describe. https://github.com/continue-revolution/sd-webui-animatediff#lcm\r\n\r\nDude, what a BLESSING! Installing Animatediff just added the LCM sampler as well. Also you can totally increase the step count to 10-15 and get even better quality it seems. Thank you!\n> > use my extension to use LCM sampler in exaly the way you describe. https://github.com/continue-revolution/sd-webui-animatediff#lcm\r\n> \r\n> Dude, what a BLESSING! Installing Animatediff just added the LCM sampler as well. Also you can totally increase the step count to 10-15 and get even better quality it seems. Thank you!\r\n\r\nI am getting an image per second on 3090Ti, with TensorRT (no LCM) it is around 3.5 second per image.\r\nBut the results are so bad, its not worth using, i have tried 8-15 steps, 2 cfg scale, results come up almost identical to each other in all ways, without deep colors, do you get decent results?\n> I am getting an image per second on 3090Ti, with TensorRT (no LCM) it is around 3.5 second per image. But the results are so bad, its not worth using, i have tried 8-15 steps, 2 cfg scale, results come up almost identical to each other in all ways, without deep colors, do you get decent results?\r\n\r\nI'm using an RTX 3060 12GB and I'm getting surprisingly good results. My settings are lora weight of 0.75, LCM sampler, 6-8 steps and CFG of 2.0. Try these settings and see if things improve at all for you.\n> I'm awaiting the integration of the LCM sampler into AUTOMATIC1111, While AUTOMATIC1111 is an excellent program, the implementation of new features, such as the LCM sampler and consistency VAE, appears to be sluggish. The consistency VAE is currently only accessible in beta, while comfyui already offers both consistency VAE and LCM sampler support at no time :(\r\n\r\nAnd with LCM you want TinyVAE.  It turns 70ms 512x512 4 step generations into 45ms on my 4090.\r\nFor some reason the way A1111 is fusing the Lora must be different.  It only make about a 2.5X speed up instead of the 10X I get in a simple diffusers pipeline.\nYou need to understand several things\r\n1. Most likely comfyanonymous is paid by SAI so that he can work full time on ComfyUI, but we A1111 developers are paid nothing.\r\n2. Although A1111 is much more convenient to use compared to ComfyUI, the internal is much harder to modify. If you read my source code of AnimateDiff, you will understand this much deeper. However ComfyUI is made of a bunch of nodes which almost don\u2019t affect each other.\r\n3. A1111 takes cautious step to add new features - most features are not universal so it\u2019s better to come with an extension. This is actually better for us programmers - our extensions will not be abruptly broken.\r\n4. It is not easy to write an elegant implementation to convert diffusers based research into A1111. It is actually easy to write a tab that force you to use diffusers, but I don\u2019t want to do that.\r\n5. Theoretically A1111 should not be slower than anything else. A known reason that A1111 is \u201cslower\u201d than diffusers is that - some samplers require 2 unet forward. In diffusers, step means the number of unet forwards; in A1111, step means the number of sampler forwards. There might be other reasons.\n> You need to understand several things\r\n> \r\n> 1. Most likely comfyanonymous is paid by SAI so that he can work full time on ComfyUI, but we A1111 developers are paid nothing.\r\n> 2. Although A1111 is much more convenient to use compared to ComfyUI, the internal is much harder to modify. If you read my source code of AnimateDiff, you will understand this much deeper. However ComfyUI is made of a bunch of nodes which almost don\u2019t affect each other.\r\n> 3. A1111 takes cautious step to add new features - most features are not universal so it\u2019s better to come with an extension. This is actually better for us programmers - our extensions will not be abruptly broken.\r\n> 4. It is not easy to write an elegant implementation to convert diffusers based research into A1111. It is actually easy to write a tab that force you to use diffusers, but I don\u2019t want to do that.\r\n> 5. Theoretically A1111 should not be slower than anything else. A known reason that A1111 is \u201cslower\u201d than diffusers is that - some samplers require 2 unet forward. In diffusers, step means the number of unet forwards; in A1111, step means the number of sampler forwards. There might be other reasons.\r\n\r\nDo you feel like Automatic1111 is a dead-end in the long run? Should we all be putting more effort into learning ComfyUI?\n> Do you feel like Automatic1111 is a dead-end in the long run? Should we all be putting more effort into learning ComfyUI?\r\n\r\nI've never had a second having any thoughts like this, and I will almost certainly stick to A1111. I believe that creating a great user experience is our (programmer's) mission, and you, users, should focus more on how to use, and teach us how to actually use our software. A lot of people are much better at using my extension than myself. Designing a user-friendly software is never easy in any UI, but I enjoy that.\r\n\r\nA1111 has done his best. A1111 WebUI is easy-to-use, fast and memory efficient. We should not critisize A1111 before having any clear evidence. Dispite the fact that it is tricky to hook some functions, some other functions are designed so good that it can easily fit my need.\r\n\r\nThat said, we programmers do need money. Working for love is never sustainable, unless we are as rich as Mark Zuckerberg. Mark has already been extremely rich through Facebook, so he open-sourced almost everything about ML in Meta. Sam is not as rich as Mark, so OpenAI becomes CloseAI.\nThey have updated the algorithm of sampler\r\nhttps://github.com/huggingface/diffusers/pull/5836/files\n@light-and-ray How do I use this update? I tested lcm and there are some problems in controlnet inpaint, it doesn't recognize red anymore.\n> > > use my extension to use LCM sampler in exaly the way you describe. https://github.com/continue-revolution/sd-webui-animatediff#lcm\r\n> > \r\n> > \r\n> > Dude, what a BLESSING! Installing Animatediff just added the LCM sampler as well. Also you can totally increase the step count to 10-15 and get even better quality it seems. Thank you!\r\n> \r\n> I am getting an image per second on 3090Ti, with TensorRT (no LCM) it is around 3.5 second per image. But the results are so bad, its not worth using, i have tried 8-15 steps, 2 cfg scale, results come up almost identical to each other in all ways, without deep colors, do you get decent results?\r\n\r\nplease can you elaborate how ti install TENSOR RT? I have an RTX 2060, do i need to install a driver from NVIDIA, or just the webui tensor rt extension??????\n@continue-revolution cold you apply this update in your extension?\n\n> They have updated the algorithm of sampler\n> https://github.com/huggingface/diffusers/pull/5836/files\n\n\npost a feature request in my repo and AT the original author of LCM. He will make the decision.\nAfter trying it out, it's really fantastic. For the Sampling method, select Euler a, Euler, or LCM. Set the Sampling steps to 6-8, and choose the lora option as either <lora:lcm-lora-sdv1-5:0.6> or <lora:lcm-lora-sdv1-5:0.7>. For the CFG Scale, select either 1.2 or 1.5.\r\n![xyz_grid-0002-3834625937](https://github.com/AUTOMATIC1111/stable-diffusion-webui/assets/131566897/29cff4f8-5c64-49d4-b867-b5fa8d152718)\r\n![xyz_grid-0003-3834625937](https://github.com/AUTOMATIC1111/stable-diffusion-webui/assets/131566897/72859fe7-a59f-4dd0-9c65-7bd8cdeefc81)\r\n\n> After trying it out, it's really fantastic. For the Sampling method, select Euler a, Euler, or LCM. Set the Sampling steps to 6-8, and choose the lora option as either lora:lcm-lora-sdv1-5:0.6 or lora:lcm-lora-sdv1-5:0.7. For the CFG Scale, select either 1.2 or 1.5. ![xyz_grid-0002-3834625937](https://user-images.githubusercontent.com/131566897/284911371-29cff4f8-5c64-49d4-b867-b5fa8d152718.jpg) ![xyz_grid-0003-3834625937](https://user-images.githubusercontent.com/131566897/284911389-72859fe7-a59f-4dd0-9c65-7bd8cdeefc81.jpg)\r\n\r\nyes but for high quality animation, you can do even 15 steps...... try it with the LCM just like you did..... but ONLY for animatediff animations... not for photos......\n@LIQUIDMIND111 Why do you say NOT for photos? The images aren't perfect, but after an Ultimate Upscale application, the results look really, really nice. \ud83e\udd37\ud83c\udffc\u200d\u2642\ufe0f\nsorry i meant that for photos you dont need as much steps as for\r\nanimations, but i have done photos with LCM at 12 steps and look good also,\r\nbut i meant for photos is enough with 6 steps, but for animatediff i do\r\nanimations with LCM at 14 steps and look WAY BETTER than 8 steps.... so it\r\ngives GOOD result to do MORE steps than what the authors recommend.\r\n\r\nOn Fri, Nov 24, 2023 at 1:23\u202fAM DarthBuckeye ***@***.***>\r\nwrote:\r\n\r\n> @LIQUIDMIND111 <https://github.com/LIQUIDMIND111> Why do you say NOT for\r\n> photos? The images aren't perfect, but after an Ultimate Upscale\r\n> application, the results look really, really nice. \ud83e\udd37\ud83c\udffc\u200d\u2642\ufe0f\r\n>\r\n> \u2014\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/AUTOMATIC1111/stable-diffusion-webui/issues/13952#issuecomment-1825162937>,\r\n> or unsubscribe\r\n> <https://github.com/notifications/unsubscribe-auth/A22MJBASEMLYAXFKM3SDZTLYGAVM5AVCNFSM6AAAAAA7HPKZ5WVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMYTQMRVGE3DEOJTG4>\r\n> .\r\n> You are receiving this because you were mentioned.Message ID:\r\n> ***@***.***>\r\n>\r\n\r\n\r\n-- \r\n\"It was from the Mind of GOD, the Universe was created.\"\r\n- - -- --- -)( ((---IMAGINATION IS CREATION---)) )(- --- -- - -\r\n           - -- -- --- -)( ((---liquid[)(]mind---)) )(- --- -- -- -\r\n\n@continue-revolution \r\nIt seems that LCM in animatediff  is not compatible with regional-prompter. I found in my test that after installing regional-prompter (Did not choose Active), it takes 5 seconds to draw, but after uninstalling regional-prompter, the same parameters only take 1 second to draw.", "created_at": "2024-01-08T12:54:13Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 14276, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-14276", "issue_numbers": ["14263"], "base_commit": "f92d61497a426a19818625c3ccdaae9beeb82b31", "patch": "diff --git a/modules/styles.py b/modules/styles.py\nindex 7fb6c2e1177..81d9800d184 100644\n--- a/modules/styles.py\n+++ b/modules/styles.py\n@@ -98,10 +98,8 @@ def __init__(self, path: str):\n         self.path = path\r\n \r\n         folder, file = os.path.split(self.path)\r\n-        self.default_file = file.split(\"*\")[0] + \".csv\"\r\n-        if self.default_file == \".csv\":\r\n-            self.default_file = \"styles.csv\"\r\n-        self.default_path = os.path.join(folder, self.default_file)\r\n+        filename, _, ext = file.partition('*')\r\n+        self.default_path = os.path.join(folder, filename + ext)\r\n \r\n         self.prompt_fields = [field for field in PromptStyle._fields if field != \"path\"]\r\n \r\n@@ -155,10 +153,8 @@ def load_from_csv(self, path: str):\n                     row[\"name\"], prompt, negative_prompt, path\r\n                 )\r\n \r\n-    def get_style_paths(self) -> list():\r\n-        \"\"\"\r\n-        Returns a list of all distinct paths, including the default path, of\r\n-        files that styles are loaded from.\"\"\"\r\n+    def get_style_paths(self) -> set:\r\n+        \"\"\"Returns a set of all distinct paths of files that styles are loaded from.\"\"\"\r\n         # Update any styles without a path to the default path\r\n         for style in list(self.styles.values()):\r\n             if not style.path:\r\n@@ -172,9 +168,9 @@ def get_style_paths(self) -> list():\n                 style_paths.add(style.path)\r\n \r\n         # Remove any paths for styles that are just list dividers\r\n-        style_paths.remove(\"do_not_save\")\r\n+        style_paths.discard(\"do_not_save\")\r\n \r\n-        return list(style_paths)\r\n+        return style_paths\r\n \r\n     def get_style_prompts(self, styles):\r\n         return [self.styles.get(x, self.no_style).prompt for x in styles]\r\n@@ -196,20 +192,7 @@ def save_styles(self, path: str = None) -> None:\n         # The path argument is deprecated, but kept for backwards compatibility\r\n         _ = path\r\n \r\n-        # Update any styles without a path to the default path\r\n-        for style in list(self.styles.values()):\r\n-            if not style.path:\r\n-                self.styles[style.name] = style._replace(path=self.default_path)\r\n-\r\n-        # Create a list of all distinct paths, including the default path\r\n-        style_paths = set()\r\n-        style_paths.add(self.default_path)\r\n-        for _, style in self.styles.items():\r\n-            if style.path:\r\n-                style_paths.add(style.path)\r\n-\r\n-        # Remove any paths for styles that are just list dividers\r\n-        style_paths.remove(\"do_not_save\")\r\n+        style_paths = self.get_style_paths()\r\n \r\n         csv_names = [os.path.split(path)[1].lower() for path in style_paths]\r\n \r\n", "test_patch": "", "problem_statement": "[Bug]: KeyError: \"do_not_save\" when trying to save a prompt\n### Is there an existing issue for this?\n\n- [X] I have searched the existing issues and checked the recent builds/commits\n\n### What happened?\n\nWhen I try to save a prompt, it errors in the console saying\r\n```\r\n  File \"/home/ciel/stable-diffusion/stable-diffusion-webui/modules/styles.py\", line 212, in save_styles\r\n    style_paths.remove(\"do_not_save\")\r\nKeyError: 'do_not_save'\r\n```\r\nand the file is not modified\r\nI manually commented it out and it doesn't seem to break anything, except that it is saved to styles.csv.csv instead of styles.csv\n\n### Steps to reproduce the problem\n\nTry to save a prompt\r\n\n\n### What should have happened?\n\nSave into style.csv with no error\n\n### Sysinfo\n\n{\r\n    \"Platform\": \"Linux-6.6.4-zen1-1-zen-x86_64-with-glibc2.38\",\r\n    \"Python\": \"3.11.4\",\r\n    \"Version\": \"v1.7.0-RC-5-gf92d6149\",\r\n    \"Commit\": \"f92d61497a426a19818625c3ccdaae9beeb82b31\",\r\n    \"Script path\": \"/home/ciel/stable-diffusion/stable-diffusion-webui\",\r\n    \"Data path\": \"/home/ciel/stable-diffusion/stable-diffusion-webui\",\r\n    \"Extensions dir\": \"/home/ciel/stable-diffusion/stable-diffusion-webui/extensions\",\r\n    \"Checksum\": \"e15aad6adb98a2a0ad13cad2b45b61b03565ef4f258783021da82b4ef7f37fa9\",\r\n    \"Commandline\": [\r\n        \"launch.py\"\r\n    ],\r\n    \"Torch env info\": {\r\n        \"torch_version\": \"2.2.0\",\r\n        \"is_debug_build\": \"False\",\r\n        \"cuda_compiled_version\": \"N/A\",\r\n        \"gcc_version\": \"(GCC) 13.2.1 20230801\",\r\n        \"clang_version\": \"16.0.6\",\r\n        \"cmake_version\": \"version 3.26.4\",\r\n        \"os\": \"Arch Linux (x86_64)\",\r\n        \"libc_version\": \"glibc-2.38\",\r\n        \"python_version\": \"3.11.4 (main, Jul  5 2023, 13:45:01) [GCC 11.2.0] (64-bit runtime)\",\r\n        \"python_platform\": \"Linux-6.6.4-zen1-1-zen-x86_64-with-glibc2.38\",\r\n        \"is_cuda_available\": \"True\",\r\n        \"cuda_runtime_version\": null,\r\n        \"cuda_module_loading\": \"LAZY\",\r\n        \"nvidia_driver_version\": null,\r\n        \"nvidia_gpu_models\": \"AMD Radeon RX 7900 XTX (gfx1100)\",\r\n        \"cudnn_version\": null,\r\n        \"pip_version\": \"pip3\",\r\n        \"pip_packages\": [\r\n            \"numpy==1.23.5\",\r\n            \"open-clip-torch==2.20.0\",\r\n            \"pytorch-lightning==1.9.4\",\r\n            \"pytorch-triton-rocm==2.1.0+dafe145982\",\r\n            \"torch==2.2.0.dev20231208+rocm5.6\",\r\n            \"torchdiffeq==0.2.3\",\r\n            \"torchmetrics==1.2.1\",\r\n            \"torchsde==0.2.6\",\r\n            \"torchvision==0.17.0.dev20231208+rocm5.6\"\r\n        ],\r\n        \"conda_packages\": [\r\n            \"numpy                     1.26.2          py311h24aa872_0  \",\r\n            \"numpy-base                1.26.2          py311hbfb1bba_0  \",\r\n            \"open-clip-torch           2.20.0                   pypi_0    pypi\",\r\n            \"pytorch-lightning         1.9.4                    pypi_0    pypi\",\r\n            \"pytorch-triton-rocm       2.1.0+dafe145982          pypi_0    pypi\",\r\n            \"torch                     2.2.0.dev20231208+rocm5.7          pypi_0    pypi\",\r\n            \"torchaudio                2.2.0.dev20231208+rocm5.7          pypi_0    pypi\",\r\n            \"torchdiffeq               0.2.3                    pypi_0    pypi\",\r\n            \"torchmetrics              1.2.1                    pypi_0    pypi\",\r\n            \"torchsde                  0.2.5                    pypi_0    pypi\",\r\n            \"torchvision               0.17.0.dev20231208+rocm5.7          pypi_0    pypi\"\r\n        ],\r\n        \"hip_compiled_version\": \"5.6.31061-8c743ae5d\",\r\n        \"hip_runtime_version\": \"5.6.31061\",\r\n        \"miopen_runtime_version\": \"2.20.0\",\r\n        \"caching_allocator_config\": \"\",\r\n        \"is_xnnpack_available\": \"True\",\r\n        \"cpu_info\": [\r\n            \"Architecture:                       x86_64\",\r\n            \"CPU op-mode(s):                     32-bit, 64-bit\",\r\n            \"Address sizes:                      48 bits physical, 48 bits virtual\",\r\n            \"Byte Order:                         Little Endian\",\r\n            \"CPU(s):                             32\",\r\n            \"On-line CPU(s) list:                0-31\",\r\n            \"Vendor ID:                          AuthenticAMD\",\r\n            \"Model name:                         AMD Ryzen 9 5950X 16-Core Processor\",\r\n            \"CPU family:                         25\",\r\n            \"Model:                              33\",\r\n            \"Thread(s) per core:                 2\",\r\n            \"Core(s) per socket:                 16\",\r\n            \"Socket(s):                          1\",\r\n            \"Stepping:                           0\",\r\n            \"Frequency boost:                    disabled\",\r\n            \"CPU(s) scaling MHz:                 49%\",\r\n            \"CPU max MHz:                        6279.4922\",\r\n            \"CPU min MHz:                        2200.0000\",\r\n            \"BogoMIPS:                           8383.88\",\r\n            \"Flags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local user_shstk clzero irperf xsaveerptr rdpru wbnoinvd arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip pku ospke vaes vpclmulqdq rdpid overflow_recov succor smca fsrm debug_swap\",\r\n            \"Virtualization:                     AMD-V\",\r\n            \"L1d cache:                          512 KiB (16 instances)\",\r\n            \"L1i cache:                          512 KiB (16 instances)\",\r\n            \"L2 cache:                           8 MiB (16 instances)\",\r\n            \"L3 cache:                           64 MiB (2 instances)\",\r\n            \"NUMA node(s):                       1\",\r\n            \"NUMA node0 CPU(s):                  0-31\",\r\n            \"Vulnerability Gather data sampling: Not affected\",\r\n            \"Vulnerability Itlb multihit:        Not affected\",\r\n            \"Vulnerability L1tf:                 Not affected\",\r\n            \"Vulnerability Mds:                  Not affected\",\r\n            \"Vulnerability Meltdown:             Not affected\",\r\n            \"Vulnerability Mmio stale data:      Not affected\",\r\n            \"Vulnerability Retbleed:             Not affected\",\r\n            \"Vulnerability Spec rstack overflow: Vulnerable: Safe RET, no microcode\",\r\n            \"Vulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl\",\r\n            \"Vulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\",\r\n            \"Vulnerability Spectre v2:           Mitigation; Retpolines, IBPB conditional, IBRS_FW, STIBP always-on, RSB filling, PBRSB-eIBRS Not affected\",\r\n            \"Vulnerability Srbds:                Not affected\",\r\n            \"Vulnerability Tsx async abort:      Not affected\"\r\n        ]\r\n    },\r\n    \"Exceptions\": [],\r\n    \"CPU\": {\r\n        \"model\": \"\",\r\n        \"count logical\": 32,\r\n        \"count physical\": 16\r\n    },\r\n    \"RAM\": {\r\n        \"total\": \"31GB\",\r\n        \"used\": \"6GB\",\r\n        \"free\": \"20GB\",\r\n        \"active\": \"7GB\",\r\n        \"inactive\": \"2GB\",\r\n        \"buffers\": \"172MB\",\r\n        \"cached\": \"5GB\",\r\n        \"shared\": \"199MB\"\r\n    },\r\n    \"Extensions\": [\r\n        {\r\n            \"name\": \"clip-interrogator-ext\",\r\n            \"path\": \"/home/ciel/stable-diffusion/stable-diffusion-webui/extensions/clip-interrogator-ext\",\r\n            \"version\": \"0f1a4591\",\r\n            \"branch\": \"main\",\r\n            \"remote\": \"https://github.com/pharmapsychotic/clip-interrogator-ext.git\"\r\n        },\r\n        {\r\n            \"name\": \"latent-upscale\",\r\n            \"path\": \"/home/ciel/stable-diffusion/stable-diffusion-webui/extensions/latent-upscale\",\r\n            \"version\": \"b9f75f44\",\r\n            \"branch\": \"main\",\r\n            \"remote\": \"https://github.com/feynlee/latent-upscale.git\"\r\n        },\r\n        {\r\n            \"name\": \"sd-webui-controlnet\",\r\n            \"path\": \"/home/ciel/stable-diffusion/stable-diffusion-webui/extensions/sd-webui-controlnet\",\r\n            \"version\": \"feea1f65\",\r\n            \"branch\": \"main\",\r\n            \"remote\": \"https://github.com/Mikubill/sd-webui-controlnet.git\"\r\n        },\r\n        {\r\n            \"name\": \"ultimate-upscale-for-automatic1111\",\r\n            \"path\": \"/home/ciel/stable-diffusion/stable-diffusion-webui/extensions/ultimate-upscale-for-automatic1111\",\r\n            \"version\": \"728ffcec\",\r\n            \"branch\": \"master\",\r\n            \"remote\": \"https://github.com/Coyote-A/ultimate-upscale-for-automatic1111.git\"\r\n        }\r\n    ],\r\n    \"Inactive extensions\": [],\r\n    \"Environment\": {\r\n        \"GIT\": \"git\",\r\n        \"GRADIO_ANALYTICS_ENABLED\": \"False\",\r\n        \"TORCH_COMMAND\": \"pip install --pre torch torchvision --index-url https://download.pytorch.org/whl/nightly/rocm5.6\"\r\n    },\r\n    \"Config\": {\r\n        \"samples_save\": true,\r\n        \"samples_format\": \"png\",\r\n        \"samples_filename_pattern\": \"\",\r\n        \"save_images_add_number\": true,\r\n        \"save_images_replace_action\": \"Replace\",\r\n        \"grid_save\": true,\r\n        \"grid_format\": \"png\",\r\n        \"grid_extended_filename\": false,\r\n        \"grid_only_if_multiple\": true,\r\n        \"grid_prevent_empty_spots\": false,\r\n        \"grid_zip_filename_pattern\": \"\",\r\n        \"n_rows\": -1,\r\n        \"font\": \"\",\r\n        \"grid_text_active_color\": \"#000000\",\r\n        \"grid_text_inactive_color\": \"#999999\",\r\n        \"grid_background_color\": \"#ffffff\",\r\n        \"save_images_before_face_restoration\": false,\r\n        \"save_images_before_highres_fix\": false,\r\n        \"save_images_before_color_correction\": false,\r\n        \"save_mask\": false,\r\n        \"save_mask_composite\": false,\r\n        \"jpeg_quality\": 80,\r\n        \"webp_lossless\": false,\r\n        \"export_for_4chan\": true,\r\n        \"img_downscale_threshold\": 4.0,\r\n        \"target_side_length\": 4000,\r\n        \"img_max_size_mp\": 200,\r\n        \"use_original_name_batch\": true,\r\n        \"use_upscaler_name_as_suffix\": false,\r\n        \"save_selected_only\": true,\r\n        \"save_init_img\": false,\r\n        \"temp_dir\": \"\",\r\n        \"clean_temp_dir_at_start\": false,\r\n        \"save_incomplete_images\": false,\r\n        \"notification_audio\": true,\r\n        \"notification_volume\": 100,\r\n        \"outdir_samples\": \"\",\r\n        \"outdir_txt2img_samples\": \"outputs/txt2img-images\",\r\n        \"outdir_img2img_samples\": \"outputs/img2img-images\",\r\n        \"outdir_extras_samples\": \"outputs/extras-images\",\r\n        \"outdir_grids\": \"\",\r\n        \"outdir_txt2img_grids\": \"outputs/txt2img-grids\",\r\n        \"outdir_img2img_grids\": \"outputs/img2img-grids\",\r\n        \"outdir_save\": \"log/images\",\r\n        \"outdir_init_images\": \"outputs/init-images\",\r\n        \"save_to_dirs\": true,\r\n        \"grid_save_to_dirs\": true,\r\n        \"use_save_to_dirs_for_ui\": false,\r\n        \"directories_filename_pattern\": \"[date]\",\r\n        \"directories_max_prompt_words\": 8,\r\n        \"ESRGAN_tile\": 192,\r\n        \"ESRGAN_tile_overlap\": 8,\r\n        \"realesrgan_enabled_models\": [\r\n            \"R-ESRGAN 4x+\",\r\n            \"R-ESRGAN 4x+ Anime6B\"\r\n        ],\r\n        \"upscaler_for_img2img\": null,\r\n        \"face_restoration\": false,\r\n        \"face_restoration_model\": \"CodeFormer\",\r\n        \"code_former_weight\": 0.5,\r\n        \"face_restoration_unload\": false,\r\n        \"auto_launch_browser\": \"Local\",\r\n        \"enable_console_prompts\": false,\r\n        \"show_warnings\": false,\r\n        \"show_gradio_deprecation_warnings\": true,\r\n        \"memmon_poll_rate\": 8,\r\n        \"samples_log_stdout\": false,\r\n        \"multiple_tqdm\": true,\r\n        \"print_hypernet_extra\": false,\r\n        \"list_hidden_files\": true,\r\n        \"disable_mmap_load_safetensors\": false,\r\n        \"hide_ldm_prints\": true,\r\n        \"dump_stacks_on_signal\": false,\r\n        \"api_enable_requests\": true,\r\n        \"api_forbid_local_requests\": true,\r\n        \"api_useragent\": \"\",\r\n        \"unload_models_when_training\": false,\r\n        \"pin_memory\": false,\r\n        \"save_optimizer_state\": false,\r\n        \"save_training_settings_to_txt\": true,\r\n        \"dataset_filename_word_regex\": \"\",\r\n        \"dataset_filename_join_string\": \" \",\r\n        \"training_image_repeats_per_epoch\": 1,\r\n        \"training_write_csv_every\": 500,\r\n        \"training_xattention_optimizations\": false,\r\n        \"training_enable_tensorboard\": false,\r\n        \"training_tensorboard_save_images\": false,\r\n        \"training_tensorboard_flush_every\": 120,\r\n        \"sd_model_checkpoint\": \"AOM3A1B_orangemixs.safetensors [5493a0ec49]\",\r\n        \"sd_checkpoints_limit\": 1,\r\n        \"sd_checkpoints_keep_in_cpu\": true,\r\n        \"sd_checkpoint_cache\": 0,\r\n        \"sd_unet\": \"Automatic\",\r\n        \"enable_quantization\": false,\r\n        \"enable_emphasis\": true,\r\n        \"enable_batch_seeds\": true,\r\n        \"comma_padding_backtrack\": 20,\r\n        \"CLIP_stop_at_last_layers\": 1,\r\n        \"upcast_attn\": true,\r\n        \"randn_source\": \"GPU\",\r\n        \"tiling\": false,\r\n        \"hires_fix_refiner_pass\": \"second pass\",\r\n        \"sdxl_crop_top\": 0,\r\n        \"sdxl_crop_left\": 0,\r\n        \"sdxl_refiner_low_aesthetic_score\": 2.5,\r\n        \"sdxl_refiner_high_aesthetic_score\": 6.0,\r\n        \"sd_vae_checkpoint_cache\": 1,\r\n        \"sd_vae\": \"orangemix.vae.pt\",\r\n        \"sd_vae_overrides_per_model_preferences\": true,\r\n        \"auto_vae_precision\": true,\r\n        \"sd_vae_encode_method\": \"Full\",\r\n        \"sd_vae_decode_method\": \"Full\",\r\n        \"inpainting_mask_weight\": 1.0,\r\n        \"initial_noise_multiplier\": 1.0,\r\n        \"img2img_extra_noise\": 0.0,\r\n        \"img2img_color_correction\": false,\r\n        \"img2img_fix_steps\": false,\r\n        \"img2img_background_color\": \"#ffffff\",\r\n        \"img2img_editor_height\": 720,\r\n        \"img2img_sketch_default_brush_color\": \"#ffffff\",\r\n        \"img2img_inpaint_mask_brush_color\": \"#ffffff\",\r\n        \"img2img_inpaint_sketch_default_brush_color\": \"#ffffff\",\r\n        \"return_mask\": false,\r\n        \"return_mask_composite\": false,\r\n        \"img2img_batch_show_results_limit\": 32,\r\n        \"cross_attention_optimization\": \"Automatic\",\r\n        \"s_min_uncond\": 0.0,\r\n        \"token_merging_ratio\": 0.0,\r\n        \"token_merging_ratio_img2img\": 0.0,\r\n        \"token_merging_ratio_hr\": 0.0,\r\n        \"pad_cond_uncond\": false,\r\n        \"persistent_cond_cache\": true,\r\n        \"batch_cond_uncond\": true,\r\n        \"use_old_emphasis_implementation\": false,\r\n        \"use_old_karras_scheduler_sigmas\": false,\r\n        \"no_dpmpp_sde_batch_determinism\": false,\r\n        \"use_old_hires_fix_width_height\": false,\r\n        \"dont_fix_second_order_samplers_schedule\": false,\r\n        \"hires_fix_use_firstpass_conds\": false,\r\n        \"use_old_scheduling\": false,\r\n        \"interrogate_keep_models_in_memory\": false,\r\n        \"interrogate_return_ranks\": false,\r\n        \"interrogate_clip_num_beams\": 1,\r\n        \"interrogate_clip_min_length\": 24,\r\n        \"interrogate_clip_max_length\": 48,\r\n        \"interrogate_clip_dict_limit\": 1500,\r\n        \"interrogate_clip_skip_categories\": [],\r\n        \"interrogate_deepbooru_score_threshold\": 0.5,\r\n        \"deepbooru_sort_alpha\": true,\r\n        \"deepbooru_use_spaces\": true,\r\n        \"deepbooru_escape\": true,\r\n        \"deepbooru_filter_tags\": \"\",\r\n        \"extra_networks_show_hidden_directories\": true,\r\n        \"extra_networks_dir_button_function\": false,\r\n        \"extra_networks_hidden_models\": \"When searched\",\r\n        \"extra_networks_default_multiplier\": 1.0,\r\n        \"extra_networks_card_width\": 0,\r\n        \"extra_networks_card_height\": 0,\r\n        \"extra_networks_card_text_scale\": 1.0,\r\n        \"extra_networks_card_show_desc\": true,\r\n        \"extra_networks_card_order_field\": \"Path\",\r\n        \"extra_networks_card_order\": \"Ascending\",\r\n        \"extra_networks_add_text_separator\": \" \",\r\n        \"ui_extra_networks_tab_reorder\": \"\",\r\n        \"textual_inversion_print_at_load\": false,\r\n        \"textual_inversion_add_hashes_to_infotext\": true,\r\n        \"sd_hypernetwork\": \"None\",\r\n        \"keyedit_precision_attention\": 0.1,\r\n        \"keyedit_precision_extra\": 0.05,\r\n        \"keyedit_delimiters\": \".,\\\\/!?%^*;:{}=`~() \",\r\n        \"keyedit_delimiters_whitespace\": [\r\n            \"Tab\",\r\n            \"Carriage Return\",\r\n            \"Line Feed\"\r\n        ],\r\n        \"disable_token_counters\": false,\r\n        \"return_grid\": true,\r\n        \"do_not_show_images\": false,\r\n        \"js_modal_lightbox\": true,\r\n        \"js_modal_lightbox_initially_zoomed\": true,\r\n        \"js_modal_lightbox_gamepad\": false,\r\n        \"js_modal_lightbox_gamepad_repeat\": 250,\r\n        \"gallery_height\": \"\",\r\n        \"compact_prompt_box\": false,\r\n        \"samplers_in_dropdown\": true,\r\n        \"dimensions_and_batch_together\": true,\r\n        \"sd_checkpoint_dropdown_use_short\": false,\r\n        \"hires_fix_show_sampler\": false,\r\n        \"hires_fix_show_prompts\": false,\r\n        \"txt2img_settings_accordion\": false,\r\n        \"img2img_settings_accordion\": false,\r\n        \"localization\": \"None\",\r\n        \"quicksettings_list\": [\r\n            \"sd_model_checkpoint\"\r\n        ],\r\n        \"ui_tab_order\": [],\r\n        \"hidden_tabs\": [],\r\n        \"ui_reorder_list\": [],\r\n        \"gradio_theme\": \"Default\",\r\n        \"gradio_themes_cache\": true,\r\n        \"show_progress_in_title\": true,\r\n        \"send_seed\": true,\r\n        \"send_size\": true,\r\n        \"enable_pnginfo\": true,\r\n        \"save_txt\": false,\r\n        \"add_model_name_to_info\": true,\r\n        \"add_model_hash_to_info\": true,\r\n        \"add_vae_name_to_info\": true,\r\n        \"add_vae_hash_to_info\": true,\r\n        \"add_user_name_to_info\": false,\r\n        \"add_version_to_infotext\": true,\r\n        \"disable_weights_auto_swap\": true,\r\n        \"infotext_skip_pasting\": [],\r\n        \"infotext_styles\": \"Apply if any\",\r\n        \"show_progressbar\": true,\r\n        \"live_previews_enable\": false,\r\n        \"live_previews_image_format\": \"png\",\r\n        \"show_progress_grid\": true,\r\n        \"show_progress_every_n_steps\": 5,\r\n        \"show_progress_type\": \"Approx NN\",\r\n        \"live_preview_allow_lowvram_full\": false,\r\n        \"live_preview_content\": \"Prompt\",\r\n        \"live_preview_refresh_period\": 300.0,\r\n        \"live_preview_fast_interrupt\": false,\r\n        \"hide_samplers\": [],\r\n        \"eta_ddim\": 0.0,\r\n        \"eta_ancestral\": 1.0,\r\n        \"ddim_discretize\": \"uniform\",\r\n        \"s_churn\": 0.0,\r\n        \"s_tmin\": 0.0,\r\n        \"s_tmax\": 0.0,\r\n        \"s_noise\": 1.0,\r\n        \"k_sched_type\": \"Automatic\",\r\n        \"sigma_min\": 0.0,\r\n        \"sigma_max\": 0.0,\r\n        \"rho\": 0.0,\r\n        \"eta_noise_seed_delta\": 0,\r\n        \"always_discard_next_to_last_sigma\": false,\r\n        \"sgm_noise_multiplier\": false,\r\n        \"uni_pc_variant\": \"bh1\",\r\n        \"uni_pc_skip_type\": \"time_uniform\",\r\n        \"uni_pc_order\": 3,\r\n        \"uni_pc_lower_order_final\": true,\r\n        \"postprocessing_enable_in_main_ui\": [],\r\n        \"postprocessing_operation_order\": [],\r\n        \"upscaling_max_images_in_cache\": 5,\r\n        \"postprocessing_existing_caption_action\": \"Ignore\",\r\n        \"disabled_extensions\": [],\r\n        \"disable_all_extensions\": \"none\",\r\n        \"restore_config_state_file\": \"\",\r\n        \"sd_checkpoint_hash\": \"5493a0ec491f5961dbdc1c861404088a6ae9bd4007f6a3a7c5dee8789cdc1361\",\r\n        \"ldsr_steps\": 100,\r\n        \"ldsr_cached\": false,\r\n        \"SCUNET_tile\": 256,\r\n        \"SCUNET_tile_overlap\": 8,\r\n        \"SWIN_tile\": 192,\r\n        \"SWIN_tile_overlap\": 8,\r\n        \"SWIN_torch_compile\": false,\r\n        \"hypertile_enable_unet\": false,\r\n        \"hypertile_enable_unet_secondpass\": false,\r\n        \"hypertile_max_depth_unet\": 3,\r\n        \"hypertile_max_tile_unet\": 256,\r\n        \"hypertile_swap_size_unet\": 3,\r\n        \"hypertile_enable_vae\": false,\r\n        \"hypertile_max_depth_vae\": 3,\r\n        \"hypertile_max_tile_vae\": 128,\r\n        \"hypertile_swap_size_vae\": 3,\r\n        \"control_net_detectedmap_dir\": \"detected_maps\",\r\n        \"control_net_models_path\": \"\",\r\n        \"control_net_modules_path\": \"\",\r\n        \"control_net_unit_count\": 3,\r\n        \"control_net_model_cache_size\": 1,\r\n        \"control_net_inpaint_blur_sigma\": 7,\r\n        \"control_net_no_high_res_fix\": false,\r\n        \"control_net_no_detectmap\": false,\r\n        \"control_net_detectmap_autosaving\": false,\r\n        \"control_net_allow_script_control\": false,\r\n        \"control_net_sync_field_args\": true,\r\n        \"controlnet_show_batch_images_in_ui\": false,\r\n        \"controlnet_increment_seed_during_batch\": false,\r\n        \"controlnet_disable_openpose_edit\": false,\r\n        \"controlnet_ignore_noninpaint_mask\": false,\r\n        \"lora_functional\": false,\r\n        \"sd_lora\": \"None\",\r\n        \"lora_preferred_name\": \"Alias from file\",\r\n        \"lora_add_hashes_to_infotext\": true,\r\n        \"lora_show_all\": false,\r\n        \"lora_hide_unknown_for_versions\": [],\r\n        \"lora_in_memory_limit\": 0,\r\n        \"extra_options_txt2img\": [],\r\n        \"extra_options_img2img\": [],\r\n        \"extra_options_cols\": 1,\r\n        \"extra_options_accordion\": false,\r\n        \"canvas_hotkey_zoom\": \"Alt\",\r\n        \"canvas_hotkey_adjust\": \"Ctrl\",\r\n        \"canvas_hotkey_move\": \"F\",\r\n        \"canvas_hotkey_fullscreen\": \"S\",\r\n        \"canvas_hotkey_reset\": \"R\",\r\n        \"canvas_hotkey_overlap\": \"O\",\r\n        \"canvas_show_tooltip\": true,\r\n        \"canvas_auto_expand\": true,\r\n        \"canvas_blur_prompt\": false,\r\n        \"canvas_disabled_functions\": [\r\n            \"Overlap\"\r\n        ]\r\n    },\r\n    \"Startup\": {\r\n        \"total\": 11.257086753845215,\r\n        \"records\": {\r\n            \"initial startup\": 0.02352619171142578,\r\n            \"prepare environment/checks\": 3.457069396972656e-05,\r\n            \"prepare environment/git version info\": 0.009780406951904297,\r\n            \"prepare environment/torch GPU test\": 2.7273693084716797,\r\n            \"prepare environment/clone repositores\": 0.038356781005859375,\r\n            \"prepare environment/run extensions installers/sd-webui-controlnet\": 0.14071893692016602,\r\n            \"prepare environment/run extensions installers/ultimate-upscale-for-automatic1111\": 2.288818359375e-05,\r\n            \"prepare environment/run extensions installers/clip-interrogator-ext\": 2.8869497776031494,\r\n            \"prepare environment/run extensions installers/latent-upscale\": 5.626678466796875e-05,\r\n            \"prepare environment/run extensions installers\": 3.0277533531188965,\r\n            \"prepare environment\": 5.820652484893799,\r\n            \"launcher\": 0.0008344650268554688,\r\n            \"import torch\": 2.0337331295013428,\r\n            \"import gradio\": 0.6256029605865479,\r\n            \"setup paths\": 0.9430902004241943,\r\n            \"import ldm\": 0.0025310516357421875,\r\n            \"import sgm\": 2.384185791015625e-06,\r\n            \"initialize shared\": 0.047745466232299805,\r\n            \"other imports\": 0.5719733238220215,\r\n            \"opts onchange\": 0.0002732276916503906,\r\n            \"setup SD model\": 0.0003185272216796875,\r\n            \"setup codeformer\": 0.07199668884277344,\r\n            \"setup gfpgan\": 0.009232521057128906,\r\n            \"set samplers\": 2.8371810913085938e-05,\r\n            \"list extensions\": 0.0010488033294677734,\r\n            \"restore config state file\": 5.4836273193359375e-06,\r\n            \"list SD models\": 0.004712820053100586,\r\n            \"list localizations\": 0.0001246929168701172,\r\n            \"load scripts/custom_code.py\": 0.001154184341430664,\r\n            \"load scripts/img2imgalt.py\": 0.0002789497375488281,\r\n            \"load scripts/loopback.py\": 0.0001888275146484375,\r\n            \"load scripts/outpainting_mk_2.py\": 0.0002484321594238281,\r\n            \"load scripts/poor_mans_outpainting.py\": 0.0001766681671142578,\r\n            \"load scripts/postprocessing_caption.py\": 0.0001506805419921875,\r\n            \"load scripts/postprocessing_codeformer.py\": 0.00015020370483398438,\r\n            \"load scripts/postprocessing_create_flipped_copies.py\": 0.00014519691467285156,\r\n            \"load scripts/postprocessing_focal_crop.py\": 0.00043463706970214844,\r\n            \"load scripts/postprocessing_gfpgan.py\": 0.00014495849609375,\r\n            \"load scripts/postprocessing_split_oversized.py\": 0.00015592575073242188,\r\n            \"load scripts/postprocessing_upscale.py\": 0.00021982192993164062,\r\n            \"load scripts/processing_autosized_crop.py\": 0.0001621246337890625,\r\n            \"load scripts/prompt_matrix.py\": 0.0001780986785888672,\r\n            \"load scripts/prompts_from_file.py\": 0.0001876354217529297,\r\n            \"load scripts/sd_upscale.py\": 0.00016450881958007812,\r\n            \"load scripts/xyz_grid.py\": 0.0010995864868164062,\r\n            \"load scripts/ldsr_model.py\": 0.11085081100463867,\r\n            \"load scripts/lora_script.py\": 0.05980086326599121,\r\n            \"load scripts/scunet_model.py\": 0.011086463928222656,\r\n            \"load scripts/swinir_model.py\": 0.010489225387573242,\r\n            \"load scripts/hotkey_config.py\": 0.0001678466796875,\r\n            \"load scripts/extra_options_section.py\": 0.00020551681518554688,\r\n            \"load scripts/hypertile_script.py\": 0.019654512405395508,\r\n            \"load scripts/hypertile_xyz.py\": 8.058547973632812e-05,\r\n            \"load scripts/clip_interrogator_ext.py\": 0.02592325210571289,\r\n            \"load scripts/latent_upscale.py\": 0.0007441043853759766,\r\n            \"load scripts/adapter.py\": 0.0003275871276855469,\r\n            \"load scripts/api.py\": 0.12074923515319824,\r\n            \"load scripts/batch_hijack.py\": 0.0005114078521728516,\r\n            \"load scripts/cldm.py\": 0.00022983551025390625,\r\n            \"load scripts/controlmodel_ipadapter.py\": 0.00032711029052734375,\r\n            \"load scripts/controlnet.py\": 0.0494229793548584,\r\n            \"load scripts/controlnet_diffusers.py\": 0.0001556873321533203,\r\n            \"load scripts/controlnet_lllite.py\": 0.0001430511474609375,\r\n            \"load scripts/controlnet_lora.py\": 0.00012731552124023438,\r\n            \"load scripts/controlnet_model_guess.py\": 0.00011944770812988281,\r\n            \"load scripts/controlnet_version.py\": 0.0001239776611328125,\r\n            \"load scripts/enums.py\": 0.0003447532653808594,\r\n            \"load scripts/external_code.py\": 6.246566772460938e-05,\r\n            \"load scripts/global_state.py\": 0.0003178119659423828,\r\n            \"load scripts/hook.py\": 0.0002903938293457031,\r\n            \"load scripts/infotext.py\": 9.560585021972656e-05,\r\n            \"load scripts/logging.py\": 0.00016260147094726562,\r\n            \"load scripts/lvminthin.py\": 0.0001952648162841797,\r\n            \"load scripts/movie2movie.py\": 0.00022029876708984375,\r\n            \"load scripts/processor.py\": 0.00023818016052246094,\r\n            \"load scripts/utils.py\": 0.00011324882507324219,\r\n            \"load scripts/xyz_grid_support.py\": 0.0003902912139892578,\r\n            \"load scripts/ultimate-upscale.py\": 0.00045228004455566406,\r\n            \"load scripts/refiner.py\": 0.00011444091796875,\r\n            \"load scripts/seed.py\": 0.00012302398681640625,\r\n            \"load scripts\": 0.41962695121765137,\r\n            \"load upscalers\": 0.001577138900756836,\r\n            \"refresh VAE\": 0.0006160736083984375,\r\n            \"refresh textual inversion templates\": 2.86102294921875e-05,\r\n            \"scripts list_optimizers\": 0.00027680397033691406,\r\n            \"scripts list_unets\": 4.76837158203125e-06,\r\n            \"reload hypernetworks\": 0.0027685165405273438,\r\n            \"initialize extra networks\": 0.004837512969970703,\r\n            \"scripts before_ui_callback\": 0.00041604042053222656,\r\n            \"create ui\": 0.4426920413970947,\r\n            \"gradio launch\": 0.23865938186645508,\r\n            \"add APIs\": 0.003912210464477539,\r\n            \"app_started_callback/lora_script.py\": 0.0001537799835205078,\r\n            \"app_started_callback/clip_interrogator_ext.py\": 0.0003566741943359375,\r\n            \"app_started_callback/api.py\": 0.0010819435119628906,\r\n            \"app_started_callback\": 0.001596689224243164\r\n        }\r\n    },\r\n    \"Packages\": [\r\n        \"absl-py==2.0.0\",\r\n        \"accelerate==0.21.0\",\r\n        \"addict==2.4.0\",\r\n        \"aenum==3.1.15\",\r\n        \"aiofiles==23.2.1\",\r\n        \"aiohttp==3.9.1\",\r\n        \"aiosignal==1.3.1\",\r\n        \"altair==5.2.0\",\r\n        \"antlr4-python3-runtime==4.9.3\",\r\n        \"anyio==3.7.1\",\r\n        \"attrs==23.1.0\",\r\n        \"basicsr==1.4.2\",\r\n        \"beautifulsoup4==4.12.2\",\r\n        \"blendmodes==2022\",\r\n        \"boltons==23.1.1\",\r\n        \"cachetools==5.3.2\",\r\n        \"certifi==2022.12.7\",\r\n        \"cffi==1.16.0\",\r\n        \"charset-normalizer==2.1.1\",\r\n        \"clean-fid==0.1.35\",\r\n        \"click==8.1.7\",\r\n        \"clip-interrogator==0.6.0\",\r\n        \"clip==1.0\",\r\n        \"contourpy==1.2.0\",\r\n        \"cssselect2==0.7.0\",\r\n        \"cycler==0.12.1\",\r\n        \"deprecation==2.1.0\",\r\n        \"einops==0.4.1\",\r\n        \"facexlib==0.3.0\",\r\n        \"fastapi==0.94.0\",\r\n        \"ffmpy==0.3.1\",\r\n        \"filelock==3.9.0\",\r\n        \"filterpy==1.4.5\",\r\n        \"flatbuffers==23.5.26\",\r\n        \"fonttools==4.46.0\",\r\n        \"frozenlist==1.4.0\",\r\n        \"fsspec==2023.12.1\",\r\n        \"ftfy==6.1.3\",\r\n        \"future==0.18.3\",\r\n        \"fvcore==0.1.5.post20221221\",\r\n        \"gdown==4.7.1\",\r\n        \"gfpgan==1.3.8\",\r\n        \"gitdb==4.0.11\",\r\n        \"gitpython==3.1.32\",\r\n        \"google-auth-oauthlib==1.1.0\",\r\n        \"google-auth==2.25.1\",\r\n        \"gradio-client==0.5.0\",\r\n        \"gradio==3.41.2\",\r\n        \"grpcio==1.60.0\",\r\n        \"h11==0.12.0\",\r\n        \"httpcore==0.15.0\",\r\n        \"httpx==0.24.1\",\r\n        \"huggingface-hub==0.19.4\",\r\n        \"idna==3.4\",\r\n        \"imageio==2.33.0\",\r\n        \"importlib-metadata==7.0.0\",\r\n        \"importlib-resources==6.1.1\",\r\n        \"inflection==0.5.1\",\r\n        \"iopath==0.1.9\",\r\n        \"jinja2==3.1.2\",\r\n        \"jsonmerge==1.8.0\",\r\n        \"jsonschema-specifications==2023.11.2\",\r\n        \"jsonschema==4.20.0\",\r\n        \"kiwisolver==1.4.5\",\r\n        \"kornia==0.6.7\",\r\n        \"lark==1.1.2\",\r\n        \"lazy-loader==0.3\",\r\n        \"lightning-utilities==0.10.0\",\r\n        \"llvmlite==0.41.1\",\r\n        \"lmdb==1.4.1\",\r\n        \"lpips==0.1.4\",\r\n        \"lxml==4.9.3\",\r\n        \"markdown==3.5.1\",\r\n        \"markupsafe==2.1.3\",\r\n        \"matplotlib==3.8.2\",\r\n        \"mediapipe==0.10.8\",\r\n        \"mpmath==1.2.1\",\r\n        \"multidict==6.0.4\",\r\n        \"networkx==3.0rc1\",\r\n        \"numba==0.58.1\",\r\n        \"numpy==1.23.5\",\r\n        \"oauthlib==3.2.2\",\r\n        \"omegaconf==2.2.3\",\r\n        \"open-clip-torch==2.20.0\",\r\n        \"opencv-contrib-python==4.8.1.78\",\r\n        \"opencv-python==4.8.1.78\",\r\n        \"orjson==3.9.10\",\r\n        \"packaging==23.2\",\r\n        \"pandas==2.1.4\",\r\n        \"piexif==1.1.3\",\r\n        \"pillow==9.5.0\",\r\n        \"pip==23.1.2\",\r\n        \"platformdirs==4.1.0\",\r\n        \"portalocker==2.8.2\",\r\n        \"protobuf==3.20.0\",\r\n        \"psutil==5.9.5\",\r\n        \"pyasn1-modules==0.3.0\",\r\n        \"pyasn1==0.5.1\",\r\n        \"pycparser==2.21\",\r\n        \"pydantic==1.10.13\",\r\n        \"pydub==0.25.1\",\r\n        \"pyparsing==3.1.1\",\r\n        \"pysocks==1.7.1\",\r\n        \"python-dateutil==2.8.2\",\r\n        \"python-multipart==0.0.6\",\r\n        \"pytorch-lightning==1.9.4\",\r\n        \"pytorch-triton-rocm==2.1.0+dafe145982\",\r\n        \"pytz==2023.3.post1\",\r\n        \"pywavelets==1.5.0\",\r\n        \"pyyaml==6.0.1\",\r\n        \"realesrgan==0.3.0\",\r\n        \"referencing==0.32.0\",\r\n        \"regex==2023.10.3\",\r\n        \"reportlab==4.0.7\",\r\n        \"requests-oauthlib==1.3.1\",\r\n        \"requests==2.28.1\",\r\n        \"resize-right==0.0.2\",\r\n        \"rpds-py==0.13.2\",\r\n        \"rsa==4.9\",\r\n        \"safetensors==0.3.1\",\r\n        \"scikit-image==0.21.0\",\r\n        \"scipy==1.11.4\",\r\n        \"semantic-version==2.10.0\",\r\n        \"sentencepiece==0.1.99\",\r\n        \"setuptools==65.5.0\",\r\n        \"six==1.16.0\",\r\n        \"smmap==5.0.1\",\r\n        \"sniffio==1.3.0\",\r\n        \"sounddevice==0.4.6\",\r\n        \"soupsieve==2.5\",\r\n        \"starlette==0.26.1\",\r\n        \"svglib==1.5.1\",\r\n        \"sympy==1.11.1\",\r\n        \"tabulate==0.9.0\",\r\n        \"tb-nightly==2.16.0a20231208\",\r\n        \"tensorboard-data-server==0.7.2\",\r\n        \"termcolor==2.4.0\",\r\n        \"tf-keras-nightly==2.16.0.dev2023120810\",\r\n        \"tifffile==2023.9.26\",\r\n        \"timm==0.9.2\",\r\n        \"tinycss2==1.2.1\",\r\n        \"tokenizers==0.13.3\",\r\n        \"tomesd==0.1.3\",\r\n        \"tomli==2.0.1\",\r\n        \"toolz==0.12.0\",\r\n        \"torch==2.2.0.dev20231208+rocm5.6\",\r\n        \"torchdiffeq==0.2.3\",\r\n        \"torchmetrics==1.2.1\",\r\n        \"torchsde==0.2.6\",\r\n        \"torchvision==0.17.0.dev20231208+rocm5.6\",\r\n        \"tqdm==4.66.1\",\r\n        \"trampoline==0.1.2\",\r\n        \"transformers==4.30.2\",\r\n        \"typing-extensions==4.8.0\",\r\n        \"tzdata==2023.3\",\r\n        \"urllib3==1.26.13\",\r\n        \"uvicorn==0.24.0.post1\",\r\n        \"wcwidth==0.2.12\",\r\n        \"webencodings==0.5.1\",\r\n        \"websockets==11.0.3\",\r\n        \"werkzeug==3.0.1\",\r\n        \"yacs==0.1.8\",\r\n        \"yapf==0.40.2\",\r\n        \"yarl==1.9.4\",\r\n        \"zipp==3.17.0\"\r\n    ]\r\n}\n\n### What browsers do you use to access the UI ?\n\nMozilla Firefox\n\n### Console logs\n\n```Shell\n\u276f ./webui.sh                                                         (base) \r\n\r\n################################################################\r\nInstall script for stable-diffusion + Web UI\r\nTested on Debian 11 (Bullseye)\r\n################################################################\r\n\r\n################################################################\r\nRunning on ciel user\r\n################################################################\r\n\r\n################################################################\r\nCreate and activate python venv\r\n################################################################\r\n\r\n################################################################\r\nLaunching launch.py...\r\n################################################################\r\nUsing TCMalloc: libtcmalloc_minimal.so.4\r\nPython 3.11.4 (main, Jul  5 2023, 13:45:01) [GCC 11.2.0]\r\nVersion: v1.7.0-RC-5-gf92d6149\r\nCommit hash: f92d61497a426a19818625c3ccdaae9beeb82b31\r\nLaunching Web UI with arguments: \r\nno module 'xformers'. Processing without...\r\nno module 'xformers'. Processing without...\r\nNo module 'xformers'. Proceeding without it.\r\n2023-12-09 17:08:09,876 - ControlNet - INFO - ControlNet v1.1.422\r\nControlNet preprocessor location: /home/ciel/stable-diffusion/stable-diffusion-webui/extensions/sd-webui-controlnet/annotator/downloads\r\n2023-12-09 17:08:09,921 - ControlNet - INFO - ControlNet v1.1.422\r\nLoading weights [5493a0ec49] from /home/ciel/stable-diffusion/stable-diffusion-webui/models/Stable-diffusion/AOM3A1B_orangemixs.safetensors\r\nRunning on local URL:  http://127.0.0.1:7860\r\n\r\nTo create a public link, set `share=True` in `launch()`.\r\nCreating model from config: /home/ciel/stable-diffusion/stable-diffusion-webui/configs/v1-inference.yaml\r\nStartup time: 8.9s (prepare environment: 4.0s, import torch: 2.0s, import gradio: 0.5s, setup paths: 0.8s, other imports: 0.5s, load scripts: 0.4s, create ui: 0.4s, gradio launch: 0.2s).\r\nLoading VAE weights specified in settings: /home/ciel/stable-diffusion/stable-diffusion-webui/models/VAE/orangemix.vae.pt\r\nApplying attention optimization: Doggettx... done.\r\nModel loaded in 2.6s (load weights from disk: 0.6s, create model: 0.2s, apply weights to model: 1.4s, load VAE: 0.2s, calculate empty prompt: 0.1s).\r\nTraceback (most recent call last):\r\n  File \"/home/ciel/stable-diffusion/stable-diffusion-webui/venv/lib/python3.11/site-packages/gradio/routes.py\", line 488, in run_predict\r\n    output = await app.get_blocks().process_api(\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ciel/stable-diffusion/stable-diffusion-webui/venv/lib/python3.11/site-packages/gradio/blocks.py\", line 1431, in process_api\r\n    result = await self.call_function(\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ciel/stable-diffusion/stable-diffusion-webui/venv/lib/python3.11/site-packages/gradio/blocks.py\", line 1103, in call_function\r\n    prediction = await anyio.to_thread.run_sync(\r\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ciel/stable-diffusion/stable-diffusion-webui/venv/lib/python3.11/site-packages/anyio/to_thread.py\", line 33, in run_sync\r\n    return await get_asynclib().run_sync_in_worker_thread(\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ciel/stable-diffusion/stable-diffusion-webui/venv/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\r\n    return await future\r\n           ^^^^^^^^^^^^\r\n  File \"/home/ciel/stable-diffusion/stable-diffusion-webui/venv/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 807, in run\r\n    result = context.run(func, *args)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ciel/stable-diffusion/stable-diffusion-webui/venv/lib/python3.11/site-packages/gradio/utils.py\", line 707, in wrapper\r\n    response = f(*args, **kwargs)\r\n               ^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ciel/stable-diffusion/stable-diffusion-webui/modules/ui_prompt_styles.py\", line 27, in save_style\r\n    shared.prompt_styles.save_styles(shared.styles_filename)\r\n  File \"/home/ciel/stable-diffusion/stable-diffusion-webui/modules/styles.py\", line 212, in save_styles\r\n    style_paths.remove(\"do_not_save\")\r\nKeyError: 'do_not_save'\n```\n\n\n### Additional information\n\nI'm running dev branch due to the Navi3 bug, checking out master after launch seems to result in the same issue, but it could have just been jit-ed, didn't test very in-depth\n", "hints_text": "I also encountered the same problem", "created_at": "2023-12-10T05:19:37Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 14186, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-14186", "issue_numbers": ["13985"], "base_commit": "b4776ea3a236c07041940ba78a50e25bc5c9a06f", "patch": "diff --git a/modules/import_hook.py b/modules/import_hook.py\nindex 28c67dfa897..eba9a372929 100644\n--- a/modules/import_hook.py\n+++ b/modules/import_hook.py\n@@ -3,3 +3,14 @@\n # this will break any attempt to import xformers which will prevent stability diffusion repo from trying to use it\n if \"--xformers\" not in \"\".join(sys.argv):\n     sys.modules[\"xformers\"] = None\n+\n+# Hack to fix a changed import in torchvision 0.17+, which otherwise breaks\n+# basicsr; see https://github.com/AUTOMATIC1111/stable-diffusion-webui/issues/13985\n+try:\n+    import torchvision.transforms.functional_tensor  # noqa: F401\n+except ImportError:\n+    try:\n+        import torchvision.transforms.functional as functional\n+        sys.modules[\"torchvision.transforms.functional_tensor\"] = functional\n+    except ImportError:\n+        pass  # shrug...\n", "test_patch": "", "problem_statement": "[Bug]: ModuleNotFoundError: No module named 'torchvision.transforms.functional_tensor' torchvision 0.17 promblem\n### Is there an existing issue for this?\n\n- [X] I have searched the existing issues and checked the recent builds/commits\n\n### What happened?\n\nModuleNotFoundError: No module named 'torchvision.transforms.functional_tensor'\r\ncrash\r\nhttps://github.com/pytorch/pytorch/issues/103973#issuecomment-1813303128\n\n### Steps to reproduce the problem\n\n1. Install\r\nhttps://github.com/pytorch/pytorch/issues/103973#issuecomment-1813214452\r\n2. Run webui\n\n### What should have happened?\n\nRun normaly\n\n### Sysinfo\n\nAMD Radeon VII\r\nAMD\u00ae Fx(tm)-9590 eight-core processor \u00d7 8 (not supporting pci atomics)\r\n32 gb ram\r\nUbuntu 22.04.3 LTS x64\n\n### What browsers do you use to access the UI ?\n\nMozilla Firefox\n\n### Console logs\n\n```Shell\n(venv) b_cansin@b-cansin-ubuntu:/media/b_cansin/ai/ai/stable-diffusion-webui$ ./webui.sh\r\n\r\n################################################################\r\nInstall script for stable-diffusion + Web UI\r\nTested on Debian 11 (Bullseye)\r\n################################################################\r\n\r\n################################################################\r\nRunning on b_cansin user\r\n################################################################\r\n\r\n################################################################\r\nRepo already cloned, using it as install directory\r\n################################################################\r\n\r\n################################################################\r\npython venv already activate or run without venv: /media/b_cansin/ai/ai/stable-diffusion-webui/venv\r\n################################################################\r\n\r\n################################################################\r\nLaunching launch.py...\r\n################################################################\r\nUsing TCMalloc: libtcmalloc_minimal.so.4\r\nfatal: No names found, cannot describe anything.\r\nPython 3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0]\r\nVersion: 1.6.1\r\nCommit hash: 7ba3923d5b494b7756d0b12f33acb3716d830b9a\r\nLaunching Web UI with arguments: \r\n2023-11-16 00:38:11.762329: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\nno module 'xformers'. Processing without...\r\nno module 'xformers'. Processing without...\r\nNo module 'xformers'. Proceeding without it.\r\n*** Error setting up CodeFormer\r\n    Traceback (most recent call last):\r\n      File \"/media/b_cansin/ai/ai/stable-diffusion-webui/modules/codeformer_model.py\", line 30, in setup_model\r\n        from modules.codeformer.codeformer_arch import CodeFormer\r\n      File \"/media/b_cansin/ai/ai/stable-diffusion-webui/modules/codeformer/codeformer_arch.py\", line 9, in <module>\r\n        from modules.codeformer.vqgan_arch import VQAutoEncoder, ResBlock\r\n      File \"/media/b_cansin/ai/ai/stable-diffusion-webui/modules/codeformer/vqgan_arch.py\", line 11, in <module>\r\n        from basicsr.utils import get_root_logger\r\n      File \"/media/b_cansin/ai/ai/stable-diffusion-webui/venv/lib/python3.10/site-packages/basicsr/__init__.py\", line 4, in <module>\r\n        from .data import *\r\n      File \"/media/b_cansin/ai/ai/stable-diffusion-webui/venv/lib/python3.10/site-packages/basicsr/data/__init__.py\", line 22, in <module>\r\n        _dataset_modules = [importlib.import_module(f'basicsr.data.{file_name}') for file_name in dataset_filenames]\r\n      File \"/media/b_cansin/ai/ai/stable-diffusion-webui/venv/lib/python3.10/site-packages/basicsr/data/__init__.py\", line 22, in <listcomp>\r\n        _dataset_modules = [importlib.import_module(f'basicsr.data.{file_name}') for file_name in dataset_filenames]\r\n      File \"/usr/lib/python3.10/importlib/__init__.py\", line 126, in import_module\r\n        return _bootstrap._gcd_import(name[level:], package, level)\r\n      File \"/media/b_cansin/ai/ai/stable-diffusion-webui/venv/lib/python3.10/site-packages/basicsr/data/realesrgan_dataset.py\", line 11, in <module>\r\n        from basicsr.data.degradations import circular_lowpass_kernel, random_mixed_kernels\r\n      File \"/media/b_cansin/ai/ai/stable-diffusion-webui/venv/lib/python3.10/site-packages/basicsr/data/degradations.py\", line 8, in <module>\r\n        from torchvision.transforms.functional_tensor import rgb_to_grayscale\r\n    ModuleNotFoundError: No module named 'torchvision.transforms.functional_tensor'\r\n\r\n---\r\nTraceback (most recent call last):\r\n  File \"/media/b_cansin/ai/ai/stable-diffusion-webui/launch.py\", line 48, in <module>\r\n    main()\r\n  File \"/media/b_cansin/ai/ai/stable-diffusion-webui/launch.py\", line 44, in main\r\n    start()\r\n  File \"/media/b_cansin/ai/ai/stable-diffusion-webui/modules/launch_utils.py\", line 436, in start\r\n    webui.webui()\r\n  File \"/media/b_cansin/ai/ai/stable-diffusion-webui/webui.py\", line 52, in webui\r\n    initialize.initialize()\r\n  File \"/media/b_cansin/ai/ai/stable-diffusion-webui/modules/initialize.py\", line 71, in initialize\r\n    from modules import gfpgan_model\r\n  File \"/media/b_cansin/ai/ai/stable-diffusion-webui/modules/gfpgan_model.py\", line 4, in <module>\r\n    import gfpgan\r\n  File \"/media/b_cansin/ai/ai/stable-diffusion-webui/venv/lib/python3.10/site-packages/gfpgan/__init__.py\", line 3, in <module>\r\n    from .data import *\r\n  File \"/media/b_cansin/ai/ai/stable-diffusion-webui/venv/lib/python3.10/site-packages/gfpgan/data/__init__.py\", line 10, in <module>\r\n    _dataset_modules = [importlib.import_module(f'gfpgan.data.{file_name}') for file_name in dataset_filenames]\r\n  File \"/media/b_cansin/ai/ai/stable-diffusion-webui/venv/lib/python3.10/site-packages/gfpgan/data/__init__.py\", line 10, in <listcomp>\r\n    _dataset_modules = [importlib.import_module(f'gfpgan.data.{file_name}') for file_name in dataset_filenames]\r\n  File \"/usr/lib/python3.10/importlib/__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"/media/b_cansin/ai/ai/stable-diffusion-webui/venv/lib/python3.10/site-packages/gfpgan/data/ffhq_degradation_dataset.py\", line 7, in <module>\r\n    from basicsr.data import degradations as degradations\r\n  File \"/media/b_cansin/ai/ai/stable-diffusion-webui/venv/lib/python3.10/site-packages/basicsr/__init__.py\", line 4, in <module>\r\n    from .data import *\r\n  File \"/media/b_cansin/ai/ai/stable-diffusion-webui/venv/lib/python3.10/site-packages/basicsr/data/__init__.py\", line 22, in <module>\r\n    _dataset_modules = [importlib.import_module(f'basicsr.data.{file_name}') for file_name in dataset_filenames]\r\n  File \"/media/b_cansin/ai/ai/stable-diffusion-webui/venv/lib/python3.10/site-packages/basicsr/data/__init__.py\", line 22, in <listcomp>\r\n    _dataset_modules = [importlib.import_module(f'basicsr.data.{file_name}') for file_name in dataset_filenames]\r\n  File \"/usr/lib/python3.10/importlib/__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"/media/b_cansin/ai/ai/stable-diffusion-webui/venv/lib/python3.10/site-packages/basicsr/data/realesrgan_dataset.py\", line 11, in <module>\r\n    from basicsr.data.degradations import circular_lowpass_kernel, random_mixed_kernels\r\n  File \"/media/b_cansin/ai/ai/stable-diffusion-webui/venv/lib/python3.10/site-packages/basicsr/data/degradations.py\", line 8, in <module>\r\n    from torchvision.transforms.functional_tensor import rgb_to_grayscale\r\nModuleNotFoundError: No module named 'torchvision.transforms.functional_tensor'\r\n(venv) b_cansin@b-cansin-ubuntu:/media/b_cansin/ai/ai/stable-diffusion-webui$\n```\n\n\n### Additional information\n\nAs @hongxiayang said in here:\r\nhttps://github.com/pytorch/pytorch/issues/103973#issuecomment-1813380169\r\nNeed support torchvision 0.17 because of torchvision.transforms.functional deprecated and removed in 0.17. Could we get dev version that support 0.17?\r\nAfter than we will test if this wheel fix non pci atomics problem of ROCM\n", "hints_text": "This looks like it should probably be opened at [basicsr's repo](https://github.com/XPixelGroup/BasicSR),as I believe A1111 just pulls from that repo, it hasn't been updated in a while though. \r\n\r\nIn the short term - try:\r\n\r\nOpen `./stable-diffusion-webui/venv/lib/python3.10/site-packages/basicsr/data/degradations.py` and on line 8, simply change:\r\n\r\n```python\r\nfrom torchvision.transforms.functional_tensor import rgb_to_grayscale\r\n```\r\nto:\r\n```python\r\nfrom torchvision.transforms.functional import rgb_to_grayscale\r\n```\r\n\r\nWhich should at least get you past this step.\nOpened a PR on basicsr repo https://github.com/XPixelGroup/BasicSR/pull/650\n> This looks like it should probably be opened at [basicsr's repo](https://github.com/XPixelGroup/BasicSR),as I believe A1111 just pulls from that repo, it hasn't been updated in a while though.\r\n> \r\n> In the short term - try:\r\n> \r\n> Open `./stable-diffusion-webui/venv/lib/python3.10/site-packages/basicsr/data/degradations.py` and on line 8, simply change:\r\n> \r\n> ```python\r\n> from torchvision.transforms.functional_tensor import rgb_to_grayscale\r\n> ```\r\n> \r\n> to:\r\n> \r\n> ```python\r\n> from torchvision.transforms.functional import rgb_to_grayscale\r\n> ```\r\n> \r\n> Which should at least get you past this step.\r\n\r\nYes that fix my problem now I can finally gen with pytorch 2 without black image or pci atomics problem\r\n\r\n![Screenshot from 2023-11-16 13-58-13](https://github.com/AUTOMATIC1111/stable-diffusion-webui/assets/52115509/66ae618c-d132-43fc-b273-d1389d8dc96f)\r\n\r\n\n> This looks like it should probably be opened at [basicsr's repo](https://github.com/XPixelGroup/BasicSR),as I believe A1111 just pulls from that repo, it hasn't been updated in a while though.\r\n> \r\n> In the short term - try:\r\n> \r\n> Open `./stable-diffusion-webui/venv/lib/python3.10/site-packages/basicsr/data/degradations.py` and on line 8, simply change:\r\n> \r\n> ```python\r\n> from torchvision.transforms.functional_tensor import rgb_to_grayscale\r\n> ```\r\n> \r\n> to:\r\n> \r\n> ```python\r\n> from torchvision.transforms.functional import rgb_to_grayscale\r\n> ```\r\n> \r\n> Which should at least get you past this step.\r\n\r\nLazy people (like me) can use sed (on Mac and Linux):\r\n\r\nmac\r\n```\r\nsed -i '' 's/from torchvision.transforms.functional_tensor import rgb_to_grayscale/from torchvision.transforms.functional import rgb_to_grayscale/' venv/lib/python3.10/site-packages/basicsr/data/degradations.py\r\n```\r\n\r\nlinux\r\n```\r\nsed -i 's/from torchvision.transforms.functional_tensor import rgb_to_grayscale/from torchvision.transforms.functional import rgb_to_grayscale/' venv/lib/python3.10/site-packages/basicsr/data/degradations.py\r\n```\r\n\r\nMac requires an additional parameter for the backup file, but the rest is the same for both.", "created_at": "2023-12-03T14:51:20Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 14145, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-14145", "issue_numbers": ["14071"], "base_commit": "f0f100e67b78f686dc73cf3c8cad422e45cc9b8a", "patch": "diff --git a/modules/processing.py b/modules/processing.py\nindex ac58ef86940..d73c8bfc1c9 100644\n--- a/modules/processing.py\n+++ b/modules/processing.py\n@@ -864,6 +864,34 @@ def process_images_inner(p: StableDiffusionProcessing) -> Processed:\n             if p.n_iter > 1:\r\n                 shared.state.job = f\"Batch {n+1} out of {p.n_iter}\"\r\n \r\n+            def rescale_zero_terminal_snr_abar(alphas_cumprod):\r\n+                alphas_bar_sqrt = alphas_cumprod.sqrt()\r\n+\r\n+                # Store old values.\r\n+                alphas_bar_sqrt_0 = alphas_bar_sqrt[0].clone()\r\n+                alphas_bar_sqrt_T = alphas_bar_sqrt[-1].clone()\r\n+\r\n+                # Shift so the last timestep is zero.\r\n+                alphas_bar_sqrt -= (alphas_bar_sqrt_T)\r\n+\r\n+                # Scale so the first timestep is back to the old value.\r\n+                alphas_bar_sqrt *= alphas_bar_sqrt_0 / (alphas_bar_sqrt_0 - alphas_bar_sqrt_T)\r\n+\r\n+                # Convert alphas_bar_sqrt to betas\r\n+                alphas_bar = alphas_bar_sqrt**2  # Revert sqrt\r\n+                alphas_bar[-1] = 4.8973451890853435e-08\r\n+                return alphas_bar\r\n+\r\n+            if hasattr(p.sd_model, 'alphas_cumprod') and hasattr(p.sd_model, 'alphas_cumprod_original'):\r\n+                p.sd_model.alphas_cumprod = p.sd_model.alphas_cumprod_original.to(shared.device)\r\n+\r\n+                if opts.use_downcasted_alpha_bar:\r\n+                    p.extra_generation_params['Downcast alphas_cumprod'] = opts.use_downcasted_alpha_bar\r\n+                    p.sd_model.alphas_cumprod = p.sd_model.alphas_cumprod.half().to(shared.device)\r\n+                if opts.sd_noise_schedule == \"Zero Terminal SNR\":\r\n+                    p.extra_generation_params['Noise Schedule'] = opts.sd_noise_schedule\r\n+                    p.sd_model.alphas_cumprod = rescale_zero_terminal_snr_abar(p.sd_model.alphas_cumprod).to(shared.device)\r\n+\r\n             with devices.without_autocast() if devices.unet_needs_upcast else devices.autocast():\r\n                 samples_ddim = p.sample(conditioning=p.c, unconditional_conditioning=p.uc, seeds=p.seeds, subseeds=p.subseeds, subseed_strength=p.subseed_strength, prompts=p.prompts)\r\n \r\ndiff --git a/modules/sd_models.py b/modules/sd_models.py\nindex 841402e8629..5a19a00a50a 100644\n--- a/modules/sd_models.py\n+++ b/modules/sd_models.py\n@@ -374,6 +374,7 @@ def load_model_weights(model, checkpoint_info: CheckpointInfo, state_dict, timer\n \r\n     if shared.cmd_opts.no_half:\r\n         model.float()\r\n+        model.alphas_cumprod_original = model.alphas_cumprod\r\n         devices.dtype_unet = torch.float32\r\n         timer.record(\"apply float()\")\r\n     else:\r\n@@ -387,7 +388,11 @@ def load_model_weights(model, checkpoint_info: CheckpointInfo, state_dict, timer\n         if shared.cmd_opts.upcast_sampling and depth_model:\r\n             model.depth_model = None\r\n \r\n+        alphas_cumprod = model.alphas_cumprod\r\n+        model.alphas_cumprod = None\r\n         model.half()\r\n+        model.alphas_cumprod = alphas_cumprod\r\n+        model.alphas_cumprod_original = alphas_cumprod\r\n         model.first_stage_model = vae\r\n         if depth_model:\r\n             model.depth_model = depth_model\r\n@@ -642,6 +647,7 @@ def load_model(checkpoint_info=None, already_loaded_state_dict=None):\n     else:\r\n         weight_dtype_conversion = {\r\n             'first_stage_model': None,\r\n+            'alphas_cumprod': None,\r\n             '': torch.float16,\r\n         }\r\n \r\ndiff --git a/modules/sd_samplers_timesteps.py b/modules/sd_samplers_timesteps.py\nindex b17a8f93c2b..c4bd5c12775 100644\n--- a/modules/sd_samplers_timesteps.py\n+++ b/modules/sd_samplers_timesteps.py\n@@ -36,7 +36,7 @@ def __init__(self, model, *args, **kwargs):\n         self.inner_model = model\r\n \r\n     def predict_eps_from_z_and_v(self, x_t, t, v):\r\n-        return self.inner_model.sqrt_alphas_cumprod[t.to(torch.int), None, None, None] * v + self.inner_model.sqrt_one_minus_alphas_cumprod[t.to(torch.int), None, None, None] * x_t\r\n+        return torch.sqrt(self.inner_model.alphas_cumprod)[t.to(torch.int), None, None, None] * v + torch.sqrt(1 - self.inner_model.alphas_cumprod)[t.to(torch.int), None, None, None] * x_t\r\n \r\n     def forward(self, input, timesteps, **kwargs):\r\n         model_output = self.inner_model.apply_model(input, timesteps, **kwargs)\r\ndiff --git a/modules/shared_options.py b/modules/shared_options.py\nindex 04e68a712ef..bc3d56dec60 100644\n--- a/modules/shared_options.py\n+++ b/modules/shared_options.py\n@@ -218,6 +218,7 @@\n     \"dont_fix_second_order_samplers_schedule\": OptionInfo(False, \"Do not fix prompt schedule for second order samplers.\"),\r\n     \"hires_fix_use_firstpass_conds\": OptionInfo(False, \"For hires fix, calculate conds of second pass using extra networks of first pass.\"),\r\n     \"use_old_scheduling\": OptionInfo(False, \"Use old prompt editing timelines.\", infotext=\"Old prompt editing timelines\").info(\"For [red:green:N]; old: If N < 1, it's a fraction of steps (and hires fix uses range from 0 to 1), if N >= 1, it's an absolute number of steps; new: If N has a decimal point in it, it's a fraction of steps (and hires fix uses range from 1 to 2), othewrwise it's an absolute number of steps\"),\r\n+    \"use_downcasted_alpha_bar\": OptionInfo(False, \"Downcast model alphas_cumprod to fp16 before sampling. For reproducing old seeds.\", infotext=\"Downcast alphas_cumprod\")\r\n }))\r\n \r\n options_templates.update(options_section(('interrogate', \"Interrogate\"), {\r\n@@ -335,6 +336,7 @@\n     'uni_pc_skip_type': OptionInfo(\"time_uniform\", \"UniPC skip type\", gr.Radio, {\"choices\": [\"time_uniform\", \"time_quadratic\", \"logSNR\"]}, infotext='UniPC skip type'),\r\n     'uni_pc_order': OptionInfo(3, \"UniPC order\", gr.Slider, {\"minimum\": 1, \"maximum\": 50, \"step\": 1}, infotext='UniPC order').info(\"must be < sampling steps\"),\r\n     'uni_pc_lower_order_final': OptionInfo(True, \"UniPC lower order final\", infotext='UniPC lower order final'),\r\n+    'sd_noise_schedule': OptionInfo(\"Default\", \"Noise schedule for sampling\", gr.Radio, {\"choices\": [\"Default\", \"Zero Terminal SNR\"]}, infotext=\"Noise Schedule\").info(\"for use with zero terminal SNR trained models\")\r\n }))\r\n \r\n options_templates.update(options_section(('postprocessing', \"Postprocessing\", \"postprocessing\"), {\r\n", "test_patch": "", "problem_statement": "[Bug]: alphas_cumprod are downcasted to half precision during model load despite existing at full precision during sampling\n### Is there an existing issue for this?\n\n- [X] I have searched the existing issues and checked the recent builds/commits\n\n### What happened?\n\nDuring model loading, alphas_cumprod is downcasted to half precision along with the rest of the model.  However, any sampler will always use those values as full precision floats.  Not only is this not saving memory, it also noticeably changes results due to the loss of precision, and it may in fact be one of the most major factors changing results between mixed and full precision inference.  I am considering this to be a bug since it serves no clear optimization purpose and alters results in a way that is less aligned with model training.  I have also tested a crude fix for the issue and have found that it does not impact generation speed in a significant manner.\r\n\r\nPerhaps more importantly, this problem is an obstacle to implementing zero terminal SNR noise schedules like in #13052, since downcasting the values that we derive SNR from will result in sampler sigmas being rounded to infinity shortly after they pass 65000.  It is possible to use a zero terminal SNR noise schedule that even plays nicely with k-diffusion samplers by setting the last alpha_bar value to a very small number like `4.8973451890853435e-08`, but in fp16 format this number will be rounded down to zero which will break things.\n\n### Steps to reproduce the problem\n\n1. Load a model\r\n2. model.half() gets called in `modules/sd_models.py`, causing `model.alphas_cumprod` to be downcast\r\n3. Observe the loss of precision\r\n4. Inspect the dtype of alphas_cumprod values (or values directly derived from them, like the sigma schedule) as used within a K-diffusion sampler\r\n5. Observe that they are being used in full-precision float format\n\n### What should have happened?\n\nDowncasting the model to half precision should be implemented in a way that leaves alphas_cumprod alone.  Since this change will change seeds, a compatibility option should be added that will downcast and upcast a copy of alphas_cumprod right before sampling in order to simulate old behavior.  Implementing that compatibility option would also provide all of the necessary infrastructure to cleanly implement an option to use a zero terminal SNR noise schedule so #13052 may as well be implemented at the same time.\n\n### Sysinfo\n\n[sysinfo-2023-11-23-17-00.txt](https://github.com/AUTOMATIC1111/stable-diffusion-webui/files/13452397/sysinfo-2023-11-23-17-00.txt)\r\n\n\n### What browsers do you use to access the UI ?\n\nMozilla Firefox\n\n### Console logs\n\n```Shell\nThis is an internal issue, logs won't help here.\n```\n\n\n### Additional information\n\n_No response_\n", "hints_text": "", "created_at": "2023-11-29T22:55:07Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 13957, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-13957", "issue_numbers": ["13895"], "base_commit": "5e80d9ee99c5899e5e2b130408ffb65a0585a62a", "patch": "diff --git a/modules/shared_options.py b/modules/shared_options.py\nindex 00b273faa54..1d2dca797ce 100644\n--- a/modules/shared_options.py\n+++ b/modules/shared_options.py\n@@ -224,6 +224,7 @@\n \r\n options_templates.update(options_section(('extra_networks', \"Extra Networks\"), {\r\n     \"extra_networks_show_hidden_directories\": OptionInfo(True, \"Show hidden directories\").info(\"directory is hidden if its name starts with \\\".\\\".\"),\r\n+    \"extra_networks_dir_button_function\": OptionInfo(False, \"Add a '/' to the beginning of directory buttons\").info(\"Buttons will display the contents of the selected directory without acting as a search filter.\"),\r\n     \"extra_networks_hidden_models\": OptionInfo(\"When searched\", \"Show cards for models in hidden directories\", gr.Radio, {\"choices\": [\"Always\", \"When searched\", \"Never\"]}).info('\"When searched\" option will only show the item when the search string has 4 characters or more'),\r\n     \"extra_networks_default_multiplier\": OptionInfo(1.0, \"Default multiplier for extra networks\", gr.Slider, {\"minimum\": 0.0, \"maximum\": 2.0, \"step\": 0.01}),\r\n     \"extra_networks_card_width\": OptionInfo(0, \"Card width for Extra Networks\").info(\"in pixels\"),\r\ndiff --git a/modules/ui_extra_networks.py b/modules/ui_extra_networks.py\nindex 063bd7b80e6..27a37295fc5 100644\n--- a/modules/ui_extra_networks.py\n+++ b/modules/ui_extra_networks.py\n@@ -138,8 +138,13 @@ def create_html(self, tabname):\n                         continue\r\n \r\n                     subdir = os.path.abspath(x)[len(parentdir):].replace(\"\\\\\", \"/\")\r\n-                    while subdir.startswith(\"/\"):\r\n-                        subdir = subdir[1:]\r\n+\r\n+                    if shared.opts.extra_networks_dir_button_function:\r\n+                        if not subdir.startswith(\"/\"):\r\n+                            subdir = \"/\" + subdir\r\n+                    else:\r\n+                        while subdir.startswith(\"/\"):\r\n+                            subdir = subdir[1:]\r\n \r\n                     is_empty = len(os.listdir(x)) == 0\r\n                     if not is_empty and not subdir.endswith(\"/\"):\r\n", "test_patch": "", "problem_statement": "[Bug]: Lora folders get mixed\n### Is there an existing issue for this?\n\n- [x] I have searched the existing issues and checked the recent builds/commits\n\n### What happened?\n\nI have sorted Loras into the folders `NSFW` and `SFW`, but the NSFW folder is mixed into the SFW folder in the UI when I select only the SFW.\n\n### Steps to reproduce the problem\n\nSort loras into folders where a folder has a substring as its name from another folder. \n\n### What should have happened?\n\nthe folders should not get mixed\n\n### Sysinfo\n\n{\r\n    \"Platform\": \"Windows-10-10.0.19045-SP0\",\r\n    \"Python\": \"3.10.10\",\r\n    \"Version\": \"v1.6.0-2-g4afaaf8a\",\r\n    \"Commit\": \"4afaaf8a020c1df457bcf7250cb1c7f609699fa7\",\r\n    \"Script path\": \"Y:\\\\stable-diffusion-webui\",\r\n    \"Data path\": \"Y:\\\\stable-diffusion-webui\",\r\n    \"Extensions dir\": \"Y:\\\\stable-diffusion-webui\\\\extensions\",\r\n    \"Checksum\": \"6516ab9f4a0cd3e7e03b87fc5aacadbd06a705d2a59483ec32a84f01e2d32b22\",\r\n    \"Commandline\": [\r\n        \"launch.py\",\r\n        \"--ckpt-dir\",\r\n        \"Y:\\\\stable_difiusion\\\\models\",\r\n        \"--medvram-sdxl\",\r\n        \"--no-download-sd-model\",\r\n        \"--xformers\",\r\n        \"--lora-dir\",\r\n        \"Y:\\\\stable_difiusion\\\\Loaras\"\r\n    ],\r\n    \"Torch env info\": {\r\n        \"torch_version\": \"2.0.1+cu118\",\r\n        \"is_debug_build\": \"False\",\r\n        \"cuda_compiled_version\": \"11.8\",\r\n        \"gcc_version\": \"(MinGW.org GCC Build-2) 9.2.0\\r\",\r\n        \"clang_version\": null,\r\n        \"cmake_version\": null,\r\n        \"os\": \"Microsoft Windows 10 Pro\",\r\n        \"libc_version\": \"N/A\",\r\n        \"python_version\": \"3.10.10 (tags/v3.10.10:aad5f6a, Feb  7 2023, 17:20:36) [MSC v.1929 64 bit (AMD64)] (64-bit runtime)\",\r\n        \"python_platform\": \"Windows-10-10.0.19045-SP0\",\r\n        \"is_cuda_available\": \"True\",\r\n        \"cuda_runtime_version\": \"11.3.58\\r\",\r\n        \"cuda_module_loading\": \"LAZY\",\r\n        \"nvidia_driver_version\": \"536.23\",\r\n        \"nvidia_gpu_models\": \"GPU 0: NVIDIA GeForce RTX 3060\",\r\n        \"cudnn_version\": null,\r\n        \"pip_version\": \"pip3\",\r\n        \"pip_packages\": [\r\n            \"numpy==1.23.5\",\r\n            \"open-clip-torch==2.20.0\",\r\n            \"pytorch-lightning==1.9.4\",\r\n            \"torch==2.0.1+cu118\",\r\n            \"torchdiffeq==0.2.3\",\r\n            \"torchmetrics==1.0.1\",\r\n            \"torchsde==0.2.5\",\r\n            \"torchvision==0.15.2+cu118\"\r\n        ],\r\n        \"conda_packages\": null,\r\n        \"hip_compiled_version\": \"N/A\",\r\n        \"hip_runtime_version\": \"N/A\",\r\n        \"miopen_runtime_version\": \"N/A\",\r\n        \"caching_allocator_config\": \"\",\r\n        \"is_xnnpack_available\": \"True\",\r\n        \"cpu_info\": [\r\n            \"Architecture=9\",\r\n            \"CurrentClockSpeed=3792\",\r\n            \"DeviceID=CPU0\",\r\n            \"Family=198\",\r\n            \"L2CacheSize=2048\",\r\n            \"L2CacheSpeed=\",\r\n            \"Manufacturer=GenuineIntel\",\r\n            \"MaxClockSpeed=3792\",\r\n            \"Name=Intel(R) Core(TM) i7-10700K CPU @ 3.80GHz\",\r\n            \"ProcessorType=3\",\r\n            \"Revision=\"\r\n        ]\r\n    },\r\n    \"Exceptions\": [],\r\n    \"CPU\": {\r\n        \"model\": \"Intel64 Family 6 Model 165 Stepping 5, GenuineIntel\",\r\n        \"count logical\": 16,\r\n        \"count physical\": 8\r\n    },\r\n    \"RAM\": {\r\n        \"total\": \"32GB\",\r\n        \"used\": \"16GB\",\r\n        \"free\": \"16GB\"\r\n    },\r\n    \"Extensions\": [\r\n        {\r\n            \"name\": \"--sd-webui-ar-plus\",\r\n            \"path\": \"Y:\\\\stable-diffusion-webui\\\\extensions\\\\--sd-webui-ar-plus\",\r\n            \"version\": \"a7ac063a\",\r\n            \"branch\": \"main\",\r\n            \"remote\": \"https://github.com/LEv145/--sd-webui-ar-plus\"\r\n        },\r\n        {\r\n            \"name\": \"DWPose\",\r\n            \"path\": \"Y:\\\\stable-diffusion-webui\\\\extensions\\\\DWPose\",\r\n            \"version\": \"81e6fb58\",\r\n            \"branch\": \"onnx\",\r\n            \"remote\": \"https://github.com/IDEA-Research/DWPose\"\r\n        },\r\n        {\r\n            \"name\": \"LoraTester\",\r\n            \"path\": \"Y:\\\\stable-diffusion-webui\\\\extensions\\\\LoraTester\",\r\n            \"version\": \"\",\r\n            \"branch\": null,\r\n            \"remote\": null\r\n        },\r\n        {\r\n            \"name\": \"SD-WebUI-BatchCheckpointPrompt\",\r\n            \"path\": \"Y:\\\\stable-diffusion-webui\\\\extensions\\\\SD-WebUI-BatchCheckpointPrompt\",\r\n            \"version\": \"6d3c2856\",\r\n            \"branch\": \"main\",\r\n            \"remote\": \"https://github.com/h43lb1t0/SD-WebUI-BatchCheckpointPrompt\"\r\n        },\r\n        {\r\n            \"name\": \"Stable-Diffusion-Webui-Civitai-Helper\",\r\n            \"path\": \"Y:\\\\stable-diffusion-webui\\\\extensions\\\\Stable-Diffusion-Webui-Civitai-Helper\",\r\n            \"version\": \"ac2f087e\",\r\n            \"branch\": \"master\",\r\n            \"remote\": \"https://github.com/zixaphir/Stable-Diffusion-Webui-Civitai-Helper.git\"\r\n        },\r\n        {\r\n            \"name\": \"StyleSelectorXL\",\r\n            \"path\": \"Y:\\\\stable-diffusion-webui\\\\extensions\\\\StyleSelectorXL\",\r\n            \"version\": \"5a093f5a\",\r\n            \"branch\": \"main\",\r\n            \"remote\": \"https://github.com/ahgsql/StyleSelectorXL.git\"\r\n        },\r\n        {\r\n            \"name\": \"a-sd-dynamic-prompts\",\r\n            \"path\": \"Y:\\\\stable-diffusion-webui\\\\extensions\\\\a-sd-dynamic-prompts\",\r\n            \"version\": \"39c06b30\",\r\n            \"branch\": \"main\",\r\n            \"remote\": \"https://github.com/adieyal/sd-dynamic-prompts.git\"\r\n        },\r\n        {\r\n            \"name\": \"adetailer\",\r\n            \"path\": \"Y:\\\\stable-diffusion-webui\\\\extensions\\\\adetailer\",\r\n            \"version\": \"6b41b3db\",\r\n            \"branch\": \"main\",\r\n            \"remote\": \"https://github.com/Bing-su/adetailer.git\"\r\n        },\r\n        {\r\n            \"name\": \"multidiffusion-upscaler-for-automatic1111\",\r\n            \"path\": \"Y:\\\\stable-diffusion-webui\\\\extensions\\\\multidiffusion-upscaler-for-automatic1111\",\r\n            \"version\": \"fbb24736\",\r\n            \"branch\": \"main\",\r\n            \"remote\": \"https://github.com/pkuliyi2015/multidiffusion-upscaler-for-automatic1111\"\r\n        },\r\n        {\r\n            \"name\": \"sd-webui-controlnet\",\r\n            \"path\": \"Y:\\\\stable-diffusion-webui\\\\extensions\\\\sd-webui-controlnet\",\r\n            \"version\": \"fce6775a\",\r\n            \"branch\": \"main\",\r\n            \"remote\": \"https://github.com/Mikubill/sd-webui-controlnet.git\"\r\n        },\r\n        {\r\n            \"name\": \"sd-webui-roop\",\r\n            \"path\": \"Y:\\\\stable-diffusion-webui\\\\extensions\\\\sd-webui-roop\",\r\n            \"version\": \"9a16e728\",\r\n            \"branch\": \"main\",\r\n            \"remote\": \"https://github.com/s0md3v/sd-webui-roop\"\r\n        },\r\n        {\r\n            \"name\": \"stable-diffusion-webui-images-browser\",\r\n            \"path\": \"Y:\\\\stable-diffusion-webui\\\\extensions\\\\stable-diffusion-webui-images-browser\",\r\n            \"version\": \"08fc2647\",\r\n            \"branch\": \"main\",\r\n            \"remote\": \"https://github.com/AlUlkesh/stable-diffusion-webui-images-browser.git\"\r\n        },\r\n        {\r\n            \"name\": \"stable-diffusion-webui-wd14-tagger\",\r\n            \"path\": \"Y:\\\\stable-diffusion-webui\\\\extensions\\\\stable-diffusion-webui-wd14-tagger\",\r\n            \"version\": \"e72d984b\",\r\n            \"branch\": \"master\",\r\n            \"remote\": \"https://github.com/picobyte/stable-diffusion-webui-wd14-tagger.git\"\r\n        },\r\n        {\r\n            \"name\": \"ultimate-upscale-for-automatic1111\",\r\n            \"path\": \"Y:\\\\stable-diffusion-webui\\\\extensions\\\\ultimate-upscale-for-automatic1111\",\r\n            \"version\": \"728ffcec\",\r\n            \"branch\": \"master\",\r\n            \"remote\": \"https://github.com/Coyote-A/ultimate-upscale-for-automatic1111.git\"\r\n        }\r\n    ],\r\n    \"Inactive extensions\": [\r\n        {\r\n            \"name\": \"CloneCleaner\",\r\n            \"path\": \"Y:\\\\stable-diffusion-webui\\\\extensions\\\\CloneCleaner\",\r\n            \"version\": \"bf304e4d\",\r\n            \"branch\": \"main\",\r\n            \"remote\": \"https://github.com/h43lb1t0/CloneCleaner\"\r\n        },\r\n        {\r\n            \"name\": \"CloneCleaner - Kopie\",\r\n            \"path\": \"Y:\\\\stable-diffusion-webui\\\\extensions\\\\CloneCleaner - Kopie\",\r\n            \"version\": \"bf304e4d\",\r\n            \"branch\": \"main\",\r\n            \"remote\": \"https://github.com/h43lb1t0/CloneCleaner\"\r\n        },\r\n        {\r\n            \"name\": \"batch-face-swap\",\r\n            \"path\": \"Y:\\\\stable-diffusion-webui\\\\extensions\\\\batch-face-swap\",\r\n            \"version\": \"942ebae6\",\r\n            \"branch\": \"main\",\r\n            \"remote\": \"https://github.com/kex0/batch-face-swap.git\"\r\n        },\r\n        {\r\n            \"name\": \"sd-webui-faceswaplab\",\r\n            \"path\": \"Y:\\\\stable-diffusion-webui\\\\extensions\\\\sd-webui-faceswaplab\",\r\n            \"version\": \"42d1c75b\",\r\n            \"branch\": \"main\",\r\n            \"remote\": \"https://github.com/glucauze/sd-webui-faceswaplab\"\r\n        },\r\n        {\r\n            \"name\": \"sd-webui-sdxl-refiner-hack\",\r\n            \"path\": \"Y:\\\\stable-diffusion-webui\\\\extensions\\\\sd-webui-sdxl-refiner-hack\",\r\n            \"version\": \"78f4606a\",\r\n            \"branch\": \"main\",\r\n            \"remote\": \"https://github.com/h43lb1t0/sd-webui-sdxl-refiner-hack.git\"\r\n        }\r\n    ],\r\n    \"Environment\": {\r\n        \"COMMANDLINE_ARGS\": \" --ckpt-dir Y:\\\\\\\\stable_difiusion\\\\\\\\models --medvram-sdxl --no-download-sd-model --xformers --lora-dir Y:\\\\\\\\stable_difiusion\\\\\\\\Loaras \",\r\n        \"GRADIO_ANALYTICS_ENABLED\": \"False\"\r\n    },\r\n    \"Config\": {\r\n        \"samples_save\": true,\r\n        \"samples_format\": \"png\",\r\n        \"samples_filename_pattern\": \"\",\r\n        \"save_images_add_number\": true,\r\n        \"grid_save\": true,\r\n        \"grid_format\": \"png\",\r\n        \"grid_extended_filename\": false,\r\n        \"grid_only_if_multiple\": true,\r\n        \"grid_prevent_empty_spots\": false,\r\n        \"grid_zip_filename_pattern\": \"\",\r\n        \"n_rows\": -1,\r\n        \"font\": \"\",\r\n        \"grid_text_active_color\": \"#000000\",\r\n        \"grid_text_inactive_color\": \"#999999\",\r\n        \"grid_background_color\": \"#ffffff\",\r\n        \"enable_pnginfo\": true,\r\n        \"save_txt\": false,\r\n        \"save_images_before_face_restoration\": false,\r\n        \"save_images_before_highres_fix\": true,\r\n        \"save_images_before_color_correction\": false,\r\n        \"save_mask\": false,\r\n        \"save_mask_composite\": false,\r\n        \"jpeg_quality\": 80,\r\n        \"webp_lossless\": false,\r\n        \"export_for_4chan\": false,\r\n        \"img_downscale_threshold\": 4.0,\r\n        \"target_side_length\": 4000,\r\n        \"img_max_size_mp\": 200,\r\n        \"use_original_name_batch\": true,\r\n        \"use_upscaler_name_as_suffix\": false,\r\n        \"save_selected_only\": true,\r\n        \"save_init_img\": false,\r\n        \"temp_dir\": \"\",\r\n        \"clean_temp_dir_at_start\": false,\r\n        \"outdir_samples\": \"\",\r\n        \"outdir_txt2img_samples\": \"outputs/txt2img-images\",\r\n        \"outdir_img2img_samples\": \"outputs/img2img-images\",\r\n        \"outdir_extras_samples\": \"outputs/extras-images\",\r\n        \"outdir_grids\": \"\",\r\n        \"outdir_txt2img_grids\": \"outputs/txt2img-grids\",\r\n        \"outdir_img2img_grids\": \"outputs/img2img-grids\",\r\n        \"outdir_save\": \"log/images\",\r\n        \"outdir_init_images\": \"outputs/init-images\",\r\n        \"save_to_dirs\": true,\r\n        \"grid_save_to_dirs\": true,\r\n        \"use_save_to_dirs_for_ui\": false,\r\n        \"directories_filename_pattern\": \"[date]\",\r\n        \"directories_max_prompt_words\": 8,\r\n        \"ESRGAN_tile\": 192,\r\n        \"ESRGAN_tile_overlap\": 8,\r\n        \"realesrgan_enabled_models\": [\r\n            \"R-ESRGAN 4x+\",\r\n            \"R-ESRGAN 4x+ Anime6B\"\r\n        ],\r\n        \"upscaler_for_img2img\": null,\r\n        \"face_restoration_model\": \"GFPGAN\",\r\n        \"code_former_weight\": 0.5,\r\n        \"face_restoration_unload\": true,\r\n        \"show_warnings\": false,\r\n        \"memmon_poll_rate\": 40,\r\n        \"samples_log_stdout\": false,\r\n        \"multiple_tqdm\": true,\r\n        \"print_hypernet_extra\": false,\r\n        \"list_hidden_files\": true,\r\n        \"disable_mmap_load_safetensors\": false,\r\n        \"unload_models_when_training\": false,\r\n        \"pin_memory\": false,\r\n        \"save_optimizer_state\": false,\r\n        \"save_training_settings_to_txt\": true,\r\n        \"dataset_filename_word_regex\": \"\",\r\n        \"dataset_filename_join_string\": \" \",\r\n        \"training_image_repeats_per_epoch\": 1,\r\n        \"training_write_csv_every\": 500,\r\n        \"training_xattention_optimizations\": false,\r\n        \"training_enable_tensorboard\": false,\r\n        \"training_tensorboard_save_images\": false,\r\n        \"training_tensorboard_flush_every\": 120,\r\n        \"sd_model_checkpoint\": \"sdxl\\\\realvisxlV20_v20Bakedvae.safetensors [74dda471cc]\",\r\n        \"sd_checkpoint_cache\": 0,\r\n        \"sd_vae_checkpoint_cache\": 0,\r\n        \"sd_vae\": \"sdxl_vae-fp-16.safetensors\",\r\n        \"sd_vae_as_default\": true,\r\n        \"sd_unet\": \"Automatic\",\r\n        \"inpainting_mask_weight\": 1.0,\r\n        \"initial_noise_multiplier\": 1.0,\r\n        \"img2img_color_correction\": false,\r\n        \"img2img_fix_steps\": true,\r\n        \"img2img_background_color\": \"#ffffff\",\r\n        \"enable_quantization\": false,\r\n        \"enable_emphasis\": true,\r\n        \"enable_batch_seeds\": true,\r\n        \"comma_padding_backtrack\": 20,\r\n        \"CLIP_stop_at_last_layers\": 1,\r\n        \"upcast_attn\": false,\r\n        \"auto_vae_precision\": true,\r\n        \"randn_source\": \"GPU\",\r\n        \"sdxl_crop_top\": 0,\r\n        \"sdxl_crop_left\": 0,\r\n        \"sdxl_refiner_low_aesthetic_score\": 2.5,\r\n        \"sdxl_refiner_high_aesthetic_score\": 6.0,\r\n        \"cross_attention_optimization\": \"sdp - scaled dot product\",\r\n        \"s_min_uncond\": 0.0,\r\n        \"token_merging_ratio\": 0.0,\r\n        \"token_merging_ratio_img2img\": 0.0,\r\n        \"token_merging_ratio_hr\": 0.0,\r\n        \"pad_cond_uncond\": false,\r\n        \"experimental_persistent_cond_cache\": false,\r\n        \"use_old_emphasis_implementation\": false,\r\n        \"use_old_karras_scheduler_sigmas\": false,\r\n        \"no_dpmpp_sde_batch_determinism\": false,\r\n        \"use_old_hires_fix_width_height\": false,\r\n        \"dont_fix_second_order_samplers_schedule\": false,\r\n        \"hires_fix_use_firstpass_conds\": false,\r\n        \"interrogate_keep_models_in_memory\": false,\r\n        \"interrogate_return_ranks\": false,\r\n        \"interrogate_clip_num_beams\": 1,\r\n        \"interrogate_clip_min_length\": 24,\r\n        \"interrogate_clip_max_length\": 48,\r\n        \"interrogate_clip_dict_limit\": 1500,\r\n        \"interrogate_clip_skip_categories\": [],\r\n        \"interrogate_deepbooru_score_threshold\": 0.5,\r\n        \"deepbooru_sort_alpha\": true,\r\n        \"deepbooru_use_spaces\": true,\r\n        \"deepbooru_escape\": true,\r\n        \"deepbooru_filter_tags\": \"\",\r\n        \"extra_networks_show_hidden_directories\": true,\r\n        \"extra_networks_hidden_models\": \"When searched\",\r\n        \"extra_networks_default_multiplier\": 1.0,\r\n        \"extra_networks_card_width\": 0,\r\n        \"extra_networks_card_height\": 0,\r\n        \"extra_networks_card_text_scale\": 1.0,\r\n        \"extra_networks_card_show_desc\": true,\r\n        \"extra_networks_add_text_separator\": \" \",\r\n        \"ui_extra_networks_tab_reorder\": \"\",\r\n        \"textual_inversion_print_at_load\": false,\r\n        \"textual_inversion_add_hashes_to_infotext\": true,\r\n        \"sd_hypernetwork\": \"None\",\r\n        \"localization\": \"None\",\r\n        \"gradio_theme\": \"Default\",\r\n        \"img2img_editor_height\": 720,\r\n        \"return_grid\": true,\r\n        \"return_mask\": false,\r\n        \"return_mask_composite\": false,\r\n        \"do_not_show_images\": false,\r\n        \"send_seed\": true,\r\n        \"send_size\": true,\r\n        \"js_modal_lightbox\": true,\r\n        \"js_modal_lightbox_initially_zoomed\": true,\r\n        \"js_modal_lightbox_gamepad\": false,\r\n        \"js_modal_lightbox_gamepad_repeat\": 250,\r\n        \"show_progress_in_title\": true,\r\n        \"samplers_in_dropdown\": true,\r\n        \"dimensions_and_batch_together\": true,\r\n        \"keyedit_precision_attention\": 0.1,\r\n        \"keyedit_precision_extra\": 0.05,\r\n        \"keyedit_delimiters\": \".,\\\\/!?%^*;:{}=`~()\",\r\n        \"keyedit_move\": true,\r\n        \"quicksettings_list\": [\r\n            \"sd_model_checkpoint\",\r\n            \"sd_vae\",\r\n            \"CLIP_stop_at_last_layers\",\r\n            \"sd_unet\"\r\n        ],\r\n        \"ui_tab_order\": [],\r\n        \"hidden_tabs\": [],\r\n        \"ui_reorder_list\": [],\r\n        \"hires_fix_show_sampler\": false,\r\n        \"hires_fix_show_prompts\": true,\r\n        \"disable_token_counters\": false,\r\n        \"add_model_hash_to_info\": true,\r\n        \"add_model_name_to_info\": true,\r\n        \"add_user_name_to_info\": false,\r\n        \"add_version_to_infotext\": true,\r\n        \"disable_weights_auto_swap\": true,\r\n        \"infotext_styles\": \"Apply if any\",\r\n        \"show_progressbar\": true,\r\n        \"live_previews_enable\": true,\r\n        \"live_previews_image_format\": \"webp\",\r\n        \"show_progress_grid\": true,\r\n        \"show_progress_every_n_steps\": 10,\r\n        \"show_progress_type\": \"Full\",\r\n        \"live_preview_content\": \"Prompt\",\r\n        \"live_preview_refresh_period\": 1000,\r\n        \"hide_samplers\": [],\r\n        \"eta_ddim\": 0.0,\r\n        \"eta_ancestral\": 1.0,\r\n        \"ddim_discretize\": \"uniform\",\r\n        \"s_churn\": 0.0,\r\n        \"s_tmin\": 0.0,\r\n        \"s_noise\": 1.0,\r\n        \"k_sched_type\": \"Automatic\",\r\n        \"sigma_min\": 0.0,\r\n        \"sigma_max\": 0.0,\r\n        \"rho\": 0.0,\r\n        \"eta_noise_seed_delta\": 0,\r\n        \"always_discard_next_to_last_sigma\": false,\r\n        \"uni_pc_variant\": \"bh1\",\r\n        \"uni_pc_skip_type\": \"time_uniform\",\r\n        \"uni_pc_order\": 3,\r\n        \"uni_pc_lower_order_final\": true,\r\n        \"postprocessing_enable_in_main_ui\": [],\r\n        \"postprocessing_operation_order\": [],\r\n        \"upscaling_max_images_in_cache\": 5,\r\n        \"disabled_extensions\": [\r\n            \"CloneCleaner\",\r\n            \"CloneCleaner - Kopie\",\r\n            \"batch-face-swap\",\r\n            \"sd-webui-faceswaplab\",\r\n            \"sd-webui-sdxl-refiner-hack\"\r\n        ],\r\n        \"disable_all_extensions\": \"none\",\r\n        \"restore_config_state_file\": \"\",\r\n        \"sd_checkpoint_hash\": \"74dda471cc0a288d4b672bda0316dd2ce8d46878363b0203f1591c12e0c03e01\",\r\n        \"ldsr_steps\": 100,\r\n        \"ldsr_cached\": false,\r\n        \"SCUNET_tile\": 256,\r\n        \"SCUNET_tile_overlap\": 8,\r\n        \"SWIN_tile\": 192,\r\n        \"SWIN_tile_overlap\": 8,\r\n        \"lora_functional\": false,\r\n        \"sd_lora\": \"None\",\r\n        \"lora_preferred_name\": \"Alias from file\",\r\n        \"lora_add_hashes_to_infotext\": true,\r\n        \"lora_show_all\": false,\r\n        \"lora_hide_unknown_for_versions\": [],\r\n        \"extra_options\": [],\r\n        \"extra_options_accordion\": false,\r\n        \"canvas_hotkey_zoom\": \"Alt\",\r\n        \"canvas_hotkey_adjust\": \"Ctrl\",\r\n        \"canvas_hotkey_move\": \"F\",\r\n        \"canvas_hotkey_fullscreen\": \"S\",\r\n        \"canvas_hotkey_reset\": \"R\",\r\n        \"canvas_hotkey_overlap\": \"O\",\r\n        \"canvas_show_tooltip\": true,\r\n        \"canvas_blur_prompt\": false,\r\n        \"canvas_disabled_functions\": [\r\n            \"Overlap\"\r\n        ],\r\n        \"sdxl_base_model\": \"sdxl\\\\sd_xl_base_1.0.safetensors\",\r\n        \"sdxl_refiner_model\": \"sdxl\\\\sd_xl_refiner_1.0.safetensors\",\r\n        \"sd_vae_overrides_per_model_preferences\": false,\r\n        \"save_incomplete_images\": false,\r\n        \"face_restoration\": false,\r\n        \"auto_launch_browser\": \"Local\",\r\n        \"show_gradio_deprecation_warnings\": true,\r\n        \"hide_ldm_prints\": true,\r\n        \"api_enable_requests\": true,\r\n        \"api_forbid_local_requests\": true,\r\n        \"api_useragent\": \"\",\r\n        \"sd_checkpoints_limit\": 1,\r\n        \"sd_checkpoints_keep_in_cpu\": true,\r\n        \"tiling\": false,\r\n        \"hires_fix_refiner_pass\": \"second pass\",\r\n        \"sd_vae_encode_method\": \"Full\",\r\n        \"sd_vae_decode_method\": \"Full\",\r\n        \"img2img_extra_noise\": 0,\r\n        \"img2img_sketch_default_brush_color\": \"#ffffff\",\r\n        \"img2img_inpaint_mask_brush_color\": \"#ffffff\",\r\n        \"img2img_inpaint_sketch_default_brush_color\": \"#ffffff\",\r\n        \"persistent_cond_cache\": true,\r\n        \"batch_cond_uncond\": true,\r\n        \"use_old_scheduling\": false,\r\n        \"lora_in_memory_limit\": 0,\r\n        \"gradio_themes_cache\": true,\r\n        \"gallery_height\": \"\",\r\n        \"extra_options_txt2img\": [],\r\n        \"extra_options_img2img\": [],\r\n        \"extra_options_cols\": 1,\r\n        \"live_preview_allow_lowvram_full\": false,\r\n        \"live_preview_fast_interrupt\": false,\r\n        \"s_tmax\": 0,\r\n        \"sgm_noise_multiplier\": false,\r\n        \"canvas_auto_expand\": true,\r\n        \"promptRegex\": \"{prompt}\",\r\n        \"batchCountRegex\": \"\\\\{\\\\{count:[0-9]+\\\\}\\\\}\",\r\n        \"clipSkipRegex\": \"\\\\{\\\\{clip_skip:[0-9]+\\\\}\\\\}\",\r\n        \"negPromptRegex\": \"\\\\{\\\\{neg:(.*?)\\\\}\\\\}\",\r\n        \"ch_open_url_with_js\": true,\r\n        \"ch_always_display\": true,\r\n        \"ch_show_btn_on_thumb\": true,\r\n        \"ch_max_size_preview\": true,\r\n        \"ch_skip_nsfw_preview\": false,\r\n        \"ch_dl_webui_metadata\": true,\r\n        \"ch_proxy\": \"\",\r\n        \"styles_ui\": \"select-list\",\r\n        \"enable_styleselector_by_default\": true,\r\n        \"prompt_database_path\": \"prompt_tree.yml\",\r\n        \"enable_clonecleaner_by_default\": true,\r\n        \"ad_max_models\": 2,\r\n        \"ad_save_previews\": false,\r\n        \"ad_save_images_before\": false,\r\n        \"ad_only_seleted_scripts\": true,\r\n        \"ad_script_names\": \"dynamic_prompting,dynamic_thresholding,wildcard_recursive,wildcards,lora_block_weight\",\r\n        \"ad_bbox_sortby\": \"None\",\r\n        \"control_net_detectedmap_dir\": \"detected_maps\",\r\n        \"control_net_models_path\": \"Y:\\\\stable_difiusion\\\\controll-net-models\",\r\n        \"control_net_modules_path\": \"\",\r\n        \"control_net_unit_count\": 3,\r\n        \"control_net_model_cache_size\": 1,\r\n        \"control_net_inpaint_blur_sigma\": 7,\r\n        \"control_net_no_high_res_fix\": true,\r\n        \"control_net_no_detectmap\": false,\r\n        \"control_net_detectmap_autosaving\": false,\r\n        \"control_net_allow_script_control\": true,\r\n        \"control_net_sync_field_args\": true,\r\n        \"controlnet_show_batch_images_in_ui\": false,\r\n        \"controlnet_increment_seed_during_batch\": false,\r\n        \"controlnet_disable_control_type\": false,\r\n        \"controlnet_disable_openpose_edit\": false,\r\n        \"controlnet_ignore_noninpaint_mask\": true,\r\n        \"image_browser_active_tabs\": \"txt2img, img2img, txt2img-grids, img2img-grids, Extras, Favorites, Others, All, Maintenance\",\r\n        \"image_browser_hidden_components\": [],\r\n        \"image_browser_with_subdirs\": true,\r\n        \"image_browser_preload\": false,\r\n        \"image_browser_copy_image\": false,\r\n        \"image_browser_delete_message\": true,\r\n        \"image_browser_txt_files\": true,\r\n        \"image_browser_debug_level\": \"0 - none\",\r\n        \"image_browser_delete_recycle\": true,\r\n        \"image_browser_scan_exif\": true,\r\n        \"image_browser_mod_shift\": false,\r\n        \"image_browser_mod_ctrl_shift\": false,\r\n        \"image_browser_ranking_pnginfo\": false,\r\n        \"image_browser_page_columns\": 6.0,\r\n        \"image_browser_page_rows\": 6.0,\r\n        \"image_browser_pages_perload\": 20.0,\r\n        \"image_browser_height_auto\": false,\r\n        \"image_browser_use_thumbnail\": false,\r\n        \"image_browser_thumbnail_size\": 200.0,\r\n        \"image_browser_swipe\": false,\r\n        \"image_browser_img_tooltips\": true,\r\n        \"image_browser_show_progress\": true,\r\n        \"image_browser_info_add\": false,\r\n        \"tagger_out_filename_fmt\": \"[name].[output_extension]\",\r\n        \"tagger_count_threshold\": 100,\r\n        \"tagger_batch_recursive\": true,\r\n        \"tagger_auto_serde_json\": true,\r\n        \"tagger_store_images\": false,\r\n        \"tagger_weighted_tags_files\": false,\r\n        \"tagger_verbose\": false,\r\n        \"tagger_repl_us\": true,\r\n        \"tagger_repl_us_excl\": \"0_0, (o)_(o), +_+, +_-, ._., <o>_<o>, <|>_<|>, =_=, >_<, 3_3, 6_9, >_o, @_@, ^_^, o_o, u_u, x_x, |_|, ||_||\",\r\n        \"tagger_escape\": false,\r\n        \"tagger_batch_size\": 1024.0,\r\n        \"tagger_hf_cache_dir\": \"Y:\\\\stable-diffusion-webui\\\\models\\\\interrogators\",\r\n        \"dp_ignore_whitespace\": false,\r\n        \"dp_write_raw_template\": true,\r\n        \"dp_write_prompts_to_file\": false,\r\n        \"dp_parser_variant_start\": \"{\",\r\n        \"dp_parser_variant_end\": \"}\",\r\n        \"dp_parser_wildcard_wrap\": \"__\",\r\n        \"dp_limit_jinja_prompts\": false,\r\n        \"dp_auto_purge_cache\": false,\r\n        \"dp_wildcard_manager_no_dedupe\": false,\r\n        \"dp_wildcard_manager_no_sort\": false,\r\n        \"dp_wildcard_manager_shuffle\": false,\r\n        \"dp_magicprompt_default_model\": \"Gustavosta/MagicPrompt-Stable-Diffusion\",\r\n        \"dp_magicprompt_batch_size\": 1,\r\n        \"civitai_link_key\": \"600cbeee8abb2a152e3fa4166c3c12b118476f326bded6bf8eaa04016882340bc6a575ae9927ad946db547a992481bf93c010c7bd981916e5c55d4d5b5f8673d\",\r\n        \"civitai_link_logging\": true,\r\n        \"civitai_api_key\": \"\",\r\n        \"civitai_download_previews\": true,\r\n        \"civitai_download_triggers\": true,\r\n        \"civitai_nsfw_previews\": true,\r\n        \"civitai_download_missing_models\": false,\r\n        \"civitai_hashify_resources\": false,\r\n        \"civitai_folder_model\": \"\",\r\n        \"civitai_folder_lora\": \"\",\r\n        \"civitai_folder_lyco\": \"\",\r\n        \"styleRegex\": \"\\\\{\\\\{style:(.*?)\\\\}\\\\}\",\r\n        \"ch_hide_buttons\": [\r\n            \"add_trigger_words_button\"\r\n        ],\r\n        \"ch_nsfw_preview_threshold\": \"Allow All\",\r\n        \"ch_use_sdwebui_sha256\": false\r\n    },\r\n    \"Startup\": {\r\n        \"total\": 33.19213247299194,\r\n        \"records\": {\r\n            \"initial startup\": 0.0,\r\n            \"prepare environment/checks\": 0.0,\r\n            \"prepare environment/git version info\": 0.04684615135192871,\r\n            \"prepare environment/torch GPU test\": 1.6322832107543945,\r\n            \"prepare environment/clone repositores\": 0.13394856452941895,\r\n            \"prepare environment/run extensions installers/--sd-webui-ar-plus\": 0.0,\r\n            \"prepare environment/run extensions installers/a-sd-dynamic-prompts\": 0.15370750427246094,\r\n            \"prepare environment/run extensions installers/adetailer\": 0.11597299575805664,\r\n            \"prepare environment/run extensions installers/DWPose\": 0.0,\r\n            \"prepare environment/run extensions installers/LoraTester\": 0.0,\r\n            \"prepare environment/run extensions installers/multidiffusion-upscaler-for-automatic1111\": 0.0,\r\n            \"prepare environment/run extensions installers/SD-WebUI-BatchCheckpointPrompt\": 0.0,\r\n            \"prepare environment/run extensions installers/sd-webui-controlnet\": 3.7403218746185303,\r\n            \"prepare environment/run extensions installers/sd-webui-roop\": 9.727271556854248,\r\n            \"prepare environment/run extensions installers/Stable-Diffusion-Webui-Civitai-Helper\": 0.0,\r\n            \"prepare environment/run extensions installers/stable-diffusion-webui-images-browser\": 0.09660506248474121,\r\n            \"prepare environment/run extensions installers/stable-diffusion-webui-wd14-tagger\": 2.630730152130127,\r\n            \"prepare environment/run extensions installers/StyleSelectorXL\": 0.0,\r\n            \"prepare environment/run extensions installers/ultimate-upscale-for-automatic1111\": 0.0,\r\n            \"prepare environment/run extensions installers\": 16.464609146118164,\r\n            \"prepare environment\": 18.324591159820557,\r\n            \"launcher\": 0.0,\r\n            \"import torch\": 5.275165557861328,\r\n            \"import gradio\": 0.6702303886413574,\r\n            \"setup paths\": 0.6705753803253174,\r\n            \"import ldm\": 0.0,\r\n            \"import sgm\": 0.0,\r\n            \"initialize shared\": 0.20056796073913574,\r\n            \"other imports\": 0.5286440849304199,\r\n            \"opts onchange\": 0.0,\r\n            \"setup SD model\": 0.0,\r\n            \"setup codeformer\": 0.10487008094787598,\r\n            \"setup gfpgan\": 0.03128409385681152,\r\n            \"set samplers\": 0.0,\r\n            \"list extensions\": 0.0,\r\n            \"restore config state file\": 0.0,\r\n            \"list SD models\": 0.06757664680480957,\r\n            \"list localizations\": 0.0,\r\n            \"load scripts/custom_code.py\": 0.015624284744262695,\r\n            \"load scripts/img2imgalt.py\": 0.0,\r\n            \"load scripts/loopback.py\": 0.0,\r\n            \"load scripts/outpainting_mk_2.py\": 0.0,\r\n            \"load scripts/poor_mans_outpainting.py\": 0.0,\r\n            \"load scripts/postprocessing_codeformer.py\": 0.0,\r\n            \"load scripts/postprocessing_gfpgan.py\": 0.0,\r\n            \"load scripts/postprocessing_upscale.py\": 0.0,\r\n            \"load scripts/prompt_matrix.py\": 0.0,\r\n            \"load scripts/prompts_from_file.py\": 0.0,\r\n            \"load scripts/refiner.py\": 0.0,\r\n            \"load scripts/sd_upscale.py\": 0.0,\r\n            \"load scripts/seed.py\": 0.0,\r\n            \"load scripts/xyz_grid.py\": 0.0,\r\n            \"load scripts/sd-webui-ar.py\": 0.0,\r\n            \"load scripts/batchCheckpointsPrompt.py\": 0.0156252384185791,\r\n            \"load scripts/BatchParams.py\": 0.0,\r\n            \"load scripts/CivitaihelperPrompts.py\": 0.0,\r\n            \"load scripts/Logger.py\": 0.0,\r\n            \"load scripts/Save.py\": 0.0,\r\n            \"load scripts/Utils.py\": 0.0,\r\n            \"load scripts/settings.py\": 1.016012191772461,\r\n            \"load scripts/civitai_helper.py\": 0.04943561553955078,\r\n            \"load scripts/StyleSelectorXL.py\": 0.06249713897705078,\r\n            \"load scripts/dynamic_prompting.py\": 0.03782081604003906,\r\n            \"load scripts/!adetailer.py\": 2.0369460582733154,\r\n            \"load scripts/tilediffusion.py\": 0.0,\r\n            \"load scripts/tilevae.py\": 0.0,\r\n            \"load scripts/adapter.py\": 0.0,\r\n            \"load scripts/api.py\": 0.5474202632904053,\r\n            \"load scripts/batch_hijack.py\": 0.0,\r\n            \"load scripts/cldm.py\": 0.0,\r\n            \"load scripts/controlmodel_ipadapter.py\": 0.0,\r\n            \"load scripts/controlnet.py\": 0.16927409172058105,\r\n            \"load scripts/controlnet_diffusers.py\": 0.0,\r\n            \"load scripts/controlnet_lllite.py\": 0.0,\r\n            \"load scripts/controlnet_lora.py\": 0.0,\r\n            \"load scripts/controlnet_model_guess.py\": 0.0,\r\n            \"load scripts/controlnet_version.py\": 0.0,\r\n            \"load scripts/external_code.py\": 0.0,\r\n            \"load scripts/global_state.py\": 0.0,\r\n            \"load scripts/hook.py\": 0.0,\r\n            \"load scripts/infotext.py\": 0.0,\r\n            \"load scripts/logging.py\": 0.0,\r\n            \"load scripts/lvminthin.py\": 0.0,\r\n            \"load scripts/movie2movie.py\": 0.0,\r\n            \"load scripts/processor.py\": 0.0,\r\n            \"load scripts/utils.py\": 0.0,\r\n            \"load scripts/xyz_grid_support.py\": 0.0,\r\n            \"load scripts/cimage.py\": 0.0,\r\n            \"load scripts/faceswap.py\": 0.0,\r\n            \"load scripts/roop_logging.py\": 0.015624284744262695,\r\n            \"load scripts/roop_version.py\": 0.0,\r\n            \"load scripts/swapper.py\": 0.0,\r\n            \"load scripts/image_browser.py\": 0.05338907241821289,\r\n            \"load scripts/tagger.py\": 0.10026717185974121,\r\n            \"load scripts/ultimate-upscale.py\": 0.0,\r\n            \"load scripts/ldsr_model.py\": 0.04687380790710449,\r\n            \"load scripts/lora_script.py\": 0.20015931129455566,\r\n            \"load scripts/scunet_model.py\": 0.022171735763549805,\r\n            \"load scripts/swinir_model.py\": 0.031249284744262695,\r\n            \"load scripts/hotkey_config.py\": 0.015598058700561523,\r\n            \"load scripts/extra_options_section.py\": 0.0,\r\n            \"load scripts\": 4.435988426208496,\r\n            \"load upscalers\": 0.0,\r\n            \"refresh VAE\": 0.0,\r\n            \"refresh textual inversion templates\": 0.0,\r\n            \"scripts list_optimizers\": 0.0,\r\n            \"scripts list_unets\": 0.0,\r\n            \"reload hypernetworks\": 0.0,\r\n            \"initialize extra networks\": 0.01599431037902832,\r\n            \"scripts before_ui_callback\": 0.0,\r\n            \"create ui\": 2.5042202472686768,\r\n            \"gradio launch\": 0.4093282222747803,\r\n            \"add APIs\": 0.0,\r\n            \"app_started_callback/api.py\": 0.0,\r\n            \"app_started_callback/tagger.py\": 0.0,\r\n            \"app_started_callback/lora_script.py\": 0.0,\r\n            \"app_started_callback\": 0.0\r\n        }\r\n    },\r\n    \"Packages\": [\r\n        \"-rotobuf==3.20.0\",\r\n        \"absl-py==1.4.0\",\r\n        \"accelerate==0.21.0\",\r\n        \"addict==2.4.0\",\r\n        \"aenum==3.1.15\",\r\n        \"aiofiles==23.1.0\",\r\n        \"aiohttp==3.8.5\",\r\n        \"aiosignal==1.3.1\",\r\n        \"albumentations==1.3.1\",\r\n        \"altair==5.0.1\",\r\n        \"antlr4-python3-runtime==4.9.3\",\r\n        \"anyio==3.7.1\",\r\n        \"astunparse==1.6.3\",\r\n        \"async-timeout==4.0.2\",\r\n        \"attrs==23.1.0\",\r\n        \"basicsr==1.4.2\",\r\n        \"beautifulsoup4==4.12.2\",\r\n        \"bidict==0.22.1\",\r\n        \"blendmodes==2022\",\r\n        \"boltons==23.0.0\",\r\n        \"cachetools==5.3.1\",\r\n        \"certifi==2023.7.22\",\r\n        \"cffi==1.15.1\",\r\n        \"charset-normalizer==3.2.0\",\r\n        \"clean-fid==0.1.35\",\r\n        \"click==8.1.6\",\r\n        \"clip==1.0\",\r\n        \"colorama==0.4.6\",\r\n        \"colored==2.2.3\",\r\n        \"coloredlogs==15.0.1\",\r\n        \"contourpy==1.1.0\",\r\n        \"cssselect2==0.7.0\",\r\n        \"cycler==0.11.0\",\r\n        \"cython==3.0.2\",\r\n        \"deepdanbooru==1.0.2\",\r\n        \"deprecation==2.1.0\",\r\n        \"dynamicprompts==0.29.0\",\r\n        \"easydict==1.10\",\r\n        \"einops==0.4.1\",\r\n        \"exceptiongroup==1.1.2\",\r\n        \"facexlib==0.3.0\",\r\n        \"fastapi==0.94.0\",\r\n        \"ffmpy==0.3.1\",\r\n        \"filelock==3.12.2\",\r\n        \"filterpy==1.4.5\",\r\n        \"flatbuffers==23.5.26\",\r\n        \"fonttools==4.41.1\",\r\n        \"frozenlist==1.4.0\",\r\n        \"fsspec==2023.6.0\",\r\n        \"ftfy==6.1.1\",\r\n        \"future==0.18.3\",\r\n        \"fvcore==0.1.5.post20221221\",\r\n        \"gast==0.5.4\",\r\n        \"gdown==4.7.1\",\r\n        \"gfpgan==1.3.8\",\r\n        \"gitdb==4.0.10\",\r\n        \"gitpython==3.1.32\",\r\n        \"google-auth-oauthlib==1.0.0\",\r\n        \"google-auth==2.22.0\",\r\n        \"google-pasta==0.2.0\",\r\n        \"gradio-client==0.5.0\",\r\n        \"gradio==3.41.2\",\r\n        \"grpcio==1.56.2\",\r\n        \"h11==0.12.0\",\r\n        \"h5py==3.9.0\",\r\n        \"httpcore==0.15.0\",\r\n        \"httpx==0.24.1\",\r\n        \"huggingface-hub==0.16.4\",\r\n        \"humanfriendly==10.0\",\r\n        \"idna==3.4\",\r\n        \"ifnude==0.0.3\",\r\n        \"imageio==2.31.1\",\r\n        \"importlib-metadata==6.8.0\",\r\n        \"importlib-resources==6.1.0\",\r\n        \"inflection==0.5.1\",\r\n        \"insightface==0.7.3\",\r\n        \"iopath==0.1.9\",\r\n        \"jinja2==3.1.2\",\r\n        \"joblib==1.3.2\",\r\n        \"jsonmerge==1.8.0\",\r\n        \"jsonschema-specifications==2023.7.1\",\r\n        \"jsonschema==4.18.4\",\r\n        \"keras==2.14.0\",\r\n        \"kiwisolver==1.4.4\",\r\n        \"kornia==0.6.7\",\r\n        \"lark==1.1.2\",\r\n        \"lazy-loader==0.3\",\r\n        \"libclang==16.0.6\",\r\n        \"lightning-utilities==0.9.0\",\r\n        \"linkify-it-py==2.0.2\",\r\n        \"llvmlite==0.40.1\",\r\n        \"lmdb==1.4.1\",\r\n        \"lpips==0.1.4\",\r\n        \"lxml==4.9.3\",\r\n        \"markdown-it-py==2.2.0\",\r\n        \"markdown==3.4.4\",\r\n        \"markupsafe==2.1.3\",\r\n        \"matplotlib==3.7.2\",\r\n        \"mdit-py-plugins==0.3.3\",\r\n        \"mdurl==0.1.2\",\r\n        \"mediapipe==0.10.7\",\r\n        \"ml-dtypes==0.2.0\",\r\n        \"mpmath==1.3.0\",\r\n        \"multidict==6.0.4\",\r\n        \"networkx==3.1\",\r\n        \"numba==0.57.1\",\r\n        \"numpy==1.23.5\",\r\n        \"nvidia-cublas-cu11==11.11.3.6\",\r\n        \"nvidia-cuda-nvrtc-cu11==11.8.89\",\r\n        \"nvidia-cuda-runtime-cu11==11.8.89\",\r\n        \"nvidia-cudnn-cu11==8.9.4.25\",\r\n        \"oauthlib==3.2.2\",\r\n        \"omegaconf==2.2.3\",\r\n        \"onnx-graphsurgeon==0.3.27\",\r\n        \"onnx==1.14.0\",\r\n        \"onnxruntime-gpu==1.16.0\",\r\n        \"onnxruntime==1.15.0\",\r\n        \"open-clip-torch==2.20.0\",\r\n        \"opencv-contrib-python==4.8.0.76\",\r\n        \"opencv-python-headless==4.8.1.78\",\r\n        \"opencv-python==4.7.0.72\",\r\n        \"opt-einsum==3.3.0\",\r\n        \"orjson==3.9.2\",\r\n        \"packaging==23.1\",\r\n        \"pandas==2.0.3\",\r\n        \"piexif==1.1.3\",\r\n        \"pillow==9.5.0\",\r\n        \"pip==23.3.1\",\r\n        \"platformdirs==3.9.1\",\r\n        \"polygraphy==0.49.0\",\r\n        \"portalocker==2.8.2\",\r\n        \"prettytable==3.9.0\",\r\n        \"protobuf==3.20.3\",\r\n        \"psutil==5.9.5\",\r\n        \"py-cpuinfo==9.0.0\",\r\n        \"pyasn1-modules==0.3.0\",\r\n        \"pyasn1==0.5.0\",\r\n        \"pycparser==2.21\",\r\n        \"pydantic==1.10.12\",\r\n        \"pydub==0.25.1\",\r\n        \"pygments==2.15.1\",\r\n        \"pyparsing==3.0.9\",\r\n        \"pyreadline3==3.4.1\",\r\n        \"pysocks==1.7.1\",\r\n        \"python-dateutil==2.8.2\",\r\n        \"python-engineio==4.7.1\",\r\n        \"python-multipart==0.0.6\",\r\n        \"python-socketio==5.7.2\",\r\n        \"pytorch-lightning==1.9.4\",\r\n        \"pytz==2023.3\",\r\n        \"pywavelets==1.4.1\",\r\n        \"pywin32==306\",\r\n        \"pyyaml==6.0.1\",\r\n        \"qudida==0.0.4\",\r\n        \"realesrgan==0.3.0\",\r\n        \"referencing==0.30.0\",\r\n        \"regex==2023.6.3\",\r\n        \"reportlab==4.0.5\",\r\n        \"requests-oauthlib==1.3.1\",\r\n        \"requests==2.31.0\",\r\n        \"resize-right==0.0.2\",\r\n        \"rich==13.5.3\",\r\n        \"rpds-py==0.9.2\",\r\n        \"rsa==4.9\",\r\n        \"safetensors==0.3.1\",\r\n        \"scikit-image==0.21.0\",\r\n        \"scikit-learn==1.3.1\",\r\n        \"scipy==1.11.1\",\r\n        \"seaborn==0.12.2\",\r\n        \"semantic-version==2.10.0\",\r\n        \"send2trash==1.8.2\",\r\n        \"sentencepiece==0.1.99\",\r\n        \"setuptools==65.5.0\",\r\n        \"simple-websocket==1.0.0\",\r\n        \"six==1.16.0\",\r\n        \"smmap==5.0.0\",\r\n        \"sniffio==1.3.0\",\r\n        \"sounddevice==0.4.6\",\r\n        \"soupsieve==2.4.1\",\r\n        \"starlette==0.26.1\",\r\n        \"svglib==1.5.1\",\r\n        \"sympy==1.12\",\r\n        \"tabulate==0.9.0\",\r\n        \"tb-nightly==2.14.0a20230727\",\r\n        \"tensorboard-data-server==0.7.1\",\r\n        \"tensorboard==2.14.1\",\r\n        \"tensorflow-estimator==2.14.0\",\r\n        \"tensorflow-intel==2.14.0\",\r\n        \"tensorflow-io-gcs-filesystem==0.31.0\",\r\n        \"tensorflow==2.14.0\",\r\n        \"tensorrt-bindings==9.0.1.post11.dev4\",\r\n        \"tensorrt-libs==9.0.1.post11.dev4\",\r\n        \"tensorrt==9.0.1.post11.dev4\",\r\n        \"termcolor==2.3.0\",\r\n        \"thop==0.1.1.post2209072238\",\r\n        \"threadpoolctl==3.2.0\",\r\n        \"tifffile==2023.7.18\",\r\n        \"timm==0.9.2\",\r\n        \"tinycss2==1.2.1\",\r\n        \"tokenizers==0.13.3\",\r\n        \"tomesd==0.1.3\",\r\n        \"tomli==2.0.1\",\r\n        \"toolz==0.12.0\",\r\n        \"torch==2.0.1+cu118\",\r\n        \"torchdiffeq==0.2.3\",\r\n        \"torchmetrics==1.0.1\",\r\n        \"torchsde==0.2.5\",\r\n        \"torchvision==0.15.2+cu118\",\r\n        \"tqdm==4.65.0\",\r\n        \"trampoline==0.1.2\",\r\n        \"transformers==4.30.2\",\r\n        \"typing-extensions==4.7.1\",\r\n        \"tzdata==2023.3\",\r\n        \"uc-micro-py==1.0.2\",\r\n        \"ultralytics==8.0.199\",\r\n        \"urllib3==1.26.16\",\r\n        \"uvicorn==0.23.1\",\r\n        \"wcwidth==0.2.6\",\r\n        \"webencodings==0.5.1\",\r\n        \"websocket-client==1.6.3\",\r\n        \"websockets==11.0.3\",\r\n        \"werkzeug==2.3.6\",\r\n        \"wheel==0.41.0\",\r\n        \"wrapt==1.14.1\",\r\n        \"wsproto==1.2.0\",\r\n        \"xformers==0.0.20\",\r\n        \"yacs==0.1.8\",\r\n        \"yapf==0.40.1\",\r\n        \"yarl==1.9.2\",\r\n        \"zipp==3.16.2\"\r\n    ]\r\n}\r\n\r\n\n\n### What browsers do you use to access the UI ?\n\nMozilla Firefox\n\n### Console logs\n\n```Shell\nnothing relevant\n```\n\n\n### Additional information\n\n_No response_\n", "hints_text": "This is because when you click the folder \"SFW/\" its actually doing a search for any Lora with \"SFW/\" in the name. So it found that the folder \"NSFW/\" contains \"SFW/\".\r\n\r\nI have tested it \"/SFW/\" this should show you only this one folder. So that may be the fix, instead of just one forward slash when clicking folders, there should be 2, on in front and one in back.", "created_at": "2023-11-12T14:26:53Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 13829, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-13829", "issue_numbers": ["13813"], "base_commit": "464fbcd92118bf00173b9982325fe6348201313e", "patch": "diff --git a/javascript/edit-attention.js b/javascript/edit-attention.js\nindex 04464100699..688c2f112d6 100644\n--- a/javascript/edit-attention.js\n+++ b/javascript/edit-attention.js\n@@ -28,7 +28,7 @@ function keyupEditAttention(event) {\n         if (afterParen == -1) return false;\n \n         let afterOpeningParen = after.indexOf(OPEN);\n-        if (afterOpeningParen != -1 && afterOpeningParen < beforeParen) return false;\n+        if (afterOpeningParen != -1 && afterOpeningParen < afterParen) return false;\n \n         // Set the selection to the text between the parenthesis\n         const parenContent = text.substring(beforeParen + 1, selectionStart + afterParen);\n", "test_patch": "", "problem_statement": "[Bug]: Incorrect key based prompt manipulation\n### Is there an existing issue for this?\n\n- [X] I have searched the existing issues and checked the recent builds/commits\n\n### What happened?\n\nManipulating prompts with ctrl-up/down in the dev branch has faulty behaviour.\n\n### Steps to reproduce the problem\n\nGiven\r\n\r\n`word1 word2  <lora:L1:1>  <lora:L2:1>`\r\n\r\nWith the cursor within the first LoRA, pressing ctrl-up gives\r\n\r\n`(word1 word2  <lora:1.1):L1:1>  <lora:L2:1>` with **word1 word2 <lora** selected\r\n\r\n\r\n\r\n\n\n### What should have happened?\n\n`word1 word2  <lora:L1:1.1>  <lora:L2:1>`\r\n\n\n### Sysinfo\n\n{\r\n    \"Platform\": \"Linux-5.14.0-284.30.1.el9_2.x86_64-x86_64-with-glibc2.34\",\r\n    \"Python\": \"3.10.9\",\r\n    \"Version\": \"v1.6.0-274-g8d3c88a4\",\r\n    \"Commit\": \"8d3c88a426a12a593ca34257d214a0389da4d7f5\",\r\n    \"Checksum\": \"2978b4a451cd03e5100058b62374fef917ff8f233104f8ce07256669c5a02dd3\",\r\n    \"Commandline\": [\r\n        \"launch.py\",\r\n        \"--xformers\"\r\n    ],\r\n    \"Torch env info\": {\r\n        \"torch_version\": \"2.0.1+cu118\",\r\n        \"is_debug_build\": \"False\",\r\n        \"cuda_compiled_version\": \"11.8\",\r\n        \"gcc_version\": \"(GCC) 11.3.1 20221121 (Red Hat 11.3.1-4)\",\r\n        \"clang_version\": null,\r\n        \"cmake_version\": \"version 3.26.4\",\r\n        \"os\": \"AlmaLinux release 9.2 (Turquoise Kodkod) (x86_64)\",\r\n        \"libc_version\": \"glibc-2.34\",\r\n        \"python_version\": \"3.10.9 (main, Mar  1 2023, 18:23:06) [GCC 11.2.0] (64-bit runtime)\",\r\n        \"python_platform\": \"Linux-5.14.0-284.30.1.el9_2.x86_64-x86_64-with-glibc2.34\",\r\n        \"is_cuda_available\": \"True\",\r\n        \"cuda_runtime_version\": null,\r\n        \"cuda_module_loading\": \"LAZY\",\r\n        \"nvidia_driver_version\": \"535.104.12\",\r\n        \"nvidia_gpu_models\": \"GPU 0: NVIDIA GeForce RTX 4090\",\r\n        \"cudnn_version\": null,\r\n        \"pip_version\": \"pip3\",\r\n        \"pip_packages\": [\r\n            \"numpy==1.23.5\",\r\n            \"open-clip-torch==2.20.0\",\r\n            \"pytorch-lightning==1.9.4\",\r\n            \"torch==2.0.1+cu118\",\r\n            \"torchdiffeq==0.2.3\",\r\n            \"torchmetrics==1.0.1\",\r\n            \"torchsde==0.2.6\",\r\n            \"torchvision==0.15.2+cu118\"\r\n        ],\r\n        \"conda_packages\": [\r\n            \"_tflow_select             2.3.0                       mkl  \",\r\n            \"blas                      1.0                         mkl  \",\r\n            \"cudatoolkit               11.3.1               h2bc3f7f_2  \",\r\n            \"mkl                       2021.4.0           h06a4308_640  \",\r\n            \"mkl-service               2.4.0           py310h7f8727e_0  \",\r\n            \"mkl_fft                   1.3.1           py310hd6ae3a3_0  \",\r\n            \"mkl_random                1.2.2           py310h00e6091_0  \",\r\n            \"numpy                     1.23.5          py310hd5efca6_0  \",\r\n            \"numpy-base                1.23.5          py310h8e6c178_0  \",\r\n            \"numpydoc                  1.5.0           py310h06a4308_0  \",\r\n            \"pytorch                   1.12.1          cpu_py310hb1f1ab4_1  \",\r\n            \"tensorflow                2.11.0          mkl_py310hb40ee82_0  \",\r\n            \"tensorflow-base           2.11.0          mkl_py310he5f8e37_0  \"\r\n        ],\r\n        \"hip_compiled_version\": \"N/A\",\r\n        \"hip_runtime_version\": \"N/A\",\r\n        \"miopen_runtime_version\": \"N/A\",\r\n        \"caching_allocator_config\": \"\",\r\n        \"is_xnnpack_available\": \"True\",\r\n        \"cpu_info\": [\r\n            \"Architecture:                    x86_64\",\r\n            \"CPU op-mode(s):                  32-bit, 64-bit\",\r\n            \"Address sizes:                   46 bits physical, 48 bits virtual\",\r\n            \"Byte Order:                      Little Endian\",\r\n            \"CPU(s):                          32\",\r\n            \"On-line CPU(s) list:             0-31\",\r\n            \"Vendor ID:                       GenuineIntel\",\r\n            \"Model name:                      13th Gen Intel(R) Core(TM) i9-13900K\",\r\n            \"CPU family:                      6\",\r\n            \"Model:                           183\",\r\n            \"Thread(s) per core:              2\",\r\n            \"Core(s) per socket:              24\",\r\n            \"Socket(s):                       1\",\r\n            \"Stepping:                        1\",\r\n            \"CPU max MHz:                     5800.0000\",\r\n            \"CPU min MHz:                     800.0000\",\r\n            \"BogoMIPS:                        5990.40\",\r\n            \"Flags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid rdseed adx smap clflushopt clwb intel_pt sha_ni xsaveopt xsavec xgetbv1 xsaves split_lock_detect avx_vnni dtherm ida arat pln pts hwp hwp_notify hwp_act_window hwp_epp hwp_pkg_req hfi umip pku ospke waitpkg gfni vaes vpclmulqdq tme rdpid movdiri movdir64b fsrm md_clear serialize pconfig arch_lbr ibt flush_l1d arch_capabilities\",\r\n            \"Virtualization:                  VT-x\",\r\n            \"L1d cache:                       896 KiB (24 instances)\",\r\n            \"L1i cache:                       1.3 MiB (24 instances)\",\r\n            \"L2 cache:                        32 MiB (12 instances)\",\r\n            \"L3 cache:                        36 MiB (1 instance)\",\r\n            \"NUMA node(s):                    1\",\r\n            \"NUMA node0 CPU(s):               0-31\",\r\n            \"Vulnerability Itlb multihit:     Not affected\",\r\n            \"Vulnerability L1tf:              Not affected\",\r\n            \"Vulnerability Mds:               Not affected\",\r\n            \"Vulnerability Meltdown:          Not affected\",\r\n            \"Vulnerability Mmio stale data:   Not affected\",\r\n            \"Vulnerability Retbleed:          Not affected\",\r\n            \"Vulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl\",\r\n            \"Vulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization\",\r\n            \"Vulnerability Spectre v2:        Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\",\r\n            \"Vulnerability Srbds:             Not affected\",\r\n            \"Vulnerability Tsx async abort:   Not affected\"\r\n        ]\r\n    },\r\n    \"Exceptions\": [],\r\n    \"CPU\": {\r\n        \"model\": \"x86_64\",\r\n        \"count logical\": 32,\r\n        \"count physical\": 24\r\n    },\r\n    \"RAM\": {\r\n        \"total\": \"62GB\",\r\n        \"used\": \"6GB\",\r\n        \"free\": \"42GB\",\r\n        \"active\": \"4GB\",\r\n        \"inactive\": \"15GB\",\r\n        \"buffers\": \"5MB\",\r\n        \"cached\": \"14GB\",\r\n        \"shared\": \"236MB\"\r\n    },\r\n    \"Extensions\": [\r\n        {\r\n            \"name\": \"CFG-Schedule-for-Automatic1111-SD\",\r\n            \"version\": \"84892c88\",\r\n            \"branch\": \"main\",\r\n            \"remote\": \"https://github.com/guzuligo/CFG-Schedule-for-Automatic1111-SD.git\"\r\n        },\r\n        {\r\n            \"name\": \"adetailer\",\r\n            \"version\": \"887155c8\",\r\n            \"branch\": \"main\",\r\n            \"remote\": \"https://github.com/Bing-su/adetailer.git\"\r\n        },\r\n        {\r\n            \"name\": \"clip-interrogator-ext\",\r\n            \"version\": \"0f1a4591\",\r\n            \"branch\": \"main\",\r\n            \"remote\": \"https://github.com/pharmapsychotic/clip-interrogator-ext.git\"\r\n        },\r\n        {\r\n            \"name\": \"dddetailer\",\r\n            \"version\": \"f82fe898\",\r\n            \"branch\": \"master\",\r\n            \"remote\": \"https://github.com/Bing-su/dddetailer.git\"\r\n        },\r\n        {\r\n            \"name\": \"embedding-inspector\",\r\n            \"version\": \"448b6d06\",\r\n            \"branch\": \"main\",\r\n            \"remote\": \"https://github.com/tkalayci71/embedding-inspector.git\"\r\n        },\r\n        {\r\n            \"name\": \"gigadiffusion\",\r\n            \"version\": \"e1f9fb20\",\r\n            \"branch\": \"master\",\r\n            \"remote\": \"https://github.com/jpohhhh/gigadiffusion\"\r\n        },\r\n        {\r\n            \"name\": \"loopback_scaler\",\r\n            \"version\": \"184a53e6\",\r\n            \"branch\": \"main\",\r\n            \"remote\": \"https://github.com/Elldreth/loopback_scaler.git\"\r\n        },\r\n        {\r\n            \"name\": \"model_preset_manager\",\r\n            \"version\": \"4e25eebd\",\r\n            \"branch\": \"main\",\r\n            \"remote\": \"https://github.com/rifeWithKaiju/model_preset_manager.git\"\r\n        },\r\n        {\r\n            \"name\": \"sd-dynamic-thresholding\",\r\n            \"version\": \"a046fe9c\",\r\n            \"branch\": \"master\",\r\n            \"remote\": \"https://github.com/mcmonkeyprojects/sd-dynamic-thresholding.git\"\r\n        },\r\n        {\r\n            \"name\": \"sd-webui-3d-open-pose-editor\",\r\n            \"version\": \"f2d5aac5\",\r\n            \"branch\": \"main\",\r\n            \"remote\": \"https://github.com/nonnonstop/sd-webui-3d-open-pose-editor.git\"\r\n        },\r\n        {\r\n            \"name\": \"sd-webui-controlnet\",\r\n            \"version\": \"a43e5742\",\r\n            \"branch\": \"main\",\r\n            \"remote\": \"https://github.com/Mikubill/sd-webui-controlnet.git\"\r\n        },\r\n        {\r\n            \"name\": \"sd-webui-infinite-image-browsing\",\r\n            \"version\": \"add16701\",\r\n            \"branch\": \"main\",\r\n            \"remote\": \"https://github.com/zanllp/sd-webui-infinite-image-browsing.git\"\r\n        },\r\n        {\r\n            \"name\": \"sd-webui-openpose-editor\",\r\n            \"version\": \"be6f54fa\",\r\n            \"branch\": \"main\",\r\n            \"remote\": \"https://github.com/huchenlei/sd-webui-openpose-editor.git\"\r\n        },\r\n        {\r\n            \"name\": \"sd-webui-regional-prompter\",\r\n            \"version\": \"8d96c92e\",\r\n            \"branch\": \"main\",\r\n            \"remote\": \"https://github.com/hako-mikan/sd-webui-regional-prompter.git\"\r\n        },\r\n        {\r\n            \"name\": \"sd-webui-segment-anything\",\r\n            \"version\": \"d80220ec\",\r\n            \"branch\": \"master\",\r\n            \"remote\": \"https://github.com/continue-revolution/sd-webui-segment-anything.git\"\r\n        },\r\n        {\r\n            \"name\": \"stable-diffusion-webui-sonar\",\r\n            \"version\": \"15f92859\",\r\n            \"branch\": \"main\",\r\n            \"remote\": \"https://github.com/Kahsolt/stable-diffusion-webui-sonar.git\"\r\n        },\r\n        {\r\n            \"name\": \"stable-diffusion-webui-tokenizer\",\r\n            \"version\": \"ac6d541c\",\r\n            \"branch\": \"master\",\r\n            \"remote\": \"https://github.com/AUTOMATIC1111/stable-diffusion-webui-tokenizer.git\"\r\n        },\r\n        {\r\n            \"name\": \"stable-diffusion-webui-two-shot\",\r\n            \"version\": \"6b55dd52\",\r\n            \"branch\": \"main\",\r\n            \"remote\": \"https://github.com/ashen-sensored/stable-diffusion-webui-two-shot.git\"\r\n        },\r\n        {\r\n            \"name\": \"stable-diffusion-webui-zoomimage\",\r\n            \"version\": \"c8919a49\",\r\n            \"branch\": \"main\",\r\n            \"remote\": \"https://github.com/viyiviyi/stable-diffusion-webui-zoomimage.git\"\r\n        },\r\n        {\r\n            \"name\": \"ultimate-upscale-for-automatic1111\",\r\n            \"version\": \"728ffcec\",\r\n            \"branch\": \"master\",\r\n            \"remote\": \"https://github.com/Coyote-A/ultimate-upscale-for-automatic1111.git\"\r\n        }\r\n    ],\r\n    \"Inactive extensions\": [],\r\n    \"Environment\": {\r\n        \"COMMANDLINE_ARGS\": \"--xformers\",\r\n        \"GIT\": \"git\",\r\n        \"GRADIO_ANALYTICS_ENABLED\": \"False\"\r\n    },\r\n    \"Config\": {\r\n        \"samples_save\": true,\r\n        \"samples_format\": \"png\",\r\n        \"samples_filename_pattern\": \"\",\r\n        \"save_images_add_number\": true,\r\n        \"grid_save\": true,\r\n        \"grid_format\": \"png\",\r\n        \"grid_extended_filename\": false,\r\n        \"grid_only_if_multiple\": true,\r\n        \"grid_prevent_empty_spots\": false,\r\n        \"grid_zip_filename_pattern\": \"\",\r\n        \"n_rows\": -1,\r\n        \"enable_pnginfo\": true,\r\n        \"save_txt\": false,\r\n        \"save_images_before_face_restoration\": false,\r\n        \"save_images_before_highres_fix\": false,\r\n        \"save_images_before_color_correction\": false,\r\n        \"save_mask\": false,\r\n        \"save_mask_composite\": false,\r\n        \"jpeg_quality\": 95,\r\n        \"webp_lossless\": false,\r\n        \"export_for_4chan\": true,\r\n        \"img_downscale_threshold\": 8.0,\r\n        \"target_side_length\": 6000.0,\r\n        \"img_max_size_mp\": 200,\r\n        \"use_original_name_batch\": true,\r\n        \"use_upscaler_name_as_suffix\": false,\r\n        \"save_selected_only\": true,\r\n        \"save_init_img\": false,\r\n        \"temp_dir\": \"\",\r\n        \"clean_temp_dir_at_start\": false,\r\n        \"outdir_samples\": \"\",\r\n        \"outdir_txt2img_samples\": \"outputs/txt2img-images\",\r\n        \"outdir_img2img_samples\": \"outputs/img2img-images\",\r\n        \"outdir_extras_samples\": \"outputs/extras-images\",\r\n        \"outdir_grids\": \"\",\r\n        \"outdir_txt2img_grids\": \"outputs/txt2img-grids\",\r\n        \"outdir_img2img_grids\": \"outputs/img2img-grids\",\r\n        \"outdir_save\": \"log/images\",\r\n        \"outdir_init_images\": \"outputs/init-images\",\r\n        \"save_to_dirs\": true,\r\n        \"grid_save_to_dirs\": true,\r\n        \"use_save_to_dirs_for_ui\": false,\r\n        \"directories_filename_pattern\": \"[date]\",\r\n        \"directories_max_prompt_words\": 8,\r\n        \"ESRGAN_tile\": 192,\r\n        \"ESRGAN_tile_overlap\": 8,\r\n        \"realesrgan_enabled_models\": [\r\n            \"R-ESRGAN 4x+\",\r\n            \"R-ESRGAN General WDN 4xV3\"\r\n        ],\r\n        \"upscaler_for_img2img\": null,\r\n        \"face_restoration_model\": \"CodeFormer\",\r\n        \"code_former_weight\": 0.5,\r\n        \"face_restoration_unload\": false,\r\n        \"show_warnings\": false,\r\n        \"memmon_poll_rate\": 8,\r\n        \"samples_log_stdout\": false,\r\n        \"multiple_tqdm\": false,\r\n        \"print_hypernet_extra\": false,\r\n        \"list_hidden_files\": true,\r\n        \"unload_models_when_training\": false,\r\n        \"pin_memory\": false,\r\n        \"save_optimizer_state\": false,\r\n        \"save_training_settings_to_txt\": true,\r\n        \"dataset_filename_word_regex\": \"\",\r\n        \"dataset_filename_join_string\": \" \",\r\n        \"training_image_repeats_per_epoch\": 1,\r\n        \"training_write_csv_every\": 500,\r\n        \"training_xattention_optimizations\": false,\r\n        \"training_enable_tensorboard\": false,\r\n        \"training_tensorboard_save_images\": false,\r\n        \"training_tensorboard_flush_every\": 120,\r\n        \"sd_model_checkpoint\": \"\",\r\n        \"sd_checkpoint_cache\": 1,\r\n        \"sd_vae_checkpoint_cache\": 1,\r\n        \"sd_vae\": \"vae-ft-mse-840000-ema-pruned.safetensors\",\r\n        \"sd_vae_as_default\": true,\r\n        \"sd_unet\": \"Automatic\",\r\n        \"inpainting_mask_weight\": 1.0,\r\n        \"initial_noise_multiplier\": 1.0,\r\n        \"img2img_color_correction\": true,\r\n        \"img2img_fix_steps\": false,\r\n        \"img2img_background_color\": \"#ffffff\",\r\n        \"enable_quantization\": false,\r\n        \"enable_emphasis\": true,\r\n        \"enable_batch_seeds\": true,\r\n        \"comma_padding_backtrack\": 20,\r\n        \"CLIP_stop_at_last_layers\": 1,\r\n        \"upcast_attn\": false,\r\n        \"randn_source\": \"CPU\",\r\n        \"cross_attention_optimization\": \"Automatic\",\r\n        \"s_min_uncond\": 0.0,\r\n        \"token_merging_ratio\": 0.0,\r\n        \"token_merging_ratio_img2img\": 0.0,\r\n        \"token_merging_ratio_hr\": 0.0,\r\n        \"pad_cond_uncond\": false,\r\n        \"experimental_persistent_cond_cache\": false,\r\n        \"use_old_emphasis_implementation\": false,\r\n        \"use_old_karras_scheduler_sigmas\": false,\r\n        \"no_dpmpp_sde_batch_determinism\": false,\r\n        \"use_old_hires_fix_width_height\": false,\r\n        \"dont_fix_second_order_samplers_schedule\": false,\r\n        \"hires_fix_use_firstpass_conds\": false,\r\n        \"interrogate_keep_models_in_memory\": false,\r\n        \"interrogate_return_ranks\": false,\r\n        \"interrogate_clip_num_beams\": 1,\r\n        \"interrogate_clip_min_length\": 24,\r\n        \"interrogate_clip_max_length\": 48,\r\n        \"interrogate_clip_dict_limit\": 1500,\r\n        \"interrogate_clip_skip_categories\": [],\r\n        \"interrogate_deepbooru_score_threshold\": 0.5,\r\n        \"deepbooru_sort_alpha\": true,\r\n        \"deepbooru_use_spaces\": true,\r\n        \"deepbooru_escape\": true,\r\n        \"deepbooru_filter_tags\": \"\",\r\n        \"extra_networks_show_hidden_directories\": true,\r\n        \"extra_networks_hidden_models\": \"When searched\",\r\n        \"extra_networks_default_view\": \"cards\",\r\n        \"extra_networks_default_multiplier\": 1.0,\r\n        \"extra_networks_card_width\": 0,\r\n        \"extra_networks_card_height\": 0,\r\n        \"extra_networks_add_text_separator\": \" \",\r\n        \"ui_extra_networks_tab_reorder\": \"\",\r\n        \"sd_hypernetwork\": \"None\",\r\n        \"localization\": \"None\",\r\n        \"gradio_theme\": \"Default\",\r\n        \"img2img_editor_height\": 720,\r\n        \"return_grid\": true,\r\n        \"return_mask\": false,\r\n        \"return_mask_composite\": false,\r\n        \"do_not_show_images\": false,\r\n        \"send_seed\": true,\r\n        \"send_size\": true,\r\n        \"font\": \"\",\r\n        \"js_modal_lightbox\": true,\r\n        \"js_modal_lightbox_initially_zoomed\": true,\r\n        \"js_modal_lightbox_gamepad\": false,\r\n        \"js_modal_lightbox_gamepad_repeat\": 250,\r\n        \"show_progress_in_title\": true,\r\n        \"samplers_in_dropdown\": true,\r\n        \"dimensions_and_batch_together\": true,\r\n        \"keyedit_precision_attention\": 0.1,\r\n        \"keyedit_precision_extra\": 0.05,\r\n        \"keyedit_delimiters\": \".,\\\\/!?%^*;:{}=`~()\",\r\n        \"quicksettings_list\": [\r\n            \"sd_model_checkpoint\",\r\n            \"sd_vae\",\r\n            \"CLIP_stop_at_last_layers\"\r\n        ],\r\n        \"ui_tab_order\": [],\r\n        \"hidden_tabs\": [],\r\n        \"ui_reorder_list\": [],\r\n        \"hires_fix_show_sampler\": false,\r\n        \"hires_fix_show_prompts\": false,\r\n        \"disable_token_counters\": false,\r\n        \"add_model_hash_to_info\": true,\r\n        \"add_model_name_to_info\": true,\r\n        \"add_version_to_infotext\": true,\r\n        \"disable_weights_auto_swap\": true,\r\n        \"infotext_styles\": \"Apply if any\",\r\n        \"show_progressbar\": true,\r\n        \"live_previews_enable\": true,\r\n        \"live_previews_image_format\": \"png\",\r\n        \"show_progress_grid\": true,\r\n        \"show_progress_every_n_steps\": 10,\r\n        \"show_progress_type\": \"Approx NN\",\r\n        \"live_preview_content\": \"Prompt\",\r\n        \"live_preview_refresh_period\": 1000,\r\n        \"hide_samplers\": [\r\n            \"LMS\",\r\n            \"PLMS\"\r\n        ],\r\n        \"eta_ddim\": 0.0,\r\n        \"eta_ancestral\": 1.0,\r\n        \"ddim_discretize\": \"uniform\",\r\n        \"s_churn\": 0.0,\r\n        \"s_tmin\": 0.0,\r\n        \"s_noise\": 1.0,\r\n        \"k_sched_type\": \"Automatic\",\r\n        \"sigma_min\": 0.0,\r\n        \"sigma_max\": 0.0,\r\n        \"rho\": 0.0,\r\n        \"eta_noise_seed_delta\": 0,\r\n        \"always_discard_next_to_last_sigma\": true,\r\n        \"uni_pc_variant\": \"bh1\",\r\n        \"uni_pc_skip_type\": \"time_uniform\",\r\n        \"uni_pc_order\": 3,\r\n        \"uni_pc_lower_order_final\": true,\r\n        \"postprocessing_enable_in_main_ui\": [],\r\n        \"postprocessing_operation_order\": [],\r\n        \"upscaling_max_images_in_cache\": 5,\r\n        \"disabled_extensions\": [],\r\n        \"disable_all_extensions\": \"none\",\r\n        \"restore_config_state_file\": \"\",\r\n        \"sd_checkpoint_hash\": \"be33e1574d746469ffee55239376c244a64d0e632548b48f251847c266c50ce7\",\r\n        \"ldsr_steps\": 100,\r\n        \"ldsr_cached\": false,\r\n        \"SCUNET_tile\": 256,\r\n        \"SCUNET_tile_overlap\": 8,\r\n        \"SWIN_tile\": 192,\r\n        \"SWIN_tile_overlap\": 8,\r\n        \"lora_functional\": false,\r\n        \"sd_lora\": \"None\",\r\n        \"lora_preferred_name\": \"Alias from file\",\r\n        \"lora_add_hashes_to_infotext\": true,\r\n        \"extra_options\": [],\r\n        \"extra_options_accordion\": false,\r\n        \"canvas_hotkey_zoom\": \"Alt\",\r\n        \"canvas_hotkey_adjust\": \"Ctrl\",\r\n        \"canvas_hotkey_move\": \"F\",\r\n        \"canvas_hotkey_fullscreen\": \"S\",\r\n        \"canvas_hotkey_reset\": \"R\",\r\n        \"canvas_hotkey_overlap\": \"O\",\r\n        \"canvas_show_tooltip\": true,\r\n        \"canvas_disabled_functions\": [\r\n            \"Overlap\"\r\n        ],\r\n        \"openpose3d_use_online_version\": false,\r\n        \"control_net_model_config\": \"models/cldm_v15.yaml\",\r\n        \"control_net_model_adapter_config\": \"models/t2iadapter_sketch_sd14v1.yaml\",\r\n        \"control_net_detectedmap_dir\": \"detected_maps\",\r\n        \"control_net_models_path\": \"\",\r\n        \"control_net_modules_path\": \"\",\r\n        \"control_net_max_models_num\": 3,\r\n        \"control_net_model_cache_size\": 1,\r\n        \"control_net_no_detectmap\": false,\r\n        \"control_net_detectmap_autosaving\": false,\r\n        \"control_net_allow_script_control\": true,\r\n        \"control_net_sync_field_args\": false,\r\n        \"controlnet_show_batch_images_in_ui\": false,\r\n        \"controlnet_increment_seed_during_batch\": false,\r\n        \"controlnet_disable_control_type\": false,\r\n        \"controlnet_disable_openpose_edit\": false,\r\n        \"ad_max_models\": 2,\r\n        \"ad_save_previews\": false,\r\n        \"ad_save_images_before\": false,\r\n        \"ad_only_seleted_scripts\": true,\r\n        \"ad_script_names\": \"dynamic_prompting,dynamic_thresholding,wildcard_recursive,wildcards,lora_block_weight\",\r\n        \"ad_bbox_sortby\": \"Position (left to right)\",\r\n        \"grid_text_active_color\": \"#000000\",\r\n        \"grid_text_inactive_color\": \"#999999\",\r\n        \"grid_background_color\": \"#ffffff\",\r\n        \"SWIN_torch_compile\": false,\r\n        \"disable_mmap_load_safetensors\": false,\r\n        \"auto_vae_precision\": true,\r\n        \"sdxl_crop_top\": 0.0,\r\n        \"sdxl_crop_left\": 0.0,\r\n        \"sdxl_refiner_low_aesthetic_score\": 2.5,\r\n        \"sdxl_refiner_high_aesthetic_score\": 6.0,\r\n        \"extra_networks_card_text_scale\": 1,\r\n        \"extra_networks_card_show_desc\": true,\r\n        \"textual_inversion_print_at_load\": false,\r\n        \"textual_inversion_add_hashes_to_infotext\": true,\r\n        \"lora_show_all\": false,\r\n        \"lora_hide_unknown_for_versions\": [],\r\n        \"keyedit_move\": true,\r\n        \"add_user_name_to_info\": false,\r\n        \"canvas_blur_prompt\": false,\r\n        \"control_net_inpaint_blur_sigma\": 7,\r\n        \"dd_save_previews\": false,\r\n        \"outdir_ddetailer_previews\": \"extensions/dddetailer/outputs/masks-previews\",\r\n        \"dd_save_masks\": false,\r\n        \"outdir_ddetailer_masks\": \"extensions/dddetailer/outputs/masks\",\r\n        \"control_net_no_high_res_fix\": false,\r\n        \"regprp_debug\": false,\r\n        \"regprp_hidepmask\": false,\r\n        \"sam_use_local_groundingdino\": false,\r\n        \"sd_vae_overrides_per_model_preferences\": false,\r\n        \"batch_cond_uncond\": true,\r\n        \"save_incomplete_images\": false,\r\n        \"face_restoration\": false,\r\n        \"auto_launch_browser\": \"Disable\",\r\n        \"show_gradio_deprecation_warnings\": true,\r\n        \"hide_ldm_prints\": true,\r\n        \"api_enable_requests\": true,\r\n        \"api_forbid_local_requests\": true,\r\n        \"api_useragent\": \"\",\r\n        \"sd_checkpoints_limit\": 1,\r\n        \"sd_checkpoints_keep_in_cpu\": true,\r\n        \"tiling\": false,\r\n        \"hires_fix_refiner_pass\": \"second pass\",\r\n        \"sd_vae_encode_method\": \"Full\",\r\n        \"sd_vae_decode_method\": \"Full\",\r\n        \"img2img_extra_noise\": 0,\r\n        \"img2img_sketch_default_brush_color\": \"#ffffff\",\r\n        \"img2img_inpaint_mask_brush_color\": \"#ffffff\",\r\n        \"img2img_inpaint_sketch_default_brush_color\": \"#ffffff\",\r\n        \"persistent_cond_cache\": true,\r\n        \"use_old_scheduling\": false,\r\n        \"lora_in_memory_limit\": 0,\r\n        \"gradio_themes_cache\": true,\r\n        \"gallery_height\": \"\",\r\n        \"extra_options_txt2img\": [],\r\n        \"extra_options_img2img\": [],\r\n        \"extra_options_cols\": 1,\r\n        \"live_preview_allow_lowvram_full\": false,\r\n        \"live_preview_fast_interrupt\": false,\r\n        \"s_tmax\": 0,\r\n        \"sgm_noise_multiplier\": false,\r\n        \"canvas_auto_expand\": true,\r\n        \"controlnet_ignore_noninpaint_mask\": false,\r\n        \"control_net_unit_count\": 3,\r\n        \"save_images_replace_action\": \"Replace\",\r\n        \"enable_console_prompts\": false,\r\n        \"notification_audio\": true,\r\n        \"dump_stacks_on_signal\": false,\r\n        \"keyedit_delimiters_whitespace\": [\r\n            \"Tab\",\r\n            \"Carriage Return\",\r\n            \"Line Feed\"\r\n        ],\r\n        \"sd_checkpoint_dropdown_use_short\": false\r\n    },\r\n    \"Startup\": {\r\n        \"total\": 8.572977542877197,\r\n        \"records\": {\r\n            \"initial startup\": 0.0002636909484863281,\r\n            \"prepare environment/checks\": 2.288818359375e-05,\r\n            \"prepare environment/git version info\": 0.00865030288696289,\r\n            \"prepare environment/torch GPU test\": 0.6202812194824219,\r\n            \"prepare environment/clone repositores\": 0.015151262283325195,\r\n            \"prepare environment/run extensions installers/sd-webui-openpose-editor\": 0.00011539459228515625,\r\n            \"prepare environment/run extensions installers/sd-webui-3d-open-pose-editor\": 0.04622793197631836,\r\n            \"prepare environment/run extensions installers/CFG-Schedule-for-Automatic1111-SD\": 2.193450927734375e-05,\r\n            \"prepare environment/run extensions installers/sd-webui-controlnet\": 0.07720422744750977,\r\n            \"prepare environment/run extensions installers/sd-webui-infinite-image-browsing\": 0.07681918144226074,\r\n            \"prepare environment/run extensions installers/ultimate-upscale-for-automatic1111\": 2.6702880859375e-05,\r\n            \"prepare environment/run extensions installers/model_preset_manager\": 6.67572021484375e-06,\r\n            \"prepare environment/run extensions installers/stable-diffusion-webui-zoomimage\": 4.76837158203125e-06,\r\n            \"prepare environment/run extensions installers/adetailer\": 0.035430908203125,\r\n            \"prepare environment/run extensions installers/gigadiffusion\": 2.4318695068359375e-05,\r\n            \"prepare environment/run extensions installers/sd-webui-segment-anything\": 0.022768497467041016,\r\n            \"prepare environment/run extensions installers/clip-interrogator-ext\": 1.0450770854949951,\r\n            \"prepare environment/run extensions installers/stable-diffusion-webui-tokenizer\": 3.266334533691406e-05,\r\n            \"prepare environment/run extensions installers/embedding-inspector\": 6.198883056640625e-06,\r\n            \"prepare environment/run extensions installers/dddetailer\": 2.3495004177093506,\r\n            \"prepare environment/run extensions installers/loopback_scaler\": 3.0517578125e-05,\r\n            \"prepare environment/run extensions installers/stable-diffusion-webui-two-shot\": 6.4373016357421875e-06,\r\n            \"prepare environment/run extensions installers/sd-webui-regional-prompter\": 5.245208740234375e-06,\r\n            \"prepare environment/run extensions installers/stable-diffusion-webui-sonar\": 4.5299530029296875e-06,\r\n            \"prepare environment/run extensions installers/sd-dynamic-thresholding\": 4.291534423828125e-06,\r\n            \"prepare environment/run extensions installers\": 3.653320789337158,\r\n            \"prepare environment\": 4.314878463745117,\r\n            \"launcher\": 0.00033283233642578125,\r\n            \"import torch\": 1.0496821403503418,\r\n            \"import gradio\": 0.3185546398162842,\r\n            \"setup paths\": 0.4733433723449707,\r\n            \"import ldm\": 0.001397848129272461,\r\n            \"import sgm\": 3.337860107421875e-06,\r\n            \"initialize shared\": 0.11992859840393066,\r\n            \"other imports\": 0.17567729949951172,\r\n            \"opts onchange\": 0.00017499923706054688,\r\n            \"setup SD model\": 0.0001442432403564453,\r\n            \"setup codeformer\": 0.09760928153991699,\r\n            \"setup gfpgan\": 0.0033431053161621094,\r\n            \"set samplers\": 2.1219253540039062e-05,\r\n            \"list extensions\": 9.632110595703125e-05,\r\n            \"restore config state file\": 3.0994415283203125e-06,\r\n            \"list SD models\": 0.01830267906188965,\r\n            \"list localizations\": 8.869171142578125e-05,\r\n            \"load scripts/custom_code.py\": 0.0005245208740234375,\r\n            \"load scripts/img2imgalt.py\": 0.00014138221740722656,\r\n            \"load scripts/loopback.py\": 6.222724914550781e-05,\r\n            \"load scripts/outpainting_mk_2.py\": 7.128715515136719e-05,\r\n            \"load scripts/poor_mans_outpainting.py\": 4.553794860839844e-05,\r\n            \"load scripts/postprocessing_codeformer.py\": 3.9577484130859375e-05,\r\n            \"load scripts/postprocessing_gfpgan.py\": 3.266334533691406e-05,\r\n            \"load scripts/postprocessing_upscale.py\": 6.318092346191406e-05,\r\n            \"load scripts/prompt_matrix.py\": 4.8160552978515625e-05,\r\n            \"load scripts/prompts_from_file.py\": 5.555152893066406e-05,\r\n            \"load scripts/refiner.py\": 4.506111145019531e-05,\r\n            \"load scripts/sd_upscale.py\": 4.291534423828125e-05,\r\n            \"load scripts/seed.py\": 5.1975250244140625e-05,\r\n            \"load scripts/xyz_grid.py\": 0.0007011890411376953,\r\n            \"load scripts/CFG Auto.py\": 9.226799011230469e-05,\r\n            \"load scripts/CFG Schedule.py\": 6.556510925292969e-05,\r\n            \"load scripts/!adetailer.py\": 0.7333564758300781,\r\n            \"load scripts/clip_interrogator_ext.py\": 0.02101302146911621,\r\n            \"load scripts/dddetailer.py\": 0.25907015800476074,\r\n            \"load scripts/embedding_inspector.py\": 0.008678197860717773,\r\n            \"load scripts/gigadiffusion.py\": 0.00029158592224121094,\r\n            \"load scripts/loopback_scaler.py\": 8.559226989746094e-05,\r\n            \"load scripts/interface.py\": 4.3392181396484375e-05,\r\n            \"load scripts/main.py\": 0.00883030891418457,\r\n            \"load scripts/dynamic_thresholding.py\": 0.008682489395141602,\r\n            \"load scripts/openpose_editor.py\": 0.026071786880493164,\r\n            \"load scripts/adapter.py\": 0.0003192424774169922,\r\n            \"load scripts/api.py\": 0.13697385787963867,\r\n            \"load scripts/batch_hijack.py\": 0.00020956993103027344,\r\n            \"load scripts/cldm.py\": 9.298324584960938e-05,\r\n            \"load scripts/controlmodel_ipadapter.py\": 0.0001468658447265625,\r\n            \"load scripts/controlnet.py\": 0.0415036678314209,\r\n            \"load scripts/controlnet_diffusers.py\": 0.00012874603271484375,\r\n            \"load scripts/controlnet_lllite.py\": 9.322166442871094e-05,\r\n            \"load scripts/controlnet_lora.py\": 7.152557373046875e-05,\r\n            \"load scripts/controlnet_model_guess.py\": 6.890296936035156e-05,\r\n            \"load scripts/controlnet_version.py\": 8.106231689453125e-05,\r\n            \"load scripts/external_code.py\": 3.695487976074219e-05,\r\n            \"load scripts/global_state.py\": 0.00014066696166992188,\r\n            \"load scripts/hook.py\": 0.00030541419982910156,\r\n            \"load scripts/infotext.py\": 0.00013780593872070312,\r\n            \"load scripts/logging.py\": 0.0001125335693359375,\r\n            \"load scripts/lvminthin.py\": 0.00014257431030273438,\r\n            \"load scripts/movie2movie.py\": 7.271766662597656e-05,\r\n            \"load scripts/processor.py\": 0.00012683868408203125,\r\n            \"load scripts/utils.py\": 7.343292236328125e-05,\r\n            \"load scripts/xyz_grid_support.py\": 0.0001201629638671875,\r\n            \"load scripts/iib_setup.py\": 0.023550033569335938,\r\n            \"load scripts/attention.py\": 0.00013756752014160156,\r\n            \"load scripts/latent.py\": 0.0005583763122558594,\r\n            \"load scripts/regions.py\": 0.00015401840209960938,\r\n            \"load scripts/rp.py\": 0.010063886642456055,\r\n            \"load scripts/rps.py\": 0.00547027587890625,\r\n            \"load scripts/auto.py\": 0.00017023086547851562,\r\n            \"load scripts/dino.py\": 7.271766662597656e-05,\r\n            \"load scripts/process_params.py\": 6.389617919921875e-05,\r\n            \"load scripts/sam.py\": 0.015322208404541016,\r\n            \"load scripts/sonar.py\": 0.0004382133483886719,\r\n            \"load scripts/tokenizer.py\": 0.008967161178588867,\r\n            \"load scripts/sketch_helper.py\": 9.369850158691406e-05,\r\n            \"load scripts/two_shot.py\": 0.009548187255859375,\r\n            \"load scripts/ultimate-upscale.py\": 0.00025153160095214844,\r\n            \"load scripts/ldsr_model.py\": 0.009922027587890625,\r\n            \"load scripts/lora_script.py\": 0.0553278923034668,\r\n            \"load scripts/scunet_model.py\": 0.009708404541015625,\r\n            \"load scripts/swinir_model.py\": 0.009277582168579102,\r\n            \"load scripts/hotkey_config.py\": 7.152557373046875e-05,\r\n            \"load scripts/extra_options_section.py\": 7.915496826171875e-05,\r\n            \"load scripts\": 1.408433198928833,\r\n            \"load upscalers\": 0.0005857944488525391,\r\n            \"refresh VAE\": 0.00063323974609375,\r\n            \"refresh textual inversion templates\": 1.7404556274414062e-05,\r\n            \"scripts list_optimizers\": 0.00014281272888183594,\r\n            \"scripts list_unets\": 2.6226043701171875e-06,\r\n            \"reload hypernetworks\": 0.0001914501190185547,\r\n            \"initialize extra networks\": 0.0035004615783691406,\r\n            \"scripts before_ui_callback\": 0.00022172927856445312,\r\n            \"create ui\": 0.40461039543151855,\r\n            \"gradio launch\": 0.18329834938049316,\r\n            \"add APIs\": 0.0036804676055908203,\r\n            \"app_started_callback/clip_interrogator_ext.py\": 0.0002796649932861328,\r\n            \"app_started_callback/api.py\": 0.00385284423828125,\r\n            \"app_started_callback/iib_setup.py\": 0.00631403923034668,\r\n            \"app_started_callback/openpose_editor.py\": 0.0006959438323974609,\r\n            \"app_started_callback/lora_script.py\": 0.00012254714965820312,\r\n            \"app_started_callback\": 0.011267423629760742\r\n        }\r\n    },\r\n    \"Packages\": [\r\n        \"absl-py==1.4.0\",\r\n        \"accelerate==0.21.0\",\r\n        \"addict==2.4.0\",\r\n        \"aenum==3.1.15\",\r\n        \"aiofiles==23.1.0\",\r\n        \"aiohttp==3.8.4\",\r\n        \"aiosignal==1.3.1\",\r\n        \"altair==5.0.1\",\r\n        \"antlr4-python3-runtime==4.9.3\",\r\n        \"anyio==3.7.1\",\r\n        \"async-timeout==4.0.2\",\r\n        \"attrs==23.1.0\",\r\n        \"basicsr==1.4.2\",\r\n        \"beautifulsoup4==4.12.2\",\r\n        \"blendmodes==2022\",\r\n        \"blip-ci==0.0.5\",\r\n        \"boltons==23.0.0\",\r\n        \"cachetools==5.3.1\",\r\n        \"certifi==2023.5.7\",\r\n        \"cffi==1.15.1\",\r\n        \"charset-normalizer==3.2.0\",\r\n        \"clean-fid==0.1.35\",\r\n        \"click==8.1.5\",\r\n        \"clip-interrogator==0.6.0\",\r\n        \"clip==1.0\",\r\n        \"cmake==3.26.4\",\r\n        \"colorama==0.4.6\",\r\n        \"contourpy==1.1.0\",\r\n        \"cssselect2==0.7.0\",\r\n        \"cycler==0.11.0\",\r\n        \"deprecation==2.1.0\",\r\n        \"dill==0.3.7\",\r\n        \"einops==0.4.1\",\r\n        \"exceptiongroup==1.1.2\",\r\n        \"facexlib==0.3.0\",\r\n        \"fairscale==0.4.4\",\r\n        \"fastapi==0.94.0\",\r\n        \"ffmpy==0.3.0\",\r\n        \"filelock==3.12.2\",\r\n        \"filterpy==1.4.5\",\r\n        \"flatbuffers==23.5.26\",\r\n        \"fonttools==4.41.0\",\r\n        \"frozenlist==1.4.0\",\r\n        \"fsspec==2023.6.0\",\r\n        \"ftfy==6.1.1\",\r\n        \"future==0.18.3\",\r\n        \"fvcore==0.1.5.post20221221\",\r\n        \"gdown==4.7.1\",\r\n        \"gfpgan==1.3.8\",\r\n        \"gitdb==4.0.10\",\r\n        \"gitpython==3.1.32\",\r\n        \"google-auth-oauthlib==1.0.0\",\r\n        \"google-auth==2.22.0\",\r\n        \"gradio-client==0.5.0\",\r\n        \"gradio==3.41.2\",\r\n        \"grpcio==1.56.0\",\r\n        \"h11==0.12.0\",\r\n        \"httpcore==0.15.0\",\r\n        \"httpx==0.24.1\",\r\n        \"huggingface-hub==0.16.4\",\r\n        \"idna==3.4\",\r\n        \"imageio==2.31.1\",\r\n        \"importlib-metadata==6.8.0\",\r\n        \"importlib-resources==6.0.1\",\r\n        \"inflection==0.5.1\",\r\n        \"iopath==0.1.9\",\r\n        \"jinja2==3.1.2\",\r\n        \"jsonmerge==1.8.0\",\r\n        \"jsonschema-specifications==2023.6.1\",\r\n        \"jsonschema==4.18.3\",\r\n        \"kiwisolver==1.4.4\",\r\n        \"kornia==0.6.7\",\r\n        \"lark==1.1.2\",\r\n        \"lazy-loader==0.3\",\r\n        \"lightning-utilities==0.9.0\",\r\n        \"linkify-it-py==2.0.2\",\r\n        \"lit==16.0.6\",\r\n        \"llvmlite==0.40.1\",\r\n        \"lmdb==1.4.1\",\r\n        \"lpips==0.1.4\",\r\n        \"lxml==4.9.3\",\r\n        \"markdown-it-py==2.2.0\",\r\n        \"markdown==3.4.3\",\r\n        \"markupsafe==2.1.3\",\r\n        \"matplotlib==3.7.2\",\r\n        \"mdit-py-plugins==0.3.3\",\r\n        \"mdurl==0.1.2\",\r\n        \"mediapipe==0.10.7\",\r\n        \"mmcv==2.0.1\",\r\n        \"mmdet==3.1.0\",\r\n        \"mmengine==0.8.2\",\r\n        \"model-index==0.1.11\",\r\n        \"mpmath==1.3.0\",\r\n        \"multidict==6.0.4\",\r\n        \"networkx==3.1\",\r\n        \"numba==0.57.1\",\r\n        \"numpy==1.23.5\",\r\n        \"oauthlib==3.2.2\",\r\n        \"omegaconf==2.2.3\",\r\n        \"open-clip-torch==2.20.0\",\r\n        \"opencv-contrib-python==4.8.0.74\",\r\n        \"opencv-python-headless==4.8.0.74\",\r\n        \"opencv-python==4.8.0.74\",\r\n        \"opendatalab==0.0.9\",\r\n        \"openmim==0.3.9\",\r\n        \"ordered-set==4.1.0\",\r\n        \"orjson==3.9.2\",\r\n        \"packaging==23.1\",\r\n        \"pandas==2.0.3\",\r\n        \"piexif==1.1.3\",\r\n        \"pillow==9.5.0\",\r\n        \"pip==22.3.1\",\r\n        \"platformdirs==3.9.1\",\r\n        \"portalocker==2.7.0\",\r\n        \"protobuf==3.20.0\",\r\n        \"psutil==5.9.5\",\r\n        \"py-cpuinfo==9.0.0\",\r\n        \"pyasn1-modules==0.3.0\",\r\n        \"pyasn1==0.5.0\",\r\n        \"pycocotools==2.0.6\",\r\n        \"pycparser==2.21\",\r\n        \"pycryptodome==3.18.0\",\r\n        \"pydantic==1.10.11\",\r\n        \"pydub==0.25.1\",\r\n        \"pyfunctional==1.4.3\",\r\n        \"pygments==2.15.1\",\r\n        \"pyparsing==3.0.9\",\r\n        \"pysocks==1.7.1\",\r\n        \"python-dateutil==2.8.2\",\r\n        \"python-dotenv==1.0.0\",\r\n        \"python-multipart==0.0.6\",\r\n        \"pytorch-lightning==1.9.4\",\r\n        \"pytz==2023.3\",\r\n        \"pywavelets==1.4.1\",\r\n        \"pyyaml==6.0\",\r\n        \"realesrgan==0.3.0\",\r\n        \"referencing==0.29.1\",\r\n        \"regex==2023.6.3\",\r\n        \"reportlab==4.0.4\",\r\n        \"requests-oauthlib==1.3.1\",\r\n        \"requests==2.31.0\",\r\n        \"resize-right==0.0.2\",\r\n        \"rich==13.4.2\",\r\n        \"rpds-py==0.8.10\",\r\n        \"rsa==4.9\",\r\n        \"safetensors==0.3.1\",\r\n        \"scikit-image==0.21.0\",\r\n        \"scipy==1.11.1\",\r\n        \"seaborn==0.12.2\",\r\n        \"segment-anything==1.0\",\r\n        \"semantic-version==2.10.0\",\r\n        \"sentencepiece==0.1.99\",\r\n        \"setuptools==65.5.0\",\r\n        \"shapely==2.0.1\",\r\n        \"six==1.16.0\",\r\n        \"smmap==5.0.0\",\r\n        \"sniffio==1.3.0\",\r\n        \"sounddevice==0.4.6\",\r\n        \"soupsieve==2.4.1\",\r\n        \"starlette==0.26.1\",\r\n        \"supervision==0.12.0\",\r\n        \"svglib==1.5.1\",\r\n        \"sympy==1.12\",\r\n        \"tabulate==0.9.0\",\r\n        \"tb-nightly==2.14.0a20230714\",\r\n        \"tensorboard-data-server==0.7.1\",\r\n        \"termcolor==2.3.0\",\r\n        \"terminaltables==3.1.10\",\r\n        \"thop==0.1.1.post2209072238\",\r\n        \"tifffile==2023.7.10\",\r\n        \"timm==0.9.2\",\r\n        \"tinycss2==1.2.1\",\r\n        \"tokenizers==0.13.3\",\r\n        \"tomesd==0.1.3\",\r\n        \"tomli==2.0.1\",\r\n        \"toolz==0.12.0\",\r\n        \"torch==2.0.1+cu118\",\r\n        \"torchdiffeq==0.2.3\",\r\n        \"torchmetrics==1.0.1\",\r\n        \"torchsde==0.2.6\",\r\n        \"torchvision==0.15.2+cu118\",\r\n        \"tqdm==4.65.0\",\r\n        \"trampoline==0.1.2\",\r\n        \"transformers==4.30.2\",\r\n        \"triton==2.0.0\",\r\n        \"typing-extensions==4.7.1\",\r\n        \"tzdata==2023.3\",\r\n        \"uc-micro-py==1.0.2\",\r\n        \"ultralytics==8.0.200\",\r\n        \"urllib3==1.26.16\",\r\n        \"uvicorn==0.23.0\",\r\n        \"wcwidth==0.2.6\",\r\n        \"webencodings==0.5.1\",\r\n        \"websockets==11.0.3\",\r\n        \"werkzeug==2.3.6\",\r\n        \"wheel==0.40.0\",\r\n        \"xformers==0.0.20\",\r\n        \"yacs==0.1.8\",\r\n        \"yapf==0.40.1\",\r\n        \"yarl==1.9.2\",\r\n        \"zipp==3.16.2\"\r\n    ]\r\n}\n\n### What browsers do you use to access the UI ?\n\nGoogle Chrome, Brave\n\n### Console logs\n\n```Shell\nn/a\n```\n\n\n### Additional information\n\n_No response_\n", "hints_text": "Seems to have been broken by 464fbcd92118bf00173b9982325fe6348201313e. It still works if you manually select `lora:L1`", "created_at": "2023-11-03T07:01:26Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 13797, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-13797", "issue_numbers": ["13796"], "base_commit": "1f373a2baa94a6731be9df1a665f981a0b31907a", "patch": "diff --git a/modules/prompt_parser.py b/modules/prompt_parser.py\nindex 334efeef317..86b7acb50a8 100644\n--- a/modules/prompt_parser.py\n+++ b/modules/prompt_parser.py\n@@ -5,7 +5,7 @@\n from typing import List\r\n import lark\r\n \r\n-# a prompt like this: \"fantasy landscape with a [mountain:lake:0.25] and [an oak:a christmas tree:0.75][ in foreground::0.6][ in background:0.25] [shoddy:masterful:0.5]\"\r\n+# a prompt like this: \"fantasy landscape with a [mountain:lake:0.25] and [an oak:a christmas tree:0.75][ in foreground::0.6][: in background:0.25] [shoddy:masterful:0.5]\"\r\n # will be represented with prompt_schedule like this (assuming steps=100):\r\n # [25, 'fantasy landscape with a mountain and an oak in foreground shoddy']\r\n # [50, 'fantasy landscape with a lake and an oak in foreground in background shoddy']\r\n", "test_patch": "", "problem_statement": "[Bug]: Prompt Scheduling Code Comment is Wrong\n### Is there an existing issue for this?\n\n- [X] I have searched the existing issues and checked the recent builds/commits\n\n### What happened?\n\nhttps://github.com/AUTOMATIC1111/stable-diffusion-webui/blob/5ef669de080814067961f28357256e8fe27544f4/modules/prompt_parser.py#L8\r\n\r\n```py\r\n# a prompt like this: \"fantasy landscape with a [mountain:lake:0.25] and [an oak:a christmas tree:0.75][ in foreground::0.6][ in background:0.25] [shoddy:masterful:0.5]\"\r\n# will be represented with prompt_schedule like this (assuming steps=100):\r\n# [25, 'fantasy landscape with a mountain and an oak in foreground shoddy']\r\n# [50, 'fantasy landscape with a lake and an oak in foreground in background shoddy']\r\n# [60, 'fantasy landscape with a lake and an oak in foreground in background masterful']\r\n# [75, 'fantasy landscape with a lake and an oak in background masterful']\r\n# [100, 'fantasy landscape with a lake and a christmas tree in background masterful']\r\n```\r\n\r\nNotice that `[ in background:0.25]` has only one colon. This schedule is badly formatted. Currently, this represents an \"emphasis\" rather than a schedule.\r\n\r\nIf this example is to match the output above, it needs to be `[:in background:0.25]`\n\n### Steps to reproduce the problem\n\n.\n\n### What should have happened?\n\n.\n\n### Sysinfo\n\n.\n\n### What browsers do you use to access the UI ?\n\n_No response_\n\n### Console logs\n\n```Shell\n.\n```\n\n\n### Additional information\n\n_No response_\n", "hints_text": "", "created_at": "2023-10-29T22:37:50Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 13653, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-13653", "issue_numbers": ["8898"], "base_commit": "0ce67cb61806cf43f4d726d4705a4f6fdc2540e6", "patch": "diff --git a/modules/call_queue.py b/modules/call_queue.py\nindex ddf0d57383c..01c6d17f685 100644\n--- a/modules/call_queue.py\n+++ b/modules/call_queue.py\n@@ -78,6 +78,7 @@ def f(*args, extra_outputs_array=extra_outputs, **kwargs):\n \r\n         shared.state.skipped = False\r\n         shared.state.interrupted = False\r\n+        shared.state.interrupted_next = False\r\n         shared.state.job_count = 0\r\n \r\n         if not add_stats:\r\ndiff --git a/modules/img2img.py b/modules/img2img.py\nindex 75b3d346f9a..829faa818ff 100644\n--- a/modules/img2img.py\n+++ b/modules/img2img.py\n@@ -51,7 +51,7 @@ def process_batch(p, input_dir, output_dir, inpaint_mask_dir, args, to_scale=Fal\n         if state.skipped:\r\n             state.skipped = False\r\n \r\n-        if state.interrupted:\r\n+        if state.interrupted or state.interrupted_next:\r\n             break\r\n \r\n         try:\r\ndiff --git a/modules/processing.py b/modules/processing.py\nindex f065688218a..00de2ed2e9a 100644\n--- a/modules/processing.py\n+++ b/modules/processing.py\n@@ -865,7 +865,7 @@ def process_images_inner(p: StableDiffusionProcessing) -> Processed:\n             if state.skipped:\r\n                 state.skipped = False\r\n \r\n-            if state.interrupted:\r\n+            if state.interrupted or state.interrupted_next:\r\n                 break\r\n \r\n             sd_models.reload_model_weights()  # model can be changed for example by refiner\r\ndiff --git a/modules/shared_options.py b/modules/shared_options.py\nindex e813546f6a5..7852e0ea362 100644\n--- a/modules/shared_options.py\n+++ b/modules/shared_options.py\n@@ -120,6 +120,7 @@\n     \"disable_mmap_load_safetensors\": OptionInfo(False, \"Disable memmapping for loading .safetensors files.\").info(\"fixes very slow loading speed in some cases\"),\r\n     \"hide_ldm_prints\": OptionInfo(True, \"Prevent Stability-AI's ldm/sgm modules from printing noise to console.\"),\r\n     \"dump_stacks_on_signal\": OptionInfo(False, \"Print stack traces before exiting the program with ctrl+c.\"),\r\n+    \"interrupt_after_current\": OptionInfo(False, \"Interrupt generation after current image is finished on batch processing\"),\r\n }))\r\n \r\n options_templates.update(options_section(('API', \"API\", \"system\"), {\r\ndiff --git a/modules/shared_state.py b/modules/shared_state.py\nindex a68789cc815..532fdcd8d12 100644\n--- a/modules/shared_state.py\n+++ b/modules/shared_state.py\n@@ -12,6 +12,7 @@\n class State:\r\n     skipped = False\r\n     interrupted = False\r\n+    interrupted_next = False\r\n     job = \"\"\r\n     job_no = 0\r\n     job_count = 0\r\n@@ -79,6 +80,10 @@ def interrupt(self):\n         self.interrupted = True\r\n         log.info(\"Received interrupt request\")\r\n \r\n+    def interrupt_next(self):\r\n+        self.interrupted_next = True\r\n+        log.info(\"Received interrupt request, interrupt after current job\")\r\n+\r\n     def nextjob(self):\r\n         if shared.opts.live_previews_enable and shared.opts.show_progress_every_n_steps == -1:\r\n             self.do_set_current_image()\r\n@@ -91,6 +96,7 @@ def dict(self):\n         obj = {\r\n             \"skipped\": self.skipped,\r\n             \"interrupted\": self.interrupted,\r\n+            \"interrupted_next\": self.interrupted_next,\r\n             \"job\": self.job,\r\n             \"job_count\": self.job_count,\r\n             \"job_timestamp\": self.job_timestamp,\r\n@@ -114,6 +120,7 @@ def begin(self, job: str = \"(unknown)\"):\n         self.id_live_preview = 0\r\n         self.skipped = False\r\n         self.interrupted = False\r\n+        self.interrupted_next = False\r\n         self.textinfo = None\r\n         self.job = job\r\n         devices.torch_gc()\r\ndiff --git a/modules/ui.py b/modules/ui.py\nindex 1f91a33fda2..378529c79a4 100644\n--- a/modules/ui.py\n+++ b/modules/ui.py\n@@ -177,7 +177,6 @@ def update_negative_prompt_token_counter(text, steps):\n     return update_token_counter(text, steps, is_positive=False)\r\n \r\n \r\n-\r\n def setup_progressbar(*args, **kwargs):\r\n     pass\r\n \r\ndiff --git a/scripts/loopback.py b/scripts/loopback.py\nindex 2d5feaf9b26..ad921269afb 100644\n--- a/scripts/loopback.py\n+++ b/scripts/loopback.py\n@@ -95,7 +95,7 @@ def calculate_denoising_strength(loop):\n                 processed = processing.process_images(p)\r\n \r\n                 # Generation cancelled.\r\n-                if state.interrupted:\r\n+                if state.interrupted or state.interrupted_next:\r\n                     break\r\n \r\n                 if initial_seed is None:\r\n@@ -122,8 +122,8 @@ def calculate_denoising_strength(loop):\n \r\n             p.inpainting_fill = original_inpainting_fill\r\n \r\n-            if state.interrupted:\r\n-                    break\r\n+            if state.interrupted or state.interrupted_next:\r\n+                break\r\n \r\n         if len(history) > 1:\r\n             grid = images.image_grid(history, rows=1)\r\ndiff --git a/scripts/xyz_grid.py b/scripts/xyz_grid.py\nindex 2d550994709..2deff365fa1 100644\n--- a/scripts/xyz_grid.py\n+++ b/scripts/xyz_grid.py\n@@ -696,7 +696,7 @@ def fix_axis_seeds(axis_opt, axis_list):\n         grid_infotext = [None] * (1 + len(zs))\r\n \r\n         def cell(x, y, z, ix, iy, iz):\r\n-            if shared.state.interrupted:\r\n+            if shared.state.interrupted or state.interrupted_next:\r\n                 return Processed(p, [], p.seed, \"\")\r\n \r\n             pc = copy(p)\r\n", "test_patch": "", "problem_statement": "[Feature Request]: Make this last\n### Is there an existing issue for this?\n\n- [X] I have searched the existing issues and checked the recent builds/commits\n\n### What would your feature do ?\n\n Can there be a button to make SD stop generating batch upon finishing current one?  just a third button aside from \"Skip\" \"Interrupt\" ones. \r\nWhy? because i will often put batches of a hundred in work, and more often then not I change prompts and need to restart generation, so i need to camp progress to stop when image is already on last step.\n\n### Proposed workflow\n\n1. Press \"Stop after this\" button \r\n2. Batch generation finishes current image fully and stops\r\n3. Profit - no broken images on interrupt\r\n\n\n### Additional information\n\n_No response_\n", "hints_text": "You can already do this by right clicking on either button and pressing \"cancel generate forever\" tho ?\n> You can already do this by right clicking on either button and pressing \"cancel generate forever\" tho ?\r\n\r\ndidn't know about this one, but request still stands. point is to finish current image before stopping\nSimilar to #7488, where \"Interrupt\" takes on the effect of the new button you propose", "created_at": "2023-10-16T06:17:25Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 13411, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-13411", "issue_numbers": ["13407"], "base_commit": "102b6617dacffdcc89c56badcaae6c5e83c3ff21", "patch": "diff --git a/javascript/extraNetworks.js b/javascript/extraNetworks.js\nindex 493f31af28a..e927346c332 100644\n--- a/javascript/extraNetworks.js\n+++ b/javascript/extraNetworks.js\n@@ -335,7 +335,7 @@ function extraNetworksEditUserMetadata(event, tabname, extraPage, cardName) {\n function extraNetworksRefreshSingleCard(page, tabname, name) {\n     requestGet(\"./sd_extra_networks/get-single-card\", {page: page, tabname: tabname, name: name}, function(data) {\n         if (data && data.html) {\n-            var card = gradioApp().querySelector('.card[data-name=' + JSON.stringify(name) + ']'); // likely using the wrong stringify function\n+            var card = gradioApp().querySelector(`#${tabname}_${page.replace(\" \", \"_\")}_cards > .card[data-name=\"${name}\"]`);\n \n             var newDiv = document.createElement('DIV');\n             newDiv.innerHTML = data.html;\n", "test_patch": "", "problem_statement": "[Bug]: Wrong card is updated when multiple with same name exist\n### Is there an existing issue for this?\n\n- [X] I have searched the existing issues and checked the recent builds/commits\n\n### What happened?\n\nIf a lora and an embedding share the same file name (sans the extension), changes made in the settings popup will be saved to the lora version in the file system, and to the embedding version in the web session, regardless of which tab is open. \n\n### Steps to reproduce the problem\n\n1. have both a lora and embedding of the same name (https://civitai.com/user/narugo1992/models has many loras/embeddings of the same name that are meant to be used in conjunction)\r\n\r\n3. in txt2img click on the lora tab\r\n\r\n4. click on the crossed wrench and hammer (settings icon) in the top right corner of the above mentioned lora\r\n\r\n5. make changes to the settings (such as invoking the above mentioned embedding in the activation text section, or replacing the preview)\r\n\r\n6. note that these changes have not been updated in the web session (by, for example, clicking on the lora and seeing that the activation text changes do not show up in the prompt; or by noting that the changed preview does not update)\r\n\r\n7. click on the textual inversion tab and note that the above made changes *did* occur in the aforementioned embedding there, and that clicking on said embedding invokes the lora in the prompt (with the changes made to the activation text of the lora).\r\n\r\n8. refresh the webpage and click the refresh button on the lora tab, and (after waiting for the refresh) note that the above issues are now temporarily fixed (until more changes are made to the lora/embedding settings). For example the embedding no longer invokes the lora, and the changes to the lora settings are properly registered in the lora tab, presumably because the filesystem changes were made correctly. \n\n### What should have happened?\n\nWhen changing settings for a lora in the lora tab both filesystem changes and web session changes should be made to the lora in question, and likewise with the embedding tab.\n\n### Sysinfo\n\n{\r\n    \"Platform\": \"Windows-10-10.0.22621-SP0\",\r\n    \"Python\": \"3.10.6\",\r\n    \"Version\": \"v1.6.0\",\r\n    \"Commit\": \"5ef669de080814067961f28357256e8fe27544f4\",\r\n    \"Script path\": \"E:\\\\stable-diffusion-webui\",\r\n    \"Data path\": \"E:\\\\stable-diffusion-webui\",\r\n    \"Extensions dir\": \"E:\\\\stable-diffusion-webui\\\\extensions\",\r\n    \"Checksum\": \"34b999c8571ab6b15c2dbe42face96dd692f7f82de1a066ceb86dfe553d5dacc\",\r\n    \"Commandline\": [\r\n        \"launch.py\",\r\n        \"--listen\",\r\n        \"--xformers\",\r\n        \"--medvram\",\r\n        \"--enable-insecure-extension-access\",\r\n        \"--no-half-vae\"\r\n    ],\r\n    \"Torch env info\": {\r\n        \"torch_version\": \"2.0.1+cu118\",\r\n        \"is_debug_build\": \"False\",\r\n        \"cuda_compiled_version\": \"11.8\",\r\n        \"gcc_version\": null,\r\n        \"clang_version\": null,\r\n        \"cmake_version\": null,\r\n        \"os\": \"Microsoft Windows 11 Pro\",\r\n        \"libc_version\": \"N/A\",\r\n        \"python_version\": \"3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)] (64-bit runtime)\",\r\n        \"python_platform\": \"Windows-10-10.0.22621-SP0\",\r\n        \"is_cuda_available\": \"True\",\r\n        \"cuda_runtime_version\": null,\r\n        \"cuda_module_loading\": \"LAZY\",\r\n        \"nvidia_driver_version\": \"536.99\",\r\n        \"nvidia_gpu_models\": \"GPU 0: NVIDIA GeForce RTX 3070 Ti\",\r\n        \"cudnn_version\": null,\r\n        \"pip_version\": \"pip3\",\r\n        \"pip_packages\": [\r\n            \"numpy==1.23.5\",\r\n            \"open-clip-torch==2.20.0\",\r\n            \"pytorch-lightning==1.9.4\",\r\n            \"torch==2.0.1+cu118\",\r\n            \"torchdiffeq==0.2.3\",\r\n            \"torchmetrics==1.0.1\",\r\n            \"torchsde==0.2.5\",\r\n            \"torchvision==0.15.2+cu118\"\r\n        ],\r\n        \"conda_packages\": null,\r\n        \"hip_compiled_version\": \"N/A\",\r\n        \"hip_runtime_version\": \"N/A\",\r\n        \"miopen_runtime_version\": \"N/A\",\r\n        \"caching_allocator_config\": \"\",\r\n        \"is_xnnpack_available\": \"True\",\r\n        \"cpu_info\": [\r\n            \"Architecture=9\",\r\n            \"CurrentClockSpeed=3701\",\r\n            \"DeviceID=CPU0\",\r\n            \"Family=107\",\r\n            \"L2CacheSize=3072\",\r\n            \"L2CacheSpeed=\",\r\n            \"Manufacturer=AuthenticAMD\",\r\n            \"MaxClockSpeed=3701\",\r\n            \"Name=AMD Ryzen 5 5600X 6-Core Processor             \",\r\n            \"ProcessorType=3\",\r\n            \"Revision=8448\"\r\n        ]\r\n    },\r\n    \"Exceptions\": [],\r\n    \"CPU\": {\r\n        \"model\": \"AMD64 Family 25 Model 33 Stepping 0, AuthenticAMD\",\r\n        \"count logical\": 12,\r\n        \"count physical\": 6\r\n    },\r\n    \"RAM\": {\r\n        \"total\": \"32GB\",\r\n        \"used\": \"18GB\",\r\n        \"free\": \"14GB\"\r\n    },\r\n    \"Extensions\": [\r\n        {\r\n            \"name\": \"adetailer\",\r\n            \"path\": \"E:\\\\stable-diffusion-webui\\\\extensions\\\\adetailer\",\r\n            \"version\": \"910bf3b9\",\r\n            \"branch\": \"main\",\r\n            \"remote\": \"https://github.com/Bing-su/adetailer.git\"\r\n        },\r\n        {\r\n            \"name\": \"embedding-inspector\",\r\n            \"path\": \"E:\\\\stable-diffusion-webui\\\\extensions\\\\embedding-inspector\",\r\n            \"version\": \"448b6d06\",\r\n            \"branch\": \"main\",\r\n            \"remote\": \"https://github.com/tkalayci71/embedding-inspector.git\"\r\n        },\r\n        {\r\n            \"name\": \"openpose-editor\",\r\n            \"path\": \"E:\\\\stable-diffusion-webui\\\\extensions\\\\openpose-editor\",\r\n            \"version\": \"db07b9c6\",\r\n            \"branch\": \"master\",\r\n            \"remote\": \"https://github.com/fkunn1326/openpose-editor.git\"\r\n        },\r\n        {\r\n            \"name\": \"sd-webui-additional-networks\",\r\n            \"path\": \"E:\\\\stable-diffusion-webui\\\\extensions\\\\sd-webui-additional-networks\",\r\n            \"version\": \"e9f3d622\",\r\n            \"branch\": \"main\",\r\n            \"remote\": \"https://github.com/kohya-ss/sd-webui-additional-networks.git\"\r\n        },\r\n        {\r\n            \"name\": \"sd-webui-agent-scheduler\",\r\n            \"path\": \"E:\\\\stable-diffusion-webui\\\\extensions\\\\sd-webui-agent-scheduler\",\r\n            \"version\": \"097fe4e5\",\r\n            \"branch\": \"main\",\r\n            \"remote\": \"https://github.com/ArtVentureX/sd-webui-agent-scheduler.git\"\r\n        },\r\n        {\r\n            \"name\": \"sd-webui-ar\",\r\n            \"path\": \"E:\\\\stable-diffusion-webui\\\\extensions\\\\sd-webui-ar\",\r\n            \"version\": \"3973c86a\",\r\n            \"branch\": \"main\",\r\n            \"remote\": \"https://github.com/alemelis/sd-webui-ar.git\"\r\n        },\r\n        {\r\n            \"name\": \"sd-webui-controlnet\",\r\n            \"path\": \"E:\\\\stable-diffusion-webui\\\\extensions\\\\sd-webui-controlnet\",\r\n            \"version\": \"7a4805c8\",\r\n            \"branch\": \"main\",\r\n            \"remote\": \"https://github.com/Mikubill/sd-webui-controlnet.git\"\r\n        },\r\n        {\r\n            \"name\": \"sd-webui-infinite-image-browsing\",\r\n            \"path\": \"E:\\\\stable-diffusion-webui\\\\extensions\\\\sd-webui-infinite-image-browsing\",\r\n            \"version\": \"228e045d\",\r\n            \"branch\": \"main\",\r\n            \"remote\": \"https://github.com/zanllp/sd-webui-infinite-image-browsing.git\"\r\n        },\r\n        {\r\n            \"name\": \"stable-diffusion-webui-dataset-tag-editor\",\r\n            \"path\": \"E:\\\\stable-diffusion-webui\\\\extensions\\\\stable-diffusion-webui-dataset-tag-editor\",\r\n            \"version\": \"7a2f4c53\",\r\n            \"branch\": \"main\",\r\n            \"remote\": \"https://github.com/toshiaki1729/stable-diffusion-webui-dataset-tag-editor.git\"\r\n        }\r\n    ],\r\n    \"Inactive extensions\": [\r\n        {\r\n            \"name\": \"a1111-sd-webui-locon\",\r\n            \"path\": \"E:\\\\stable-diffusion-webui\\\\extensions\\\\a1111-sd-webui-locon\",\r\n            \"version\": \"b6911354\",\r\n            \"branch\": \"main\",\r\n            \"remote\": \"https://github.com/KohakuBlueleaf/a1111-sd-webui-locon.git\"\r\n        },\r\n        {\r\n            \"name\": \"a1111-sd-webui-lycoris\",\r\n            \"path\": \"E:\\\\stable-diffusion-webui\\\\extensions\\\\a1111-sd-webui-lycoris\",\r\n            \"version\": \"8e97bf54\",\r\n            \"branch\": \"main\",\r\n            \"remote\": \"https://github.com/KohakuBlueleaf/a1111-sd-webui-lycoris.git\"\r\n        },\r\n        {\r\n            \"name\": \"sd-webui-aspect-ratio-helper\",\r\n            \"path\": \"E:\\\\stable-diffusion-webui\\\\extensions\\\\sd-webui-aspect-ratio-helper\",\r\n            \"version\": \"99fcf9b0\",\r\n            \"branch\": \"main\",\r\n            \"remote\": \"https://github.com/thomasasfk/sd-webui-aspect-ratio-helper.git\"\r\n        },\r\n        {\r\n            \"name\": \"stable-diffusion-webui-state\",\r\n            \"path\": \"E:\\\\stable-diffusion-webui\\\\extensions\\\\stable-diffusion-webui-state\",\r\n            \"version\": \"f2bb3809\",\r\n            \"branch\": \"main\",\r\n            \"remote\": \"https://github.com/ilian6806/stable-diffusion-webui-state.git\"\r\n        }\r\n    ],\r\n    \"Environment\": {\r\n        \"COMMANDLINE_ARGS\": \"--listen --xformers --medvram --enable-insecure-extension-access --no-half-vae\",\r\n        \"GRADIO_ANALYTICS_ENABLED\": \"False\"\r\n    },\r\n    \"Config\": {\r\n        \"samples_save\": true,\r\n        \"samples_format\": \"png\",\r\n        \"samples_filename_pattern\": \"\",\r\n        \"save_images_add_number\": true,\r\n        \"grid_save\": true,\r\n        \"grid_format\": \"png\",\r\n        \"grid_extended_filename\": false,\r\n        \"grid_only_if_multiple\": true,\r\n        \"grid_prevent_empty_spots\": false,\r\n        \"grid_zip_filename_pattern\": \"\",\r\n        \"n_rows\": 2,\r\n        \"font\": \"\",\r\n        \"grid_text_active_color\": \"#000000\",\r\n        \"grid_text_inactive_color\": \"#999999\",\r\n        \"grid_background_color\": \"#ffffff\",\r\n        \"enable_pnginfo\": true,\r\n        \"save_txt\": false,\r\n        \"save_images_before_face_restoration\": false,\r\n        \"save_images_before_highres_fix\": false,\r\n        \"save_images_before_color_correction\": false,\r\n        \"save_mask\": false,\r\n        \"save_mask_composite\": false,\r\n        \"jpeg_quality\": 80,\r\n        \"webp_lossless\": false,\r\n        \"export_for_4chan\": false,\r\n        \"img_downscale_threshold\": 4.0,\r\n        \"target_side_length\": 4000,\r\n        \"img_max_size_mp\": 200,\r\n        \"use_original_name_batch\": true,\r\n        \"use_upscaler_name_as_suffix\": false,\r\n        \"save_selected_only\": true,\r\n        \"save_init_img\": false,\r\n        \"temp_dir\": \"\",\r\n        \"clean_temp_dir_at_start\": false,\r\n        \"outdir_samples\": \"\",\r\n        \"outdir_txt2img_samples\": \"outputs/txt2img-images\",\r\n        \"outdir_img2img_samples\": \"outputs/img2img-images\",\r\n        \"outdir_extras_samples\": \"outputs/extras-images\",\r\n        \"outdir_grids\": \"\",\r\n        \"outdir_txt2img_grids\": \"outputs/txt2img-grids\",\r\n        \"outdir_img2img_grids\": \"outputs/img2img-grids\",\r\n        \"outdir_save\": \"log/images\",\r\n        \"outdir_init_images\": \"outputs/init-images\",\r\n        \"save_to_dirs\": true,\r\n        \"grid_save_to_dirs\": true,\r\n        \"use_save_to_dirs_for_ui\": false,\r\n        \"directories_filename_pattern\": \"[date]\",\r\n        \"directories_max_prompt_words\": 8,\r\n        \"ESRGAN_tile\": 192,\r\n        \"ESRGAN_tile_overlap\": 8,\r\n        \"realesrgan_enabled_models\": [\r\n            \"R-ESRGAN 4x+\",\r\n            \"R-ESRGAN 4x+ Anime6B\"\r\n        ],\r\n        \"upscaler_for_img2img\": null,\r\n        \"face_restoration_model\": \"CodeFormer\",\r\n        \"code_former_weight\": 0.5,\r\n        \"face_restoration_unload\": false,\r\n        \"show_warnings\": false,\r\n        \"memmon_poll_rate\": 8,\r\n        \"samples_log_stdout\": false,\r\n        \"multiple_tqdm\": true,\r\n        \"print_hypernet_extra\": false,\r\n        \"list_hidden_files\": true,\r\n        \"disable_mmap_load_safetensors\": false,\r\n        \"unload_models_when_training\": false,\r\n        \"pin_memory\": false,\r\n        \"save_optimizer_state\": false,\r\n        \"save_training_settings_to_txt\": true,\r\n        \"dataset_filename_word_regex\": \"\",\r\n        \"dataset_filename_join_string\": \" \",\r\n        \"training_image_repeats_per_epoch\": 1,\r\n        \"training_write_csv_every\": 500,\r\n        \"training_xattention_optimizations\": false,\r\n        \"training_enable_tensorboard\": false,\r\n        \"training_tensorboard_save_images\": false,\r\n        \"training_tensorboard_flush_every\": 120,\r\n        \"sd_model_checkpoint\": \"hassakuHentaiModel_v13.safetensors [7eb674963a]\",\r\n        \"sd_checkpoint_cache\": 0,\r\n        \"sd_vae_checkpoint_cache\": 0,\r\n        \"sd_vae\": \"None\",\r\n        \"sd_vae_as_default\": true,\r\n        \"sd_unet\": \"Automatic\",\r\n        \"inpainting_mask_weight\": 1.0,\r\n        \"initial_noise_multiplier\": 1.0,\r\n        \"img2img_color_correction\": false,\r\n        \"img2img_fix_steps\": false,\r\n        \"img2img_background_color\": \"#ffffff\",\r\n        \"enable_quantization\": false,\r\n        \"enable_emphasis\": true,\r\n        \"enable_batch_seeds\": true,\r\n        \"comma_padding_backtrack\": 20,\r\n        \"CLIP_stop_at_last_layers\": 2,\r\n        \"upcast_attn\": false,\r\n        \"auto_vae_precision\": true,\r\n        \"randn_source\": \"GPU\",\r\n        \"sdxl_crop_top\": 0,\r\n        \"sdxl_crop_left\": 0,\r\n        \"sdxl_refiner_low_aesthetic_score\": 2.5,\r\n        \"sdxl_refiner_high_aesthetic_score\": 6.0,\r\n        \"cross_attention_optimization\": \"Automatic\",\r\n        \"s_min_uncond\": 0.0,\r\n        \"token_merging_ratio\": 0.0,\r\n        \"token_merging_ratio_img2img\": 0.0,\r\n        \"token_merging_ratio_hr\": 0.0,\r\n        \"pad_cond_uncond\": false,\r\n        \"experimental_persistent_cond_cache\": false,\r\n        \"use_old_emphasis_implementation\": false,\r\n        \"use_old_karras_scheduler_sigmas\": false,\r\n        \"no_dpmpp_sde_batch_determinism\": false,\r\n        \"use_old_hires_fix_width_height\": false,\r\n        \"dont_fix_second_order_samplers_schedule\": false,\r\n        \"hires_fix_use_firstpass_conds\": false,\r\n        \"interrogate_keep_models_in_memory\": false,\r\n        \"interrogate_return_ranks\": false,\r\n        \"interrogate_clip_num_beams\": 1,\r\n        \"interrogate_clip_min_length\": 24,\r\n        \"interrogate_clip_max_length\": 48,\r\n        \"interrogate_clip_dict_limit\": 1500,\r\n        \"interrogate_clip_skip_categories\": [],\r\n        \"interrogate_deepbooru_score_threshold\": 0.5,\r\n        \"deepbooru_sort_alpha\": true,\r\n        \"deepbooru_use_spaces\": true,\r\n        \"deepbooru_escape\": true,\r\n        \"deepbooru_filter_tags\": \"\",\r\n        \"extra_networks_show_hidden_directories\": true,\r\n        \"extra_networks_hidden_models\": \"When searched\",\r\n        \"extra_networks_default_multiplier\": 1.0,\r\n        \"extra_networks_card_width\": 0,\r\n        \"extra_networks_card_height\": 0,\r\n        \"extra_networks_card_text_scale\": 1.0,\r\n        \"extra_networks_card_show_desc\": true,\r\n        \"extra_networks_add_text_separator\": \" \",\r\n        \"ui_extra_networks_tab_reorder\": \"\",\r\n        \"textual_inversion_print_at_load\": false,\r\n        \"textual_inversion_add_hashes_to_infotext\": true,\r\n        \"sd_hypernetwork\": \"None\",\r\n        \"localization\": \"None\",\r\n        \"gradio_theme\": \"Default\",\r\n        \"img2img_editor_height\": 720,\r\n        \"return_grid\": true,\r\n        \"return_mask\": false,\r\n        \"return_mask_composite\": false,\r\n        \"do_not_show_images\": false,\r\n        \"send_seed\": true,\r\n        \"send_size\": true,\r\n        \"js_modal_lightbox\": true,\r\n        \"js_modal_lightbox_initially_zoomed\": true,\r\n        \"js_modal_lightbox_gamepad\": false,\r\n        \"js_modal_lightbox_gamepad_repeat\": 250,\r\n        \"show_progress_in_title\": true,\r\n        \"samplers_in_dropdown\": true,\r\n        \"dimensions_and_batch_together\": true,\r\n        \"keyedit_precision_attention\": 0.1,\r\n        \"keyedit_precision_extra\": 0.05,\r\n        \"keyedit_delimiters\": \".,\\\\/!?%^*;:{}=`~()\",\r\n        \"keyedit_move\": true,\r\n        \"quicksettings_list\": [\r\n            \"sd_model_checkpoint\",\r\n            \"sd_vae\",\r\n            \"n_rows\"\r\n        ],\r\n        \"ui_tab_order\": [],\r\n        \"hidden_tabs\": [],\r\n        \"ui_reorder_list\": [],\r\n        \"hires_fix_show_sampler\": false,\r\n        \"hires_fix_show_prompts\": false,\r\n        \"disable_token_counters\": false,\r\n        \"add_model_hash_to_info\": true,\r\n        \"add_model_name_to_info\": true,\r\n        \"add_user_name_to_info\": false,\r\n        \"add_version_to_infotext\": true,\r\n        \"disable_weights_auto_swap\": true,\r\n        \"infotext_styles\": \"Apply if any\",\r\n        \"show_progressbar\": true,\r\n        \"live_previews_enable\": true,\r\n        \"live_previews_image_format\": \"png\",\r\n        \"show_progress_grid\": true,\r\n        \"show_progress_every_n_steps\": 10,\r\n        \"show_progress_type\": \"Approx NN\",\r\n        \"live_preview_content\": \"Prompt\",\r\n        \"live_preview_refresh_period\": 1000,\r\n        \"hide_samplers\": [],\r\n        \"eta_ddim\": 0.0,\r\n        \"eta_ancestral\": 1.0,\r\n        \"ddim_discretize\": \"uniform\",\r\n        \"s_churn\": 0.0,\r\n        \"s_tmin\": 0.0,\r\n        \"s_noise\": 1.0,\r\n        \"k_sched_type\": \"Automatic\",\r\n        \"sigma_min\": 0.0,\r\n        \"sigma_max\": 0.0,\r\n        \"rho\": 0.0,\r\n        \"eta_noise_seed_delta\": 0,\r\n        \"always_discard_next_to_last_sigma\": false,\r\n        \"uni_pc_variant\": \"bh1\",\r\n        \"uni_pc_skip_type\": \"time_uniform\",\r\n        \"uni_pc_order\": 3,\r\n        \"uni_pc_lower_order_final\": true,\r\n        \"postprocessing_enable_in_main_ui\": [],\r\n        \"postprocessing_operation_order\": [],\r\n        \"upscaling_max_images_in_cache\": 5,\r\n        \"disabled_extensions\": [\r\n            \"a1111-sd-webui-locon\",\r\n            \"a1111-sd-webui-lycoris\",\r\n            \"sd-webui-aspect-ratio-helper\",\r\n            \"stable-diffusion-webui-state\"\r\n        ],\r\n        \"disable_all_extensions\": \"none\",\r\n        \"restore_config_state_file\": \"\",\r\n        \"sd_checkpoint_hash\": \"7eb674963a0d93fcead95905b4d658d1e94e9a36230c0c732f65c3f53d8888aa\",\r\n        \"ldsr_steps\": 100,\r\n        \"ldsr_cached\": false,\r\n        \"SCUNET_tile\": 256,\r\n        \"SCUNET_tile_overlap\": 8,\r\n        \"SWIN_tile\": 192,\r\n        \"SWIN_tile_overlap\": 8,\r\n        \"lora_functional\": false,\r\n        \"sd_lora\": \"None\",\r\n        \"lora_preferred_name\": \"Alias from file\",\r\n        \"lora_add_hashes_to_infotext\": true,\r\n        \"lora_show_all\": false,\r\n        \"lora_hide_unknown_for_versions\": [],\r\n        \"extra_options\": [],\r\n        \"extra_options_accordion\": false,\r\n        \"canvas_hotkey_zoom\": \"Alt\",\r\n        \"canvas_hotkey_adjust\": \"Ctrl\",\r\n        \"canvas_hotkey_move\": \"F\",\r\n        \"canvas_hotkey_fullscreen\": \"S\",\r\n        \"canvas_hotkey_reset\": \"R\",\r\n        \"canvas_hotkey_overlap\": \"O\",\r\n        \"canvas_show_tooltip\": true,\r\n        \"canvas_blur_prompt\": false,\r\n        \"canvas_disabled_functions\": [\r\n            \"Overlap\"\r\n        ],\r\n        \"additional_networks_extra_lora_path\": \"\",\r\n        \"additional_networks_sort_models_by\": \"name\",\r\n        \"additional_networks_reverse_sort_order\": false,\r\n        \"additional_networks_model_name_filter\": \"\",\r\n        \"additional_networks_xy_grid_model_metadata\": \"\",\r\n        \"additional_networks_hash_thread_count\": 1.0,\r\n        \"additional_networks_back_up_model_when_saving\": true,\r\n        \"additional_networks_show_only_safetensors\": false,\r\n        \"additional_networks_show_only_models_with_metadata\": \"disabled\",\r\n        \"additional_networks_max_top_tags\": 20.0,\r\n        \"additional_networks_max_dataset_folders\": 20.0,\r\n        \"control_net_model_config\": \"models\\\\cldm_v15.yaml\",\r\n        \"control_net_model_adapter_config\": \"models\\\\t2iadapter_sketch_sd14v1.yaml\",\r\n        \"control_net_detectedmap_dir\": \"detected_maps\",\r\n        \"control_net_models_path\": \"\",\r\n        \"control_net_modules_path\": \"\",\r\n        \"control_net_max_models_num\": 3,\r\n        \"control_net_model_cache_size\": 1,\r\n        \"control_net_inpaint_blur_sigma\": 7,\r\n        \"control_net_no_high_res_fix\": false,\r\n        \"control_net_no_detectmap\": false,\r\n        \"control_net_detectmap_autosaving\": false,\r\n        \"control_net_allow_script_control\": false,\r\n        \"control_net_sync_field_args\": false,\r\n        \"controlnet_show_batch_images_in_ui\": false,\r\n        \"controlnet_increment_seed_during_batch\": false,\r\n        \"controlnet_disable_control_type\": false,\r\n        \"controlnet_disable_openpose_edit\": false,\r\n        \"dataset_editor_image_columns\": 6.0,\r\n        \"dataset_editor_max_res\": 0.0,\r\n        \"dataset_editor_use_temp_files\": false,\r\n        \"dataset_editor_use_raw_clip_token\": true,\r\n        \"sd_vae_overrides_per_model_preferences\": false,\r\n        \"save_incomplete_images\": false,\r\n        \"face_restoration\": false,\r\n        \"auto_launch_browser\": \"Local\",\r\n        \"show_gradio_deprecation_warnings\": true,\r\n        \"hide_ldm_prints\": true,\r\n        \"api_enable_requests\": true,\r\n        \"api_forbid_local_requests\": true,\r\n        \"api_useragent\": \"\",\r\n        \"sd_checkpoints_limit\": 1,\r\n        \"sd_checkpoints_keep_in_cpu\": true,\r\n        \"tiling\": false,\r\n        \"hires_fix_refiner_pass\": \"second pass\",\r\n        \"sd_vae_encode_method\": \"Full\",\r\n        \"sd_vae_decode_method\": \"Full\",\r\n        \"img2img_extra_noise\": 0,\r\n        \"img2img_sketch_default_brush_color\": \"#ffffff\",\r\n        \"img2img_inpaint_mask_brush_color\": \"#ffffff\",\r\n        \"img2img_inpaint_sketch_default_brush_color\": \"#ffffff\",\r\n        \"persistent_cond_cache\": true,\r\n        \"batch_cond_uncond\": true,\r\n        \"use_old_scheduling\": false,\r\n        \"lora_in_memory_limit\": 0,\r\n        \"gradio_themes_cache\": true,\r\n        \"gallery_height\": \"\",\r\n        \"extra_options_txt2img\": [],\r\n        \"extra_options_img2img\": [],\r\n        \"extra_options_cols\": 1,\r\n        \"live_preview_allow_lowvram_full\": false,\r\n        \"live_preview_fast_interrupt\": false,\r\n        \"s_tmax\": 0,\r\n        \"sgm_noise_multiplier\": false,\r\n        \"canvas_auto_expand\": true,\r\n        \"ad_max_models\": 2,\r\n        \"ad_save_previews\": false,\r\n        \"ad_save_images_before\": false,\r\n        \"ad_only_seleted_scripts\": true,\r\n        \"ad_script_names\": \"dynamic_prompting,dynamic_thresholding,wildcard_recursive,wildcards,lora_block_weight\",\r\n        \"ad_bbox_sortby\": \"None\",\r\n        \"queue_paused\": false,\r\n        \"queue_button_placement\": \"Under Generate button\",\r\n        \"queue_button_hide_checkpoint\": true,\r\n        \"queue_history_retention_days\": \"30 days\",\r\n        \"queue_keyboard_shortcut\": \"Ctrl+KeyE\",\r\n        \"queue_ui_placement\": \"As a tab\",\r\n        \"control_net_unit_count\": 3,\r\n        \"controlnet_ignore_noninpaint_mask\": false\r\n    },\r\n    \"Startup\": {\r\n        \"total\": 16.119651317596436,\r\n        \"records\": {\r\n            \"initial startup\": 0.0010001659393310547,\r\n            \"prepare environment/checks\": 0.01200556755065918,\r\n            \"prepare environment/git version info\": 0.04651784896850586,\r\n            \"prepare environment/torch GPU test\": 1.6494877338409424,\r\n            \"prepare environment/clone repositores\": 0.13556528091430664,\r\n            \"prepare environment/run extensions installers/adetailer\": 0.131927490234375,\r\n            \"prepare environment/run extensions installers/embedding-inspector\": 0.0,\r\n            \"prepare environment/run extensions installers/openpose-editor\": 0.0,\r\n            \"prepare environment/run extensions installers/sd-webui-additional-networks\": 0.0,\r\n            \"prepare environment/run extensions installers/sd-webui-agent-scheduler\": 0.07900881767272949,\r\n            \"prepare environment/run extensions installers/sd-webui-ar\": 0.0,\r\n            \"prepare environment/run extensions installers/sd-webui-controlnet\": 0.26058197021484375,\r\n            \"prepare environment/run extensions installers/sd-webui-infinite-image-browsing\": 0.26610326766967773,\r\n            \"prepare environment/run extensions installers/stable-diffusion-webui-dataset-tag-editor\": 0.0010018348693847656,\r\n            \"prepare environment/run extensions installers\": 0.7386233806610107,\r\n            \"prepare environment\": 2.6301982402801514,\r\n            \"launcher\": 0.0019989013671875,\r\n            \"import torch\": 2.702693462371826,\r\n            \"import gradio\": 0.8281288146972656,\r\n            \"setup paths\": 0.6810657978057861,\r\n            \"import ldm\": 0.004998922348022461,\r\n            \"import sgm\": 0.0,\r\n            \"initialize shared\": 0.18516278266906738,\r\n            \"other imports\": 0.5169463157653809,\r\n            \"opts onchange\": 0.0,\r\n            \"setup SD model\": 0.0009999275207519531,\r\n            \"setup codeformer\": 0.09752225875854492,\r\n            \"setup gfpgan\": 0.018000364303588867,\r\n            \"set samplers\": 0.0,\r\n            \"list extensions\": 0.0,\r\n            \"restore config state file\": 0.0,\r\n            \"list SD models\": 0.3444406986236572,\r\n            \"list localizations\": 0.0,\r\n            \"load scripts/custom_code.py\": 0.003999471664428711,\r\n            \"load scripts/img2imgalt.py\": 0.0009996891021728516,\r\n            \"load scripts/loopback.py\": 0.0,\r\n            \"load scripts/outpainting_mk_2.py\": 0.0009968280792236328,\r\n            \"load scripts/poor_mans_outpainting.py\": 0.0,\r\n            \"load scripts/postprocessing_codeformer.py\": 0.0,\r\n            \"load scripts/postprocessing_gfpgan.py\": 0.0010004043579101562,\r\n            \"load scripts/postprocessing_upscale.py\": 0.0,\r\n            \"load scripts/prompt_matrix.py\": 0.0009996891021728516,\r\n            \"load scripts/prompts_from_file.py\": 0.0,\r\n            \"load scripts/refiner.py\": 0.0,\r\n            \"load scripts/sd_upscale.py\": 0.0009996891021728516,\r\n            \"load scripts/seed.py\": 0.0,\r\n            \"load scripts/xyz_grid.py\": 0.0010013580322265625,\r\n            \"load scripts/!adetailer.py\": 1.049208164215088,\r\n            \"load scripts/embedding_inspector.py\": 0.02595806121826172,\r\n            \"load scripts/main.py\": 0.06292295455932617,\r\n            \"load scripts/additional_networks.py\": 0.0687415599822998,\r\n            \"load scripts/addnet_xyz_grid_support.py\": 0.0005855560302734375,\r\n            \"load scripts/lora_compvis.py\": 0.0005061626434326172,\r\n            \"load scripts/metadata_editor.py\": 0.0005266666412353516,\r\n            \"load scripts/model_util.py\": 0.0042002201080322266,\r\n            \"load scripts/safetensors_hack.py\": 0.0005214214324951172,\r\n            \"load scripts/util.py\": 0.0,\r\n            \"load scripts/task_scheduler.py\": 0.2628440856933594,\r\n            \"load scripts/sd-webui-ar.py\": 0.0,\r\n            \"load scripts/adapter.py\": 0.0010018348693847656,\r\n            \"load scripts/api.py\": 0.1220850944519043,\r\n            \"load scripts/batch_hijack.py\": 0.0009999275207519531,\r\n            \"load scripts/cldm.py\": 0.0,\r\n            \"load scripts/controlmodel_ipadapter.py\": 0.0,\r\n            \"load scripts/controlnet.py\": 0.09201717376708984,\r\n            \"load scripts/controlnet_diffusers.py\": 0.0010013580322265625,\r\n            \"load scripts/controlnet_lllite.py\": 0.0,\r\n            \"load scripts/controlnet_lora.py\": 0.0009987354278564453,\r\n            \"load scripts/controlnet_model_guess.py\": 0.0,\r\n            \"load scripts/controlnet_version.py\": 0.0010018348693847656,\r\n            \"load scripts/external_code.py\": 0.0,\r\n            \"load scripts/global_state.py\": 0.000997781753540039,\r\n            \"load scripts/hook.py\": 0.0010037422180175781,\r\n            \"load scripts/infotext.py\": 0.0,\r\n            \"load scripts/logging.py\": 0.0009975433349609375,\r\n            \"load scripts/lvminthin.py\": 0.0,\r\n            \"load scripts/movie2movie.py\": 0.0010006427764892578,\r\n            \"load scripts/processor.py\": 0.0,\r\n            \"load scripts/utils.py\": 0.0010023117065429688,\r\n            \"load scripts/xyz_grid_support.py\": 0.0,\r\n            \"load scripts/iib_setup.py\": 0.06318879127502441,\r\n            \"load scripts/dte_instance.py\": 0.012150764465332031,\r\n            \"load scripts/singleton.py\": 0.0,\r\n            \"load scripts/ldsr_model.py\": 0.018006086349487305,\r\n            \"load scripts/lora_script.py\": 0.8579683303833008,\r\n            \"load scripts/scunet_model.py\": 0.018000364303588867,\r\n            \"load scripts/swinir_model.py\": 0.016000032424926758,\r\n            \"load scripts/hotkey_config.py\": 0.0,\r\n            \"load scripts/extra_options_section.py\": 0.0009999275207519531,\r\n            \"load scripts\": 2.696434259414673,\r\n            \"load upscalers\": 0.002000093460083008,\r\n            \"refresh VAE\": 0.0009999275207519531,\r\n            \"refresh textual inversion templates\": 0.0,\r\n            \"scripts list_optimizers\": 0.0010056495666503906,\r\n            \"scripts list_unets\": 0.0,\r\n            \"reload hypernetworks\": 0.0010004043579101562,\r\n            \"initialize extra networks\": 0.015149354934692383,\r\n            \"scripts before_ui_callback\": 0.0020058155059814453,\r\n            \"create ui\": 0.7552030086517334,\r\n            \"gradio launch\": 4.240787982940674,\r\n            \"add APIs\": 0.00451350212097168,\r\n            \"app_started_callback/task_scheduler.py\": 0.4163961410522461,\r\n            \"app_started_callback/api.py\": 0.0010004043579101562,\r\n            \"app_started_callback/iib_setup.py\": 0.017996549606323242,\r\n            \"app_started_callback/lora_script.py\": 0.0,\r\n            \"app_started_callback\": 0.4353930950164795\r\n        }\r\n    },\r\n    \"Packages\": [\r\n        \"absl-py==1.4.0\",\r\n        \"accelerate==0.21.0\",\r\n        \"addict==2.4.0\",\r\n        \"aenum==3.1.15\",\r\n        \"aiofiles==23.1.0\",\r\n        \"aiohttp==3.8.5\",\r\n        \"aiosignal==1.3.1\",\r\n        \"altair==5.0.1\",\r\n        \"antlr4-python3-runtime==4.9.3\",\r\n        \"anyio==3.7.1\",\r\n        \"async-timeout==4.0.2\",\r\n        \"attrs==23.1.0\",\r\n        \"basicsr==1.4.2\",\r\n        \"beautifulsoup4==4.12.2\",\r\n        \"blendmodes==2022\",\r\n        \"boltons==23.0.0\",\r\n        \"cachetools==5.3.1\",\r\n        \"certifi==2023.7.22\",\r\n        \"cffi==1.15.1\",\r\n        \"charset-normalizer==3.2.0\",\r\n        \"clean-fid==0.1.35\",\r\n        \"click==8.1.6\",\r\n        \"clip==1.0\",\r\n        \"colorama==0.4.6\",\r\n        \"coloredlogs==15.0.1\",\r\n        \"contourpy==1.1.0\",\r\n        \"cssselect2==0.7.0\",\r\n        \"cycler==0.11.0\",\r\n        \"deprecation==2.1.0\",\r\n        \"dill==0.3.7\",\r\n        \"einops==0.4.1\",\r\n        \"exceptiongroup==1.1.2\",\r\n        \"facexlib==0.3.0\",\r\n        \"fastapi==0.94.0\",\r\n        \"ffmpy==0.3.1\",\r\n        \"filelock==3.12.2\",\r\n        \"filterpy==1.4.5\",\r\n        \"flatbuffers==23.5.26\",\r\n        \"fonttools==4.41.1\",\r\n        \"frozenlist==1.4.0\",\r\n        \"fsspec==2023.6.0\",\r\n        \"ftfy==6.1.1\",\r\n        \"future==0.18.3\",\r\n        \"fvcore==0.1.5.post20221221\",\r\n        \"gdown==4.7.1\",\r\n        \"gfpgan==1.3.8\",\r\n        \"gitdb==4.0.10\",\r\n        \"gitpython==3.1.32\",\r\n        \"google-auth-oauthlib==1.0.0\",\r\n        \"google-auth==2.22.0\",\r\n        \"gradio-client==0.5.0\",\r\n        \"gradio==3.41.2\",\r\n        \"greenlet==2.0.2\",\r\n        \"grpcio==1.56.2\",\r\n        \"h11==0.12.0\",\r\n        \"httpcore==0.15.0\",\r\n        \"httpx==0.24.1\",\r\n        \"huggingface-hub==0.16.4\",\r\n        \"humanfriendly==10.0\",\r\n        \"idna==3.4\",\r\n        \"imageio==2.31.1\",\r\n        \"importlib-metadata==6.8.0\",\r\n        \"importlib-resources==6.0.1\",\r\n        \"inflection==0.5.1\",\r\n        \"iopath==0.1.9\",\r\n        \"jinja2==3.1.2\",\r\n        \"jsonmerge==1.8.0\",\r\n        \"jsonschema-specifications==2023.7.1\",\r\n        \"jsonschema==4.18.4\",\r\n        \"kiwisolver==1.4.4\",\r\n        \"kornia==0.6.7\",\r\n        \"lark==1.1.2\",\r\n        \"lazy-loader==0.3\",\r\n        \"lightning-utilities==0.9.0\",\r\n        \"linkify-it-py==2.0.2\",\r\n        \"llvmlite==0.40.1\",\r\n        \"lmdb==1.4.1\",\r\n        \"lpips==0.1.4\",\r\n        \"lxml==4.9.3\",\r\n        \"markdown-it-py==2.2.0\",\r\n        \"markdown==3.4.4\",\r\n        \"markupsafe==2.1.3\",\r\n        \"matplotlib==3.7.2\",\r\n        \"mdit-py-plugins==0.3.3\",\r\n        \"mdurl==0.1.2\",\r\n        \"mediapipe==0.10.5\",\r\n        \"mpmath==1.3.0\",\r\n        \"multidict==6.0.4\",\r\n        \"networkx==3.1\",\r\n        \"numba==0.57.1\",\r\n        \"numpy==1.23.5\",\r\n        \"oauthlib==3.2.2\",\r\n        \"omegaconf==2.2.3\",\r\n        \"onnxruntime-gpu==1.16.0\",\r\n        \"open-clip-torch==2.20.0\",\r\n        \"opencv-contrib-python==4.8.0.74\",\r\n        \"opencv-python==4.8.0.74\",\r\n        \"orjson==3.9.2\",\r\n        \"packaging==23.1\",\r\n        \"pandas==2.0.3\",\r\n        \"piexif==1.1.3\",\r\n        \"pillow==9.5.0\",\r\n        \"pip==23.2.1\",\r\n        \"platformdirs==3.9.1\",\r\n        \"portalocker==2.7.0\",\r\n        \"protobuf==3.20.0\",\r\n        \"psutil==5.9.5\",\r\n        \"py-cpuinfo==9.0.0\",\r\n        \"pyasn1-modules==0.3.0\",\r\n        \"pyasn1==0.5.0\",\r\n        \"pycparser==2.21\",\r\n        \"pydantic==1.10.12\",\r\n        \"pydub==0.25.1\",\r\n        \"pyfunctional==1.4.3\",\r\n        \"pygments==2.15.1\",\r\n        \"pyparsing==3.0.9\",\r\n        \"pyreadline3==3.4.1\",\r\n        \"pysocks==1.7.1\",\r\n        \"python-dateutil==2.8.2\",\r\n        \"python-dotenv==1.0.0\",\r\n        \"python-multipart==0.0.6\",\r\n        \"pytorch-lightning==1.9.4\",\r\n        \"pytz==2023.3\",\r\n        \"pywavelets==1.4.1\",\r\n        \"pywin32==306\",\r\n        \"pyyaml==6.0.1\",\r\n        \"realesrgan==0.3.0\",\r\n        \"referencing==0.30.0\",\r\n        \"regex==2023.6.3\",\r\n        \"reportlab==4.0.4\",\r\n        \"requests-oauthlib==1.3.1\",\r\n        \"requests==2.31.0\",\r\n        \"resize-right==0.0.2\",\r\n        \"rich==13.5.2\",\r\n        \"rpds-py==0.9.2\",\r\n        \"rsa==4.9\",\r\n        \"safetensors==0.3.1\",\r\n        \"scikit-image==0.21.0\",\r\n        \"scipy==1.11.1\",\r\n        \"seaborn==0.12.2\",\r\n        \"semantic-version==2.10.0\",\r\n        \"sentencepiece==0.1.99\",\r\n        \"setuptools==63.2.0\",\r\n        \"six==1.16.0\",\r\n        \"smmap==5.0.0\",\r\n        \"sniffio==1.3.0\",\r\n        \"sounddevice==0.4.6\",\r\n        \"soupsieve==2.4.1\",\r\n        \"sqlalchemy==2.0.20\",\r\n        \"starlette==0.26.1\",\r\n        \"svglib==1.5.1\",\r\n        \"sympy==1.12\",\r\n        \"tabulate==0.9.0\",\r\n        \"tb-nightly==2.14.0a20230728\",\r\n        \"tensorboard-data-server==0.7.1\",\r\n        \"termcolor==2.3.0\",\r\n        \"tifffile==2023.7.18\",\r\n        \"timm==0.9.2\",\r\n        \"tinycss2==1.2.1\",\r\n        \"tokenizers==0.13.3\",\r\n        \"tomesd==0.1.3\",\r\n        \"tomli==2.0.1\",\r\n        \"toolz==0.12.0\",\r\n        \"torch==2.0.1+cu118\",\r\n        \"torchdiffeq==0.2.3\",\r\n        \"torchmetrics==1.0.1\",\r\n        \"torchsde==0.2.5\",\r\n        \"torchvision==0.15.2+cu118\",\r\n        \"tqdm==4.65.0\",\r\n        \"trampoline==0.1.2\",\r\n        \"transformers==4.30.2\",\r\n        \"typing-extensions==4.7.1\",\r\n        \"tzdata==2023.3\",\r\n        \"uc-micro-py==1.0.2\",\r\n        \"ultralytics==8.0.187\",\r\n        \"urllib3==1.26.16\",\r\n        \"uvicorn==0.23.1\",\r\n        \"wcwidth==0.2.6\",\r\n        \"webencodings==0.5.1\",\r\n        \"websockets==11.0.3\",\r\n        \"werkzeug==2.3.6\",\r\n        \"wheel==0.41.0\",\r\n        \"xformers==0.0.20\",\r\n        \"yacs==0.1.8\",\r\n        \"yapf==0.40.1\",\r\n        \"yarl==1.9.2\",\r\n        \"zipp==3.16.2\"\r\n    ]\r\n}\n\n### What browsers do you use to access the UI ?\n\n_No response_\n\n### Console logs\n\n```Shell\nvenv \"E:\\stable-diffusion-webui\\venv\\Scripts\\Python.exe\"\r\nPython 3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]\r\nVersion: v1.6.0\r\nCommit hash: 5ef669de080814067961f28357256e8fe27544f4\r\nLaunching Web UI with arguments: --listen --xformers --medvram --enable-insecure-extension-access --no-half-vae\r\n[-] ADetailer initialized. version: 23.9.3, num models: 11\r\n[AddNet] Updating model hashes...\r\n0it [00:00, ?it/s]\r\n[AddNet] Updating model hashes...\r\n0it [00:00, ?it/s]\r\n2023-09-26 14:05:00,898 - ControlNet - INFO - ControlNet v1.1.410\r\nControlNet preprocessor location: E:\\stable-diffusion-webui\\extensions\\sd-webui-controlnet\\annotator\\downloads\r\n2023-09-26 14:05:01,017 - ControlNet - INFO - ControlNet v1.1.410\r\nLoading weights [7eb674963a] from E:\\stable-diffusion-webui\\models\\Stable-diffusion\\hassakuHentaiModel_v13.safetensors\r\nCreating model from config: E:\\stable-diffusion-webui\\configs\\v1-inference.yaml\r\nE:\\stable-diffusion-webui\\extensions\\sd-webui-additional-networks\\scripts\\metadata_editor.py:399: GradioDeprecationWarning: The `style` method is deprecated. Please set these arguments in the constructor instead.\r\n  with gr.Row().style(equal_height=False):\r\nE:\\stable-diffusion-webui\\extensions\\sd-webui-additional-networks\\scripts\\metadata_editor.py:521: GradioDeprecationWarning: The `style` method is deprecated. Please set these arguments in the constructor instead.\r\n  cover_image = gr.Image(\r\nE:\\stable-diffusion-webui\\extensions\\stable-diffusion-webui-dataset-tag-editor\\scripts\\main.py:218: GradioDeprecationWarning: The `style` method is deprecated. Please set these arguments in the constructor instead.\r\n  with gr.Row().style(equal_height=False):\r\nE:\\stable-diffusion-webui\\extensions\\stable-diffusion-webui-dataset-tag-editor\\scripts\\tag_editor_ui\\block_dataset_gallery.py:25: GradioDeprecationWarning: The `style` method is deprecated. Please set these arguments in the constructor instead.\r\n  self.gl_dataset_images = gr.Gallery(label='Dataset Images', elem_id=\"dataset_tag_editor_dataset_gallery\").style(grid=image_columns)\r\nE:\\stable-diffusion-webui\\extensions\\stable-diffusion-webui-dataset-tag-editor\\scripts\\tag_editor_ui\\block_dataset_gallery.py:25: GradioDeprecationWarning: The 'grid' parameter will be deprecated. Please use 'columns' in the constructor instead.\r\n  self.gl_dataset_images = gr.Gallery(label='Dataset Images', elem_id=\"dataset_tag_editor_dataset_gallery\").style(grid=image_columns)\r\nE:\\stable-diffusion-webui\\extensions\\stable-diffusion-webui-dataset-tag-editor\\scripts\\tag_editor_ui\\tab_filter_by_selection.py:35: GradioDeprecationWarning: The `style` method is deprecated. Please set these arguments in the constructor instead.\r\n  self.gl_filter_images = gr.Gallery(label='Filter Images', elem_id=\"dataset_tag_editor_filter_gallery\").style(grid=image_columns)\r\nE:\\stable-diffusion-webui\\extensions\\stable-diffusion-webui-dataset-tag-editor\\scripts\\tag_editor_ui\\tab_filter_by_selection.py:35: GradioDeprecationWarning: The 'grid' parameter will be deprecated. Please use 'columns' in the constructor instead.\r\n  self.gl_filter_images = gr.Gallery(label='Filter Images', elem_id=\"dataset_tag_editor_filter_gallery\").style(grid=image_columns)\r\nRunning on local URL:  http://0.0.0.0:7860\r\nApplying attention optimization: xformers... done.\r\nModel loaded in 3.3s (load weights from disk: 0.6s, create model: 0.4s, apply weights to model: 0.5s, calculate empty prompt: 1.6s).\r\n\r\nTo create a public link, set `share=True` in `launch()`.\r\nStartup time: 20.9s (prepare environment: 4.1s, import torch: 3.2s, import gradio: 1.7s, setup paths: 1.6s, initialize shared: 0.2s, other imports: 1.2s, setup codeformer: 0.1s, list SD models: 0.3s, load scripts: 2.8s, create ui: 0.9s, gradio launch: 4.2s, app_started_callback: 0.4s).\n```\n\n\n### Additional information\n\nThere are several workarounds for this, not the least of which would be simply changing the filenames, but this is a bug nonetheless.\n", "hints_text": "", "created_at": "2023-09-27T03:11:57Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 13395, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-13395", "issue_numbers": ["12945"], "base_commit": "102b6617dacffdcc89c56badcaae6c5e83c3ff21", "patch": "diff --git a/modules/ui_extra_networks.py b/modules/ui_extra_networks.py\nindex 063bd7b80e6..60b95f21855 100644\n--- a/modules/ui_extra_networks.py\n+++ b/modules/ui_extra_networks.py\n@@ -213,9 +213,9 @@ def create_html_for_item(self, item, tabname):\n         metadata_button = \"\"\r\n         metadata = item.get(\"metadata\")\r\n         if metadata:\r\n-            metadata_button = f\"<div class='metadata-button card-button' title='Show internal metadata' onclick='extraNetworksRequestMetadata(event, {quote_js(self.name)}, {quote_js(item['name'])})'></div>\"\r\n+            metadata_button = f\"<div class='metadata-button card-button' title='Show internal metadata' onclick='extraNetworksRequestMetadata(event, {quote_js(self.name)}, {quote_js(html.escape(item['name']))})'></div>\"\r\n \r\n-        edit_button = f\"<div class='edit-button card-button' title='Edit metadata' onclick='extraNetworksEditUserMetadata(event, {quote_js(tabname)}, {quote_js(self.id_page)}, {quote_js(item['name'])})'></div>\"\r\n+        edit_button = f\"<div class='edit-button card-button' title='Edit metadata' onclick='extraNetworksEditUserMetadata(event, {quote_js(tabname)}, {quote_js(self.id_page)}, {quote_js(html.escape(item['name']))})'></div>\"\r\n \r\n         local_path = \"\"\r\n         filename = item.get(\"filename\", \"\")\r\n", "test_patch": "", "problem_statement": "[Bug]: Unable to edit metadata on a checkpoiont with an apostrophe in the file name\n### Is there an existing issue for this?\n\n- [X] I have searched the existing issues and checked the recent builds/commits\n\n### What happened?\n\nI was trying to edit the metadata of the check point \"Yamer's Realistic\", but every time I clicked the red tool icon the UI loaded that checkpoint instead. I test a few things and eventually worked that if I remove the apostrophe everything worked.\r\n\r\nbtw. I think I'm currently on the dev branch\n\n### Steps to reproduce the problem\n\n1. Have a check point with an apostrophe in the name eg. badlynamed'checkpoint.safetensors\r\n2. Open the extra networks tab, if needed\r\n3. Attempt to edit the metadata of the checkpoint by clicking the red tool icon in the upper right corner of the checkpoints card\r\n4. The UI loads the card instead of showing the edit popup\r\n\n\n### What should have happened?\n\nthe edit popup for the checkpoints metadata is shown\n\n### Sysinfo\n\n{\r\n    \"Platform\": \"Windows-10-10.0.22621-SP0\",\r\n    \"Python\": \"3.10.11\",\r\n    \"Version\": \"v1.5.1-540-g9d2299ed\",\r\n    \"Commit\": \"9d2299ed0bd6c81cae8a7ba4ca22d6a14fb27bef\",\r\n    \"Script path\": \"F:\\\\Stable\\\\Automatic1111\",\r\n    \"Data path\": \"F:\\\\Stable\\\\Automatic1111\",\r\n    \"Extensions dir\": \"F:\\\\Stable\\\\Automatic1111\\\\extensions\",\r\n    \"Checksum\": \"a64977429e1316ffe7b6350d2bd3f33237465447e98ffe671f34f8f3f61575e2\",\r\n    \"Commandline\": [\r\n        \"launch.py\",\r\n        \"--xformers\",\r\n        \"--no-half-vae\",\r\n        \"--theme\",\r\n        \"dark\",\r\n        \"--ui-config-file\",\r\n        \"XLui-config.json\",\r\n        \"--ui-settings-file\",\r\n        \"XLconfig.json\",\r\n        \"--ckpt-dir\",\r\n        \"..\\\\models\\\\Stable-diffusion\\\\SDXL\",\r\n        \"--lora-dir\",\r\n        \"..\\\\models\\\\Lora\\\\SDXL\",\r\n        \"--vae-dir\",\r\n        \"..\\\\models\\\\VAE\\\\SDXL\",\r\n        \"--embeddings-dir\",\r\n        \"..\\\\models\\\\embeddings\\\\SDXL\"\r\n    ],\r\n    \"Torch env info\": {\r\n        \"torch_version\": \"2.0.1+cu118\",\r\n        \"is_debug_build\": \"False\",\r\n        \"cuda_compiled_version\": \"11.8\",\r\n        \"gcc_version\": null,\r\n        \"clang_version\": null,\r\n        \"cmake_version\": null,\r\n        \"os\": \"Microsoft Windows 11 Home\",\r\n        \"libc_version\": \"N/A\",\r\n        \"python_version\": \"3.10.11 (tags/v3.10.11:7d4cc5a, Apr  5 2023, 00:38:17) [MSC v.1929 64 bit (AMD64)] (64-bit runtime)\",\r\n        \"python_platform\": \"Windows-10-10.0.22621-SP0\",\r\n        \"is_cuda_available\": \"True\",\r\n        \"cuda_runtime_version\": null,\r\n        \"cuda_module_loading\": \"LAZY\",\r\n        \"nvidia_driver_version\": \"537.13\",\r\n        \"nvidia_gpu_models\": \"GPU 0: NVIDIA GeForce RTX 4070\",\r\n        \"cudnn_version\": null,\r\n        \"pip_version\": \"pip3\",\r\n        \"pip_packages\": [\r\n            \"lion-pytorch==0.1.2\",\r\n            \"numpy==1.23.5\",\r\n            \"open-clip-torch==2.20.0\",\r\n            \"pytorch-lightning==1.9.4\",\r\n            \"torch==2.0.1+cu118\",\r\n            \"torchdiffeq==0.2.3\",\r\n            \"torchmetrics==1.0.1\",\r\n            \"torchsde==0.2.5\",\r\n            \"torchvision==0.15.2+cu118\"\r\n        ],\r\n        \"conda_packages\": null,\r\n        \"hip_compiled_version\": \"N/A\",\r\n        \"hip_runtime_version\": \"N/A\",\r\n        \"miopen_runtime_version\": \"N/A\",\r\n        \"caching_allocator_config\": \"\",\r\n        \"is_xnnpack_available\": \"True\",\r\n        \"cpu_info\": [\r\n            \"Architecture=9\",\r\n            \"CurrentClockSpeed=3801\",\r\n            \"DeviceID=CPU0\",\r\n            \"Family=107\",\r\n            \"L2CacheSize=6144\",\r\n            \"L2CacheSpeed=\",\r\n            \"Manufacturer=AuthenticAMD\",\r\n            \"MaxClockSpeed=3801\",\r\n            \"Name=AMD Ryzen 5 7600 6-Core Processor              \",\r\n            \"ProcessorType=3\",\r\n            \"Revision=24834\"\r\n        ]\r\n    },\r\n    \"Exceptions\": [],\r\n    \"CPU\": {\r\n        \"model\": \"AMD64 Family 25 Model 97 Stepping 2, AuthenticAMD\",\r\n        \"count logical\": 12,\r\n        \"count physical\": 6\r\n    },\r\n    \"RAM\": {\r\n        \"total\": \"63GB\",\r\n        \"used\": \"16GB\",\r\n        \"free\": \"47GB\"\r\n    },\r\n    \"Extensions\": [\r\n        {\r\n            \"name\": \"Stable-Diffusion-Webui-Civitai-Helper\",\r\n            \"path\": \"F:\\\\Stable\\\\Automatic1111\\\\extensions\\\\Stable-Diffusion-Webui-Civitai-Helper\",\r\n            \"version\": \"920ca326\",\r\n            \"branch\": \"main\",\r\n            \"remote\": \"https://github.com/butaixianran/Stable-Diffusion-Webui-Civitai-Helper.git\"\r\n        },\r\n        {\r\n            \"name\": \"adetailer\",\r\n            \"path\": \"F:\\\\Stable\\\\Automatic1111\\\\extensions\\\\adetailer\",\r\n            \"version\": \"a97b9923\",\r\n            \"branch\": \"main\",\r\n            \"remote\": \"https://github.com/Bing-su/adetailer.git\"\r\n        },\r\n        {\r\n            \"name\": \"civitai-shortcut\",\r\n            \"path\": \"F:\\\\Stable\\\\Automatic1111\\\\extensions\\\\civitai-shortcut\",\r\n            \"version\": \"53c6755f\",\r\n            \"branch\": \"main\",\r\n            \"remote\": \"https://github.com/sunnyark/civitai-shortcut.git\"\r\n        },\r\n        {\r\n            \"name\": \"sd-webui-infinite-image-browsing\",\r\n            \"path\": \"F:\\\\Stable\\\\Automatic1111\\\\extensions\\\\sd-webui-infinite-image-browsing\",\r\n            \"version\": \"38a04a4b\",\r\n            \"branch\": \"main\",\r\n            \"remote\": \"https://github.com/zanllp/sd-webui-infinite-image-browsing.git\"\r\n        },\r\n        {\r\n            \"name\": \"stable-diffusion-webui-dataset-tag-editor\",\r\n            \"path\": \"F:\\\\Stable\\\\Automatic1111\\\\extensions\\\\stable-diffusion-webui-dataset-tag-editor\",\r\n            \"version\": \"7a2f4c53\",\r\n            \"branch\": \"main\",\r\n            \"remote\": \"https://github.com/toshiaki1729/stable-diffusion-webui-dataset-tag-editor.git\"\r\n        }\r\n    ],\r\n    \"Inactive extensions\": [\r\n        {\r\n            \"name\": \"PBRemTools\",\r\n            \"path\": \"F:\\\\Stable\\\\Automatic1111\\\\extensions\\\\PBRemTools\",\r\n            \"version\": \"91e16d32\",\r\n            \"branch\": \"main\",\r\n            \"remote\": \"https://github.com/mattyamonaca/PBRemTools.git\"\r\n        },\r\n        {\r\n            \"name\": \"sd-webui-controlnet\",\r\n            \"path\": \"F:\\\\Stable\\\\Automatic1111\\\\extensions\\\\sd-webui-controlnet\",\r\n            \"version\": \"510bd66e\",\r\n            \"branch\": \"main\",\r\n            \"remote\": \"https://github.com/Mikubill/sd-webui-controlnet.git\"\r\n        },\r\n        {\r\n            \"name\": \"sd-webui-model-converter\",\r\n            \"path\": \"F:\\\\Stable\\\\Automatic1111\\\\extensions\\\\sd-webui-model-converter\",\r\n            \"version\": \"b1f10148\",\r\n            \"branch\": \"main\",\r\n            \"remote\": \"https://github.com/Akegarasu/sd-webui-model-converter.git\"\r\n        },\r\n        {\r\n            \"name\": \"sd-webui-prompt-all-in-one\",\r\n            \"path\": \"F:\\\\Stable\\\\Automatic1111\\\\extensions\\\\sd-webui-prompt-all-in-one\",\r\n            \"version\": \"2cbbcf95\",\r\n            \"branch\": \"main\",\r\n            \"remote\": \"https://github.com/Physton/sd-webui-prompt-all-in-one\"\r\n        },\r\n        {\r\n            \"name\": \"training-picker\",\r\n            \"path\": \"F:\\\\Stable\\\\Automatic1111\\\\extensions\\\\training-picker\",\r\n            \"version\": \"d2784b09\",\r\n            \"branch\": \"master\",\r\n            \"remote\": \"https://github.com/Maurdekye/training-picker.git\"\r\n        }\r\n    ],\r\n    \"Environment\": {\r\n        \"COMMANDLINE_ARGS\": \"--xformers --no-half-vae --theme dark --ui-config-file XLui-config.json --ui-settings-file XLconfig.json --ckpt-dir ..\\\\\\\\models\\\\\\\\Stable-diffusion\\\\\\\\SDXL --lora-dir ..\\\\\\\\models\\\\\\\\Lora\\\\\\\\SDXL --vae-dir ..\\\\\\\\models\\\\\\\\VAE\\\\\\\\SDXL --embeddings-dir ..\\\\\\\\models\\\\\\\\embeddings\\\\\\\\SDXL\",\r\n        \"GRADIO_ANALYTICS_ENABLED\": \"False\"\r\n    },\r\n    \"Config\": {\r\n        \"samples_save\": true,\r\n        \"samples_format\": \"png\",\r\n        \"samples_filename_pattern\": \"\",\r\n        \"save_images_add_number\": true,\r\n        \"grid_save\": true,\r\n        \"grid_format\": \"png\",\r\n        \"grid_extended_filename\": false,\r\n        \"grid_only_if_multiple\": true,\r\n        \"grid_prevent_empty_spots\": false,\r\n        \"grid_zip_filename_pattern\": \"\",\r\n        \"n_rows\": -1,\r\n        \"font\": \"\",\r\n        \"grid_text_active_color\": \"#000000\",\r\n        \"grid_text_inactive_color\": \"#999999\",\r\n        \"grid_background_color\": \"#ffffff\",\r\n        \"enable_pnginfo\": true,\r\n        \"save_txt\": false,\r\n        \"save_images_before_face_restoration\": false,\r\n        \"save_images_before_highres_fix\": false,\r\n        \"save_images_before_color_correction\": false,\r\n        \"save_mask\": false,\r\n        \"save_mask_composite\": false,\r\n        \"jpeg_quality\": 80,\r\n        \"webp_lossless\": false,\r\n        \"export_for_4chan\": false,\r\n        \"img_downscale_threshold\": 4.0,\r\n        \"target_side_length\": 4000,\r\n        \"img_max_size_mp\": 200,\r\n        \"use_original_name_batch\": true,\r\n        \"use_upscaler_name_as_suffix\": false,\r\n        \"save_selected_only\": true,\r\n        \"save_init_img\": false,\r\n        \"temp_dir\": \"\",\r\n        \"clean_temp_dir_at_start\": false,\r\n        \"outdir_samples\": \"\",\r\n        \"outdir_txt2img_samples\": \"../outputs/txt2img-images\",\r\n        \"outdir_img2img_samples\": \"../outputs/img2img-images\",\r\n        \"outdir_extras_samples\": \"../outputs/extras-images\",\r\n        \"outdir_grids\": \"\",\r\n        \"outdir_txt2img_grids\": \"../outputs/txt2img-grids\",\r\n        \"outdir_img2img_grids\": \"../outputs/img2img-grids\",\r\n        \"outdir_save\": \"../outputs/images\",\r\n        \"outdir_init_images\": \"../outputs/init-images\",\r\n        \"save_to_dirs\": true,\r\n        \"grid_save_to_dirs\": true,\r\n        \"use_save_to_dirs_for_ui\": false,\r\n        \"directories_filename_pattern\": \"[date]\",\r\n        \"directories_max_prompt_words\": 8,\r\n        \"ESRGAN_tile\": 192,\r\n        \"ESRGAN_tile_overlap\": 8,\r\n        \"realesrgan_enabled_models\": [\r\n            \"R-ESRGAN 4x+\",\r\n            \"R-ESRGAN 4x+ Anime6B\"\r\n        ],\r\n        \"upscaler_for_img2img\": \"4x-UltraSharp\",\r\n        \"face_restoration_model\": \"CodeFormer\",\r\n        \"code_former_weight\": 0.5,\r\n        \"face_restoration_unload\": true,\r\n        \"show_warnings\": false,\r\n        \"memmon_poll_rate\": 8,\r\n        \"samples_log_stdout\": false,\r\n        \"multiple_tqdm\": true,\r\n        \"print_hypernet_extra\": false,\r\n        \"list_hidden_files\": true,\r\n        \"disable_mmap_load_safetensors\": false,\r\n        \"unload_models_when_training\": false,\r\n        \"pin_memory\": false,\r\n        \"save_optimizer_state\": false,\r\n        \"save_training_settings_to_txt\": true,\r\n        \"dataset_filename_word_regex\": \"\",\r\n        \"dataset_filename_join_string\": \" \",\r\n        \"training_image_repeats_per_epoch\": 1,\r\n        \"training_write_csv_every\": 500,\r\n        \"training_xattention_optimizations\": false,\r\n        \"training_enable_tensorboard\": false,\r\n        \"training_tensorboard_save_images\": false,\r\n        \"training_tensorboard_flush_every\": 120,\r\n        \"sd_model_checkpoint\": \"aaYamer's Realistic XL v3.safetensors [3c2c1c9181]\",\r\n        \"sd_checkpoint_cache\": 0,\r\n        \"sd_vae_checkpoint_cache\": 2,\r\n        \"sd_vae\": \"sdxl_vae.safetensors\",\r\n        \"sd_vae_as_default\": true,\r\n        \"sd_unet\": \"Automatic\",\r\n        \"inpainting_mask_weight\": 1.0,\r\n        \"initial_noise_multiplier\": 1.0,\r\n        \"img2img_color_correction\": false,\r\n        \"img2img_fix_steps\": false,\r\n        \"img2img_background_color\": \"#ffffff\",\r\n        \"enable_quantization\": false,\r\n        \"enable_emphasis\": true,\r\n        \"enable_batch_seeds\": true,\r\n        \"comma_padding_backtrack\": 20,\r\n        \"CLIP_stop_at_last_layers\": 1,\r\n        \"upcast_attn\": true,\r\n        \"auto_vae_precision\": true,\r\n        \"randn_source\": \"GPU\",\r\n        \"sdxl_crop_top\": 0,\r\n        \"sdxl_crop_left\": 0,\r\n        \"sdxl_refiner_low_aesthetic_score\": 2.5,\r\n        \"sdxl_refiner_high_aesthetic_score\": 6.0,\r\n        \"cross_attention_optimization\": \"Automatic\",\r\n        \"s_min_uncond\": 0.0,\r\n        \"token_merging_ratio\": 0.0,\r\n        \"token_merging_ratio_img2img\": 0.0,\r\n        \"token_merging_ratio_hr\": 0.0,\r\n        \"pad_cond_uncond\": false,\r\n        \"experimental_persistent_cond_cache\": false,\r\n        \"use_old_emphasis_implementation\": false,\r\n        \"use_old_karras_scheduler_sigmas\": false,\r\n        \"no_dpmpp_sde_batch_determinism\": false,\r\n        \"use_old_hires_fix_width_height\": false,\r\n        \"dont_fix_second_order_samplers_schedule\": false,\r\n        \"hires_fix_use_firstpass_conds\": false,\r\n        \"interrogate_keep_models_in_memory\": false,\r\n        \"interrogate_return_ranks\": false,\r\n        \"interrogate_clip_num_beams\": 1,\r\n        \"interrogate_clip_min_length\": 24,\r\n        \"interrogate_clip_max_length\": 48,\r\n        \"interrogate_clip_dict_limit\": 1500,\r\n        \"interrogate_clip_skip_categories\": [],\r\n        \"interrogate_deepbooru_score_threshold\": 0.5,\r\n        \"deepbooru_sort_alpha\": true,\r\n        \"deepbooru_use_spaces\": true,\r\n        \"deepbooru_escape\": true,\r\n        \"deepbooru_filter_tags\": \"\",\r\n        \"extra_networks_show_hidden_directories\": true,\r\n        \"extra_networks_hidden_models\": \"Always\",\r\n        \"extra_networks_default_multiplier\": 1.0,\r\n        \"extra_networks_card_width\": 0,\r\n        \"extra_networks_card_height\": 0,\r\n        \"extra_networks_card_text_scale\": 1.0,\r\n        \"extra_networks_card_show_desc\": true,\r\n        \"extra_networks_add_text_separator\": \" \",\r\n        \"ui_extra_networks_tab_reorder\": \"Checkpoints,Lora\",\r\n        \"textual_inversion_print_at_load\": false,\r\n        \"textual_inversion_add_hashes_to_infotext\": true,\r\n        \"sd_hypernetwork\": \"None\",\r\n        \"localization\": \"None\",\r\n        \"gradio_theme\": \"Default\",\r\n        \"img2img_editor_height\": 720,\r\n        \"return_grid\": true,\r\n        \"return_mask\": false,\r\n        \"return_mask_composite\": false,\r\n        \"do_not_show_images\": false,\r\n        \"send_seed\": true,\r\n        \"send_size\": true,\r\n        \"js_modal_lightbox\": false,\r\n        \"js_modal_lightbox_initially_zoomed\": true,\r\n        \"js_modal_lightbox_gamepad\": false,\r\n        \"js_modal_lightbox_gamepad_repeat\": 250,\r\n        \"show_progress_in_title\": true,\r\n        \"samplers_in_dropdown\": true,\r\n        \"dimensions_and_batch_together\": true,\r\n        \"keyedit_precision_attention\": 0.1,\r\n        \"keyedit_precision_extra\": 0.05,\r\n        \"keyedit_delimiters\": \".,\\\\/!?%^*;:{}=`~()\",\r\n        \"keyedit_move\": true,\r\n        \"quicksettings_list\": [\r\n            \"sd_model_checkpoint\",\r\n            \"sd_vae\"\r\n        ],\r\n        \"ui_tab_order\": [],\r\n        \"hidden_tabs\": [],\r\n        \"ui_reorder_list\": [],\r\n        \"hires_fix_show_sampler\": false,\r\n        \"hires_fix_show_prompts\": false,\r\n        \"disable_token_counters\": false,\r\n        \"add_model_hash_to_info\": true,\r\n        \"add_model_name_to_info\": true,\r\n        \"add_user_name_to_info\": false,\r\n        \"add_version_to_infotext\": false,\r\n        \"disable_weights_auto_swap\": true,\r\n        \"infotext_styles\": \"Apply if any\",\r\n        \"show_progressbar\": true,\r\n        \"live_previews_enable\": true,\r\n        \"live_previews_image_format\": \"png\",\r\n        \"show_progress_grid\": true,\r\n        \"show_progress_every_n_steps\": 2,\r\n        \"show_progress_type\": \"Approx NN\",\r\n        \"live_preview_content\": \"Prompt\",\r\n        \"live_preview_refresh_period\": 100.0,\r\n        \"hide_samplers\": [],\r\n        \"eta_ddim\": 0.0,\r\n        \"eta_ancestral\": 1.0,\r\n        \"ddim_discretize\": \"uniform\",\r\n        \"s_churn\": 0.0,\r\n        \"s_tmin\": 0.0,\r\n        \"s_noise\": 1.0,\r\n        \"k_sched_type\": \"Automatic\",\r\n        \"sigma_min\": 0.0,\r\n        \"sigma_max\": 0.0,\r\n        \"rho\": 0.0,\r\n        \"eta_noise_seed_delta\": 0,\r\n        \"always_discard_next_to_last_sigma\": false,\r\n        \"uni_pc_variant\": \"bh1\",\r\n        \"uni_pc_skip_type\": \"time_uniform\",\r\n        \"uni_pc_order\": 3,\r\n        \"uni_pc_lower_order_final\": true,\r\n        \"postprocessing_enable_in_main_ui\": [],\r\n        \"postprocessing_operation_order\": [],\r\n        \"upscaling_max_images_in_cache\": 5,\r\n        \"disabled_extensions\": [\r\n            \"PBRemTools\",\r\n            \"sd-webui-controlnet\",\r\n            \"sd-webui-model-converter\",\r\n            \"sd-webui-prompt-all-in-one\",\r\n            \"training-picker\"\r\n        ],\r\n        \"disable_all_extensions\": \"none\",\r\n        \"restore_config_state_file\": \"\",\r\n        \"sd_checkpoint_hash\": \"3c2c1c9181d08f68bdd93b7c3d15824033d4bfeb3c9e1e434611ee151e553ff5\",\r\n        \"ldsr_steps\": 100,\r\n        \"ldsr_cached\": false,\r\n        \"SCUNET_tile\": 256,\r\n        \"SCUNET_tile_overlap\": 8,\r\n        \"SWIN_tile\": 192,\r\n        \"SWIN_tile_overlap\": 8,\r\n        \"lora_functional\": false,\r\n        \"sd_lora\": \"None\",\r\n        \"lora_preferred_name\": \"Alias from file\",\r\n        \"lora_add_hashes_to_infotext\": true,\r\n        \"lora_show_all\": false,\r\n        \"lora_hide_unknown_for_versions\": [],\r\n        \"extra_options\": [],\r\n        \"extra_options_accordion\": false,\r\n        \"canvas_hotkey_zoom\": \"Alt\",\r\n        \"canvas_hotkey_adjust\": \"Ctrl\",\r\n        \"canvas_hotkey_move\": \"F\",\r\n        \"canvas_hotkey_fullscreen\": \"S\",\r\n        \"canvas_hotkey_reset\": \"R\",\r\n        \"canvas_hotkey_overlap\": \"O\",\r\n        \"canvas_show_tooltip\": true,\r\n        \"canvas_blur_prompt\": false,\r\n        \"canvas_disabled_functions\": [\r\n            \"Overlap\"\r\n        ],\r\n        \"dataset_editor_image_columns\": 6.0,\r\n        \"dataset_editor_max_res\": 0.0,\r\n        \"dataset_editor_use_temp_files\": false,\r\n        \"dataset_editor_use_raw_clip_token\": true,\r\n        \"sd_vae_overrides_per_model_preferences\": false,\r\n        \"save_incomplete_images\": false,\r\n        \"face_restoration\": false,\r\n        \"auto_launch_browser\": \"false\",\r\n        \"show_gradio_deprecation_warnings\": false,\r\n        \"hide_ldm_prints\": true,\r\n        \"sd_checkpoints_limit\": 1,\r\n        \"sd_checkpoints_keep_in_cpu\": true,\r\n        \"tiling\": false,\r\n        \"sd_vae_encode_method\": \"Full\",\r\n        \"sd_vae_decode_method\": \"Full\",\r\n        \"img2img_extra_noise\": 0,\r\n        \"img2img_sketch_default_brush_color\": \"#ffffff\",\r\n        \"img2img_inpaint_mask_brush_color\": \"#ffffff\",\r\n        \"img2img_inpaint_sketch_default_brush_color\": \"#ffffff\",\r\n        \"persistent_cond_cache\": true,\r\n        \"lora_in_memory_limit\": 0,\r\n        \"gradio_themes_cache\": true,\r\n        \"extra_options_txt2img\": [],\r\n        \"extra_options_img2img\": [],\r\n        \"extra_options_cols\": 1,\r\n        \"s_tmax\": 0,\r\n        \"canvas_auto_expand\": true,\r\n        \"gallery_height\": \"\",\r\n        \"live_preview_fast_interrupt\": false,\r\n        \"ad_max_models\": 2,\r\n        \"ad_save_previews\": false,\r\n        \"ad_save_images_before\": false,\r\n        \"ad_only_seleted_scripts\": true,\r\n        \"ad_script_names\": \"dynamic_prompting,dynamic_thresholding,wildcard_recursive,wildcards,lora_block_weight\",\r\n        \"ad_bbox_sortby\": \"None\"\r\n    },\r\n    \"Startup\": {\r\n        \"total\": 18.911141395568848,\r\n        \"records\": {\r\n            \"initial startup\": 0.0011115074157714844,\r\n            \"prepare environment/checks\": 0.016119003295898438,\r\n            \"prepare environment/git version info\": 0.08761262893676758,\r\n            \"prepare environment/torch GPU test\": 2.0806050300598145,\r\n            \"prepare environment/clone repositores\": 0.17142415046691895,\r\n            \"prepare environment/run extensions installers/adetailer\": 0.10271978378295898,\r\n            \"prepare environment/run extensions installers/civitai-shortcut\": 0.0,\r\n            \"prepare environment/run extensions installers/sd-webui-infinite-image-browsing\": 0.2934441566467285,\r\n            \"prepare environment/run extensions installers/Stable-Diffusion-Webui-Civitai-Helper\": 0.0,\r\n            \"prepare environment/run extensions installers/stable-diffusion-webui-dataset-tag-editor\": 0.0,\r\n            \"prepare environment/run extensions installers\": 0.3961639404296875,\r\n            \"prepare environment\": 2.819159507751465,\r\n            \"launcher\": 0.0023238658905029297,\r\n            \"import torch\": 7.045945644378662,\r\n            \"import gradio\": 0.9382307529449463,\r\n            \"setup paths\": 1.058117389678955,\r\n            \"import ldm\": 0.00902247428894043,\r\n            \"import sgm\": 0.0,\r\n            \"initialize shared\": 0.23809814453125,\r\n            \"other imports\": 0.6863973140716553,\r\n            \"opts onchange\": 0.0010640621185302734,\r\n            \"setup SD model\": 0.0012471675872802734,\r\n            \"setup codeformer\": 0.16878795623779297,\r\n            \"setup gfpgan\": 0.06298232078552246,\r\n            \"set samplers\": 0.0,\r\n            \"list extensions\": 0.001016855239868164,\r\n            \"restore config state file\": 0.0,\r\n            \"list SD models\": 0.058971405029296875,\r\n            \"list localizations\": 0.0,\r\n            \"load scripts/custom_code.py\": 0.0030007362365722656,\r\n            \"load scripts/img2imgalt.py\": 0.0011336803436279297,\r\n            \"load scripts/loopback.py\": 0.0,\r\n            \"load scripts/outpainting_mk_2.py\": 0.0,\r\n            \"load scripts/poor_mans_outpainting.py\": 0.0012125968933105469,\r\n            \"load scripts/postprocessing_codeformer.py\": 0.0,\r\n            \"load scripts/postprocessing_gfpgan.py\": 0.0010843276977539062,\r\n            \"load scripts/postprocessing_upscale.py\": 0.0,\r\n            \"load scripts/prompt_matrix.py\": 0.0010118484497070312,\r\n            \"load scripts/prompts_from_file.py\": 0.0010287761688232422,\r\n            \"load scripts/refiner.py\": 0.0,\r\n            \"load scripts/sd_upscale.py\": 0.0010187625885009766,\r\n            \"load scripts/seed.py\": 0.0,\r\n            \"load scripts/xyz_grid.py\": 0.0022134780883789062,\r\n            \"load scripts/civitai_helper.py\": 0.8866167068481445,\r\n            \"load scripts/!adetailer.py\": 2.879211664199829,\r\n            \"load scripts/civitai_shortcut.py\": 0.0456390380859375,\r\n            \"load scripts/iib_setup.py\": 0.07819533348083496,\r\n            \"load scripts/dte_instance.py\": 0.01780414581298828,\r\n            \"load scripts/main.py\": 0.05234932899475098,\r\n            \"load scripts/singleton.py\": 0.0,\r\n            \"load scripts/ldsr_model.py\": 0.029529333114624023,\r\n            \"load scripts/lora_script.py\": 0.12224340438842773,\r\n            \"load scripts/scunet_model.py\": 0.026175260543823242,\r\n            \"load scripts/swinir_model.py\": 0.026006221771240234,\r\n            \"load scripts/hotkey_config.py\": 0.0,\r\n            \"load scripts/extra_options_section.py\": 0.0009965896606445312,\r\n            \"load scripts\": 4.17647123336792,\r\n            \"load upscalers\": 0.01675128936767578,\r\n            \"refresh VAE\": 0.0030100345611572266,\r\n            \"refresh textual inversion templates\": 0.001003265380859375,\r\n            \"scripts list_optimizers\": 0.0,\r\n            \"scripts list_unets\": 0.0,\r\n            \"reload hypernetworks\": 0.004995584487915039,\r\n            \"initialize extra networks\": 0.01602935791015625,\r\n            \"scripts before_ui_callback\": 0.0009980201721191406,\r\n            \"create ui\": 1.4426805973052979,\r\n            \"gradio launch\": 0.20941734313964844,\r\n            \"add APIs\": 0.005007743835449219,\r\n            \"app_started_callback/iib_setup.py\": 0.00953531265258789,\r\n            \"app_started_callback/lora_script.py\": 0.0,\r\n            \"app_started_callback\": 0.00953531265258789\r\n        }\r\n    },\r\n    \"Packages\": [\r\n        \"-rotobuf==3.20.0\",\r\n        \"absl-py==1.4.0\",\r\n        \"accelerate==0.21.0\",\r\n        \"addict==2.4.0\",\r\n        \"aenum==3.1.15\",\r\n        \"aiofiles==23.1.0\",\r\n        \"aiohttp==3.8.5\",\r\n        \"aiosignal==1.3.1\",\r\n        \"aliyun-python-sdk-alimt==3.2.0\",\r\n        \"aliyun-python-sdk-core==2.13.10\",\r\n        \"altair==5.0.1\",\r\n        \"antlr4-python3-runtime==4.9.3\",\r\n        \"anyio==3.7.1\",\r\n        \"astunparse==1.6.3\",\r\n        \"async-timeout==4.0.2\",\r\n        \"attrs==23.1.0\",\r\n        \"basicsr==1.4.2\",\r\n        \"beautifulsoup4==4.12.2\",\r\n        \"bitsandbytes==0.35.4\",\r\n        \"blendmodes==2022\",\r\n        \"boltons==23.0.0\",\r\n        \"boto3==1.28.16\",\r\n        \"botocore==1.31.16\",\r\n        \"cachetools==5.3.1\",\r\n        \"certifi==2023.7.22\",\r\n        \"cffi==1.15.1\",\r\n        \"chardet==5.1.0\",\r\n        \"charset-normalizer==3.2.0\",\r\n        \"clean-fid==0.1.35\",\r\n        \"click==8.1.6\",\r\n        \"clip==1.0\",\r\n        \"colorama==0.4.6\",\r\n        \"coloredlogs==15.0.1\",\r\n        \"contourpy==1.1.0\",\r\n        \"cryptography==41.0.2\",\r\n        \"cssselect2==0.7.0\",\r\n        \"cycler==0.11.0\",\r\n        \"dadaptation==3.1\",\r\n        \"deepdanbooru==1.0.2\",\r\n        \"deprecation==2.1.0\",\r\n        \"diffusers==0.16.1\",\r\n        \"dill==0.3.7\",\r\n        \"discord-webhook==1.1.0\",\r\n        \"einops==0.4.1\",\r\n        \"exceptiongroup==1.1.2\",\r\n        \"facexlib==0.3.0\",\r\n        \"fastapi==0.94.0\",\r\n        \"ffmpy==0.3.1\",\r\n        \"filelock==3.12.2\",\r\n        \"filterpy==1.4.5\",\r\n        \"flatbuffers==23.5.26\",\r\n        \"fonttools==4.41.1\",\r\n        \"frozenlist==1.4.0\",\r\n        \"fsspec==2023.6.0\",\r\n        \"ftfy==6.1.1\",\r\n        \"future==0.18.3\",\r\n        \"fvcore==0.1.5.post20221221\",\r\n        \"gast==0.4.0\",\r\n        \"gdown==4.7.1\",\r\n        \"gfpgan==1.3.8\",\r\n        \"gitdb==4.0.10\",\r\n        \"gitpython==3.1.32\",\r\n        \"google-auth-oauthlib==1.0.0\",\r\n        \"google-auth==2.22.0\",\r\n        \"google-pasta==0.2.0\",\r\n        \"gradio-client==0.3.0\",\r\n        \"gradio==3.39.0\",\r\n        \"grpcio==1.56.2\",\r\n        \"h11==0.12.0\",\r\n        \"h5py==3.9.0\",\r\n        \"httpcore==0.15.0\",\r\n        \"httpx==0.24.1\",\r\n        \"huggingface-hub==0.16.4\",\r\n        \"humanfriendly==10.0\",\r\n        \"idna==3.4\",\r\n        \"imageio==2.31.1\",\r\n        \"importlib-metadata==6.8.0\",\r\n        \"inflection==0.5.1\",\r\n        \"iopath==0.1.9\",\r\n        \"jinja2==3.1.2\",\r\n        \"jmespath==0.10.0\",\r\n        \"joblib==1.3.2\",\r\n        \"jsonmerge==1.8.0\",\r\n        \"jsonschema-specifications==2023.7.1\",\r\n        \"jsonschema==4.18.4\",\r\n        \"keras==2.13.1\",\r\n        \"kiwisolver==1.4.4\",\r\n        \"kornia==0.6.7\",\r\n        \"lark==1.1.2\",\r\n        \"lazy-loader==0.3\",\r\n        \"libclang==16.0.6\",\r\n        \"lightning-utilities==0.9.0\",\r\n        \"linkify-it-py==2.0.2\",\r\n        \"lion-pytorch==0.1.2\",\r\n        \"llvmlite==0.40.1\",\r\n        \"lmdb==1.4.1\",\r\n        \"lpips==0.1.4\",\r\n        \"lxml==4.9.3\",\r\n        \"markdown-it-py==2.2.0\",\r\n        \"markdown==3.4.4\",\r\n        \"markupsafe==2.1.3\",\r\n        \"matplotlib==3.7.2\",\r\n        \"mdit-py-plugins==0.3.3\",\r\n        \"mdurl==0.1.2\",\r\n        \"mediapipe==0.10.3\",\r\n        \"mpmath==1.3.0\",\r\n        \"multidict==6.0.4\",\r\n        \"multiprocess==0.70.15\",\r\n        \"networkx==3.1\",\r\n        \"numba==0.57.1\",\r\n        \"numpy==1.23.5\",\r\n        \"oauthlib==3.2.2\",\r\n        \"omegaconf==2.2.3\",\r\n        \"onnx==1.14.0\",\r\n        \"onnxruntime-gpu==1.15.1\",\r\n        \"open-clip-torch==2.20.0\",\r\n        \"openai==0.27.8\",\r\n        \"opencv-contrib-python==4.8.0.74\",\r\n        \"opencv-python-headless==4.8.0.74\",\r\n        \"opencv-python==4.8.0.74\",\r\n        \"opt-einsum==3.3.0\",\r\n        \"orjson==3.9.2\",\r\n        \"packaging==23.1\",\r\n        \"pandas==2.0.3\",\r\n        \"pathos==0.3.1\",\r\n        \"piexif==1.1.3\",\r\n        \"pillow==9.5.0\",\r\n        \"pip==22.2.1\",\r\n        \"platformdirs==3.9.1\",\r\n        \"portalocker==2.7.0\",\r\n        \"pox==0.3.3\",\r\n        \"ppft==1.7.6.7\",\r\n        \"protobuf==3.20.3\",\r\n        \"psutil==5.9.5\",\r\n        \"py-cpuinfo==9.0.0\",\r\n        \"pyasn1-modules==0.3.0\",\r\n        \"pyasn1==0.5.0\",\r\n        \"pycparser==2.21\",\r\n        \"pydantic==1.10.12\",\r\n        \"pydub==0.25.1\",\r\n        \"pyexecjs==1.5.1\",\r\n        \"pyfunctional==1.4.3\",\r\n        \"pygments==2.15.1\",\r\n        \"pyparsing==3.0.9\",\r\n        \"pyreadline3==3.4.1\",\r\n        \"pysocks==1.7.1\",\r\n        \"python-dateutil==2.8.2\",\r\n        \"python-dotenv==1.0.0\",\r\n        \"python-multipart==0.0.6\",\r\n        \"pytorch-lightning==1.9.4\",\r\n        \"pytz==2023.3\",\r\n        \"pywavelets==1.4.1\",\r\n        \"pywin32==306\",\r\n        \"pyyaml==6.0.1\",\r\n        \"realesrgan==0.3.0\",\r\n        \"referencing==0.30.0\",\r\n        \"regex==2023.6.3\",\r\n        \"reportlab==4.0.4\",\r\n        \"requests-oauthlib==1.3.1\",\r\n        \"requests==2.31.0\",\r\n        \"resize-right==0.0.2\",\r\n        \"rich==13.5.2\",\r\n        \"rpds-py==0.9.2\",\r\n        \"rsa==4.9\",\r\n        \"s3transfer==0.6.1\",\r\n        \"safetensors==0.3.1\",\r\n        \"scikit-image==0.21.0\",\r\n        \"scikit-learn==1.3.0\",\r\n        \"scipy==1.11.1\",\r\n        \"seaborn==0.12.2\",\r\n        \"segment-anything==1.0\",\r\n        \"segmentation-refinement==0.6\",\r\n        \"semantic-version==2.10.0\",\r\n        \"sentencepiece==0.1.99\",\r\n        \"setuptools==63.2.0\",\r\n        \"six==1.16.0\",\r\n        \"smmap==5.0.0\",\r\n        \"sniffio==1.3.0\",\r\n        \"sounddevice==0.4.6\",\r\n        \"soupsieve==2.4.1\",\r\n        \"starlette==0.26.1\",\r\n        \"svglib==1.5.1\",\r\n        \"sympy==1.12\",\r\n        \"tabulate==0.9.0\",\r\n        \"tb-nightly==2.14.0a20230725\",\r\n        \"tensorboard-data-server==0.7.1\",\r\n        \"tensorboard==2.13.0\",\r\n        \"tensorflow-estimator==2.13.0\",\r\n        \"tensorflow-intel==2.13.0\",\r\n        \"tensorflow-io-gcs-filesystem==0.31.0\",\r\n        \"tensorflow-io==0.31.0\",\r\n        \"tensorflow==2.13.0\",\r\n        \"termcolor==2.3.0\",\r\n        \"threadpoolctl==3.2.0\",\r\n        \"tifffile==2023.7.18\",\r\n        \"timm==0.9.2\",\r\n        \"tinycss2==1.2.1\",\r\n        \"tokenizers==0.13.3\",\r\n        \"tomesd==0.1.3\",\r\n        \"tomli==2.0.1\",\r\n        \"toolz==0.12.0\",\r\n        \"torch==2.0.1+cu118\",\r\n        \"torchdiffeq==0.2.3\",\r\n        \"torchmetrics==1.0.1\",\r\n        \"torchsde==0.2.5\",\r\n        \"torchvision==0.15.2+cu118\",\r\n        \"tqdm==4.65.0\",\r\n        \"trampoline==0.1.2\",\r\n        \"transformers==4.30.2\",\r\n        \"typing-extensions==4.5.0\",\r\n        \"tzdata==2023.3\",\r\n        \"uc-micro-py==1.0.2\",\r\n        \"ultralytics==8.0.164\",\r\n        \"urllib3==1.26.16\",\r\n        \"uvicorn==0.23.1\",\r\n        \"wcwidth==0.2.6\",\r\n        \"webencodings==0.5.1\",\r\n        \"websockets==11.0.3\",\r\n        \"werkzeug==2.3.6\",\r\n        \"wheel==0.41.0\",\r\n        \"wrapt==1.15.0\",\r\n        \"xformers==0.0.20\",\r\n        \"yacs==0.1.8\",\r\n        \"yapf==0.40.1\",\r\n        \"yarl==1.9.2\",\r\n        \"zipp==3.16.2\"\r\n    ]\r\n}\n\n### What browsers do you use to access the UI ?\n\nMozilla Firefox, Google Chrome, Other\n\n### Console logs\n\n```Shell\nvenv \"F:\\Stable\\Automatic1111\\venv\\Scripts\\Python.exe\"\r\nPython 3.10.11 (tags/v3.10.11:7d4cc5a, Apr  5 2023, 00:38:17) [MSC v.1929 64 bit (AMD64)]\r\nVersion: v1.5.1-540-g9d2299ed\r\nCommit hash: 9d2299ed0bd6c81cae8a7ba4ca22d6a14fb27bef\r\n\r\n\r\nLaunching Web UI with arguments: --xformers --no-half-vae --theme dark --ui-config-file XLui-config.json --ui-settings-file XLconfig.json --ckpt-dir ..\\models\\Stable-diffusion\\SDXL --lora-dir ..\\models\\Lora\\SDXL --vae-dir ..\\models\\VAE\\SDXL --embeddings-dir ..\\models\\embeddings\\SDXL\r\nCivitai Helper: Get Custom Model Folder\r\nCivitai Helper: Load setting from: F:\\Stable\\Automatic1111\\extensions\\Stable-Diffusion-Webui-Civitai-Helper\\setting.json\r\n[-] ADetailer initialized. version: 23.8.1, num models: 9\r\nLoading weights [f70a7d3ff2] from ..\\models\\Stable-diffusion\\SDXL\\Standard\\RealitiesEdge XL v2.safetensors\r\nCivitai Shortcut: v1.6.4\r\nCivitai Shortcut: shortcut update start\r\nCreating model from config: F:\\Stable\\Automatic1111\\repositories\\generative-models\\configs\\inference\\sd_xl_base.yaml\r\nRunning on local URL:  http://127.0.0.1:7860\r\n\r\nTo create a public link, set `share=True` in `launch()`.\r\nStartup time: 18.9s (prepare environment: 2.8s, import torch: 7.0s, import gradio: 0.9s, setup paths: 1.1s, initialize shared: 0.2s, other imports: 0.7s, setup codeformer: 0.2s, load scripts: 4.2s, create ui: 1.4s, gradio launch: 0.2s).\r\nLoading VAE weights specified in settings: ..\\models\\VAE\\SDXL\\sdxl_vae.safetensors\r\nApplying attention optimization: xformers... done.\r\nModel loaded in 13.2s (load weights from disk: 1.2s, create model: 0.7s, apply weights to model: 9.2s, apply half(): 1.2s, load VAE: 0.1s, calculate empty prompt: 0.7s).\r\nReusing loaded model RealitiesEdge XL v2.safetensors [f70a7d3ff2] to load Yamer's Realistic XL v3.safetensors [3c2c1c9181]\r\nLoading weights [3c2c1c9181] from ..\\models\\Stable-diffusion\\SDXL\\Standard\\Yamer's Realistic XL v3.safetensors\r\nLoading VAE weights specified in settings: cached sdxl_vae.safetensors\r\nApplying attention optimization: xformers... done.\r\nWeights loaded in 4.1s (send model to cpu: 1.5s, load weights from disk: 0.6s, apply weights to model: 0.7s, move model to device: 1.2s).\r\nCivitai Shortcut: shortcut update end\r\nReusing loaded model Yamer's Realistic XL v3.safetensors [3c2c1c9181] to load aaYamer's Realistic XL v3.safetensors\r\nCalculating sha256 for ..\\models\\Stable-diffusion\\SDXL\\Standard\\aaYamer's Realistic XL v3.safetensors: 3c2c1c9181d08f68bdd93b7c3d15824033d4bfeb3c9e1e434611ee151e553ff5\r\nLoading weights [3c2c1c9181] from ..\\models\\Stable-diffusion\\SDXL\\Standard\\aaYamer's Realistic XL v3.safetensors\r\nLoading VAE weights specified in settings: cached sdxl_vae.safetensors\r\nApplying attention optimization: xformers... done.\r\nWeights loaded in 8.5s (send model to cpu: 1.2s, calculate hash: 5.0s, load weights from disk: 0.5s, apply weights to model: 0.7s, move model to device: 1.0s).\n```\n\n\n### Additional information\n\n_No response_\n", "hints_text": "", "created_at": "2023-09-26T04:14:56Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 13084, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-13084", "issue_numbers": ["13080"], "base_commit": "d39440bfb9d3b20338fc23a78e6655b1e2f7c1d5", "patch": "diff --git a/modules/hypernetworks/hypernetwork.py b/modules/hypernetworks/hypernetwork.py\nindex 70f1cbd26b6..65b63f2f81f 100644\n--- a/modules/hypernetworks/hypernetwork.py\n+++ b/modules/hypernetworks/hypernetwork.py\n@@ -468,7 +468,7 @@ def create_hypernetwork(name, enable_sizes, overwrite_old, layer_structure=None,\n     shared.reload_hypernetworks()\r\n \r\n \r\n-def train_hypernetwork(id_task, hypernetwork_name, learn_rate, batch_size, gradient_step, data_root, log_directory, training_width, training_height, varsize, steps, clip_grad_mode, clip_grad_value, shuffle_tags, tag_drop_out, latent_sampling_method, use_weight, create_image_every, save_hypernetwork_every, template_filename, preview_from_txt2img, preview_prompt, preview_negative_prompt, preview_steps, preview_sampler_index, preview_cfg_scale, preview_seed, preview_width, preview_height):\r\n+def train_hypernetwork(id_task, hypernetwork_name:str, learn_rate:float, batch_size:int, gradient_step:int, data_root:str, log_directory:str, training_width:int, training_height:int, varsize:bool, steps:int, clip_grad_mode:str, clip_grad_value:float, shuffle_tags:bool, tag_drop_out:bool, latent_sampling_method:str, use_weight:bool, create_image_every:int, save_hypernetwork_every:int, template_filename:str, preview_from_txt2img:bool, preview_prompt:str, preview_negative_prompt:str, preview_steps:int, preview_sampler_name:str, preview_cfg_scale:float, preview_seed:int, preview_width:int, preview_height:int):\r\n     from modules import images, processing\r\n \r\n     save_hypernetwork_every = save_hypernetwork_every or 0\r\n@@ -698,7 +698,7 @@ def train_hypernetwork(id_task, hypernetwork_name, learn_rate, batch_size, gradi\n                         p.prompt = preview_prompt\r\n                         p.negative_prompt = preview_negative_prompt\r\n                         p.steps = preview_steps\r\n-                        p.sampler_name = sd_samplers.samplers[preview_sampler_index].name\r\n+                        p.sampler_name = sd_samplers.samplers_map[preview_sampler_name.lower()]\r\n                         p.cfg_scale = preview_cfg_scale\r\n                         p.seed = preview_seed\r\n                         p.width = preview_width\r\ndiff --git a/modules/textual_inversion/textual_inversion.py b/modules/textual_inversion/textual_inversion.py\nindex aa79dc09843..401a0a2ab02 100644\n--- a/modules/textual_inversion/textual_inversion.py\n+++ b/modules/textual_inversion/textual_inversion.py\n@@ -386,7 +386,7 @@ def validate_train_inputs(model_name, learn_rate, batch_size, gradient_step, dat\n         assert log_directory, \"Log directory is empty\"\r\n \r\n \r\n-def train_embedding(id_task, embedding_name, learn_rate, batch_size, gradient_step, data_root, log_directory, training_width, training_height, varsize, steps, clip_grad_mode, clip_grad_value, shuffle_tags, tag_drop_out, latent_sampling_method, use_weight, create_image_every, save_embedding_every, template_filename, save_image_with_stored_embedding, preview_from_txt2img, preview_prompt, preview_negative_prompt, preview_steps, preview_sampler_index, preview_cfg_scale, preview_seed, preview_width, preview_height):\r\n+def train_embedding(id_task, embedding_name, learn_rate, batch_size, gradient_step, data_root, log_directory, training_width, training_height, varsize, steps, clip_grad_mode, clip_grad_value, shuffle_tags, tag_drop_out, latent_sampling_method, use_weight, create_image_every, save_embedding_every, template_filename, save_image_with_stored_embedding, preview_from_txt2img, preview_prompt, preview_negative_prompt, preview_steps, preview_sampler_name, preview_cfg_scale, preview_seed, preview_width, preview_height):\r\n     from modules import processing\r\n \r\n     save_embedding_every = save_embedding_every or 0\r\n@@ -590,7 +590,7 @@ def train_embedding(id_task, embedding_name, learn_rate, batch_size, gradient_st\n                         p.prompt = preview_prompt\r\n                         p.negative_prompt = preview_negative_prompt\r\n                         p.steps = preview_steps\r\n-                        p.sampler_name = sd_samplers.samplers[preview_sampler_index].name\r\n+                        p.sampler_name = sd_samplers.samplers_map[preview_sampler_name.lower()]\r\n                         p.cfg_scale = preview_cfg_scale\r\n                         p.seed = preview_seed\r\n                         p.width = preview_width\r\n", "test_patch": "", "problem_statement": "[Bug]: Colab error while training hypernetwork\n### Is there an existing issue for this?\r\n\r\n- [X] I have searched the existing issues and checked the recent builds/commits\r\n\r\n### What happened?\r\n\r\nI've been using fast-stable-diffusion notebook in google Colab. And training process is stopped at the step when an image should be saved to textual_inversion log. I\u2019ve already tried to change checkpoint models, training settings and materials\r\n\r\nTraceback (most recent call last):\r\nFile \"/sd/stable-diffusion-webui/modules/hypernetworks/hypernetwork.py\", line 701, in train_hypernetwork\r\np.sampler_name = sd_samplers.samplers[preview_sampler_index].name\r\nTypeError: list indices must be integers or slices, not str\r\n\r\n### Steps to reproduce the problem\r\n\r\n1. Go to .... \r\n2. Press ....\r\n3. ...\r\n4...\r\n\r\n### What should have happened?\r\n\r\nSave a training progress image to log directory every N steps\r\n\r\n### Sysinfo\r\n\r\n-\r\n\r\n### What browsers do you use to access the UI ?\r\n\r\nApple Safari\r\n\r\n### Console logs\r\n\r\n```Shell\r\nStartup time: 38.3s (import torch: 15.0s, import gradio: 2.0s, setup paths: 4.0s, initialize shared: 0.5s, other imports: 3.3s, setup codeformer: 0.6s, setup gfpgan: 0.1s, list SD models: 0.2s, load scripts: 4.2s, create ui: 3.2s, gradio launch: 4.9s, add APIs: 0.2s).\r\n9aba26abdfcd46073e0a1d42027a3a3bcc969f562d58a03637bf0a0ded6586c9\r\nLoading weights [9aba26abdf] from /content/gdrive/MyDrive/sd/stable-diffusion-webui/models/Stable-diffusion/deliberate_v2.safetensors\r\nCreating model from config: /content/gdrive/MyDrive/sd/stable-diffusion-webui/configs/v1-inference.yaml\r\nDownloading (\u2026)olve/main/vocab.json100% 961k/961k [00:00<00:00, 15.4MB/s]\r\nDownloading (\u2026)olve/main/merges.txt100% 525k/525k [00:00<00:00, 7.23MB/s]\r\nDownloading (\u2026)cial_tokens_map.json100% 389/389 [00:00<00:00, 1.92MB/s]\r\nDownloading (\u2026)okenizer_config.json100% 905/905 [00:00<00:00, 6.89MB/s]\r\nDownloading (\u2026)lve/main/config.json100% 4.52k/4.52k [00:00<00:00, 27.0MB/s]\r\nApplying attention optimization: xformers... done.\r\nModel loaded in 47.5s (calculate hash: 24.9s, load weights from disk: 1.3s, create model: 1.8s, apply weights to model: 15.5s, calculate empty prompt: 3.9s).\r\nPreprocessing [Image 10/11]: 100% 11/11 [00:04<00:00,  2.63it/s]\r\nApplying attention optimization: xformers... done.\r\n*** Error completing request\r\n*** Arguments: ('task(egtmx3f2ofk35e0)', '', '2e-5:800, 8e-6:1600, 5e-6', 1, 1, '/content/gdrive/MyDrive/sd/Processed', 'textual_inversion', 512, 512, False, 10000, 'disabled', '0.1', False, 0, 'once', False, 100, 100, 'custom.txt', True, 'a photo of a young woman ', '', 20, 'Euler a', 7, -1, 512, 512) {}\r\n    Traceback (most recent call last):\r\n      File \"/content/gdrive/MyDrive/sd/stable-diffusion-webui/modules/call_queue.py\", line 57, in f\r\n        res = list(func(*args, **kwargs))\r\n      File \"/content/gdrive/MyDrive/sd/stable-diffusion-webui/modules/call_queue.py\", line 36, in f\r\n        res = func(*args, **kwargs)\r\n      File \"/content/gdrive/MyDrive/sd/stable-diffusion-webui/modules/hypernetworks/ui.py\", line 25, in train_hypernetwork\r\n        hypernetwork, filename = modules.hypernetworks.hypernetwork.train_hypernetwork(*args)\r\n      File \"/content/gdrive/MyDrive/sd/stable-diffusion-webui/modules/hypernetworks/hypernetwork.py\", line 477, in train_hypernetwork\r\n        textual_inversion.validate_train_inputs(hypernetwork_name, learn_rate, batch_size, gradient_step, data_root, template_file, template_filename, steps, save_hypernetwork_every, create_image_every, log_directory, name=\"hypernetwork\")\r\n      File \"/content/gdrive/MyDrive/sd/stable-diffusion-webui/modules/textual_inversion/textual_inversion.py\", line 366, in validate_train_inputs\r\n        assert model_name, f\"{name} not selected\"\r\n    AssertionError: hypernetwork not selected\r\nCalculating sha256 for /content/gdrive/MyDrive/sd/stable-diffusion-webui/models/hypernetworks/Joki .pt: ee9e9198f019b24aad90f2d54ee7b78f2e9f557763f594e1bb9cd1521fbf6dc7\r\nTraining at rate of 2e-05 until step 800\r\nPreparing dataset...\r\n100% 11/11 [00:02<00:00,  4.34it/s]\r\nTraining hypernetwork [Epoch 9: 1/11]loss: 0.0650765:   1% 99/10000 [01:14<1:52:48,  1.46it/s] *** Exception in training hypernetwork\r\n    Traceback (most recent call last):\r\n      File \"/content/gdrive/MyDrive/sd/stable-diffusion-webui/modules/hypernetworks/hypernetwork.py\", line 701, in train_hypernetwork\r\n        p.sampler_name = sd_samplers.samplers[preview_sampler_index].name\r\n    TypeError: list indices must be integers or slices, not str\r\n```\r\n\r\n\r\n### Additional information\r\n\r\n_No response_\n", "hints_text": "", "created_at": "2023-09-05T13:37:11Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 12976, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-12976", "issue_numbers": ["12943"], "base_commit": "d39440bfb9d3b20338fc23a78e6655b1e2f7c1d5", "patch": "diff --git a/modules/processing_scripts/seed.py b/modules/processing_scripts/seed.py\nindex 6b6ff987d2d..dc9c2da5000 100644\n--- a/modules/processing_scripts/seed.py\n+++ b/modules/processing_scripts/seed.py\n@@ -29,8 +29,8 @@ def ui(self, is_img2img):\n             else:\r\n                 self.seed = gr.Number(label='Seed', value=-1, elem_id=self.elem_id(\"seed\"), min_width=100, precision=0)\r\n \r\n-            random_seed = ToolButton(ui.random_symbol, elem_id=self.elem_id(\"random_seed\"), label='Random seed')\r\n-            reuse_seed = ToolButton(ui.reuse_symbol, elem_id=self.elem_id(\"reuse_seed\"), label='Reuse seed')\r\n+            random_seed = ToolButton(ui.random_symbol, elem_id=self.elem_id(\"random_seed\"), tooltip=\"Set seed to -1, which will cause a new random number to be used every time\")\r\n+            reuse_seed = ToolButton(ui.reuse_symbol, elem_id=self.elem_id(\"reuse_seed\"), tooltip=\"Reuse seed from last generation, mostly useful if it was randomized\")\r\n \r\n             seed_checkbox = gr.Checkbox(label='Extra', elem_id=self.elem_id(\"subseed_show\"), value=False)\r\n \r\ndiff --git a/modules/ui.py b/modules/ui.py\nindex 579bab9800c..89173053866 100644\n--- a/modules/ui.py\n+++ b/modules/ui.py\n@@ -215,9 +215,9 @@ def __init__(self, is_img2img):\n                     )\r\n \r\n                 with gr.Row(elem_id=f\"{id_part}_tools\"):\r\n-                    self.paste = ToolButton(value=paste_symbol, elem_id=\"paste\")\r\n-                    self.clear_prompt_button = ToolButton(value=clear_prompt_symbol, elem_id=f\"{id_part}_clear_prompt\")\r\n-                    self.restore_progress_button = ToolButton(value=restore_progress_symbol, elem_id=f\"{id_part}_restore_progress\", visible=False)\r\n+                    self.paste = ToolButton(value=paste_symbol, elem_id=\"paste\", tooltip=\"Read generation parameters from prompt or last generation if prompt is empty into user interface.\")\r\n+                    self.clear_prompt_button = ToolButton(value=clear_prompt_symbol, elem_id=f\"{id_part}_clear_prompt\", tooltip=\"Clear prompt\")\r\n+                    self.restore_progress_button = ToolButton(value=restore_progress_symbol, elem_id=f\"{id_part}_restore_progress\", visible=False, tooltip=\"Restore progress\")\r\n \r\n                     self.token_counter = gr.HTML(value=\"<span>0/75</span>\", elem_id=f\"{id_part}_token_counter\", elem_classes=[\"token-counter\"])\r\n                     self.token_button = gr.Button(visible=False, elem_id=f\"{id_part}_token_button\")\r\n@@ -348,7 +348,7 @@ def create_ui():\n                                 height = gr.Slider(minimum=64, maximum=2048, step=8, label=\"Height\", value=512, elem_id=\"txt2img_height\")\r\n \r\n                             with gr.Column(elem_id=\"txt2img_dimensions_row\", scale=1, elem_classes=\"dimensions-tools\"):\r\n-                                res_switch_btn = ToolButton(value=switch_values_symbol, elem_id=\"txt2img_res_switch_btn\", label=\"Switch dims\")\r\n+                                res_switch_btn = ToolButton(value=switch_values_symbol, elem_id=\"txt2img_res_switch_btn\", tooltip=\"Switch width/height\")\r\n \r\n                             if opts.dimensions_and_batch_together:\r\n                                 with gr.Column(elem_id=\"txt2img_column_batch\"):\r\n@@ -661,8 +661,8 @@ def copy_image(img):\n                                                 width = gr.Slider(minimum=64, maximum=2048, step=8, label=\"Width\", value=512, elem_id=\"img2img_width\")\r\n                                                 height = gr.Slider(minimum=64, maximum=2048, step=8, label=\"Height\", value=512, elem_id=\"img2img_height\")\r\n                                             with gr.Column(elem_id=\"img2img_dimensions_row\", scale=1, elem_classes=\"dimensions-tools\"):\r\n-                                                res_switch_btn = ToolButton(value=switch_values_symbol, elem_id=\"img2img_res_switch_btn\")\r\n-                                                detect_image_size_btn = ToolButton(value=detect_image_size_symbol, elem_id=\"img2img_detect_image_size_btn\")\r\n+                                                res_switch_btn = ToolButton(value=switch_values_symbol, elem_id=\"img2img_res_switch_btn\", tooltip=\"Switch width/height\")\r\n+                                                detect_image_size_btn = ToolButton(value=detect_image_size_symbol, elem_id=\"img2img_detect_image_size_btn\", tooltip=\"Auto detect size from img2img\")\r\n \r\n                                     with gr.Tab(label=\"Resize by\", elem_id=\"img2img_tab_resize_by\") as tab_scale_by:\r\n                                         scale_by = gr.Slider(minimum=0.05, maximum=4.0, step=0.05, label=\"Scale\", value=1.0, elem_id=\"img2img_scale\")\r\ndiff --git a/modules/ui_extra_networks.py b/modules/ui_extra_networks.py\nindex 063bd7b80e6..21eed6a1d6b 100644\n--- a/modules/ui_extra_networks.py\n+++ b/modules/ui_extra_networks.py\n@@ -374,7 +374,7 @@ def create_ui(interface: gr.Blocks, unrelated_tabs, tabname):\n \r\n     edit_search = gr.Textbox('', show_label=False, elem_id=tabname+\"_extra_search\", elem_classes=\"search\", placeholder=\"Search...\", visible=False, interactive=True)\r\n     dropdown_sort = gr.Dropdown(choices=['Default Sort', 'Date Created', 'Date Modified', 'Name'], value='Default Sort', elem_id=tabname+\"_extra_sort\", elem_classes=\"sort\", multiselect=False, visible=False, show_label=False, interactive=True, label=tabname+\"_extra_sort_order\")\r\n-    button_sortorder = ToolButton(switch_values_symbol, elem_id=tabname+\"_extra_sortorder\", elem_classes=\"sortorder\", visible=False)\r\n+    button_sortorder = ToolButton(switch_values_symbol, elem_id=tabname+\"_extra_sortorder\", elem_classes=\"sortorder\", visible=False, tooltip=\"Invert sort order\")\r\n     button_refresh = gr.Button('Refresh', elem_id=tabname+\"_extra_refresh\", visible=False)\r\n     checkbox_show_dirs = gr.Checkbox(True, label='Show dirs', elem_id=tabname+\"_extra_show_dirs\", elem_classes=\"show-dirs\", visible=False)\r\n \r\ndiff --git a/scripts/postprocessing_upscale.py b/scripts/postprocessing_upscale.py\nindex edb70ac01ca..eb42a29e521 100644\n--- a/scripts/postprocessing_upscale.py\n+++ b/scripts/postprocessing_upscale.py\n@@ -29,7 +29,7 @@ def ui(self):\n                                 upscaling_resize_w = gr.Slider(minimum=64, maximum=2048, step=8, label=\"Width\", value=512, elem_id=\"extras_upscaling_resize_w\")\r\n                                 upscaling_resize_h = gr.Slider(minimum=64, maximum=2048, step=8, label=\"Height\", value=512, elem_id=\"extras_upscaling_resize_h\")\r\n                             with gr.Column(elem_id=\"upscaling_dimensions_row\", scale=1, elem_classes=\"dimensions-tools\"):\r\n-                                upscaling_res_switch_btn = ToolButton(value=switch_values_symbol, elem_id=\"upscaling_res_switch_btn\")\r\n+                                upscaling_res_switch_btn = ToolButton(value=switch_values_symbol, elem_id=\"upscaling_res_switch_btn\", tooltip=\"Switch width/height\")\r\n                                 upscaling_crop = gr.Checkbox(label='Crop to fit', value=True, elem_id=\"extras_upscaling_crop\")\r\n \r\n             with FormRow():\r\n", "test_patch": "", "problem_statement": "[Bug]: Some buttons no longer display tooltips\n### Is there an existing issue for this?\n\n- [X] I have searched the existing issues and checked the recent builds/commits\n\n### What happened?\n\nSome buttons, particularly the read generation parameters button `\u2199` and clear prompt button `\ud83d\uddd1\ufe0f`, no longer display their tooltips in 1.6.0.\n\n### Steps to reproduce the problem\n\n1. Hover over one of the aforementioned buttons\r\n2. Note the tooltip does not appear\n\n### What should have happened?\n\nTooltips should appear just as they did in earlier versions.\n\n### Sysinfo\n\n[sysinfo.txt](https://github.com/AUTOMATIC1111/stable-diffusion-webui/files/12482405/sysinfo-2023-08-31-05-08.txt)\n\n### What browsers do you use to access the UI ?\n\nMozilla Firefox\n\n### Console logs\n\n```Shell\nn/a\n```\n\n\n### Additional information\n\n_No response_\n", "hints_text": "", "created_at": "2023-09-01T23:02:03Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 12975, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-12975", "issue_numbers": ["12961"], "base_commit": "d39440bfb9d3b20338fc23a78e6655b1e2f7c1d5", "patch": "diff --git a/modules/ui_prompt_styles.py b/modules/ui_prompt_styles.py\nindex 85eb3a6417e..64d379ef65e 100644\n--- a/modules/ui_prompt_styles.py\n+++ b/modules/ui_prompt_styles.py\n@@ -4,6 +4,7 @@\n \r\n styles_edit_symbol = '\\U0001f58c\\uFE0F'  # \ud83d\udd8c\ufe0f\r\n styles_materialize_symbol = '\\U0001f4cb'  # \ud83d\udccb\r\n+styles_copy_symbol = '\\U0001f4dd'  # \ud83d\udcdd\r\n \r\n \r\n def select_style(name):\r\n@@ -62,6 +63,7 @@ def __init__(self, tabname, main_ui_prompt, main_ui_negative_prompt):\n                 self.selection = gr.Dropdown(label=\"Styles\", elem_id=f\"{tabname}_styles_edit_select\", choices=list(shared.prompt_styles.styles), value=[], allow_custom_value=True, info=\"Styles allow you to add custom text to prompt. Use the {prompt} token in style text, and it will be replaced with user's prompt when applying style. Otherwise, style's text will be added to the end of the prompt.\")\r\n                 ui_common.create_refresh_button([self.dropdown, self.selection], shared.prompt_styles.reload, lambda: {\"choices\": list(shared.prompt_styles.styles)}, f\"refresh_{tabname}_styles\")\r\n                 self.materialize = ui_components.ToolButton(value=styles_materialize_symbol, elem_id=f\"{tabname}_style_apply\", tooltip=\"Apply all selected styles from the style selction dropdown in main UI to the prompt.\")\r\n+                self.copy = ui_components.ToolButton(value=styles_copy_symbol, elem_id=f\"{tabname}_style_copy\", tooltip=\"Copy main UI prompt to style.\")\r\n \r\n             with gr.Row():\r\n                 self.prompt = gr.Textbox(label=\"Prompt\", show_label=True, elem_id=f\"{tabname}_edit_style_prompt\", lines=3)\r\n@@ -103,6 +105,13 @@ def __init__(self, tabname, main_ui_prompt, main_ui_negative_prompt):\n             show_progress=False,\r\n         ).then(fn=None, _js=\"function(){update_\"+tabname+\"_tokens(); closePopup();}\", show_progress=False)\r\n \r\n+        self.copy.click(\r\n+            fn=lambda p, n: (p, n),\r\n+            inputs=[main_ui_prompt, main_ui_negative_prompt],\r\n+            outputs=[self.prompt, self.neg_prompt],\r\n+            show_progress=False,\r\n+        )\r\n+\r\n         ui_common.setup_dialog(button_show=edit_button, dialog=styles_dialog, button_close=self.close)\r\n \r\n \r\n", "test_patch": "", "problem_statement": "[Feature Request]: Where is the save style button?\n### Is there an existing issue for this?\n\n- [X] I have searched the existing issues and checked the recent builds/commits\n\n### What would your feature do ?\n\nIs it possible to make the old implementation of save style as well?\r\nNot being able to save the currently typed prompt is very troublesome.\r\nWhy do we have to open the edit screen and copy/paste the prompt?\n\n### Proposed workflow\n\nRestore old implementation of save styles button\n\n### Additional information\n\n_No response_\n", "hints_text": "", "created_at": "2023-09-01T22:23:32Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 12588, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-12588", "issue_numbers": ["10260"], "base_commit": "85fcb7b8dfe7b3dd06931943f095c77f1043dc25", "patch": "diff --git a/modules/img2img.py b/modules/img2img.py\nindex ac9fd3f843d..328cb0e9e24 100644\n--- a/modules/img2img.py\n+++ b/modules/img2img.py\n@@ -129,7 +129,7 @@ def img2img(id_task: str, mode: int, prompt: str, negative_prompt: str, prompt_s\n         mask = None\r\n     elif mode == 2:  # inpaint\r\n         image, mask = init_img_with_mask[\"image\"], init_img_with_mask[\"mask\"]\r\n-        mask = mask.split()[-1].convert(\"L\").point(lambda x: 255 if x > 128 else 0)\r\n+        mask = processing.create_binary_mask(mask)\r\n         image = image.convert(\"RGB\")\r\n     elif mode == 3:  # inpaint sketch\r\n         image = inpaint_color_sketch\r\ndiff --git a/modules/processing.py b/modules/processing.py\nindex 1d098302ead..e62db62fd9c 100755\n--- a/modules/processing.py\n+++ b/modules/processing.py\n@@ -81,6 +81,12 @@ def apply_overlay(image, paste_loc, index, overlays):\n \r\n     return image\r\n \r\n+def create_binary_mask(image):\r\n+    if image.mode == 'RGBA' and image.getextrema()[-1] != (255, 255):\r\n+        image = image.split()[-1].convert(\"L\").point(lambda x: 255 if x > 128 else 0)\r\n+    else:\r\n+        image = image.convert('L')\r\n+    return image\r\n \r\n def txt2img_image_conditioning(sd_model, x, width, height):\r\n     if sd_model.model.conditioning_key in {'hybrid', 'concat'}: # Inpainting models\r\n@@ -1385,7 +1391,9 @@ def init(self, all_prompts, all_seeds, all_subseeds):\n         image_mask = self.image_mask\r\n \r\n         if image_mask is not None:\r\n-            image_mask = image_mask.convert('L')\r\n+            # image_mask is passed in as RGBA by Gradio to support alpha masks,\r\n+            # but we still want to support binary masks.\r\n+            image_mask = create_binary_mask(image_mask)\r\n \r\n             if self.inpainting_mask_invert:\r\n                 image_mask = ImageOps.invert(image_mask)\r\ndiff --git a/modules/ui.py b/modules/ui.py\nindex a6b1f964b0a..c98d9849632 100644\n--- a/modules/ui.py\n+++ b/modules/ui.py\n@@ -598,7 +598,7 @@ def update_orig(image, state):\n \r\n                     with gr.TabItem('Inpaint upload', id='inpaint_upload', elem_id=\"img2img_inpaint_upload_tab\") as tab_inpaint_upload:\r\n                         init_img_inpaint = gr.Image(label=\"Image for img2img\", show_label=False, source=\"upload\", interactive=True, type=\"pil\", elem_id=\"img_inpaint_base\")\r\n-                        init_mask_inpaint = gr.Image(label=\"Mask\", source=\"upload\", interactive=True, type=\"pil\", elem_id=\"img_inpaint_mask\")\r\n+                        init_mask_inpaint = gr.Image(label=\"Mask\", source=\"upload\", interactive=True, type=\"pil\", image_mode=\"RGBA\", elem_id=\"img_inpaint_mask\")\r\n \r\n                     with gr.TabItem('Batch', id='batch', elem_id=\"img2img_batch_tab\") as tab_batch:\r\n                         hidden = '<br>Disabled when launched with --hide-ui-dir-config.' if shared.cmd_opts.hide_ui_dir_config else ''\r\n", "test_patch": "", "problem_statement": "img2img api   ValueError: Coordinate 'right' is less than 'left'\n### Is there an existing issue for this?\n\n- [X] I have searched the existing issues and checked the recent builds/commits\n\n### What happened?\n\nif i use mask  and  set inpaint_full_res =true\r\n\r\npyton will tip ValueError: Coordinate 'right' is less than 'left'\r\n![image](https://github.com/AUTOMATIC1111/stable-diffusion-webui/assets/3301385/89cc9654-ed2f-4c75-b61a-3249608bcfcf)\r\n\n\n### Steps to reproduce the problem\n\n1. Go to .... \r\n2. Press ....\r\n3. ...\r\n\n\n### What should have happened?\n\ni hope it can working\n\n### Commit where the problem happens\n\nSHA-1: 5ab7f213bec2f816f9c5644becb32eb72c8ffb89\n\n### What platforms do you use to access the UI ?\n\nWindows\n\n### What browsers do you use to access the UI ?\n\nGoogle Chrome\n\n### Command Line Arguments\n\n```Shell\n--api --listen\n```\n\n\n### List of extensions\n\nno\n\n### Console logs\n\n```Shell\nno\n```\n\n\n### Additional information\n\n_No response_\n", "hints_text": "This happens when you're inpainting but there's no mask. I noticed this as well, some bug remove the mask randomly, not sure how to reproduce this.\nmet the problem here, any solution? \nGen picture and mask, have similar exception\r\n<details>\r\n  <summary>Picture and mask</summary>\r\n both are 512*512\r\n \r\n![mask](https://github.com/AUTOMATIC1111/stable-diffusion-webui/assets/33497292/7ac7c55c-c9ad-4b40-b7df-8ef39c8b1539)\r\n![image](https://github.com/AUTOMATIC1111/stable-diffusion-webui/assets/33497292/e6690e3e-9dae-4dac-9394-0aaf6753cda5)\r\n</details>\nI have the same exception.\n> \r\n\r\nDraw on the image mate. :)\n> > \r\n> \r\n> Draw on the image mate. :)\r\n\r\nBut how set, that it is an inpant, not just img2img?\n> \r\n\r\nLike this. Note the size should be for your mask, so maybe double if you want it higher resolution - but not for the entire image, when you use inpaint.\r\n![3 - LARGE image still paints only inside mask](https://github.com/AUTOMATIC1111/stable-diffusion-webui/assets/3269738/0a8549b0-e341-43ea-a993-cfc2c33fb9ec)\r\n\n@andupotorac here we discuss an api, how to create curl request for inpanting draw?\n> @andupotorac here we discuss an api, how to create curl request for inpanting draw?\r\n\r\nOh, didn't notice that. Can't help with the API.\nI had this error, many times.\r\n\r\nToday I'm using inpaint upload. Based on the testimony above I replaced transparency in my mask file with white, and the error was gone, but it's attempting to inpaint the white area. Disjointedly.\r\n\r\n\r\n![problem0](https://github.com/AUTOMATIC1111/stable-diffusion-webui/assets/25666053/7ce46689-71ef-4437-bfb7-6195b7fe2052)\r\n\r\n![problem1](https://github.com/AUTOMATIC1111/stable-diffusion-webui/assets/25666053/793cd688-5dcb-4d54-9c51-9d09b1d36c5a)\r\n\r\n\nThe problem here is the webui expects the mask input to be a binary mask (RGB, only white and black), not an alpha mask. That being said some conversion should ideally happen automatically to correct this.\r\n\r\nEdit: Actually looks like some upstream breaking change with Gradio broke this. Will have a fix shortly.", "created_at": "2023-08-15T18:27:58Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 12526, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-12526", "issue_numbers": ["11465"], "base_commit": "f093c9d39d0fe9951a8f5c570027cecc68778ef2", "patch": "diff --git a/modules/launch_utils.py b/modules/launch_utils.py\nindex 4fc254a25ed..9eda7c9d3f4 100644\n--- a/modules/launch_utils.py\n+++ b/modules/launch_utils.py\n@@ -173,9 +173,12 @@ def git_clone(url, dir, name, commithash=None):\n         if current_hash == commithash:\r\n             return\r\n \r\n+        if run_git(dir, name, 'config --get remote.origin.url', None, f\"Couldn't determine {name}'s origin URL\", live=False).strip() != url:\r\n+            run_git(dir, name, f'remote set-url origin \"{url}\"', None, f\"Failed to set {name}'s origin URL\", live=False)\r\n+\r\n         run_git(dir, name, 'fetch', f\"Fetching updates for {name}...\", f\"Couldn't fetch {name}\", autofix=False)\r\n \r\n-        run_git(dir, name, 'checkout', f\"Checking out commit for {name} with hash: {commithash}...\", f\"Couldn't checkout commit {commithash} for {name}\", live=True)\r\n+        run_git(dir, name, f'checkout {commithash}', f\"Checking out commit for {name} with hash: {commithash}...\", f\"Couldn't checkout commit {commithash} for {name}\", live=True)\r\n \r\n         return\r\n \r\ndiff --git a/modules/mac_specific.py b/modules/mac_specific.py\nindex bce527ccc98..89256c5b060 100644\n--- a/modules/mac_specific.py\n+++ b/modules/mac_specific.py\n@@ -52,9 +52,6 @@ def cumsum_fix(input, cumsum_func, *args, **kwargs):\n \n \n if has_mps:\n-    # MPS fix for randn in torchsde\n-    CondFunc('torchsde._brownian.brownian_interval._randn', lambda _, size, dtype, device, seed: torch.randn(size, dtype=dtype, device=torch.device(\"cpu\"), generator=torch.Generator(torch.device(\"cpu\")).manual_seed(int(seed))).to(device), lambda _, size, dtype, device, seed: device.type == 'mps')\n-\n     if platform.mac_ver()[0].startswith(\"13.2.\"):\n         # MPS workaround for https://github.com/pytorch/pytorch/issues/95188, thanks to danieldk (https://github.com/explosion/curated-transformers/pull/124)\n         CondFunc('torch.nn.functional.linear', lambda _, input, weight, bias: (torch.matmul(input, weight.t()) + bias) if bias is not None else torch.matmul(input, weight.t()), lambda _, input, weight, bias: input.numel() > 10485760)\ndiff --git a/modules/sd_hijack_optimizations.py b/modules/sd_hijack_optimizations.py\nindex 0e810eec8a9..7f9e328d05a 100644\n--- a/modules/sd_hijack_optimizations.py\n+++ b/modules/sd_hijack_optimizations.py\n@@ -1,6 +1,7 @@\n from __future__ import annotations\r\n import math\r\n import psutil\r\n+import platform\r\n \r\n import torch\r\n from torch import einsum\r\n@@ -94,7 +95,10 @@ def apply(self):\n class SdOptimizationSubQuad(SdOptimization):\r\n     name = \"sub-quadratic\"\r\n     cmd_opt = \"opt_sub_quad_attention\"\r\n-    priority = 10\r\n+\r\n+    @property\r\n+    def priority(self):\r\n+        return 1000 if shared.device.type == 'mps' else 10\r\n \r\n     def apply(self):\r\n         ldm.modules.attention.CrossAttention.forward = sub_quad_attention_forward\r\n@@ -120,7 +124,7 @@ class SdOptimizationInvokeAI(SdOptimization):\n \r\n     @property\r\n     def priority(self):\r\n-        return 1000 if not torch.cuda.is_available() else 10\r\n+        return 1000 if shared.device.type != 'mps' and not torch.cuda.is_available() else 10\r\n \r\n     def apply(self):\r\n         ldm.modules.attention.CrossAttention.forward = split_cross_attention_forward_invokeAI\r\n@@ -427,7 +431,10 @@ def sub_quad_attention(q, k, v, q_chunk_size=1024, kv_chunk_size=None, kv_chunk_\n     qk_matmul_size_bytes = batch_x_heads * bytes_per_token * q_tokens * k_tokens\r\n \r\n     if chunk_threshold is None:\r\n-        chunk_threshold_bytes = int(get_available_vram() * 0.9) if q.device.type == 'mps' else int(get_available_vram() * 0.7)\r\n+        if q.device.type == 'mps':\r\n+            chunk_threshold_bytes = 268435456 * (2 if platform.processor() == 'i386' else bytes_per_token)\r\n+        else:\r\n+            chunk_threshold_bytes = int(get_available_vram() * 0.7)\r\n     elif chunk_threshold == 0:\r\n         chunk_threshold_bytes = None\r\n     else:\r\ndiff --git a/modules/sd_samplers_timesteps_impl.py b/modules/sd_samplers_timesteps_impl.py\nindex 48d7e6491c4..d32e35213e4 100644\n--- a/modules/sd_samplers_timesteps_impl.py\n+++ b/modules/sd_samplers_timesteps_impl.py\n@@ -11,7 +11,7 @@\n def ddim(model, x, timesteps, extra_args=None, callback=None, disable=None, eta=0.0):\r\n     alphas_cumprod = model.inner_model.inner_model.alphas_cumprod\r\n     alphas = alphas_cumprod[timesteps]\r\n-    alphas_prev = alphas_cumprod[torch.nn.functional.pad(timesteps[:-1], pad=(1, 0))].to(torch.float64)\r\n+    alphas_prev = alphas_cumprod[torch.nn.functional.pad(timesteps[:-1], pad=(1, 0))].to(torch.float64 if x.device.type != 'mps' else torch.float32)\r\n     sqrt_one_minus_alphas = torch.sqrt(1 - alphas)\r\n     sigmas = eta * np.sqrt((1 - alphas_prev.cpu().numpy()) / (1 - alphas.cpu()) * (1 - alphas.cpu() / alphas_prev.cpu().numpy()))\r\n \r\n@@ -42,7 +42,7 @@ def ddim(model, x, timesteps, extra_args=None, callback=None, disable=None, eta=\n def plms(model, x, timesteps, extra_args=None, callback=None, disable=None):\r\n     alphas_cumprod = model.inner_model.inner_model.alphas_cumprod\r\n     alphas = alphas_cumprod[timesteps]\r\n-    alphas_prev = alphas_cumprod[torch.nn.functional.pad(timesteps[:-1], pad=(1, 0))].to(torch.float64)\r\n+    alphas_prev = alphas_cumprod[torch.nn.functional.pad(timesteps[:-1], pad=(1, 0))].to(torch.float64 if x.device.type != 'mps' else torch.float32)\r\n     sqrt_one_minus_alphas = torch.sqrt(1 - alphas)\r\n \r\n     extra_args = {} if extra_args is None else extra_args\r\ndiff --git a/modules/sub_quadratic_attention.py b/modules/sub_quadratic_attention.py\nindex 497568eb51b..ae4ee4bbec0 100644\n--- a/modules/sub_quadratic_attention.py\n+++ b/modules/sub_quadratic_attention.py\n@@ -58,7 +58,7 @@ def _summarize_chunk(\n     scale: float,\n ) -> AttnChunk:\n     attn_weights = torch.baddbmm(\n-        torch.empty(1, 1, 1, device=query.device, dtype=query.dtype),\n+        torch.zeros(1, 1, 1, device=query.device, dtype=query.dtype),\n         query,\n         key.transpose(1,2),\n         alpha=scale,\n@@ -121,7 +121,7 @@ def _get_attention_scores_no_kv_chunking(\n     scale: float,\n ) -> Tensor:\n     attn_scores = torch.baddbmm(\n-        torch.empty(1, 1, 1, device=query.device, dtype=query.dtype),\n+        torch.zeros(1, 1, 1, device=query.device, dtype=query.dtype),\n         query,\n         key.transpose(1,2),\n         alpha=scale,\ndiff --git a/webui-macos-env.sh b/webui-macos-env.sh\nindex 6354e73ba72..24bc5c42615 100644\n--- a/webui-macos-env.sh\n+++ b/webui-macos-env.sh\n@@ -12,8 +12,6 @@ fi\n export install_dir=\"$HOME\"\n export COMMANDLINE_ARGS=\"--skip-torch-cuda-test --upcast-sampling --no-half-vae --use-cpu interrogate\"\n export TORCH_COMMAND=\"pip install torch==2.0.1 torchvision==0.15.2\"\n-export K_DIFFUSION_REPO=\"https://github.com/brkirch/k-diffusion.git\"\n-export K_DIFFUSION_COMMIT_HASH=\"51c9778f269cedb55a4d88c79c0246d35bdadb71\"\n export PYTORCH_ENABLE_MPS_FALLBACK=1\n \n ####################################################################\n", "test_patch": "", "problem_statement": "[Bug]: Samplers DPM++ 2M SDE and DPM++ 2M SDE Karras do not show up on Mac, because webui-macos-env.sh has not been updated\n### Is there an existing issue for this?\r\n\r\n- [X] I have searched the existing issues and checked the recent builds/commits\r\n\r\n### What happened?\r\n\r\nI have updated my installation to the most recent commit (394ffa7b0a7fff3ec484bcd084e673a8b301ccc8). The samplers DPM++ 2M SDE and DPM++ 2M SDE Karras are listed in sd_samplers_kdiffusion.py:\r\n```\r\nsamplers_k_diffusion = [\r\n    ('Euler a', 'sample_euler_ancestral', ['k_euler_a', 'k_euler_ancestral'], {\"uses_ensd\": True}),\r\n    ...\r\n    ('DPM++ 2M SDE', 'sample_dpmpp_2m_sde', ['k_dpmpp_2m_sde_ka'], {\"brownian_noise\": True}),\r\n    ...\r\n    ('DPM++ 2M SDE Karras', 'sample_dpmpp_2m_sde', ['k_dpmpp_2m_sde_ka'], {'scheduler': 'karras', \"brownian_noise\": True}),\r\n]\r\n```\r\nBut they do not show up in the sampler dropdown in the WebUI:\r\n<img width=\"1012\" alt=\"current\" src=\"https://github.com/AUTOMATIC1111/stable-diffusion-webui/assets/9304811/193a100c-448e-4d20-9bf2-a8f127022bb1\">__\r\n\r\n### Steps to reproduce the problem\r\n\r\n1. On a Mac, update Automatic1111 WebUI from an older version to the current version using `git pull`.\r\n2. Start the WebUI.\r\n3. Look at the dropdown list of samplers.\r\n\r\n\r\n### What should have happened?\r\n\r\nThe dropdown list should have listed the two missing samplers, DPM++ 2M SDE and DPM++ 2M SDE Karras.\r\n\r\n### Version or Commit where the problem happens\r\n\r\n394ffa7b0a7fff3ec484bcd084e673a8b301ccc8\r\n\r\n### What Python version are you running on ?\r\n\r\nPython 3.10.x\r\n\r\n### What platforms do you use to access the UI ?\r\n\r\nMacOS\r\n\r\n### What device are you running WebUI on?\r\n\r\nOther GPUs\r\n\r\n### Cross attention optimization\r\n\r\nAutomatic\r\n\r\n### What browsers do you use to access the UI ?\r\n\r\nMozilla Firefox, Apple Safari\r\n\r\n### Command Line Arguments\r\n\r\n```Shell\r\nLaunching Web UI with arguments: --skip-torch-cuda-test --upcast-sampling --no-half-vae --use-cpu interrogate\r\n```\r\n\r\n\r\n### List of extensions\r\n\r\n<img width=\"1148\" alt=\"extensions\" src=\"https://github.com/AUTOMATIC1111/stable-diffusion-webui/assets/9304811/c396a984-81d2-4cfa-beb6-6c4d08029aa2\">\r\n\r\n### Console logs\r\n\r\n```Shell\r\nLast login: Tue Jun 27 22:00:42 on ttys000\r\nRestored session: Di 27 Jun 2023 22:03:54 CEST\r\nUSERNAME@Air-von-USERNAME ~ % bash\r\n\r\nThe default interactive shell is now zsh.\r\nTo update your account to use zsh, please run `chsh -s /bin/zsh`.\r\nFor more details, please visit https://support.apple.com/kb/HT208050.\r\nbash-3.2$ cd stable-diffusion-webui\r\nbash-3.2$ ./webui.sh\r\n\r\n################################################################\r\nInstall script for stable-diffusion + Web UI\r\nTested on Debian 11 (Bullseye)\r\n################################################################\r\n\r\n################################################################\r\nRunning on USERNAME user\r\n################################################################\r\n\r\n################################################################\r\nRepo already cloned, using it as install directory\r\n################################################################\r\n\r\n################################################################\r\nCreate and activate python venv\r\n################################################################\r\n\r\n################################################################\r\nLaunching launch.py...\r\n################################################################\r\nPython 3.10.12 (main, Jun  7 2023, 00:38:32) [Clang 14.0.3 (clang-1403.0.22.14.1)]\r\nVersion: v1.4.0\r\nCommit hash: 394ffa7b0a7fff3ec484bcd084e673a8b301ccc8\r\nInstalling requirements\r\n\r\n\r\n\r\nLaunching Web UI with arguments: --skip-torch-cuda-test --upcast-sampling --no-half-vae --use-cpu interrogate\r\nNo module 'xformers'. Proceeding without it.\r\nWarning: caught exception 'Torch not compiled with CUDA enabled', memory monitor disabled\r\n[-] ADetailer initialized. version: 23.6.3, num models: 13\r\n2023-06-27 22:18:33,941 - ControlNet - INFO - ControlNet v1.1.227\r\nControlNet preprocessor location: /Users/USERNAME/stable-diffusion-webui/extensions/sd-webui-controlnet/annotator/downloads\r\n2023-06-27 22:18:33,975 - ControlNet - INFO - ControlNet v1.1.227\r\nImage Browser: ImageReward is not installed, cannot be used.\r\nLoading weights [ec41bd2a82] from /Users/USERNAME/stable-diffusion-webui/models/Stable-diffusion/photon_v1.safetensors\r\nCreating model from config: /Users/USERNAME/stable-diffusion-webui/configs/v1-inference.yaml\r\nLatentDiffusion: Running in eps-prediction mode\r\nDiffusionWrapper has 859.52 M params.\r\nRunning on local URL:  http://127.0.0.1:7860\r\n\r\nTo create a public link, set `share=True` in `launch()`.\r\nStartup time: 5.7s (import torch: 0.7s, import gradio: 0.5s, import ldm: 0.2s, other imports: 0.4s, list SD models: 0.1s, load scripts: 2.7s, create ui: 0.6s, gradio launch: 0.3s).\r\npreload_extensions_git_metadata for 15 extensions took 0.45s\r\nLoading VAE weights specified in settings: /Users/USERNAME/stable-diffusion-webui/models/VAE/vae-ft-mse-840000-ema-pruned.safetensors\r\nApplying attention optimization: InvokeAI... done.\r\nTextual inversion embeddings loaded(27): Asian-Less-Neg, bad-hands-5, bad-image-v2-39000, bad-picture-chill-75v, bad_pictures, bad_prompt_version2, BadDream, badhandsv5-neg, badhandv4, BadNegAnatomyV1-neg, CyberRealistic_Negative-neg, easynegative, epiCNegative, kkw-ph1, kkw-ph1-neg, negative_hand-neg, NegfeetV2, ng_deepnegative_v1_75t, np_simple_negatives, PureErosFace_V1, realisticvision-negative-embedding, rmadanegative402_sd15-neg, rmadanegative4_sd15-neg, ulzzang-6500, UnrealisticDream, verybadimagenegative_v1.2-6400, verybadimagenegative_v1.3\r\nTextual inversion embeddings skipped(7): PhotoHelper, nrealfixer, CinemaHelper, PortraitHelper, rmadanegative-neg, VintageHelper, nfixer\r\nModel loaded in 2.6s (load weights from disk: 0.1s, create model: 0.9s, apply weights to model: 0.4s, apply half(): 0.3s, load VAE: 0.2s, move model to device: 0.5s, calculate empty prompt: 0.1s).\r\nSaving backup of webui/extension state to /Users/USERNAME/stable-diffusion-webui/config_states/2023_06_27-22_26_52_Backup (pre-update).json.\r\n\r\n################################################################\r\nLaunching launch.py...\r\n################################################################\r\nPython 3.10.12 (main, Jun  7 2023, 00:38:32) [Clang 14.0.3 (clang-1403.0.22.14.1)]\r\nVersion: v1.4.0\r\nCommit hash: 394ffa7b0a7fff3ec484bcd084e673a8b301ccc8\r\nInstalling requirements\r\n\r\n\r\n\r\nLaunching Web UI with arguments: --skip-torch-cuda-test --upcast-sampling --no-half-vae --use-cpu interrogate\r\nNo module 'xformers'. Proceeding without it.\r\nWarning: caught exception 'Torch not compiled with CUDA enabled', memory monitor disabled\r\n[-] ADetailer initialized. version: 23.6.3, num models: 13\r\n2023-06-27 22:26:59,763 - ControlNet - INFO - ControlNet v1.1.227\r\nControlNet preprocessor location: /Users/USERNAME/stable-diffusion-webui/extensions/sd-webui-controlnet/annotator/downloads\r\n2023-06-27 22:26:59,798 - ControlNet - INFO - ControlNet v1.1.227\r\nImage Browser: ImageReward is not installed, cannot be used.\r\nLoading weights [ec41bd2a82] from /Users/USERNAME/stable-diffusion-webui/models/Stable-diffusion/photon_v1.safetensors\r\nCreating model from config: /Users/USERNAME/stable-diffusion-webui/configs/v1-inference.yaml\r\nLatentDiffusion: Running in eps-prediction mode\r\nDiffusionWrapper has 859.52 M params.\r\nRunning on local URL:  http://127.0.0.1:7860\r\n\r\nTo create a public link, set `share=True` in `launch()`.\r\nStartup time: 5.1s (import torch: 0.7s, import gradio: 0.5s, import ldm: 0.3s, other imports: 0.4s, list SD models: 0.1s, load scripts: 2.2s, initialize extra networks: 0.1s, create ui: 0.5s, gradio launch: 0.2s).\r\npreload_extensions_git_metadata for 15 extensions took 0.31s\r\nLoading VAE weights specified in settings: /Users/USERNAME/stable-diffusion-webui/models/VAE/vae-ft-mse-840000-ema-pruned.safetensors\r\nApplying attention optimization: InvokeAI... done.\r\nTextual inversion embeddings loaded(27): Asian-Less-Neg, bad-hands-5, bad-image-v2-39000, bad-picture-chill-75v, bad_pictures, bad_prompt_version2, BadDream, badhandsv5-neg, badhandv4, BadNegAnatomyV1-neg, CyberRealistic_Negative-neg, easynegative, epiCNegative, kkw-ph1, kkw-ph1-neg, negative_hand-neg, NegfeetV2, ng_deepnegative_v1_75t, np_simple_negatives, PureErosFace_V1, realisticvision-negative-embedding, rmadanegative402_sd15-neg, rmadanegative4_sd15-neg, ulzzang-6500, UnrealisticDream, verybadimagenegative_v1.2-6400, verybadimagenegative_v1.3\r\nTextual inversion embeddings skipped(7): PhotoHelper, nrealfixer, CinemaHelper, PortraitHelper, rmadanegative-neg, VintageHelper, nfixer\r\nModel loaded in 3.3s (load weights from disk: 0.1s, create model: 0.8s, apply weights to model: 0.4s, apply half(): 1.1s, load VAE: 0.2s, move model to device: 0.4s, calculate empty prompt: 0.1s).\r\n```\r\n\r\n\r\n### Additional information\r\n\r\n(see the following comment)\n", "hints_text": "", "created_at": "2023-08-13T14:34:19Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 12387, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-12387", "issue_numbers": ["7984"], "base_commit": "fd67eafc65ce15c13614dda0f9af4b6263aa03dc", "patch": "diff --git a/webui.py b/webui.py\nindex a5b115759af..86a62a920ae 100644\n--- a/webui.py\n+++ b/webui.py\n@@ -341,6 +341,7 @@ def api_only():\n     setup_middleware(app)\r\n     api = create_api(app)\r\n \r\n+    modules.script_callbacks.before_ui_callback()\r\n     modules.script_callbacks.app_started_callback(None, app)\r\n \r\n     print(f\"Startup time: {startup_timer.summary()}.\")\r\n", "test_patch": "", "problem_statement": "[Bug]: Lora cannot be loaded in API mode\n### Is there an existing issue for this?\r\n\r\n- [X] I have searched the existing issues and checked the recent builds/commits\r\n\r\n### What happened?\r\n\r\nwhen  I run webui --api  to start a simple api only server, and I post a http request ,it show \r\n\r\n**Skipping unknown extra network: lora**\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 40/40 [00:11<00:00,  3.39it/s]\r\nINFO:     127.0.0.1:38420 - \"POST /sdapi/v1/txt2img HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:38647 - \"POST /sdapi/v1/png-info HTTP/1.1\" 200 OK\r\n\r\nAnd I check the web.py, It seems that lora is not registered\uff0cfor some reason I can't access the web page, so I don't konw if lora works\r\n\r\n### Steps to reproduce the problem\r\n\r\n1. download release package \r\n2. download  diffusion and lora checkpoint file\r\n3. run webui.sh to prepare env\r\n4. exec ./webui.sh --nowebapi\r\n5. and it occurs\r\n\r\n\r\n### What should have happened?\r\n**Skipping unknown extra network: lora** shouldn't happen\r\n\r\n\r\n### Commit where the problem happens\r\n\r\nrelease v1.0.0-pre\r\n\r\n### What platforms do you use to access the UI ?\r\n\r\n_No response_\r\n\r\n### What browsers do you use to access the UI ?\r\n\r\n_No response_\r\n\r\n### Command Line Arguments\r\n\r\n```Shell\r\n./webui.sh --nowebui\r\n```\r\n\r\n\r\n### List of extensions\r\n\r\nNo\r\n\r\n### Console logs\r\n\r\n```Shell\r\nSkipping unknown extra network: lora\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 40/40 [00:10<00:00,  3.68it/s]\r\nINFO:     127.0.0.1:39473 - \"POST /sdapi/v1/txt2img HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:39758 - \"POST /sdapi/v1/png-info HTTP/1.1\" 200 OK\r\n```\r\n\r\n\r\n### Additional information\r\n\r\n_No response_\n", "hints_text": "Do you mind sharing your POST payload?\n> Do you mind sharing your POST payload?\r\n\r\nI got the same warning, lora model was ignored.\r\n```json\r\n{\r\n\t\"prompt\": \"best quality, ultra high res, (photorealistic:1.4), 1girl, brown blazer, black skirt, glasses, thighhighs, ((T shirt)), (upper body), (Kpop idol), (aegyo sal:1), (platinum   blonde hair:1), ((puffy eyes)), looking at viewer, facing front, smiling, <lora:koreanDollLikeness_v10:0.66>\",\r\n\t\"negative_prompt\": \"nsfw, paintings, sketches, (worst quality:2), (low quality:2), (normal quality:2), lowres, normal quality, ((monochrome)), ((grayscale)), skin spots, acnes, age spot, glan\",\r\n\t\"steps\": 28,\r\n\t\"cfg_scale\": 8,\r\n\t\"width\": 768,\r\n\t\"height\": 1024,\r\n\t\"n_iter\": 4,\r\n\t\"sampler_index\": \"DPM++ SDE Karras\"\r\n}\r\n```\r\n\n> \r\nthis is my payload, and I can load the lora with webui \r\n```\r\npayload = {\r\n    \"prompt\": \"<lora:koreanDollLikeness_v10:1>, (8k, RAW photo, best quality, masterpiece:1.2),(realistic, photo-realistic:1.37),omertosa,1lady, (aegyo sal:1),city,professional lighting,photon mapping, radiosity, physically-based rendering,  blue backlight bokeh, smile, mature,head portrait,sunlight,blone,short jacket,school uniform, hoodie, short hair, floating hair\",\r\n    \"Negative prompt\": \"EasyNegative, paintings, sketches, EasyNegative, paintings, sketches,  lowres, normal quality, ((monochrome)), ((grayscale)),skin spots, acnes, skin blemishes, age spot,glans,extra fingers,fewer fingers,strange fingers, bad hand, high heel shoes, fat ass,hole,kid,teen,cute,naked, Ripped jeans,hole jeans, fat thigh,6 fingers, bikini,underwear, nsfw, nude, briefs,knickers, underpants,panties, swim suit,leg open\",\r\n    \"steps\": 40,\r\n    \"Sampler\": \"DPM++ 2M Karras\",\r\n    \"CFG scale\": \"7\",\r\n    \"Seed\": 3534095324,\r\n    \"Model\": \"chilloutmix_NiPrunedFp32\",\r\n    \"height\":\"768\",\r\n    \"width\":\"512\",\r\n    \"Denoising strength\": 1,\r\n    \"Hires upscale\":1,\r\n    \"Hires steps\":40,\r\n    \"Hires upscaler\":\"Latent\"\r\n}\r\n```\n> > \r\n> \r\n> this is my payload, and I can load the lora with webui\r\n> \r\n> ```\r\n> payload = {\r\n>     \"prompt\": \"<lora:koreanDollLikeness_v10:1>, (8k, RAW photo, best quality, masterpiece:1.2),(realistic, photo-realistic:1.37),omertosa,1lady, (aegyo sal:1),city,professional lighting,photon mapping, radiosity, physically-based rendering,  blue backlight bokeh, smile, mature,head portrait,sunlight,blone,short jacket,school uniform, hoodie, short hair, floating hair\",\r\n>     \"Negative prompt\": \"EasyNegative, paintings, sketches, EasyNegative, paintings, sketches,  lowres, normal quality, ((monochrome)), ((grayscale)),skin spots, acnes, skin blemishes, age spot,glans,extra fingers,fewer fingers,strange fingers, bad hand, high heel shoes, fat ass,hole,kid,teen,cute,naked, Ripped jeans,hole jeans, fat thigh,6 fingers, bikini,underwear, nsfw, nude, briefs,knickers, underpants,panties, swim suit,leg open\",\r\n>     \"steps\": 40,\r\n>     \"Sampler\": \"DPM++ 2M Karras\",\r\n>     \"CFG scale\": \"7\",\r\n>     \"Seed\": 3534095324,\r\n>     \"Model\": \"chilloutmix_NiPrunedFp32\",\r\n>     \"height\":\"768\",\r\n>     \"width\":\"512\",\r\n>     \"Denoising strength\": 1,\r\n>     \"Hires upscale\":1,\r\n>     \"Hires steps\":40,\r\n>     \"Hires upscaler\":\"Latent\"\r\n> }\r\n> ```\r\nIs your Superresolution valid? I don't seem to increase the image resolution, even though \"Hires upscale\": 2. By the way, have you tried the hires.fox of img2img?\r\n\r\n\nhttps://github.com/AUTOMATIC1111/stable-diffusion-webui/blob/0cc0ee1bcb4c24a8c9715f66cede06601bfc00c8/modules/extra_networks.py#L69\r\n\r\nI found this variable `extra_network_registry`={'hypernet': <modules.extra_networks_hypernet.ExtraNetworkHypernet object at 0x000002181B293F70>}, lora is not registried after api launch.\r\nAnd lora was loaded as extension in this line:\r\nhttps://github.com/AUTOMATIC1111/stable-diffusion-webui/blob/0cc0ee1bcb4c24a8c9715f66cede06601bfc00c8/webui.py#L203\r\nI copyed `modules.script_callbacks.before_ui_callback()` into def api_only(), and works well\n> https://github.com/AUTOMATIC1111/stable-diffusion-webui/blob/0cc0ee1bcb4c24a8c9715f66cede06601bfc00c8/modules/extra_networks.py#L69\r\n> \r\n> I found this variable `extra_network_registry`={'hypernet': <modules.extra_networks_hypernet.ExtraNetworkHypernet object at 0x000002181B293F70>}, lora is not registried after api launch. And lora was loaded as extension in this line:\r\n> \r\n> https://github.com/AUTOMATIC1111/stable-diffusion-webui/blob/0cc0ee1bcb4c24a8c9715f66cede06601bfc00c8/webui.py#L203\r\n> \r\n> \r\n> I copyed `modules.script_callbacks.before_ui_callback()` into def api_only(), and works well\r\n\r\nYes, it's a bug with the \"API\"\n> https://github.com/AUTOMATIC1111/stable-diffusion-webui/blob/0cc0ee1bcb4c24a8c9715f66cede06601bfc00c8/modules/extra_networks.py#L69\r\n> \r\n> I found this variable `extra_network_registry`={'hypernet': <modules.extra_networks_hypernet.ExtraNetworkHypernet object at 0x000002181B293F70>}, lora is not registried after api launch. And lora was loaded as extension in this line:\r\n> \r\n> https://github.com/AUTOMATIC1111/stable-diffusion-webui/blob/0cc0ee1bcb4c24a8c9715f66cede06601bfc00c8/webui.py#L203\r\n> \r\n> I copyed `modules.script_callbacks.before_ui_callback()` into def api_only(), and works well\r\n\r\nafter adding this, it works, thank you very much\n> https://github.com/AUTOMATIC1111/stable-diffusion-webui/blob/0cc0ee1bcb4c24a8c9715f66cede06601bfc00c8/modules/extra_networks.py#L69\r\n> \r\n> I found this variable `extra_network_registry`={'hypernet': <modules.extra_networks_hypernet.ExtraNetworkHypernet object at 0x000002181B293F70>}, lora is not registried after api launch. And lora was loaded as extension in this line:\r\n> \r\n> https://github.com/AUTOMATIC1111/stable-diffusion-webui/blob/0cc0ee1bcb4c24a8c9715f66cede06601bfc00c8/webui.py#L203\r\n> \r\n> \r\n> I copyed `modules.script_callbacks.before_ui_callback()` into def api_only(), and works well\r\n\r\nworks for me, well done! Thanks!\n> https://github.com/AUTOMATIC1111/stable-diffusion-webui/blob/0cc0ee1bcb4c24a8c9715f66cede06601bfc00c8/modules/extra_networks.py#L69\r\n> \r\n> I found this variable `extra_network_registry`={'hypernet': <modules.extra_networks_hypernet.ExtraNetworkHypernet object at 0x000002181B293F70>}, lora is not registried after api launch. And lora was loaded as extension in this line:\r\n> \r\n> https://github.com/AUTOMATIC1111/stable-diffusion-webui/blob/0cc0ee1bcb4c24a8c9715f66cede06601bfc00c8/webui.py#L203\r\n> \r\n> \r\n> I copyed `modules.script_callbacks.before_ui_callback()` into def api_only(), and works well\r\n\r\nThank you bro. it works\n> > https://github.com/AUTOMATIC1111/stable-diffusion-webui/blob/0cc0ee1bcb4c24a8c9715f66cede06601bfc00c8/modules/extra_networks.py#L69\r\n> > \r\n> > I found this variable `extra_network_registry`={'hypernet': <modules.extra_networks_hypernet.ExtraNetworkHypernet object at 0x000002181B293F70>}, lora is not registried after api launch. And lora was loaded as extension in this line:\r\n> > https://github.com/AUTOMATIC1111/stable-diffusion-webui/blob/0cc0ee1bcb4c24a8c9715f66cede06601bfc00c8/webui.py#L203\r\n> > \r\n> > I copyed `modules.script_callbacks.before_ui_callback()` into def api_only(), and works well\r\n> \r\n> Thank you bro. it works\r\n\r\n@tenpha I changed it so that I won't go wrong, but it still doesn't work. What about you?\n> https://github.com/AUTOMATIC1111/stable-diffusion-webui/blob/0cc0ee1bcb4c24a8c9715f66cede06601bfc00c8/modules/extra_networks.py#L69\r\n> \r\n> I found this variable `extra_network_registry`={'hypernet': <modules.extra_networks_hypernet.ExtraNetworkHypernet object at 0x000002181B293F70>}, lora is not registried after api launch. And lora was loaded as extension in this line:\r\n> \r\n> https://github.com/AUTOMATIC1111/stable-diffusion-webui/blob/0cc0ee1bcb4c24a8c9715f66cede06601bfc00c8/webui.py#L203\r\n> \r\n> \r\n> I copyed `modules.script_callbacks.before_ui_callback()` into def api_only(), and works well\r\n\r\nIm a noob in coding, can you explain where is this  \"def api_only()\" \n@Noobmaster6978 \r\n![image](https://user-images.githubusercontent.com/24261680/233010531-df8c95eb-8e03-492e-913e-7a62dc85414d.png)\r\n\n> https://github.com/AUTOMATIC1111/stable-diffusion-webui/blob/0cc0ee1bcb4c24a8c9715f66cede06601bfc00c8/modules/extra_networks.py#L69\r\n> \r\n> \u6211\u53d1\u73b0\u8fd9\u4e2a\u53d8\u91cf`extra_network_registry`={'hypernet': <modules.extra_networks_hypernet.ExtraNetworkHypernet object at 0x000002181B293F70>}\uff0clora \u5728 api \u542f\u52a8\u540e\u6ca1\u6709\u6ce8\u518c\u3002 \u5e76\u4e14 lora \u5728\u8fd9\u4e00\u884c\u4e2d\u4f5c\u4e3a\u6269\u5c55\u52a0\u8f7d\uff1a\r\n> \r\n> https://github.com/AUTOMATIC1111/stable-diffusion-webui/blob/0cc0ee1bcb4c24a8c9715f66cede06601bfc00c8/webui.py#L203\r\n> \r\n> \r\n> \u6211\u590d\u5236`modules.script_callbacks.before_ui_callback()`\u5230 def api_only()\uff0c\u6548\u679c\u5f88\u597d\r\n\r\nit works\n> > https://github.com/AUTOMATIC1111/stable-diffusion-webui/blob/0cc0ee1bcb4c24a8c9715f66cede06601bfc00c8/modules/extra_networks.py#L69\r\n> > \r\n> > \u6211\u53d1\u73b0\u8fd9\u4e2a\u53d8\u91cf`extra_network_registry`={'hypernet': <modules.extra_networks_hypernet.ExtraNetworkHypernet object at 0x000002181B293F70>}\uff0clora \u5728 api \u542f\u52a8\u540e\u6ca1\u6709\u6ce8\u518c\u3002 \u5e76\u4e14 lora \u5728\u8fd9\u4e00\u884c\u4e2d\u4f5c\u4e3a\u6269\u5c55\u52a0\u8f7d\uff1a\r\n> > https://github.com/AUTOMATIC1111/stable-diffusion-webui/blob/0cc0ee1bcb4c24a8c9715f66cede06601bfc00c8/webui.py#L203\r\n> > \r\n> > \u6211\u590d\u5236`modules.script_callbacks.before_ui_callback()`\u5230 def api_only()\uff0c\u6548\u679c\u5f88\u597d\r\n> \r\n> it works\r\n\r\nThank you for your time. But I'm on Collab so can't edit directly, So I downloaded the \"webui.py\" and edited it locally then reuploaded it in its folder in place of the old file in google drive. But after running SD when I checked, the file was back to the original one and my issue didn't got resolved. It's still showing the error: \"Skipping unknown extra network: [loraname_version]\". What am I doing wrong?\r\n\nTried this solution to fix the bug for the past few days. Its not working for me\r\n\nTried this solution to fix the bug for the past few days. Its not working for me\r\n\n> https://github.com/AUTOMATIC1111/stable-diffusion-webui/blob/0cc0ee1bcb4c24a8c9715f66cede06601bfc00c8/modules/extra_networks.py#L69\r\n> \r\n> I found this variable `extra_network_registry`={'hypernet': <modules.extra_networks_hypernet.ExtraNetworkHypernet object at 0x000002181B293F70>}, lora is not registried after api launch. And lora was loaded as extension in this line:\r\n> \r\n> https://github.com/AUTOMATIC1111/stable-diffusion-webui/blob/0cc0ee1bcb4c24a8c9715f66cede06601bfc00c8/webui.py#L203\r\n> \r\n> \r\n> I copyed `modules.script_callbacks.before_ui_callback()` into def api_only(), and works well\r\n\r\nThank you. it works\n\r\n\r\n> https://github.com/AUTOMATIC1111/stable-diffusion-webui/blob/0cc0ee1bcb4c24a8c9715f66cede06601bfc00c8/modules/extra_networks.py#L69\r\n> \r\n> I found this variable `extra_network_registry`={'hypernet': <modules.extra_networks_hypernet.ExtraNetworkHypernet object at 0x000002181B293F70>}, lora is not registried after api launch. And lora was loaded as extension in this line:\r\n> \r\n> https://github.com/AUTOMATIC1111/stable-diffusion-webui/blob/0cc0ee1bcb4c24a8c9715f66cede06601bfc00c8/webui.py#L203\r\n> \r\n> \r\n> I copyed `modules.script_callbacks.before_ui_callback()` into def api_only(), and works well\r\n\r\nYes, this works very well! Thank you\n> Tried this solution to fix the bug for the past few days. Its not working for me\r\n\r\nEdit the webui.py file first, and then you need to restart the process\nIt doesn't work in the latest version\nupdated today our stack ( [5ab7f213]  - latest release 2nd of May 23) and this \"patch\" is still working fine.\r\n\r\nfor workflow / Docker we are using this little sed patch. \r\n`\r\nRUN sed -i -e '/    api = create_api/a\\' -e '    modules.script_callbacks.before_ui_callback()' webui.py\r\n`\nThe fix worked for me\nWhy is this still a bug and not fixed in the API >.<?", "created_at": "2023-08-07T10:15:50Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 12354, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-12354", "issue_numbers": ["8501"], "base_commit": "d86d12e9117772f041682124badc7baac7c57911", "patch": "diff --git a/modules/processing.py b/modules/processing.py\nindex aef8fafdde2..8743ac0aeb5 100644\n--- a/modules/processing.py\n+++ b/modules/processing.py\n@@ -110,7 +110,7 @@ class StableDiffusionProcessing:\n     cached_uc = [None, None]\r\n     cached_c = [None, None]\r\n \r\n-    def __init__(self, sd_model=None, outpath_samples=None, outpath_grids=None, prompt: str = \"\", styles: List[str] = None, seed: int = -1, subseed: int = -1, subseed_strength: float = 0, seed_resize_from_h: int = -1, seed_resize_from_w: int = -1, seed_enable_extras: bool = True, sampler_name: str = None, batch_size: int = 1, n_iter: int = 1, steps: int = 50, cfg_scale: float = 7.0, width: int = 512, height: int = 512, restore_faces: bool = False, tiling: bool = False, do_not_save_samples: bool = False, do_not_save_grid: bool = False, extra_generation_params: Dict[Any, Any] = None, overlay_images: Any = None, negative_prompt: str = None, eta: float = None, do_not_reload_embeddings: bool = False, denoising_strength: float = 0, ddim_discretize: str = None, s_min_uncond: float = 0.0, s_churn: float = 0.0, s_tmax: float = None, s_tmin: float = 0.0, s_noise: float = 1.0, override_settings: Dict[str, Any] = None, override_settings_restore_afterwards: bool = True, sampler_index: int = None, script_args: list = None):\r\n+    def __init__(self, sd_model=None, outpath_samples=None, outpath_grids=None, prompt: str = \"\", styles: List[str] = None, seed: int = -1, subseed: int = -1, subseed_strength: float = 0, seed_resize_from_h: int = -1, seed_resize_from_w: int = -1, seed_enable_extras: bool = True, sampler_name: str = None, batch_size: int = 1, n_iter: int = 1, steps: int = 50, cfg_scale: float = 7.0, width: int = 512, height: int = 512, restore_faces: bool = False, tiling: bool = False, do_not_save_samples: bool = False, do_not_save_grid: bool = False, extra_generation_params: Dict[Any, Any] = None, overlay_images: Any = None, negative_prompt: str = None, eta: float = None, do_not_reload_embeddings: bool = False, denoising_strength: float = 0, ddim_discretize: str = None, s_min_uncond: float = 0.0, s_churn: float = 0.0, s_tmax: float = None, s_tmin: float = 0.0, s_noise: float = None, override_settings: Dict[str, Any] = None, override_settings_restore_afterwards: bool = True, sampler_index: int = None, script_args: list = None):\r\n         if sampler_index is not None:\r\n             print(\"sampler_index argument for StableDiffusionProcessing does not do anything; use sampler_name\", file=sys.stderr)\r\n \r\n@@ -149,7 +149,7 @@ def __init__(self, sd_model=None, outpath_samples=None, outpath_grids=None, prom\n         self.s_churn = s_churn or opts.s_churn\r\n         self.s_tmin = s_tmin or opts.s_tmin\r\n         self.s_tmax = opts.data.get('s_tmax', s_tmax or 0) or float('inf')  # not representable as a standard ui option\r\n-        self.s_noise = s_noise or opts.s_noise\r\n+        self.s_noise = s_noise if s_noise is not None else opts.data.get('s_noise', 1.0)\r\n         self.override_settings = {k: v for k, v in (override_settings or {}).items() if k not in shared.restricted_opts}\r\n         self.override_settings_restore_afterwards = override_settings_restore_afterwards\r\n         self.is_using_inpainting_conditioning = False\r\n", "test_patch": "", "problem_statement": "[Bug]: Sigma noise setting is ignored\n### Is there an existing issue for this?\n\n- [X] I have searched the existing issues and checked the recent builds/commits\n\n### What happened?\n\nAttempting to use Sigma noise=0 with the Heun sampler, I notice that it is always set to 1, no matter what I set up.\r\n\r\nForcing it to be a different value with the X/Y/Z prompt matrix works, so I know what it _should_ look like, and what it is set to instead.\n\n### Steps to reproduce the problem\n\n1. Set sampler to Heun, sigma churn in \"Sampler parameters\" to 1\r\n2. Render arbitrary prompt with sigma noise=0 and sigma noise=1\n\n### What should have happened?\n\nN/A\n\n### Commit where the problem happens\n\n5cea278d3ad3a1c1a9b454387241a29bec11699b\n\n### What platforms do you use to access the UI ?\n\n_No response_\n\n### What browsers do you use to access the UI ?\n\n_No response_\n\n### Command Line Arguments\n\n```Shell\nN/A\n```\n\n\n### List of extensions\n\nembedding-inspector\r\nsd-dynamic-prompts\r\nsd-dynamic-thresholding\r\n\n\n### Console logs\n\n```Shell\nN/A\n```\n\n\n### Additional information\n\n_No response_\n", "hints_text": "also, the sigma churn slider doesn't use the full possible range (0..100)", "created_at": "2023-08-06T01:19:20Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 12352, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-12352", "issue_numbers": ["12351"], "base_commit": "c6278c15a81bf65efb65ded50368972a920cc198", "patch": "diff --git a/README.md b/README.md\nindex b796d150041..cd11f8569ef 100644\n--- a/README.md\n+++ b/README.md\n@@ -115,7 +115,7 @@ Alternatively, use online services (like Google Colab):\n 1. Install the dependencies:\r\n ```bash\r\n # Debian-based:\r\n-sudo apt install wget git python3 python3-venv\r\n+sudo apt install wget git python3 python3-venv libgl1 libglib2.0-0\r\n # Red Hat-based:\r\n sudo dnf install wget git python3\r\n # Arch-based:\r\n@@ -123,7 +123,7 @@ sudo pacman -S wget git python3\n ```\r\n 2. Navigate to the directory you would like the webui to be installed and execute the following command:\r\n ```bash\r\n-bash <(wget -qO- https://raw.githubusercontent.com/AUTOMATIC1111/stable-diffusion-webui/master/webui.sh)\r\n+wget -q https://raw.githubusercontent.com/AUTOMATIC1111/stable-diffusion-webui/master/webui.sh\r\n ```\r\n 3. Run `webui.sh`.\r\n 4. Check `webui-user.sh` for options.\r\n", "test_patch": "", "problem_statement": "[Bug]: Linux install instructions issue\n### Is there an existing issue for this?\n\n- [X] I have searched the existing issues and checked the recent builds/commits\n\n### What happened?\n\nAttempting to install on ubuntu, i get the error:\r\n\r\n################################################################\r\nClone stable-diffusion-webui\r\n################################################################\r\nfatal: could not create work tree dir 'stable-diffusion-webui': No such file or directory\r\n/dev/fd/63: line 191: cd: stable-diffusion-webui/: No such file or directory\n\n### Steps to reproduce the problem\n\n1. Spin up a nvidia docker container: nvidia/cuda:12.2.0-devel-ubuntu22.04\r\n2. Follow the install instructions: bash <(wget -qO- https://raw.githubusercontent.com/AUTOMATIC1111/stable-diffusion-webui/master/webui.sh)\r\n3. Error\r\n\n\n### What should have happened?\n\nPresumably it should have installed. It appears there is an assumption in the installer of where the install directory will be. I'm guessing this is a difference of behavior when piping in the shell script, leading to the script attempting to clone into the fd dir, which is obviously incorrect.\r\n\r\nFurther, it will start correctly when I download it. And your instructions are inconsistent at that point:\r\n\r\n```\r\nNavigate to the directory you would like the webui to be installed and execute the following command:\r\nbash <(wget -qO- https://raw.githubusercontent.com/AUTOMATIC1111/stable-diffusion-webui/master/webui.sh)\r\nRun webui.sh.\r\n```\r\n\r\nThe bash line you give already runs the webui.sh, it doesn't just download it. What is likely the more correct way of doing this would be to change that bash line to simply wget download the script, then run it separately. I.e.:\r\n\r\n```\r\nNavigate to the directory you would like the webui to be installed and execute the following command:\r\nwget -q https://raw.githubusercontent.com/AUTOMATIC1111/stable-diffusion-webui/master/webui.sh\r\nRun webui.sh.\r\n```\r\n\n\n### Version or Commit where the problem happens\n\nMaster\n\n### What Python version are you running on ?\n\nNone\n\n### What platforms do you use to access the UI ?\n\n_No response_\n\n### What device are you running WebUI on?\n\n_No response_\n\n### Cross attention optimization\n\nAutomatic\n\n### What browsers do you use to access the UI ?\n\n_No response_\n\n### Command Line Arguments\n\n```Shell\nNo\n```\n\n\n### List of extensions\n\nNA\n\n### Console logs\n\n```Shell\nNA\n```\n\n\n### Additional information\n\nNA\n", "hints_text": "", "created_at": "2023-08-05T18:08:57Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 12327, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-12327", "issue_numbers": ["4685"], "base_commit": "45601766409e531d2b4ee512bf1433600f140183", "patch": "diff --git a/modules/images.py b/modules/images.py\nindex 38aa933d6e5..ba3c43a4509 100644\n--- a/modules/images.py\n+++ b/modules/images.py\n@@ -318,7 +318,7 @@ def resize(im, w, h):\n     return res\r\n \r\n \r\n-invalid_filename_chars = '<>:\"/\\\\|?*\\n'\r\n+invalid_filename_chars = '<>:\"/\\\\|?*\\n\\r\\t'\r\n invalid_filename_prefix = ' '\r\n invalid_filename_postfix = ' .'\r\n re_nonletters = re.compile(r'[\\s' + string.punctuation + ']+')\r\n", "test_patch": "", "problem_statement": "[Bug]: incorrect file path when prompt contains new line\n### Is there an existing issue for this?\n\n- [X] I have searched the existing issues and checked the recent builds/commits\n\n### What happened?\n\nprompt:\r\n```\r\nportrait of a woman, art by\r\nHenry Asencio\t\r\nAnton Fadeev\t\r\nArtgerm\r\nBastien Lecouffe-Deharme\t\r\nCarne Griffiths\t\r\nDarek Zabrocki\t\r\nPatrice Murciano\t\r\nRuss Mills\t\r\nWLOP\r\nYoji Shinkawa\t\r\n\r\n```\r\nerror:\r\n```\r\nOSError: [Errno 22] Invalid argument: 'outputs/txt2img-images\\\\02357-757535083-portrait of a woman, art by_Henry Asencio\\t_Anton Fadeev\\t_Artgerm_Bastien Lecouffe-Deharme\\t_Carne Griffiths\\t_Darek Zabrocki\\t_Patr.png'\r\n```\n\n### Steps to reproduce the problem\n\nuse prompt with new line symbol\n\n### What should have happened?\n\nThe script should convert that to safe saving reading name\n\n### Commit where the problem happens\n\n98947d173e3f1667eba29c904f681047dea9de90\n\n### What platforms do you use to access UI ?\n\nWindows\n\n### What browsers do you use to access the UI ?\n\nGoogle Chrome\n\n### Command Line Arguments\n\n_No response_\n\n### Additional information, context and logs\n\n_No response_\n", "hints_text": "Could not reproduce. More information would be appreciated.\nThank you for looking into it.\r\nWith new insight: it is not new line, it is '\\t' tab symbol.\r\nBasically possible only when pasting.", "created_at": "2023-08-05T03:00:30Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 12319, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-12319", "issue_numbers": ["8161"], "base_commit": "c57cb6e89c51db4808b84f76f97afc9ff0f35e2a", "patch": "diff --git a/modules/prompt_parser.py b/modules/prompt_parser.py\nindex 8169a45969a..32d214e3a1a 100644\n--- a/modules/prompt_parser.py\n+++ b/modules/prompt_parser.py\n@@ -20,7 +20,7 @@\n         | \"(\" prompt \":\" prompt \")\"\r\n         | \"[\" prompt \"]\"\r\n scheduled: \"[\" [prompt \":\"] prompt \":\" [WHITESPACE] NUMBER [WHITESPACE] \"]\"\r\n-alternate: \"[\" prompt (\"|\" prompt)+ \"]\"\r\n+alternate: \"[\" prompt (\"|\" [prompt])+ \"]\"\r\n WHITESPACE: /\\s+/\r\n plain: /([^\\\\\\[\\]():|]|\\\\.)+/\r\n %import common.SIGNED_NUMBER -> NUMBER\r\n@@ -53,6 +53,10 @@ def get_learned_conditioning_prompt_schedules(prompts, steps):\n     [[3, '((a][:b:c '], [10, '((a][:b:c d']]\r\n     >>> g(\"[a|(b:1.1)]\")\r\n     [[1, 'a'], [2, '(b:1.1)'], [3, 'a'], [4, '(b:1.1)'], [5, 'a'], [6, '(b:1.1)'], [7, 'a'], [8, '(b:1.1)'], [9, 'a'], [10, '(b:1.1)']]\r\n+    >>> g(\"[fe|]male\")\r\n+    [[1, 'female'], [2, 'male'], [3, 'female'], [4, 'male'], [5, 'female'], [6, 'male'], [7, 'female'], [8, 'male'], [9, 'female'], [10, 'male']]\r\n+    >>> g(\"[fe|||]male\")\r\n+    [[1, 'female'], [2, 'male'], [3, 'male'], [4, 'male'], [5, 'female'], [6, 'male'], [7, 'male'], [8, 'male'], [9, 'female'], [10, 'male']]\r\n     \"\"\"\r\n \r\n     def collect_steps(steps, tree):\r\n@@ -78,7 +82,8 @@ def scheduled(self, args):\n                 before, after, _, when, _ = args\r\n                 yield before or () if step <= when else after\r\n             def alternate(self, args):\r\n-                yield next(args[(step - 1)%len(args)])\r\n+                args = [\"\" if not arg else arg for arg in args]\r\n+                yield args[(step - 1) % len(args)]\r\n             def start(self, args):\r\n                 def flatten(x):\r\n                     if type(x) == str:\r\n", "test_patch": "", "problem_statement": "[Bug]: alternating words with an empty option throws an exception\n### Is there an existing issue for this?\n\n- [X] I have searched the existing issues and checked the recent builds/commits\n\n### What happened?\n\nUsing an empty option in an alternation, like \"[red|] apple\", throws an exception. Easy workaround is to put a space in the empty option, like \"[red| ] apple\", and then it doesn't error out.\n\n### Steps to reproduce the problem\n\n1. type \"[red|] apple\" as the prompt\r\n2. Press generate\r\n\n\n### What should have happened?\n\ndoes not throw an exception\n\n### Commit where the problem happens\n\nd5ce044b\n\n### What platforms do you use to access the UI ?\n\nWindows\n\n### What browsers do you use to access the UI ?\n\nMozilla Firefox\n\n### Command Line Arguments\n\n```Shell\nxformers\n```\n\n\n### List of extensions\n\nwildcards, controlnet\n\n### Console logs\n\n```Shell\nError completing request\r\nArguments: ('task(wz2r40k8hiugpjz)', '[|red] thing', '\\n', [], 70, 0, True, False, 1, 1, 7, -1.0, -1.0, 0, 0, 0, False, 400, 700, True, 0.15, 2, '4x_foolhardy_Remacri', 20, 0, 0, 0, False, 'canny', 'control_sd15_canny(fef5e48e)', 0.1, None, False, 'Scale to Fit (Inner Fit)', False, False, False, False, False, False, '', '', '', 1, '', 0, '', 0, '', True, False, False, False) {}\r\nTraceback (most recent call last):\r\n  File \"D:\\ai\\sdweb\\modules\\prompt_parser.py\", line 76, in alternate\r\n    yield next(args[(step - 1)%len(args)])\r\nStopIteration\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"D:\\ai\\sdweb\\venv\\lib\\site-packages\\lark\\visitors.py\", line 116, in _call_userfunc\r\n    return f(children)\r\n  File \"D:\\ai\\sdweb\\modules\\prompt_parser.py\", line 84, in start\r\n    return ''.join(flatten(args))\r\n  File \"D:\\ai\\sdweb\\modules\\prompt_parser.py\", line 83, in flatten\r\n    yield from flatten(gen)\r\n  File \"D:\\ai\\sdweb\\modules\\prompt_parser.py\", line 83, in flatten\r\n    yield from flatten(gen)\r\n  File \"D:\\ai\\sdweb\\modules\\prompt_parser.py\", line 82, in flatten\r\n    for gen in x:\r\nRuntimeError: generator raised StopIteration\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"D:\\ai\\sdweb\\modules\\call_queue.py\", line 56, in f\r\n    res = list(func(*args, **kwargs))\r\n  File \"D:\\ai\\sdweb\\modules\\call_queue.py\", line 37, in f\r\n    res = func(*args, **kwargs)\r\n  File \"D:\\ai\\sdweb\\modules\\txt2img.py\", line 52, in txt2img\r\n    processed = process_images(p)\r\n  File \"D:\\ai\\sdweb\\modules\\processing.py\", line 476, in process_images\r\n    res = process_images_inner(p)\r\n  File \"D:\\ai\\sdweb\\modules\\processing.py\", line 604, in process_images_inner\r\n    c = get_conds_with_caching(prompt_parser.get_multicond_learned_conditioning, prompts, p.steps, cached_c)\r\n  File \"D:\\ai\\sdweb\\modules\\processing.py\", line 562, in get_conds_with_caching\r\n    cache[1] = function(shared.sd_model, required_prompts, steps)\r\n  File \"D:\\ai\\sdweb\\modules\\prompt_parser.py\", line 205, in get_multicond_learned_conditioning\r\n    learned_conditioning = get_learned_conditioning(model, prompt_flat_list, steps)\r\n  File \"D:\\ai\\sdweb\\scripts\\prompt_blending.py\", line 40, in hijacked_get_learned_conditioning\r\n    return real_get_learned_conditioning(model, switched_prompts, steps)\r\n  File \"D:\\ai\\sdweb\\modules\\prompt_parser.py\", line 129, in get_learned_conditioning\r\n    prompt_schedules = get_learned_conditioning_prompt_schedules(prompts, steps)\r\n  File \"D:\\ai\\sdweb\\modules\\prompt_parser.py\", line 102, in get_learned_conditioning_prompt_schedules\r\n    promptdict = {prompt: get_schedule(prompt) for prompt in set(prompts)}\r\n  File \"D:\\ai\\sdweb\\modules\\prompt_parser.py\", line 102, in <dictcomp>\r\n    promptdict = {prompt: get_schedule(prompt) for prompt in set(prompts)}\r\n  File \"D:\\ai\\sdweb\\modules\\prompt_parser.py\", line 100, in get_schedule\r\n    return [[t, at_step(t, tree)] for t in collect_steps(steps, tree)]\r\n  File \"D:\\ai\\sdweb\\modules\\prompt_parser.py\", line 100, in <listcomp>\r\n    return [[t, at_step(t, tree)] for t in collect_steps(steps, tree)]\r\n  File \"D:\\ai\\sdweb\\modules\\prompt_parser.py\", line 90, in at_step\r\n    return AtStep().transform(tree)\r\n  File \"D:\\ai\\sdweb\\venv\\lib\\site-packages\\lark\\visitors.py\", line 153, in transform\r\n    return self._transform_tree(tree)\r\n  File \"D:\\ai\\sdweb\\venv\\lib\\site-packages\\lark\\visitors.py\", line 149, in _transform_tree\r\n    return self._call_userfunc(tree, children)\r\n  File \"D:\\ai\\sdweb\\venv\\lib\\site-packages\\lark\\visitors.py\", line 120, in _call_userfunc\r\n    raise VisitError(tree.data, tree, e)\r\nlark.exceptions.VisitError: Error trying to process rule \"start\":\r\n\r\ngenerator raised StopIteration\n```\n\n\n### Additional information\n\n_No response_\n", "hints_text": "can you double-check your prompt? I had the same issue, and then noticed that there was an extra divider |  after the last altnernating word. As soon as I removed it, the error stopped.\nThe point is that the | at the end is there intentionally, it's trying to choose the empty string intentionally. It is a bug that you can't do that.\nAs a trivial example of why you might want this, consider `[fe|]male`. Yes, obviously you can write `[female|male]`, but it gets at the reason for wanting it.\r\n\r\nI looked into the code, but since the grammar uses a CFG with an LALR parser it looks pretty complicated to support; you'd have to add an empty production and then adjust rules to include that as an option, and I'm not familiar enough with Lark to make those sorts of changes myself. (It would be trivial to handle in some alternative parsing engines, but it doesn't make sense to switch parsers just for this.)", "created_at": "2023-08-04T17:33:52Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 12306, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-12306", "issue_numbers": ["6419"], "base_commit": "f0c1063a707a4a43823b0ed00e2a8eeb22a9ed0a", "patch": "diff --git a/modules/ui.py b/modules/ui.py\nindex 822a7660257..8b38d2136cb 100644\n--- a/modules/ui.py\n+++ b/modules/ui.py\n@@ -591,8 +591,8 @@ def create_ui():\n                 (seed_resize_from_h, \"Seed resize from-2\"),\r\n                 (toprow.ui_styles.dropdown, lambda d: d[\"Styles array\"] if isinstance(d.get(\"Styles array\"), list) else gr.update()),\r\n                 (denoising_strength, \"Denoising strength\"),\r\n-                (enable_hr, lambda d: \"Denoising strength\" in d),\r\n-                (hr_options, lambda d: gr.Row.update(visible=\"Denoising strength\" in d)),\r\n+                (enable_hr, lambda d: \"Denoising strength\" in d and (\"Hires upscale\" in d or \"Hires upscaler\" in d or \"Hires resize-1\" in d)),\r\n+                (hr_options, lambda d: gr.Row.update(visible=\"Denoising strength\" in d and (\"Hires upscale\" in d or \"Hires upscaler\" in d or \"Hires resize-1\" in d))),\r\n                 (hr_scale, \"Hires upscale\"),\r\n                 (hr_upscaler, \"Hires upscaler\"),\r\n                 (hr_second_pass_steps, \"Hires steps\"),\r\n", "test_patch": "", "problem_statement": "[Bug]: img2img -> txt2img (using pnginfo): conflict denoising with Hires.fix denoising\n### Is there an existing issue for this?\r\n\r\n- [X] I have searched the existing issues and checked the recent builds/commits\r\n\r\n### What happened?\r\n\r\nOS: Windows 11\r\nBrowser: Firefox Nightly\r\n\r\nusing pnginfo to send to txt2img activates hires.fix denoising from metadata saved by img2img (that it has denoising).\r\n\r\n### Steps to reproduce the problem\r\n\r\n1. Go to .... \r\n2. Press ....\r\n3. ...\r\n\r\n\r\n### What should have happened?\r\n\r\nhires.fix must be turned off when sending from pnginfo to txt2img (if it was saved by img2img)\r\n\r\n### Commit where the problem happens\r\n\r\ndunno\r\n\r\n### What platforms do you use to access UI ?\r\n\r\nWindows\r\n\r\n### What browsers do you use to access the UI ?\r\n\r\nMozilla Firefox\r\n\r\n### Command Line Arguments\r\n\r\n_No response_\r\n\r\n### Additional information, context and logs\r\n\r\n_No response_\n", "hints_text": "", "created_at": "2023-08-04T08:05:05Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 12304, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-12304", "issue_numbers": ["2097"], "base_commit": "f0c1063a707a4a43823b0ed00e2a8eeb22a9ed0a", "patch": "diff --git a/modules/ui.py b/modules/ui.py\nindex 822a7660257..2f3f74b5b30 100644\n--- a/modules/ui.py\n+++ b/modules/ui.py\n@@ -585,6 +585,7 @@ def create_ui():\n                 (width, \"Size-1\"),\r\n                 (height, \"Size-2\"),\r\n                 (batch_size, \"Batch size\"),\r\n+                (seed_checkbox, lambda d: \"Variation seed\" in d or \"Seed resize from-1\" in d),\r\n                 (subseed, \"Variation seed\"),\r\n                 (subseed_strength, \"Variation seed strength\"),\r\n                 (seed_resize_from_w, \"Seed resize from-1\"),\r\n@@ -972,6 +973,7 @@ def select_img2img_tab(tab):\n                 (width, \"Size-1\"),\r\n                 (height, \"Size-2\"),\r\n                 (batch_size, \"Batch size\"),\r\n+                (seed_checkbox, lambda d: \"Variation seed\" in d or \"Seed resize from-1\" in d),\r\n                 (subseed, \"Variation seed\"),\r\n                 (subseed_strength, \"Variation seed strength\"),\r\n                 (seed_resize_from_w, \"Seed resize from-1\"),\r\n", "test_patch": "", "problem_statement": "\"Send to\" buttons don't set \"Extras\" checkbox when using a variation seed.\nIf a PNG created using a variation seed is loaded into the \"PNG Info\" tab, the \"Variation seed strength\" and \"Denoising strength\" options are displayed. When using the \"Send to txt2img\" or \"Send to img2txt\" options, the seed and denoising are set, but the \"Extras\" checkbox is not set so the variation seed settings aren't applied.\r\n\r\nThe same issue occurs if an image with a variation seed is created on the txt2img tab and the \"Send to img2txt\" option is used.\r\n\r\nCommit hash: 45bf9a6264b3507473e02cc3f9aa36559f24aca2\r\n\n", "hints_text": "Can confirm this also occurs when using anything else from the Extras tab. (scale seed in my case)\n....and can confirm it also occurs for clip aestethics\nBumping this as it's actually still an issue.", "created_at": "2023-08-04T07:53:16Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 12207, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-12207", "issue_numbers": ["12206"], "base_commit": "a1eb49627af334f4f29d24f2c12c744b88b2d227", "patch": "diff --git a/javascript/ui.js b/javascript/ui.js\nindex d70a681bff7..abf23a78c70 100644\n--- a/javascript/ui.js\n+++ b/javascript/ui.js\n@@ -152,7 +152,11 @@ function submit() {\n     showSubmitButtons('txt2img', false);\n \n     var id = randomId();\n-    localStorage.setItem(\"txt2img_task_id\", id);\n+    try {\n+        localStorage.setItem(\"txt2img_task_id\", id);\n+    } catch (e) {\n+        console.warn(`Failed to save txt2img task id to localStorage: ${e}`);\n+    }\n \n     requestProgress(id, gradioApp().getElementById('txt2img_gallery_container'), gradioApp().getElementById('txt2img_gallery'), function() {\n         showSubmitButtons('txt2img', true);\n@@ -171,7 +175,11 @@ function submit_img2img() {\n     showSubmitButtons('img2img', false);\n \n     var id = randomId();\n-    localStorage.setItem(\"img2img_task_id\", id);\n+    try {\n+        localStorage.setItem(\"img2img_task_id\", id);\n+    } catch (e) {\n+        console.warn(`Failed to save img2img task id to localStorage: ${e}`);\n+    }\n \n     requestProgress(id, gradioApp().getElementById('img2img_gallery_container'), gradioApp().getElementById('img2img_gallery'), function() {\n         showSubmitButtons('img2img', true);\n@@ -191,8 +199,6 @@ function restoreProgressTxt2img() {\n     showRestoreProgressButton(\"txt2img\", false);\n     var id = localStorage.getItem(\"txt2img_task_id\");\n \n-    id = localStorage.getItem(\"txt2img_task_id\");\n-\n     if (id) {\n         requestProgress(id, gradioApp().getElementById('txt2img_gallery_container'), gradioApp().getElementById('txt2img_gallery'), function() {\n             showSubmitButtons('txt2img', true);\n", "test_patch": "", "problem_statement": "Generating via UI fails if site storage quota is over limits\n> However, Chrome still generates errors that I could not see in Terminal, but can readily view in console, but I still cannot figure out what's wrong:\r\n![Chrome-Auto1111-errors](https://github.com/AUTOMATIC1111/stable-diffusion-webui/assets/113193580/677397ef-3f47-49ca-9e93-60e84354547b)\r\n\r\n_Originally posted by UlyssesHeart in https://github.com/AUTOMATIC1111/stable-diffusion-webui/issues/12011#issuecomment-1656492093_\r\n            \n", "hints_text": "", "created_at": "2023-07-31T09:24:26Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 11854, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-11854", "issue_numbers": ["11805", "11805"], "base_commit": "871b8687a82bb2ca907d8a49c87aed7635b8fc33", "patch": "diff --git a/modules/shared.py b/modules/shared.py\nindex 6162938a974..a256d090771 100644\n--- a/modules/shared.py\n+++ b/modules/shared.py\n@@ -409,7 +409,7 @@ def list_samplers():\n }))\r\n \r\n options_templates.update(options_section(('sd', \"Stable Diffusion\"), {\r\n-    \"sd_model_checkpoint\": OptionInfo(None, \"Stable Diffusion checkpoint\", gr.Dropdown, lambda: {\"choices\": list_checkpoint_tiles()}, refresh=refresh_checkpoints),\r\n+    \"sd_model_checkpoint\": OptionInfo(\"\", \"Stable Diffusion checkpoint\", gr.Dropdown, lambda: {\"choices\": list_checkpoint_tiles()}, refresh=refresh_checkpoints),\r\n     \"sd_checkpoint_cache\": OptionInfo(0, \"Checkpoints to cache in RAM\", gr.Slider, {\"minimum\": 0, \"maximum\": 10, \"step\": 1}),\r\n     \"sd_vae_checkpoint_cache\": OptionInfo(0, \"VAE Checkpoints to cache in RAM\", gr.Slider, {\"minimum\": 0, \"maximum\": 10, \"step\": 1}),\r\n     \"sd_vae\": OptionInfo(\"Automatic\", \"SD VAE\", gr.Dropdown, lambda: {\"choices\": shared_items.sd_vae_items()}, refresh=shared_items.refresh_vae_list).info(\"choose VAE model: Automatic = use one with same filename as checkpoint; None = use VAE from checkpoint\"),\r\n", "test_patch": "", "problem_statement": "[Bug]: GET /sdapi/v1/options returns 500\n### Is there an existing issue for this?\r\n\r\n- [X] I have searched the existing issues and checked the recent builds/commits\r\n\r\n### What happened?\r\n\r\nWhen I started the webui with --nowebui and send a get request to `/sdapi/v1/options`, the response is 500. Error message is below\r\n```shell\r\n    pydantic.error_wrappers.ValidationError: 1 validation error for Options\r\n    response -> sd_model_checkpoint\r\n      value is not None (type=type_error.not_none)\r\n```\r\n\r\n### Steps to reproduce the problem\r\n\r\n1. git clone the repo \r\n2. Run `python3.10 -m venv venv`\r\n3. Run `source venv/bin/activate`\r\n4. Run `COMMANDLINE_ARGS=\"--listen --xformers --skip-torch-cuda-test --nowebui\" ./webui.sh`\r\n5. Run `curl -X 'GET' 'http://localhost:7861/sdapi/v1/options' -H 'accept: application/json'`\r\n\r\n### What should have happened?\r\n\r\nReturn 200 with a json file.\r\n\r\n### Version or Commit where the problem happens\r\n\r\n1.4.1\r\n\r\n### What Python version are you running on ?\r\n\r\nPython 3.10.x\r\n\r\n### What platforms do you use to access the UI ?\r\n\r\nLinux\r\n\r\n### What device are you running WebUI on?\r\n\r\nNvidia GPUs (RTX 20 above)\r\n\r\n### Cross attention optimization\r\n\r\nxformers\r\n\r\n### What browsers do you use to access the UI ?\r\n\r\n_No response_\r\n\r\n### Command Line Arguments\r\n\r\n```Shell\r\n--listen --xformers --skip-torch-cuda-test --nowebui\r\n```\r\n\r\n\r\n### List of extensions\r\n\r\nNone\r\n\r\n### Console logs\r\n\r\n```Shell\r\n*** API error: GET: http://127.0.0.1:7861/sdapi/v1/options {'error': 'ValidationError', 'detail': '', 'body': '', 'errors': '1 validation error for Options\\nresponse -> sd_model_checkpoint\\n  value is not None (type=type_error.not_none)'}\r\n    Traceback (most recent call last):\r\n      File \"/xxx/stable-diffusion-webui/venv/lib/python3.10/site-packages/anyio/streams/memory.py\", line 98, in receive\r\n        return self.receive_nowait()\r\n      File \"/xxx/stable-diffusion-webui/venv/lib/python3.10/site-packages/anyio/streams/memory.py\", line 93, in receive_nowait\r\n        raise WouldBlock\r\n    anyio.WouldBlock\r\n\r\n    During handling of the above exception, another exception occurred:\r\n\r\n    Traceback (most recent call last):\r\n      File \"/xxx/stable-diffusion-webui/venv/lib/python3.10/site-packages/starlette/middleware/base.py\", line 78, in call_next\r\n        message = await recv_stream.receive()\r\n      File \"/xxx/stable-diffusion-webui/venv/lib/python3.10/site-packages/anyio/streams/memory.py\", line 118, in receive\r\n        raise EndOfStream\r\n    anyio.EndOfStream\r\n\r\n    During handling of the above exception, another exception occurred:\r\n\r\n    Traceback (most recent call last):\r\n      File \"/xxx/stable-diffusion-webui/modules/api/api.py\", line 153, in exception_handling\r\n        return await call_next(request)\r\n      File \"/xxx/stable-diffusion-webui/venv/lib/python3.10/site-packages/starlette/middleware/base.py\", line 84, in call_next\r\n        raise app_exc\r\n      File \"/xxx/stable-diffusion-webui/venv/lib/python3.10/site-packages/starlette/middleware/base.py\", line 70, in coro\r\n        await self.app(scope, receive_or_disconnect, send_no_error)\r\n      File \"/xxx/stable-diffusion-webui/venv/lib/python3.10/site-packages/starlette/middleware/base.py\", line 108, in __call__\r\n        response = await self.dispatch_func(request, call_next)\r\n      File \"/xxx/stable-diffusion-webui/modules/api/api.py\", line 117, in log_and_time\r\n        res: Response = await call_next(req)\r\n      File \"/xxx/stable-diffusion-webui/venv/lib/python3.10/site-packages/starlette/middleware/base.py\", line 84, in call_next\r\n        raise app_exc\r\n      File \"/xxx/stable-diffusion-webui/venv/lib/python3.10/site-packages/starlette/middleware/base.py\", line 70, in coro\r\n        await self.app(scope, receive_or_disconnect, send_no_error)\r\n      File \"/xxx/stable-diffusion-webui/venv/lib/python3.10/site-packages/starlette/middleware/cors.py\", line 84, in __call__\r\n        await self.app(scope, receive, send)\r\n      File \"/xxx/stable-diffusion-webui/venv/lib/python3.10/site-packages/starlette/middleware/gzip.py\", line 24, in __call__\r\n        await responder(scope, receive, send)\r\n      File \"/xxx/stable-diffusion-webui/venv/lib/python3.10/site-packages/starlette/middleware/gzip.py\", line 44, in __call__\r\n        await self.app(scope, receive, self.send_with_gzip)\r\n      File \"/xxx/stable-diffusion-webui/venv/lib/python3.10/site-packages/starlette/middleware/exceptions.py\", line 79, in __call__\r\n        raise exc\r\n      File \"/xxx/stable-diffusion-webui/venv/lib/python3.10/site-packages/starlette/middleware/exceptions.py\", line 68, in __call__\r\n        await self.app(scope, receive, sender)\r\n      File \"/xxx/stable-diffusion-webui/venv/lib/python3.10/site-packages/fastapi/middleware/asyncexitstack.py\", line 21, in __call__\r\n        raise e\r\n      File \"/xxx/stable-diffusion-webui/venv/lib/python3.10/site-packages/fastapi/middleware/asyncexitstack.py\", line 18, in __call__\r\n        await self.app(scope, receive, send)\r\n      File \"/xxx/stable-diffusion-webui/venv/lib/python3.10/site-packages/starlette/routing.py\", line 718, in __call__\r\n        await route.handle(scope, receive, send)\r\n      File \"/xxx/stable-diffusion-webui/venv/lib/python3.10/site-packages/starlette/routing.py\", line 276, in handle\r\n        await self.app(scope, receive, send)\r\n      File \"/xxx/stable-diffusion-webui/venv/lib/python3.10/site-packages/starlette/routing.py\", line 66, in app\r\n        response = await func(request)\r\n      File \"/xxx/stable-diffusion-webui/venv/lib/python3.10/site-packages/fastapi/routing.py\", line 255, in app\r\n        content = await serialize_response(\r\n      File \"/xxx/stable-diffusion-webui/venv/lib/python3.10/site-packages/fastapi/routing.py\", line 141, in serialize_response\r\n        raise ValidationError(errors, field.type_)\r\n    pydantic.error_wrappers.ValidationError: 1 validation error for Options\r\n    response -> sd_model_checkpoint\r\n      value is not None (type=type_error.not_none)\r\n```\r\n\r\n\r\n### Additional information\r\n\r\n* NVIDIA driver 535.54.03\r\n* CUDA Version: V12.2.91\r\n\r\nI didn't edit any file in the local repo. I printed out `OptionsModel` and found `'sd_model_checkpoint': <class 'NoneType'>` in it. The value should be `string`.\r\n\r\nThe problem was cause by `\"sd_model_checkpoint\": OptionInfo(None, \"Stable Diffusion checkpoint\", gr.Dropdown, lambda: {\"choices\": list_checkpoint_tiles()}, refresh=refresh_checkpoints)`, and was fixed after I changed the `None` to `\"\"`. The `/sdapi/v1/options` started returning the correct json.\r\n\n[Bug]: GET /sdapi/v1/options returns 500\n### Is there an existing issue for this?\r\n\r\n- [X] I have searched the existing issues and checked the recent builds/commits\r\n\r\n### What happened?\r\n\r\nWhen I started the webui with --nowebui and send a get request to `/sdapi/v1/options`, the response is 500. Error message is below\r\n```shell\r\n    pydantic.error_wrappers.ValidationError: 1 validation error for Options\r\n    response -> sd_model_checkpoint\r\n      value is not None (type=type_error.not_none)\r\n```\r\n\r\n### Steps to reproduce the problem\r\n\r\n1. git clone the repo \r\n2. Run `python3.10 -m venv venv`\r\n3. Run `source venv/bin/activate`\r\n4. Run `COMMANDLINE_ARGS=\"--listen --xformers --skip-torch-cuda-test --nowebui\" ./webui.sh`\r\n5. Run `curl -X 'GET' 'http://localhost:7861/sdapi/v1/options' -H 'accept: application/json'`\r\n\r\n### What should have happened?\r\n\r\nReturn 200 with a json file.\r\n\r\n### Version or Commit where the problem happens\r\n\r\n1.4.1\r\n\r\n### What Python version are you running on ?\r\n\r\nPython 3.10.x\r\n\r\n### What platforms do you use to access the UI ?\r\n\r\nLinux\r\n\r\n### What device are you running WebUI on?\r\n\r\nNvidia GPUs (RTX 20 above)\r\n\r\n### Cross attention optimization\r\n\r\nxformers\r\n\r\n### What browsers do you use to access the UI ?\r\n\r\n_No response_\r\n\r\n### Command Line Arguments\r\n\r\n```Shell\r\n--listen --xformers --skip-torch-cuda-test --nowebui\r\n```\r\n\r\n\r\n### List of extensions\r\n\r\nNone\r\n\r\n### Console logs\r\n\r\n```Shell\r\n*** API error: GET: http://127.0.0.1:7861/sdapi/v1/options {'error': 'ValidationError', 'detail': '', 'body': '', 'errors': '1 validation error for Options\\nresponse -> sd_model_checkpoint\\n  value is not None (type=type_error.not_none)'}\r\n    Traceback (most recent call last):\r\n      File \"/xxx/stable-diffusion-webui/venv/lib/python3.10/site-packages/anyio/streams/memory.py\", line 98, in receive\r\n        return self.receive_nowait()\r\n      File \"/xxx/stable-diffusion-webui/venv/lib/python3.10/site-packages/anyio/streams/memory.py\", line 93, in receive_nowait\r\n        raise WouldBlock\r\n    anyio.WouldBlock\r\n\r\n    During handling of the above exception, another exception occurred:\r\n\r\n    Traceback (most recent call last):\r\n      File \"/xxx/stable-diffusion-webui/venv/lib/python3.10/site-packages/starlette/middleware/base.py\", line 78, in call_next\r\n        message = await recv_stream.receive()\r\n      File \"/xxx/stable-diffusion-webui/venv/lib/python3.10/site-packages/anyio/streams/memory.py\", line 118, in receive\r\n        raise EndOfStream\r\n    anyio.EndOfStream\r\n\r\n    During handling of the above exception, another exception occurred:\r\n\r\n    Traceback (most recent call last):\r\n      File \"/xxx/stable-diffusion-webui/modules/api/api.py\", line 153, in exception_handling\r\n        return await call_next(request)\r\n      File \"/xxx/stable-diffusion-webui/venv/lib/python3.10/site-packages/starlette/middleware/base.py\", line 84, in call_next\r\n        raise app_exc\r\n      File \"/xxx/stable-diffusion-webui/venv/lib/python3.10/site-packages/starlette/middleware/base.py\", line 70, in coro\r\n        await self.app(scope, receive_or_disconnect, send_no_error)\r\n      File \"/xxx/stable-diffusion-webui/venv/lib/python3.10/site-packages/starlette/middleware/base.py\", line 108, in __call__\r\n        response = await self.dispatch_func(request, call_next)\r\n      File \"/xxx/stable-diffusion-webui/modules/api/api.py\", line 117, in log_and_time\r\n        res: Response = await call_next(req)\r\n      File \"/xxx/stable-diffusion-webui/venv/lib/python3.10/site-packages/starlette/middleware/base.py\", line 84, in call_next\r\n        raise app_exc\r\n      File \"/xxx/stable-diffusion-webui/venv/lib/python3.10/site-packages/starlette/middleware/base.py\", line 70, in coro\r\n        await self.app(scope, receive_or_disconnect, send_no_error)\r\n      File \"/xxx/stable-diffusion-webui/venv/lib/python3.10/site-packages/starlette/middleware/cors.py\", line 84, in __call__\r\n        await self.app(scope, receive, send)\r\n      File \"/xxx/stable-diffusion-webui/venv/lib/python3.10/site-packages/starlette/middleware/gzip.py\", line 24, in __call__\r\n        await responder(scope, receive, send)\r\n      File \"/xxx/stable-diffusion-webui/venv/lib/python3.10/site-packages/starlette/middleware/gzip.py\", line 44, in __call__\r\n        await self.app(scope, receive, self.send_with_gzip)\r\n      File \"/xxx/stable-diffusion-webui/venv/lib/python3.10/site-packages/starlette/middleware/exceptions.py\", line 79, in __call__\r\n        raise exc\r\n      File \"/xxx/stable-diffusion-webui/venv/lib/python3.10/site-packages/starlette/middleware/exceptions.py\", line 68, in __call__\r\n        await self.app(scope, receive, sender)\r\n      File \"/xxx/stable-diffusion-webui/venv/lib/python3.10/site-packages/fastapi/middleware/asyncexitstack.py\", line 21, in __call__\r\n        raise e\r\n      File \"/xxx/stable-diffusion-webui/venv/lib/python3.10/site-packages/fastapi/middleware/asyncexitstack.py\", line 18, in __call__\r\n        await self.app(scope, receive, send)\r\n      File \"/xxx/stable-diffusion-webui/venv/lib/python3.10/site-packages/starlette/routing.py\", line 718, in __call__\r\n        await route.handle(scope, receive, send)\r\n      File \"/xxx/stable-diffusion-webui/venv/lib/python3.10/site-packages/starlette/routing.py\", line 276, in handle\r\n        await self.app(scope, receive, send)\r\n      File \"/xxx/stable-diffusion-webui/venv/lib/python3.10/site-packages/starlette/routing.py\", line 66, in app\r\n        response = await func(request)\r\n      File \"/xxx/stable-diffusion-webui/venv/lib/python3.10/site-packages/fastapi/routing.py\", line 255, in app\r\n        content = await serialize_response(\r\n      File \"/xxx/stable-diffusion-webui/venv/lib/python3.10/site-packages/fastapi/routing.py\", line 141, in serialize_response\r\n        raise ValidationError(errors, field.type_)\r\n    pydantic.error_wrappers.ValidationError: 1 validation error for Options\r\n    response -> sd_model_checkpoint\r\n      value is not None (type=type_error.not_none)\r\n```\r\n\r\n\r\n### Additional information\r\n\r\n* NVIDIA driver 535.54.03\r\n* CUDA Version: V12.2.91\r\n\r\nI didn't edit any file in the local repo. I printed out `OptionsModel` and found `'sd_model_checkpoint': <class 'NoneType'>` in it. The value should be `string`.\r\n\r\nThe problem was cause by `\"sd_model_checkpoint\": OptionInfo(None, \"Stable Diffusion checkpoint\", gr.Dropdown, lambda: {\"choices\": list_checkpoint_tiles()}, refresh=refresh_checkpoints)`, and was fixed after I changed the `None` to `\"\"`. The `/sdapi/v1/options` started returning the correct json.\r\n\n", "hints_text": "\n", "created_at": "2023-07-18T08:22:32Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 11748, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-11748", "issue_numbers": ["11425"], "base_commit": "e5ca9877781bf2ce45edfb9c46ba532668c50de9", "patch": "diff --git a/modules/images.py b/modules/images.py\nindex 7bbfc3e0b10..7935b122a2d 100644\n--- a/modules/images.py\n+++ b/modules/images.py\n@@ -302,12 +302,14 @@ def resize(im, w, h):\n \r\n         if ratio < src_ratio:\r\n             fill_height = height // 2 - src_h // 2\r\n-            res.paste(resized.resize((width, fill_height), box=(0, 0, width, 0)), box=(0, 0))\r\n-            res.paste(resized.resize((width, fill_height), box=(0, resized.height, width, resized.height)), box=(0, fill_height + src_h))\r\n+            if fill_height > 0:\r\n+                res.paste(resized.resize((width, fill_height), box=(0, 0, width, 0)), box=(0, 0))\r\n+                res.paste(resized.resize((width, fill_height), box=(0, resized.height, width, resized.height)), box=(0, fill_height + src_h))\r\n         elif ratio > src_ratio:\r\n             fill_width = width // 2 - src_w // 2\r\n-            res.paste(resized.resize((fill_width, height), box=(0, 0, 0, height)), box=(0, 0))\r\n-            res.paste(resized.resize((fill_width, height), box=(resized.width, 0, resized.width, height)), box=(fill_width + src_w, 0))\r\n+            if fill_width > 0:\r\n+                res.paste(resized.resize((fill_width, height), box=(0, 0, 0, height)), box=(0, 0))\r\n+                res.paste(resized.resize((fill_width, height), box=(resized.width, 0, resized.width, height)), box=(fill_width + src_w, 0))\r\n \r\n     return res\r\n \r\n", "test_patch": "", "problem_statement": "[Bug]: {'error': 'ValueError', 'detail': '', 'body': '', 'errors': 'height and width must be > 0'}\n### Is there an existing issue for this?\n\n- [X] I have searched the existing issues and checked the recent builds/commits\n\n### What happened?\n\nWhen I call the img2img API, sometimes this error is reported, but if I don't use graffiti without mask parameters, there is no problem\r\n\r\n\n\n### Steps to reproduce the problem\n\ncall img2img api with mak param\n\n### What should have happened?\n\n`API error: POST: http://127.0.0.1:7161/sdapi/v1/img2img {'error': 'ValueError', 'detail': '', 'body': '', 'errors': 'height and width must be > 0'}\r\nTraceback (most recent call last):\r\n  File \"/root/miniconda3/envs/xl_env/lib/python3.10/site-packages/anyio/streams/memory.py\", line 94, in receive\r\n    return self.receive_nowait()\r\n  File \"/root/miniconda3/envs/xl_env/lib/python3.10/site-packages/anyio/streams/memory.py\", line 89, in receive_nowait\r\n    raise WouldBlock\r\nanyio.WouldBlock\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/root/miniconda3/envs/xl_env/lib/python3.10/site-packages/starlette/middleware/base.py\", line 78, in call_next\r\n    message = await recv_stream.receive()\r\n  File \"/root/miniconda3/envs/xl_env/lib/python3.10/site-packages/anyio/streams/memory.py\", line 114, in receive\r\n    raise EndOfStream\r\nanyio.EndOfStream\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/root/autodl-tmp/stable-diffusion-webui/modules/api/api.py\", line 152, in exception_handling\r\n    return await call_next(request)\r\n  File \"/root/miniconda3/envs/xl_env/lib/python3.10/site-packages/starlette/middleware/base.py\", line 84, in call_next\r\n    raise app_exc\r\n  File \"/root/miniconda3/envs/xl_env/lib/python3.10/site-packages/starlette/middleware/base.py\", line 70, in coro\r\n    await self.app(scope, receive_or_disconnect, send_no_error)\r\n  File \"/root/miniconda3/envs/xl_env/lib/python3.10/site-packages/starlette/middleware/base.py\", line 108, in __call__\r\n    response = await self.dispatch_func(request, call_next)\r\n  File \"/root/autodl-tmp/stable-diffusion-webui/modules/api/api.py\", line 117, in log_and_time\r\n    res: Response = await call_next(req)\r\n  File \"/root/miniconda3/envs/xl_env/lib/python3.10/site-packages/starlette/middleware/base.py\", line 84, in call_next\r\n    raise app_exc\r\n  File \"/root/miniconda3/envs/xl_env/lib/python3.10/site-packages/starlette/middleware/base.py\", line 70, in coro\r\n    await self.app(scope, receive_or_disconnect, send_no_error)\r\n  File \"/root/miniconda3/envs/xl_env/lib/python3.10/site-packages/starlette/middleware/cors.py\", line 84, in __call__\r\n    await self.app(scope, receive, send)\r\n  File \"/root/miniconda3/envs/xl_env/lib/python3.10/site-packages/starlette/middleware/gzip.py\", line 24, in __call__\r\n    await responder(scope, receive, send)\r\n  File \"/root/miniconda3/envs/xl_env/lib/python3.10/site-packages/starlette/middleware/gzip.py\", line 44, in __call__\r\n    await self.app(scope, receive, self.send_with_gzip)\r\n  File \"/root/miniconda3/envs/xl_env/lib/python3.10/site-packages/starlette/middleware/exceptions.py\", line 79, in __call__\r\n    raise exc\r\n  File \"/root/miniconda3/envs/xl_env/lib/python3.10/site-packages/starlette/middleware/exceptions.py\", line 68, in __call__\r\n    await self.app(scope, receive, sender)\r\n  File \"/root/miniconda3/envs/xl_env/lib/python3.10/site-packages/fastapi/middleware/asyncexitstack.py\", line 21, in __call__\r\n    raise e\r\n  File \"/root/miniconda3/envs/xl_env/lib/python3.10/site-packages/fastapi/middleware/asyncexitstack.py\", line 18, in __call__\r\n    await self.app(scope, receive, send)\r\n  File \"/root/miniconda3/envs/xl_env/lib/python3.10/site-packages/starlette/routing.py\", line 718, in __call__\r\n    await route.handle(scope, receive, send)\r\n  File \"/root/miniconda3/envs/xl_env/lib/python3.10/site-packages/starlette/routing.py\", line 276, in handle\r\n    await self.app(scope, receive, send)\r\n  File \"/root/miniconda3/envs/xl_env/lib/python3.10/site-packages/starlette/routing.py\", line 66, in app\r\n    response = await func(request)\r\n  File \"/root/miniconda3/envs/xl_env/lib/python3.10/site-packages/fastapi/routing.py\", line 237, in app\r\n    raw_response = await run_endpoint_function(\r\n  File \"/root/miniconda3/envs/xl_env/lib/python3.10/site-packages/fastapi/routing.py\", line 165, in run_endpoint_function\r\n    return await run_in_threadpool(dependant.call, **values)\r\n  File \"/root/miniconda3/envs/xl_env/lib/python3.10/site-packages/starlette/concurrency.py\", line 41, in run_in_threadpool\r\n    return await anyio.to_thread.run_sync(func, *args)\r\n  File \"/root/miniconda3/envs/xl_env/lib/python3.10/site-packages/anyio/to_thread.py\", line 31, in run_sync\r\n    return await get_asynclib().run_sync_in_worker_thread(\r\n  File \"/root/miniconda3/envs/xl_env/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 937, in run_sync_in_worker_thread\r\n    return await future\r\n  File \"/root/miniconda3/envs/xl_env/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 867, in run\r\n    result = context.run(func, *args)\r\n  File \"/root/autodl-tmp/stable-diffusion-webui/modules/api/api.py\", line 392, in img2imgapi\r\n    processed = process_images(p)\r\n  File \"/root/autodl-tmp/stable-diffusion-webui/modules/processing.py\", line 610, in process_images\r\n    res = process_images_inner(p)\r\n  File \"/root/autodl-tmp/stable-diffusion-webui/modules/processing.py\", line 670, in process_images_inner\r\n    p.init(p.all_prompts, p.all_seeds, p.all_subseeds)\r\n  File \"/root/autodl-tmp/stable-diffusion-webui/modules/processing.py\", line 1161, in init\r\n    image_mask = images.resize_image(2, mask, self.width, self.height)\r\n  File \"/root/autodl-tmp/stable-diffusion-webui/modules/images.py\", line 305, in resize_image\r\n    res.paste(resized.resize((width, fill_height), box=(0, 0, width, 0)), box=(0, 0))\r\n  File \"/root/miniconda3/envs/xl_env/lib/python3.10/site-packages/PIL/Image.py\", line 2193, in resize\r\n    return self._new(self.im.resize(size, resample, box))\r\nValueError: height and width must be > 0`\n\n### Commit where the problem happens\n\nmaster\n\n### What Python version are you running on ?\n\nPython 3.10.x\n\n### What platforms do you use to access the UI ?\n\nLinux\n\n### What device are you running WebUI on?\n\nOther GPUs\n\n### What browsers do you use to access the UI ?\n\nGoogle Chrome\n\n### Command Line Arguments\n\n```Shell\n*\n```\n\n\n### List of extensions\n\n*\n\n### Console logs\n\n```Shell\n`API error: POST: http://127.0.0.1:7161/sdapi/v1/img2img {'error': 'ValueError', 'detail': '', 'body': '', 'errors': 'height and width must be > 0'}\r\nTraceback (most recent call last):\r\n  File \"/root/miniconda3/envs/xl_env/lib/python3.10/site-packages/anyio/streams/memory.py\", line 94, in receive\r\n    return self.receive_nowait()\r\n  File \"/root/miniconda3/envs/xl_env/lib/python3.10/site-packages/anyio/streams/memory.py\", line 89, in receive_nowait\r\n    raise WouldBlock\r\nanyio.WouldBlock\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/root/miniconda3/envs/xl_env/lib/python3.10/site-packages/starlette/middleware/base.py\", line 78, in call_next\r\n    message = await recv_stream.receive()\r\n  File \"/root/miniconda3/envs/xl_env/lib/python3.10/site-packages/anyio/streams/memory.py\", line 114, in receive\r\n    raise EndOfStream\r\nanyio.EndOfStream\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/root/autodl-tmp/stable-diffusion-webui/modules/api/api.py\", line 152, in exception_handling\r\n    return await call_next(request)\r\n  File \"/root/miniconda3/envs/xl_env/lib/python3.10/site-packages/starlette/middleware/base.py\", line 84, in call_next\r\n    raise app_exc\r\n  File \"/root/miniconda3/envs/xl_env/lib/python3.10/site-packages/starlette/middleware/base.py\", line 70, in coro\r\n    await self.app(scope, receive_or_disconnect, send_no_error)\r\n  File \"/root/miniconda3/envs/xl_env/lib/python3.10/site-packages/starlette/middleware/base.py\", line 108, in __call__\r\n    response = await self.dispatch_func(request, call_next)\r\n  File \"/root/autodl-tmp/stable-diffusion-webui/modules/api/api.py\", line 117, in log_and_time\r\n    res: Response = await call_next(req)\r\n  File \"/root/miniconda3/envs/xl_env/lib/python3.10/site-packages/starlette/middleware/base.py\", line 84, in call_next\r\n    raise app_exc\r\n  File \"/root/miniconda3/envs/xl_env/lib/python3.10/site-packages/starlette/middleware/base.py\", line 70, in coro\r\n    await self.app(scope, receive_or_disconnect, send_no_error)\r\n  File \"/root/miniconda3/envs/xl_env/lib/python3.10/site-packages/starlette/middleware/cors.py\", line 84, in __call__\r\n    await self.app(scope, receive, send)\r\n  File \"/root/miniconda3/envs/xl_env/lib/python3.10/site-packages/starlette/middleware/gzip.py\", line 24, in __call__\r\n    await responder(scope, receive, send)\r\n  File \"/root/miniconda3/envs/xl_env/lib/python3.10/site-packages/starlette/middleware/gzip.py\", line 44, in __call__\r\n    await self.app(scope, receive, self.send_with_gzip)\r\n  File \"/root/miniconda3/envs/xl_env/lib/python3.10/site-packages/starlette/middleware/exceptions.py\", line 79, in __call__\r\n    raise exc\r\n  File \"/root/miniconda3/envs/xl_env/lib/python3.10/site-packages/starlette/middleware/exceptions.py\", line 68, in __call__\r\n    await self.app(scope, receive, sender)\r\n  File \"/root/miniconda3/envs/xl_env/lib/python3.10/site-packages/fastapi/middleware/asyncexitstack.py\", line 21, in __call__\r\n    raise e\r\n  File \"/root/miniconda3/envs/xl_env/lib/python3.10/site-packages/fastapi/middleware/asyncexitstack.py\", line 18, in __call__\r\n    await self.app(scope, receive, send)\r\n  File \"/root/miniconda3/envs/xl_env/lib/python3.10/site-packages/starlette/routing.py\", line 718, in __call__\r\n    await route.handle(scope, receive, send)\r\n  File \"/root/miniconda3/envs/xl_env/lib/python3.10/site-packages/starlette/routing.py\", line 276, in handle\r\n    await self.app(scope, receive, send)\r\n  File \"/root/miniconda3/envs/xl_env/lib/python3.10/site-packages/starlette/routing.py\", line 66, in app\r\n    response = await func(request)\r\n  File \"/root/miniconda3/envs/xl_env/lib/python3.10/site-packages/fastapi/routing.py\", line 237, in app\r\n    raw_response = await run_endpoint_function(\r\n  File \"/root/miniconda3/envs/xl_env/lib/python3.10/site-packages/fastapi/routing.py\", line 165, in run_endpoint_function\r\n    return await run_in_threadpool(dependant.call, **values)\r\n  File \"/root/miniconda3/envs/xl_env/lib/python3.10/site-packages/starlette/concurrency.py\", line 41, in run_in_threadpool\r\n    return await anyio.to_thread.run_sync(func, *args)\r\n  File \"/root/miniconda3/envs/xl_env/lib/python3.10/site-packages/anyio/to_thread.py\", line 31, in run_sync\r\n    return await get_asynclib().run_sync_in_worker_thread(\r\n  File \"/root/miniconda3/envs/xl_env/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 937, in run_sync_in_worker_thread\r\n    return await future\r\n  File \"/root/miniconda3/envs/xl_env/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 867, in run\r\n    result = context.run(func, *args)\r\n  File \"/root/autodl-tmp/stable-diffusion-webui/modules/api/api.py\", line 392, in img2imgapi\r\n    processed = process_images(p)\r\n  File \"/root/autodl-tmp/stable-diffusion-webui/modules/processing.py\", line 610, in process_images\r\n    res = process_images_inner(p)\r\n  File \"/root/autodl-tmp/stable-diffusion-webui/modules/processing.py\", line 670, in process_images_inner\r\n    p.init(p.all_prompts, p.all_seeds, p.all_subseeds)\r\n  File \"/root/autodl-tmp/stable-diffusion-webui/modules/processing.py\", line 1161, in init\r\n    image_mask = images.resize_image(2, mask, self.width, self.height)\r\n  File \"/root/autodl-tmp/stable-diffusion-webui/modules/images.py\", line 305, in resize_image\r\n    res.paste(resized.resize((width, fill_height), box=(0, 0, width, 0)), box=(0, 0))\r\n  File \"/root/miniconda3/envs/xl_env/lib/python3.10/site-packages/PIL/Image.py\", line 2193, in resize\r\n    return self._new(self.im.resize(size, resample, box))\r\nValueError: height and width must be > 0`\n```\n\n\n### Additional information\n\nnothing\n", "hints_text": "same error\nsame here", "created_at": "2023-07-12T08:55:02Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 11747, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-11747", "issue_numbers": ["11729"], "base_commit": "15adff3d6d5e8ba186b3df6eee8a8d774c8f3879", "patch": "diff --git a/modules/img2img.py b/modules/img2img.py\nindex 4d9a02ccff5..664e26888cd 100644\n--- a/modules/img2img.py\n+++ b/modules/img2img.py\n@@ -10,6 +10,7 @@\n from modules.generation_parameters_copypaste import create_override_settings_dict, parse_generation_parameters\r\n from modules.processing import Processed, StableDiffusionProcessingImg2Img, process_images\r\n from modules.shared import opts, state\r\n+from modules.images import save_image\r\n import modules.shared as shared\r\n import modules.processing as processing\r\n from modules.ui import plaintext_to_html\r\n@@ -113,18 +114,18 @@ def process_batch(p, input_dir, output_dir, inpaint_mask_dir, args, to_scale=Fal\n             proc = process_images(p)\r\n \r\n         for n, processed_image in enumerate(proc.images):\r\n-            filename = image_path.name\r\n+            filename = image_path.stem\r\n+            infotext = proc.infotext(p, n)\r\n             relpath = os.path.dirname(os.path.relpath(image, input_dir))\r\n \r\n             if n > 0:\r\n-                left, right = os.path.splitext(filename)\r\n-                filename = f\"{left}-{n}{right}\"\r\n+                filename += f\"-{n}\"\r\n \r\n             if not save_normally:\r\n                 os.makedirs(os.path.join(output_dir, relpath), exist_ok=True)\r\n                 if processed_image.mode == 'RGBA':\r\n                     processed_image = processed_image.convert(\"RGB\")\r\n-                processed_image.save(os.path.join(output_dir, relpath, filename))\r\n+                save_image(processed_image, os.path.join(output_dir, relpath), None, extension=opts.samples_format, info=infotext, forced_filename=filename, save_to_dirs=False)\r\n \r\n \r\n def img2img(id_task: str, mode: int, prompt: str, negative_prompt: str, prompt_styles, init_img, sketch, init_img_with_mask, inpaint_color_sketch, inpaint_color_sketch_orig, init_img_inpaint, init_mask_inpaint, steps: int, sampler_index: int, mask_blur: int, mask_alpha: float, inpainting_fill: int, restore_faces: bool, tiling: bool, n_iter: int, batch_size: int, cfg_scale: float, image_cfg_scale: float, denoising_strength: float, seed: int, subseed: int, subseed_strength: float, seed_resize_from_h: int, seed_resize_from_w: int, seed_enable_extras: bool, selected_scale_tab: int, height: int, width: int, scale_by: float, resize_mode: int, inpaint_full_res: bool, inpaint_full_res_padding: int, inpainting_mask_invert: int, img2img_batch_input_dir: str, img2img_batch_output_dir: str, img2img_batch_inpaint_mask_dir: str, override_settings_texts, img2img_batch_use_png_info: bool, img2img_batch_png_info_props: list, img2img_batch_png_info_dir: str, request: gr.Request, *args):\r\n", "test_patch": "", "problem_statement": "[Feature Request]: [batch processing] file compression ratio is not controlled,can you add control to it?\n### Is there an existing issue for this?\n\n- [X] I have searched the existing issues and checked the recent builds/commits\n\n### What would your feature do ?\n\nwhen we are in the img2img  batch processing,the output file format defaults to highly compressed jpg,\r\nunder the resolution of 896x1200,it's only 45k,this is great,\r\nbut a lot of times,we don't want to lose image quality to save storage space,\r\nalthough i have set [save jpeg picture quality] to 100% in [settings] [image save settings],\r\nbut it didn't work,i can't control it,\r\nis there any way i can improve the quality of the picture myself?\n\n### Proposed workflow\n\n1. [settings] --[image save settings],\r\n2. [save jpeg picture quality] to 100%\r\n3. [img2img] --  [batch processing] the quality of the output picture will be controlled,\r\n\n\n### Additional information\n\n_No response_\n", "hints_text": "", "created_at": "2023-07-12T08:52:06Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 11656, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-11656", "issue_numbers": ["11655"], "base_commit": "46c2b1e202886eb1b8fcd4ed452b53e311ef076d", "patch": "diff --git a/modules/api/api.py b/modules/api/api.py\nindex 2e49526e3e2..6507f641cb6 100644\n--- a/modules/api/api.py\n+++ b/modules/api/api.py\n@@ -84,6 +84,8 @@ def encode_pil_to_base64(image):\n             image.save(output_bytes, format=\"PNG\", pnginfo=(metadata if use_metadata else None), quality=opts.jpeg_quality)\n \n         elif opts.samples_format.lower() in (\"jpg\", \"jpeg\", \"webp\"):\n+            if image.mode == \"RGBA\":\n+                image = image.convert(\"RGB\")\n             parameters = image.info.get('parameters', None)\n             exif_bytes = piexif.dump({\n                 \"Exif\": { piexif.ExifIFD.UserComment: piexif.helper.UserComment.dump(parameters or \"\", encoding=\"unicode\") }\n", "test_patch": "", "problem_statement": "[Bug]: Call ControlNet API failed\n### Is there an existing issue for this?\r\n\r\n- [X] I have searched the existing issues and checked the recent builds/commits\r\n\r\n### What happened?\r\n\r\nCall ControlNet through the API. It raise an error `cannot write mode RGBA as JPEG`.\r\nControlNet:\r\n- Preprocessor: inpaint_only+lama\r\n- Model: control_v11p_sd15_inpaint [ebff9138]\r\n- ControlNet Version: v1.1.231\r\n\r\n\r\n### Steps to reproduce the problem\r\n\r\n1. Call ControlNet through the API \"/sdapi/v1/txt2img\"\r\n2. ControlNet payload like this:\r\n```json\r\n{\r\n    \"prompt\": \"a handsome man\",\r\n    \"negative_prompt\": \"\",\r\n    \"batch_size\": 1,\r\n    \"steps\": 30,\r\n    \"width\": 1536,\r\n    \"height\": 768,\r\n    \"cfg_scale\": 7,\r\n    \"alwayson_scripts\": {\r\n        \"controlnet\": {\r\n            \"args\": [\r\n                {\r\n                    \"input_image\": \"encoded_image\",\r\n                    \"module\": \"inpaint_only+lama\",\r\n                    \"model\": \"control_v11p_sd15_inpaint [ebff9138]\",\r\n                    \"control_mode\": 2,\r\n                    \"resize_mode\": \"Resize and Fill\"\r\n                }\r\n            ]\r\n        }\r\n    }\r\n}\r\n```\r\n\r\n\r\n### What should have happened?\r\n\r\nControlNet should be called correctly, and the picture generated.\r\n\r\n### Version or Commit where the problem happens\r\n\r\nv1.3.2\r\n\r\n### What Python version are you running on ?\r\n\r\nPython 3.10.x\r\n\r\n### What platforms do you use to access the UI ?\r\n\r\nLinux\r\n\r\n### What device are you running WebUI on?\r\n\r\nNvidia GPUs (RTX 20 above)\r\n\r\n### Cross attention optimization\r\n\r\nxformers\r\n\r\n### What browsers do you use to access the UI ?\r\n\r\nGoogle Chrome\r\n\r\n### Command Line Arguments\r\n\r\n```Shell\r\n--api --xformers --server-name=0.0.0.0 --enable-insecure-extension-access --device-id=0\r\n```\r\n\r\n\r\n### List of extensions\r\n\r\nsd-webui-controlnet\r\n\r\n### Console logs\r\n\r\n```Shell\r\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Traceback (most recent call last) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\r\n\u2502 /home/jovijovi/anaconda3/envs/automatic/lib/python3.10/site-packages/PIL/Jp  \u2502\r\n\u2502 egImagePlugin.py:640 in _save                                                \u2502\r\n\u2502                                                                              \u2502\r\n\u2502   639 \u2502   try:                                                               \u2502\r\n\u2502 \u2771 640 \u2502   \u2502   rawmode = RAWMODE[im.mode]                                     \u2502\r\n\u2502   641 \u2502   except KeyError as e:                                              \u2502\r\n\u2502                                                                              \u2502\r\n\u2502 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 locals \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\r\n\u2502 \u2502 filename = ''                                                            \u2502 \u2502\r\n\u2502 \u2502       fp = <_io.BytesIO object at 0x7f6f3474f9c0>                        \u2502 \u2502\r\n\u2502 \u2502       im = <PIL.Image.Image image mode=RGBA size=1536x768 at             \u2502 \u2502\r\n\u2502 \u2502            0x7F6DB7636470>                                               \u2502 \u2502\r\n\u2502 \u2502      msg = 'cannot write mode RGBA as JPEG'                              \u2502 \u2502\r\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\r\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\r\nKeyError: 'RGBA'\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Traceback (most recent call last) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\r\n\u2502 /mnt/local/dev/generator-auto1111/modules/api/api.py:154 in                  \u2502\r\n\u2502 exception_handling                                                           \u2502\r\n\u2502                                                                              \u2502\r\n\u2502   153 \u2502   \u2502   try:                                                           \u2502\r\n\u2502 \u2771 154 \u2502   \u2502   \u2502   return await call_next(request)                            \u2502\r\n\u2502   155 \u2502   \u2502   except Exception as e:                                         \u2502\r\n\u2502                                                                              \u2502\r\n\u2502 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 locals \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\r\n\u2502 \u2502        call_next = <function                                             \u2502 \u2502\r\n\u2502 \u2502                    BaseHTTPMiddleware.__call__.<locals>.call_next at     \u2502 \u2502\r\n\u2502 \u2502                    0x7f6d8c1004c0>                                       \u2502 \u2502\r\n\u2502 \u2502                e = OSError('cannot write mode RGBA as JPEG')             \u2502 \u2502\r\n\u2502 \u2502 handle_exception = <function api_middleware.<locals>.handle_exception at \u2502 \u2502\r\n\u2502 \u2502                    0x7f6db4321a20>                                       \u2502 \u2502\r\n\u2502 \u2502          request = <starlette.requests.Request object at 0x7f6d8c55d900> \u2502 \u2502\r\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\r\n\u2502                                                                              \u2502\r\n\u2502 /home/jovijovi/anaconda3/envs/automatic/lib/python3.10/site-packages/starle  \u2502\r\n\u2502 tte/middleware/base.py:84 in call_next                                       \u2502\r\n\u2502                                                                              \u2502\r\n\u2502                           ... 23 frames hidden ...                           \u2502\r\n\u2502                                                                              \u2502\r\n\u2502 /home/jovijovi/anaconda3/envs/automatic/lib/python3.10/site-packages/PIL/Im  \u2502\r\n\u2502 age.py:2432 in save                                                          \u2502\r\n\u2502                                                                              \u2502\r\n\u2502   2431 \u2502   \u2502   try:                                                          \u2502\r\n\u2502 \u2771 2432 \u2502   \u2502   \u2502   save_handler(self, fp, filename)                          \u2502\r\n\u2502   2433 \u2502   \u2502   except Exception:                                             \u2502\r\n\u2502                                                                              \u2502\r\n\u2502 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 locals \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\r\n\u2502 \u2502      created = False                                                     \u2502 \u2502\r\n\u2502 \u2502          ext = ''                                                        \u2502 \u2502\r\n\u2502 \u2502     filename = ''                                                        \u2502 \u2502\r\n\u2502 \u2502       format = 'JPEG'                                                    \u2502 \u2502\r\n\u2502 \u2502           fp = <_io.BytesIO object at 0x7f6f3474f9c0>                    \u2502 \u2502\r\n\u2502 \u2502      open_fp = False                                                     \u2502 \u2502\r\n\u2502 \u2502       params = {                                                         \u2502 \u2502\r\n\u2502 \u2502                \u2502   'exif':                                               \u2502 \u2502\r\n\u2502 \u2502                b'Exif\\x00\\x00MM\\x00*\\x00\\x00\\x00\\x08\\x00\\x01\\x87i\\x00\\x\u2026 \u2502 \u2502\r\n\u2502 \u2502                \u2502   'quality': 80                                         \u2502 \u2502\r\n\u2502 \u2502                }                                                         \u2502 \u2502\r\n\u2502 \u2502     save_all = False                                                     \u2502 \u2502\r\n\u2502 \u2502 save_handler = <function _save at 0x7f6eae53c430>                        \u2502 \u2502\r\n\u2502 \u2502         self = <PIL.Image.Image image mode=RGBA size=1536x768 at         \u2502 \u2502\r\n\u2502 \u2502                0x7F6DB7636470>                                           \u2502 \u2502\r\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\r\n\u2502                                                                              \u2502\r\n\u2502 /home/jovijovi/anaconda3/envs/automatic/lib/python3.10/site-packages/PIL/Jp  \u2502\r\n\u2502 egImagePlugin.py:643 in _save                                                \u2502\r\n\u2502                                                                              \u2502\r\n\u2502   642 \u2502   \u2502   msg = f\"cannot write mode {im.mode} as JPEG\"                   \u2502\r\n\u2502 \u2771 643 \u2502   \u2502   raise OSError(msg) from e                                      \u2502\r\n\u2502   644                                                                        \u2502\r\n\u2502                                                                              \u2502\r\n\u2502 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 locals \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\r\n\u2502 \u2502 filename = ''                                                            \u2502 \u2502\r\n\u2502 \u2502       fp = <_io.BytesIO object at 0x7f6f3474f9c0>                        \u2502 \u2502\r\n\u2502 \u2502       im = <PIL.Image.Image image mode=RGBA size=1536x768 at             \u2502 \u2502\r\n\u2502 \u2502            0x7F6DB7636470>                                               \u2502 \u2502\r\n\u2502 \u2502      msg = 'cannot write mode RGBA as JPEG'                              \u2502 \u2502\r\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\r\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\r\nOSError: cannot write mode RGBA as JPEG\r\n```\r\n\r\n\r\n### Additional information\r\n\r\n_No response_\n", "hints_text": "", "created_at": "2023-07-06T11:38:34Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 11631, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-11631", "issue_numbers": ["11613"], "base_commit": "2151a9881f3fbb0de52dbfdc83a4dd67cdfbbda6", "patch": "diff --git a/modules/ui_extra_networks.py b/modules/ui_extra_networks.py\nindex a7d3bc792f9..1efd00b0353 100644\n--- a/modules/ui_extra_networks.py\n+++ b/modules/ui_extra_networks.py\n@@ -30,8 +30,8 @@ def fetch_file(filename: str = \"\"):\n         raise ValueError(f\"File cannot be fetched: {filename}. Must be in one of directories registered by extra pages.\")\r\n \r\n     ext = os.path.splitext(filename)[1].lower()\r\n-    if ext not in (\".png\", \".jpg\", \".jpeg\", \".webp\"):\r\n-        raise ValueError(f\"File cannot be fetched: {filename}. Only png and jpg and webp.\")\r\n+    if ext not in (\".png\", \".jpg\", \".jpeg\", \".webp\", \".gif\"):\r\n+        raise ValueError(f\"File cannot be fetched: {filename}. Only png, jpg, webp, and gif.\")\r\n \r\n     # would profit from returning 304\r\n     return FileResponse(filename, headers={\"Accept-Ranges\": \"bytes\"})\r\n", "test_patch": "", "problem_statement": "[Feature Request]: Support for GIF preview images\n### Is there an existing issue for this?\r\n\r\n- [X] I have searched the existing issues and checked the recent builds/commits\r\n\r\n### What would your feature do ?\r\n\r\nRight now preview images are either jpg or png, but now you can download Civitai previews in GIF format that contain all metadata. It would be nice to support them in preview images. \r\n\r\nAlso maybe support for webp images also. It seems to be used now also.\r\n\r\n### Proposed workflow\r\n\r\n1. Go to .... \r\n2. Press ....\r\n3. ...\r\n\r\n\r\n### Additional information\r\n\r\n_No response_\n", "hints_text": "", "created_at": "2023-07-05T09:21:35Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 11592, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-11592", "issue_numbers": ["11591"], "base_commit": "fab73f2e7d388ca99cdb3d5de7f36c0b9a1a3b1c", "patch": "diff --git a/webui.sh b/webui.sh\nindex 5c8d977cb2d..246381fc275 100755\n--- a/webui.sh\n+++ b/webui.sh\n@@ -4,26 +4,28 @@\n # change the variables in webui-user.sh instead #\n #################################################\n \n+SCRIPT_DIR=$( cd -- \"$( dirname -- \"${BASH_SOURCE[0]}\" )\" &> /dev/null && pwd )\n+\n # If run from macOS, load defaults from webui-macos-env.sh\n if [[ \"$OSTYPE\" == \"darwin\"* ]]; then\n-    if [[ -f webui-macos-env.sh ]]\n+    if [[ -f \"$SCRIPT_DIR\"/webui-macos-env.sh ]]\n         then\n-        source ./webui-macos-env.sh\n+        source \"$SCRIPT_DIR\"/webui-macos-env.sh\n     fi\n fi\n \n # Read variables from webui-user.sh\n # shellcheck source=/dev/null\n-if [[ -f webui-user.sh ]]\n+if [[ -f \"$SCRIPT_DIR\"/webui-user.sh ]]\n then\n-    source ./webui-user.sh\n+    source \"$SCRIPT_DIR\"/webui-user.sh\n fi\n \n # Set defaults\n # Install directory without trailing slash\n if [[ -z \"${install_dir}\" ]]\n then\n-    install_dir=\"$(pwd)\"\n+    install_dir=\"$SCRIPT_DIR\"\n fi\n \n # Name of the subdirectory (defaults to stable-diffusion-webui)\n", "test_patch": "", "problem_statement": "[Bug]: Launch script cannot find config/install directory if script is launched from external directory\n### Is there an existing issue for this?\n\n- [X] I have searched the existing issues and checked the recent builds/commits\n\n### What happened?\n\nThe script assumes that `$pwd` is the parent directory of the launch script. When this is not the case, it can cause undesired side effects (cloning to the repo to an unexpected directory) or a crash (if the user doesnt have write permissions in `$pwd`).\r\n\r\nIt also assumes `webui-user.sh` is in `$pwd` instead of the same directory as the launch script.\n\n### Steps to reproduce the problem\n\n1. `cd /`\r\n2. `~/stable-diffusion-webui/webui.sh --flags-go-here`\r\n3. launch script crashes, saying that it failed to write to /stable-diffusion-webui\n\n### What should have happened?\n\nExpected behaviour would be that it looks for the `webui-user.sh` script in the same directory as the launch script, such that launching the script as `~/stable-diffusion-webui/webui.sh` would search `~/stable-diffusion-webui/webui-user.sh` for the configurations, not `$pwd/stable-diffusion-webui/webui.sh`\n\n### Version or Commit where the problem happens\n\n1.4.0\n\n### What Python version are you running on ?\n\nPython 3.11.x (above, no supported yet)\n\n### What platforms do you use to access the UI ?\n\nLinux\n\n### What device are you running WebUI on?\n\nAMD GPUs (RX 5000 below)\n\n### Cross attention optimization\n\nDoggettx\n\n### What browsers do you use to access the UI ?\n\nMozilla Firefox\n\n### Command Line Arguments\n\n```Shell\nDoes not apply\n```\n\n\n### List of extensions\n\nNone\n\n### Console logs\n\n```Shell\nJul 03 02:00:43 chiyoda webui.sh[117259]: ################################################################\r\nJul 03 02:00:43 chiyoda webui.sh[117259]: Install script for stable-diffusion + Web UI\r\nJul 03 02:00:43 chiyoda webui.sh[117259]: Tested on Debian 11 (Bullseye)\r\nJul 03 02:00:43 chiyoda webui.sh[117259]: ################################################################\r\nJul 03 02:00:43 chiyoda webui.sh[117259]: ################################################################\r\nJul 03 02:00:43 chiyoda webui.sh[117259]: Running on kaneki user\r\nJul 03 02:00:43 chiyoda webui.sh[117259]: ################################################################\r\nJul 03 02:00:44 chiyoda webui.sh[117259]: ################################################################\r\nJul 03 02:00:44 chiyoda webui.sh[117259]: Clone stable-diffusion-webui\r\nJul 03 02:00:44 chiyoda webui.sh[117259]: ################################################################\r\nJul 03 02:00:44 chiyoda webui.sh[117272]: fatal: could not create work tree dir 'stable-diffusion-webui': Permission denied\r\nJul 03 02:00:44 chiyoda webui.sh[117259]: /home/kaneki/stable-diffusion-webui/webui.sh: line 179: cd: stable-diffusion-webui/: No such file or dire>\r\nJul 03 02:00:44 chiyoda webui.sh[117259]: ERROR: Can't cd to //stable-diffusion-webui/, aborting...\r\nJul 03 02:00:44 chiyoda systemd[1]: diffusion.service: Main process exited, code=exited, status=1/FAILURE\n```\n\n\n### Additional information\n\n_No response_\n", "hints_text": "", "created_at": "2023-07-03T06:58:49Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 11495, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-11495", "issue_numbers": ["11490"], "base_commit": "fab73f2e7d388ca99cdb3d5de7f36c0b9a1a3b1c", "patch": "diff --git a/javascript/edit-attention.js b/javascript/edit-attention.js\nindex ffa73147ff2..8906c8922e1 100644\n--- a/javascript/edit-attention.js\n+++ b/javascript/edit-attention.js\n@@ -100,11 +100,12 @@ function keyupEditAttention(event) {\n     if (String(weight).length == 1) weight += \".0\";\n \n     if (closeCharacter == ')' && weight == 1) {\n-        text = text.slice(0, selectionStart - 1) + text.slice(selectionStart, selectionEnd) + text.slice(selectionEnd + 5);\n+        var endParenPos = text.substring(selectionEnd).indexOf(')');\n+        text = text.slice(0, selectionStart - 1) + text.slice(selectionStart, selectionEnd) + text.slice(selectionEnd + endParenPos + 1);\n         selectionStart--;\n         selectionEnd--;\n     } else {\n-        text = text.slice(0, selectionEnd + 1) + weight + text.slice(selectionEnd + 1 + end - 1);\n+        text = text.slice(0, selectionEnd + 1) + weight + text.slice(selectionEnd + end);\n     }\n \n     target.focus();\n", "test_patch": "", "problem_statement": "[Bug]: end parenthesis being left on weight phrase when pressing ctrl-up or down when you reach 1.0\n### Is there an existing issue for this?\r\n\r\n- [X] I have searched the existing issues and checked the recent builds/commits\r\n\r\n### What happened?\r\n\r\nWhen highlighting a phrase, pressing ctrl-up / ctrl-down works as expected until you return to 1.0 and the trailing parenthesis is left in place when  the leading one is removed\r\n\r\n\r\n\r\n\r\n### Steps to reproduce the problem\r\n\r\n1. Go to the positive or negative prompt and highlight desired weight phrase.\r\n2. Use ctrl-up to set it, then ctrl-down to remove it\r\n     highlight `some phrase`\r\n\r\n     , `(some phrase:1.05)`, << set\r\n           and press ctrl-down\r\n     , `some phrase)`,  << zeroed (1.0) out with ctrl-up/down\r\nor \r\n     ,`(some phrase:0.95)`, << set\r\n          and press ctrl-up\r\n     , `some phrase)`,  << zeroed (1.0) out with ctrl-up/down\r\n\r\ndo this a few times, and it starts looking like \r\n     , `some phrase))))))`,\r\n\r\n\r\n### What should have happened?\r\n\t, (some phrase:1.05),\t<< set\r\n\t, some phrase,\t\t<< removed\r\n\r\n### Version or Commit where the problem happens\r\n\r\nversion: v1.4.0 \u2000\u2022\u2000 python: 3.10.6 \u2000\u2022\u2000 torch: 2.0.1+cu118 \u2000\u2022\u2000 xformers: 0.0.20 \u2000\u2022\u2000 gradio: 3.32.0 \u2000\u2022\u2000 checkpoint: 8914c7f790\r\n\r\n### What Python version are you running on ?\r\n\r\nPython 3.10.x\r\n\r\n### What platforms do you use to access the UI ?\r\n\r\nWindows\r\n\r\n### What device are you running WebUI on?\r\n\r\nNvidia GPUs (RTX 20 above)\r\n\r\n### Cross attention optimization\r\n\r\nxformers\r\n\r\n### What browsers do you use to access the UI ?\r\n\r\nMozilla Firefox, Google Chrome, Microsoft Edge\r\n\r\n### Command Line Arguments\r\n\r\n```Shell\r\n--xformers --autolaunch\r\n```\r\n\r\n\r\n### List of extensions\r\n\r\nBuilt-in Extensions\r\n\r\n### Console logs\r\n\r\n```Shell\r\nNothing outside the normal start up\r\n\r\nTo create a public link, set `share=True` in `launch()`.\r\nStartup time: 4.1s (import torch: 0.8s, import gradio: 0.6s, import ldm: 0.3s, other imports: 0.5s, opts onchange: 0.2s, list SD models: 0.2s, load scripts: 0.7s, create ui: 0.2s, gradio launch: 0.5s).\r\nCreating model from config: A:\\AI\\StableDiffusion\\stable-diffusion-webui\\configs\\v1-inference.yaml\r\nLatentDiffusion: Running in eps-prediction mode\r\nDiffusionWrapper has 859.52 M params.\r\npreload_extensions_git_metadata for 16 extensions took 2.31s\r\nLoading VAE weights specified in settings: A:\\AI\\StableDiffusion\\stable-diffusion-webui\\models\\VAE\\fixYourColorsVAE_vaeFtMse840000Ema.ckpt\r\nApplying attention optimization: xformers... done.\r\nTextual inversion embeddings loaded(0):\r\nModel loaded in 4.3s (load weights from disk: 0.8s, create model: 0.9s, apply weights to model: 0.9s, apply half(): 0.7s, load VAE: 0.2s, move model to device: 0.6s).\r\n```\r\n\r\n\r\n### Additional information\r\n\r\nNo errors, just a miss-function of the ctrl-up and down\n", "hints_text": "", "created_at": "2023-06-28T23:53:17Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 11325, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-11325", "issue_numbers": ["11323"], "base_commit": "59419bd64a1581caccaac04dceb66c1c069a2db1", "patch": "diff --git a/modules/processing.py b/modules/processing.py\nindex 8da738842cf..1d97e95e9ec 100644\n--- a/modules/processing.py\n+++ b/modules/processing.py\n@@ -549,7 +549,7 @@ def program_version():\n     return res\r\n \r\n \r\n-def create_infotext(p, all_prompts, all_seeds, all_subseeds, comments=None, iteration=0, position_in_batch=0):\r\n+def create_infotext(p, all_prompts, all_seeds, all_subseeds, comments=None, iteration=0, position_in_batch=0, use_main_prompt=False):\r\n     index = position_in_batch + iteration * p.batch_size\r\n \r\n     clip_skip = getattr(p, 'clip_skip', opts.CLIP_stop_at_last_layers)\r\n@@ -589,9 +589,10 @@ def create_infotext(p, all_prompts, all_seeds, all_subseeds, comments=None, iter\n \r\n     generation_params_text = \", \".join([k if k == v else f'{k}: {generation_parameters_copypaste.quote(v)}' for k, v in generation_params.items() if v is not None])\r\n \r\n+    prompt_text = p.prompt if use_main_prompt else all_prompts[index]\r\n     negative_prompt_text = f\"\\nNegative prompt: {p.all_negative_prompts[index]}\" if p.all_negative_prompts[index] else \"\"\r\n \r\n-    return f\"{all_prompts[index]}{negative_prompt_text}\\n{generation_params_text}\".strip()\r\n+    return f\"{prompt_text}{negative_prompt_text}\\n{generation_params_text}\".strip()\r\n \r\n \r\n def process_images(p: StableDiffusionProcessing) -> Processed:\r\n@@ -663,8 +664,8 @@ def process_images_inner(p: StableDiffusionProcessing) -> Processed:\n     else:\r\n         p.all_subseeds = [int(subseed) + x for x in range(len(p.all_prompts))]\r\n \r\n-    def infotext(iteration=0, position_in_batch=0):\r\n-        return create_infotext(p, p.all_prompts, p.all_seeds, p.all_subseeds, comments, iteration, position_in_batch)\r\n+    def infotext(iteration=0, position_in_batch=0, use_main_prompt=False):\r\n+        return create_infotext(p, p.all_prompts, p.all_seeds, p.all_subseeds, comments, iteration, position_in_batch, use_main_prompt)\r\n \r\n     if os.path.exists(cmd_opts.embeddings_dir) and not p.do_not_reload_embeddings:\r\n         model_hijack.embedding_db.load_textual_inversion_embeddings()\r\n@@ -824,7 +825,7 @@ def infotext(iteration=0, position_in_batch=0):\n             grid = images.image_grid(output_images, p.batch_size)\r\n \r\n             if opts.return_grid:\r\n-                text = infotext()\r\n+                text = infotext(use_main_prompt=True)\r\n                 infotexts.insert(0, text)\r\n                 if opts.enable_pnginfo:\r\n                     grid.info[\"parameters\"] = text\r\n@@ -832,7 +833,7 @@ def infotext(iteration=0, position_in_batch=0):\n                 index_of_first_image = 1\r\n \r\n             if opts.grid_save:\r\n-                images.save_image(grid, p.outpath_grids, \"grid\", p.all_seeds[0], p.all_prompts[0], opts.grid_format, info=infotext(), short_filename=not opts.grid_extended_filename, p=p, grid=True)\r\n+                images.save_image(grid, p.outpath_grids, \"grid\", p.all_seeds[0], p.all_prompts[0], opts.grid_format, info=infotext(use_main_prompt=True), short_filename=not opts.grid_extended_filename, p=p, grid=True)\r\n \r\n     if not p.disable_extra_networks and p.extra_network_data:\r\n         extra_networks.deactivate(p, p.extra_network_data)\r\n", "test_patch": "", "problem_statement": "[Bug]: Grid images of batch runs have incorrect metadata when the prompt changes over the batch (e.g. using wildcards)\n### Is there an existing issue for this?\n\n- [X] I have searched the existing issues and checked the recent builds/commits\n\n### What happened?\n\nThis is halfway between a bug and feature request. I have a fix already implemented and will submit a PR later today.\r\n\r\nThe grid image of a batch run currently gets metadata identical to the first image of the batch. This is generally useful, because users can send the grid image to txt2img and reproduce the whole batch.\r\n\r\nBut there are extensions that modify the prompt for different images of a batch, e.g. wildcard extensions like [stable-diffusion-webui-wildcards](https://github.com/AUTOMATIC1111/stable-diffusion-webui-wildcards) or [sd-dynamic-prompts](https://github.com/adieyal/sd-dynamic-prompts), or another that I'm currently working on. The hardcoded webui behavior is slightly broken for these extensions since the grid image no longer contains the original prompt with the wildcard templates (it gets overwritten by the randomly generated tokens in the first image). The extension dev can work around this problem by duplicating the original prompt in `extra_generation_params` but this is awkward, pollutes the metadata with another huge prompt string and requires additional save/reload infrastructure by the extension dev.\r\n\r\nInstead I think the batch grid should retain the original prompt used to create it (i.e. including the templates). Then the wildcards user can reproduce either individual images by sending the image to tx2img or the entire batch by sending the grid image instead.\r\n\r\nThe fix involves using `p.prompt` for grid images and `all_prompts[index]` for other images, see [line 587 of processing.py](https://github.com/stablegeniusdiffuser/stable-diffusion-webui/blob/baf6946e06249c5af9851c60171692c44ef633e0/modules/processing.py#L587).\n\n### Steps to reproduce the problem\n\n1. Create a batch using wildcards.\r\n2. Examine the metadata of the grid image.\r\n\n\n### What should have happened?\n\nKeep the original prompt in the grid image metadata as described above.\n\n### Commit where the problem happens\n\nbaf6946e (master) or 59419bd6 (dev)\n\n### What Python version are you running on ?\n\nPython 3.10.x\n\n### What platforms do you use to access the UI ?\n\nWindows\n\n### What device are you running WebUI on?\n\nNvidia GPUs (RTX 20 above)\n\n### What browsers do you use to access the UI ?\n\nGoogle Chrome\n\n### Command Line Arguments\n\n```Shell\nno\n```\n\n\n### List of extensions\n\nhttps://github.com/adieyal/sd-dynamic-prompts\n\n### Console logs\n\n```Shell\nno\n```\n\n\n### Additional information\n\n_No response_\n", "hints_text": "", "created_at": "2023-06-19T19:20:36Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 11227, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-11227", "issue_numbers": ["10141"], "base_commit": "bedcd2f377a38ef4da58c11dbe222d32b954be2f", "patch": "diff --git a/modules/call_queue.py b/modules/call_queue.py\nindex 1b5e5273856..69bf63d2b9e 100644\n--- a/modules/call_queue.py\n+++ b/modules/call_queue.py\n@@ -1,3 +1,4 @@\n+from functools import wraps\r\n import html\r\n import threading\r\n import time\r\n@@ -18,6 +19,7 @@ def f(*args, **kwargs):\n \r\n \r\n def wrap_gradio_gpu_call(func, extra_outputs=None):\r\n+    @wraps(func)\r\n     def f(*args, **kwargs):\r\n \r\n         # if the first argument is a string that says \"task(...)\", it is treated as a job id\r\n@@ -45,6 +47,7 @@ def f(*args, **kwargs):\n \r\n \r\n def wrap_gradio_call(func, extra_outputs=None, add_stats=False):\r\n+    @wraps(func)\r\n     def f(*args, extra_outputs_array=extra_outputs, **kwargs):\r\n         run_memmon = shared.opts.memmon_poll_rate > 0 and not shared.mem_mon.disabled and add_stats\r\n         if run_memmon:\r\ndiff --git a/modules/images.py b/modules/images.py\nindex 7bbfc3e0b10..1906e2ab5ff 100644\n--- a/modules/images.py\n+++ b/modules/images.py\n@@ -372,8 +372,8 @@ def get_vae_filename(self): #get the name of the VAE file.\n         'hasprompt': lambda self, *args: self.hasprompt(*args),  # accepts formats:[hasprompt<prompt1|default><prompt2>..]\r\n         'clip_skip': lambda self: opts.data[\"CLIP_stop_at_last_layers\"],\r\n         'denoising': lambda self: self.p.denoising_strength if self.p and self.p.denoising_strength else NOTHING_AND_SKIP_PREVIOUS_TEXT,\r\n+        'user': lambda self: self.p.user,\r\n         'vae_filename': lambda self: self.get_vae_filename(),\r\n-\r\n     }\r\n     default_time_format = '%Y%m%d%H%M%S'\r\n \r\ndiff --git a/modules/img2img.py b/modules/img2img.py\nindex 2c4970207fe..b07d7f2fe20 100644\n--- a/modules/img2img.py\n+++ b/modules/img2img.py\n@@ -3,6 +3,7 @@\n \r\n import numpy as np\r\n from PIL import Image, ImageOps, ImageFilter, ImageEnhance, ImageChops, UnidentifiedImageError\r\n+import gradio as gr\r\n \r\n from modules import sd_samplers\r\n from modules.generation_parameters_copypaste import create_override_settings_dict\r\n@@ -97,7 +98,7 @@ def process_batch(p, input_dir, output_dir, inpaint_mask_dir, args, to_scale=Fal\n                 processed_image.save(os.path.join(output_dir, filename))\r\n \r\n \r\n-def img2img(id_task: str, mode: int, prompt: str, negative_prompt: str, prompt_styles, init_img, sketch, init_img_with_mask, inpaint_color_sketch, inpaint_color_sketch_orig, init_img_inpaint, init_mask_inpaint, steps: int, sampler_index: int, mask_blur: int, mask_alpha: float, inpainting_fill: int, restore_faces: bool, tiling: bool, n_iter: int, batch_size: int, cfg_scale: float, image_cfg_scale: float, denoising_strength: float, seed: int, subseed: int, subseed_strength: float, seed_resize_from_h: int, seed_resize_from_w: int, seed_enable_extras: bool, selected_scale_tab: int, height: int, width: int, scale_by: float, resize_mode: int, inpaint_full_res: bool, inpaint_full_res_padding: int, inpainting_mask_invert: int, img2img_batch_input_dir: str, img2img_batch_output_dir: str, img2img_batch_inpaint_mask_dir: str, override_settings_texts, *args):\r\n+def img2img(id_task: str, mode: int, prompt: str, negative_prompt: str, prompt_styles, init_img, sketch, init_img_with_mask, inpaint_color_sketch, inpaint_color_sketch_orig, init_img_inpaint, init_mask_inpaint, steps: int, sampler_index: int, mask_blur: int, mask_alpha: float, inpainting_fill: int, restore_faces: bool, tiling: bool, n_iter: int, batch_size: int, cfg_scale: float, image_cfg_scale: float, denoising_strength: float, seed: int, subseed: int, subseed_strength: float, seed_resize_from_h: int, seed_resize_from_w: int, seed_enable_extras: bool, selected_scale_tab: int, height: int, width: int, scale_by: float, resize_mode: int, inpaint_full_res: bool, inpaint_full_res_padding: int, inpainting_mask_invert: int, img2img_batch_input_dir: str, img2img_batch_output_dir: str, img2img_batch_inpaint_mask_dir: str, override_settings_texts, request: gr.Request, *args):\r\n     override_settings = create_override_settings_dict(override_settings_texts)\r\n \r\n     is_batch = mode == 5\r\n@@ -180,6 +181,8 @@ def img2img(id_task: str, mode: int, prompt: str, negative_prompt: str, prompt_s\n     p.scripts = modules.scripts.scripts_img2img\r\n     p.script_args = args\r\n \r\n+    p.user = request.username\r\n+\r\n     if shared.cmd_opts.enable_console_prompts:\r\n         print(f\"\\nimg2img: {prompt}\", file=shared.progress_print_out)\r\n \r\ndiff --git a/modules/processing.py b/modules/processing.py\nindex 8da738842cf..d7df5db032a 100644\n--- a/modules/processing.py\n+++ b/modules/processing.py\n@@ -184,6 +184,8 @@ def __init__(self, sd_model=None, outpath_samples=None, outpath_grids=None, prom\n         self.uc = None\r\n         self.c = None\r\n \r\n+        self.user = None\r\n+\r\n     @property\r\n     def sd_model(self):\r\n         return shared.sd_model\r\n@@ -585,6 +587,7 @@ def create_infotext(p, all_prompts, all_seeds, all_subseeds, comments=None, iter\n         \"NGMS\": None if p.s_min_uncond == 0 else p.s_min_uncond,\r\n         **p.extra_generation_params,\r\n         \"Version\": program_version() if opts.add_version_to_infotext else None,\r\n+        \"User\": p.user if opts.add_user_name_to_info else None,\r\n     }\r\n \r\n     generation_params_text = \", \".join([k if k == v else f'{k}: {generation_parameters_copypaste.quote(v)}' for k, v in generation_params.items() if v is not None])\r\ndiff --git a/modules/shared.py b/modules/shared.py\nindex a0862055409..da4918b5c77 100644\n--- a/modules/shared.py\n+++ b/modules/shared.py\n@@ -493,6 +493,7 @@ def list_samplers():\n options_templates.update(options_section(('infotext', \"Infotext\"), {\r\n     \"add_model_hash_to_info\": OptionInfo(True, \"Add model hash to generation information\"),\r\n     \"add_model_name_to_info\": OptionInfo(True, \"Add model name to generation information\"),\r\n+    \"add_user_name_to_info\": OptionInfo(False, \"Add user name to generation information when authenticated\"),\r\n     \"add_version_to_infotext\": OptionInfo(True, \"Add program version to generation information\"),\r\n     \"disable_weights_auto_swap\": OptionInfo(True, \"Disregard checkpoint information from pasted infotext\").info(\"when reading generation parameters from text into UI\"),\r\n     \"infotext_styles\": OptionInfo(\"Apply if any\", \"Infer styles from prompts of pasted infotext\", gr.Radio, {\"choices\": [\"Ignore\", \"Apply\", \"Discard\", \"Apply if any\"]}).info(\"when reading generation parameters from text into UI)\").html(\"\"\"<ul style='margin-left: 1.5em'>\r\ndiff --git a/modules/txt2img.py b/modules/txt2img.py\nindex 2e7d202d7b0..6aa79f23927 100644\n--- a/modules/txt2img.py\n+++ b/modules/txt2img.py\n@@ -4,10 +4,10 @@\n from modules.shared import opts, cmd_opts\r\n import modules.shared as shared\r\n from modules.ui import plaintext_to_html\r\n+import gradio as gr\r\n \r\n \r\n-\r\n-def txt2img(id_task: str, prompt: str, negative_prompt: str, prompt_styles, steps: int, sampler_index: int, restore_faces: bool, tiling: bool, n_iter: int, batch_size: int, cfg_scale: float, seed: int, subseed: int, subseed_strength: float, seed_resize_from_h: int, seed_resize_from_w: int, seed_enable_extras: bool, height: int, width: int, enable_hr: bool, denoising_strength: float, hr_scale: float, hr_upscaler: str, hr_second_pass_steps: int, hr_resize_x: int, hr_resize_y: int, hr_sampler_index: int, hr_prompt: str, hr_negative_prompt, override_settings_texts, *args):\r\n+def txt2img(id_task: str, prompt: str, negative_prompt: str, prompt_styles, steps: int, sampler_index: int, restore_faces: bool, tiling: bool, n_iter: int, batch_size: int, cfg_scale: float, seed: int, subseed: int, subseed_strength: float, seed_resize_from_h: int, seed_resize_from_w: int, seed_enable_extras: bool, height: int, width: int, enable_hr: bool, denoising_strength: float, hr_scale: float, hr_upscaler: str, hr_second_pass_steps: int, hr_resize_x: int, hr_resize_y: int, hr_sampler_index: int, hr_prompt: str, hr_negative_prompt, override_settings_texts, request: gr.Request, *args):\r\n     override_settings = create_override_settings_dict(override_settings_texts)\r\n \r\n     p = processing.StableDiffusionProcessingTxt2Img(\r\n@@ -48,6 +48,8 @@ def txt2img(id_task: str, prompt: str, negative_prompt: str, prompt_styles, step\n     p.scripts = modules.scripts.scripts_txt2img\r\n     p.script_args = args\r\n \r\n+    p.user = request.username\r\n+\r\n     if cmd_opts.enable_console_prompts:\r\n         print(f\"\\ntxt2img: {prompt}\", file=shared.progress_print_out)\r\n \r\n", "test_patch": "", "problem_statement": "[Feature Request]: New directory/filename pattern tag that uses the username from --gradio-auth (or --api-auth)\n### Is there an existing issue for this?\r\n\r\n- [X] I have searched the existing issues and checked the recent builds/commits\r\n\r\n### What would your feature do ?\r\n\r\nAdds a new tag called something like [auth_username] to the list of tags listed here [features#filenames-format](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Features#filenames-format)\r\n\r\nThis would allow files to be placed into a predictable sub-directory based on the logged in user's name and filenames can include it as well if the user wants\r\n\r\n### Proposed workflow\r\n\r\n1. Set the image output directory or filename pattern to [auth_username]\r\n2. When using --gradio-auth images will be placed into a folder with the name of the username/account that job was started from.\r\n3. If --gradio-auth is not in use and the tag is used in the path/filename then the folder/filename will be set to \"guest\" or \"unauthenticated\"\r\n\r\n### Additional information\r\n\r\nI was going to implement this myself but there are so many files referencing each other in later versions I would need someone more familiar with the current structure of the ui to point me to the files where the tags are stored.\r\n\r\nI could also implement this a as a script for the ui that copies the output image(s) to correct folder after generation but I don't believe the username is accessible from scripts. (If it is let me know)\r\n\r\nIf someone would like to take this one I am fine with that as well.\n", "hints_text": "My feature request could be included with this one as well #10112 \nI was able to see what file needed to change for this recent pr #10133 so I will see about accessing the username\nLooks like we can get the current user with `gradio.Request` per [request-header](https://gradio.app/docs/#request-header)\r\n\r\nFrom my understanding of how gradio works it must be in a function called from the gradio ui so I'm just trying to decide on the most direct way to get the result to `images.py`\r\nI think getting it into processing.py and passing it to `images.save_image()` would be the cleanest, as it looks that is what does most (maybe all?) of the image saving.", "created_at": "2023-06-14T19:40:00Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 11123, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-11123", "issue_numbers": ["11098"], "base_commit": "cf28aed1a71ad8d9da5c6b44e0196c0629a4b246", "patch": "diff --git a/extensions-builtin/Lora/lora.py b/extensions-builtin/Lora/lora.py\nindex af93991cbeb..34ff57dd0e7 100644\n--- a/extensions-builtin/Lora/lora.py\n+++ b/extensions-builtin/Lora/lora.py\n@@ -448,7 +448,11 @@ def list_available_loras():\n             continue\r\n \r\n         name = os.path.splitext(os.path.basename(filename))[0]\r\n-        entry = LoraOnDisk(name, filename)\r\n+        try:\r\n+            entry = LoraOnDisk(name, filename)\r\n+        except OSError:  # should catch FileNotFoundError and PermissionError etc.\r\n+            errors.report(f\"Failed to load LoRA {name} from {filename}\", exc_info=True)\r\n+            continue\r\n \r\n         available_loras[name] = entry\r\n \r\n", "test_patch": "", "problem_statement": "[Bug]: FileNotFoundError on broken symlinks\n### Is there an existing issue for this?\r\n\r\n- [X] I have searched the existing issues and checked the recent builds/commits\r\n\r\n### What happened?\r\n\r\nI am getting an uncaught `FileNotFound` Exception for symlinks that are broken.\r\n\r\n### Steps to reproduce the problem\r\n\r\n1. Have a symlink in `models/Lora`\r\n2. Delete the file the link is pointing to\r\n\r\nhttps://github.com/AUTOMATIC1111/stable-diffusion-webui/blob/39ec4f06ffb2c26e1298b2c5d80874dc3fd693ac/modules/hashes.py#L51\r\n\r\nraises a `FileNotFound` exception. Either the webui relies on the directory listing implying that the files are accessible or there is an exists check somewhere which probably would succeed for broken symlinks, even when checking the mtime does not.\r\n\r\n### What should have happened?\r\n\r\nProbably the best way would be to just handle the exception and remove the file from the list. Possibly the console could print a warning.\r\n\r\n### Commit where the problem happens\r\n\r\n39ec4f06ffb2c26e1298b2c5d80874dc3fd693ac\r\n\r\n### What Python version are you running on ?\r\n\r\nPython 3.10.x\r\n\r\n### Console logs\r\n\r\n```Shell\r\n...\r\n  File \"/usr/lib/python3.10/genericpath.py\", line 55, in getmtime\r\n    return os.stat(filename).st_mtime\r\nFileNotFoundError: [Errno 2] No such file or directory: '/xxx/models/Lora/other/brokensymlink.safetensors'\r\n```\n", "hints_text": "Is... there some reason for not just removing the broken symlink?\nProbably not, but the issue happens before you delete the symlink.\r\n\r\n- Have a folder \"mylora\" somewhere\r\n- Symlink things to \"models/Lora\" in the webui folder\r\n- Rename \"ghlv23.safetensors\" to \"githublogo_v23.safetensors\" in \"mylora\"\r\n- Start Webui and get an exception\r\n\r\nOf course, the problem is solved by removing the symlink and restarting webui, but I think an unhandled exception is bug that should be fixed, even when it is triggered by an easy to resolve edge case.", "created_at": "2023-06-09T10:28:51Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 11066, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-11066", "issue_numbers": ["10022"], "base_commit": "a009fe15fd98b44aede07c47bf7938fb4280924b", "patch": "diff --git a/modules/sd_hijack_optimizations.py b/modules/sd_hijack_optimizations.py\nindex 80e48a422e0..6464ca8ef60 100644\n--- a/modules/sd_hijack_optimizations.py\n+++ b/modules/sd_hijack_optimizations.py\n@@ -605,7 +605,7 @@ def sdp_attnblock_forward(self, x):\n     q, k, v = (rearrange(t, 'b c h w -> b (h w) c') for t in (q, k, v))\r\n     dtype = q.dtype\r\n     if shared.opts.upcast_attn:\r\n-        q, k = q.float(), k.float()\r\n+        q, k, v = q.float(), k.float(), v.float()\r\n     q = q.contiguous()\r\n     k = k.contiguous()\r\n     v = v.contiguous()\r\n", "test_patch": "", "problem_statement": "[Bug]: SDPA doesn't work\n### Is there an existing issue for this?\n\n- [X] I have searched the existing issues and checked the recent builds/commits\n\n### What happened?\n\nSDPA (one of the optimizations, --opt-sdp-attention or --opt-sdp-no-mem-attention added to COMMANDLINE_ARGS) doesn't work - using txt2img ends with RuntimeError:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"D:\\tools\\Stable-Diffusion-web-UI\\modules\\call_queue.py\", line 57, in f\r\n    res = list(func(*args, **kwargs))\r\n  File \"D:\\tools\\Stable-Diffusion-web-UI\\modules\\call_queue.py\", line 37, in f\r\n    res = func(*args, **kwargs)\r\n  File \"D:\\tools\\Stable-Diffusion-web-UI\\modules\\txt2img.py\", line 56, in txt2img\r\n    processed = process_images(p)\r\n  File \"D:\\tools\\Stable-Diffusion-web-UI\\modules\\processing.py\", line 515, in process_images\r\n    res = process_images_inner(p)\r\n  File \"D:\\tools\\Stable-Diffusion-web-UI\\modules\\processing.py\", line 671, in process_images_inner\r\n    x_samples_ddim = [decode_first_stage(p.sd_model, samples_ddim[i:i+1].to(dtype=devices.dtype_vae))[0].cpu() for i in range(samples_ddim.size(0))]\r\n  File \"D:\\tools\\Stable-Diffusion-web-UI\\modules\\processing.py\", line 671, in <listcomp>\r\n    x_samples_ddim = [decode_first_stage(p.sd_model, samples_ddim[i:i+1].to(dtype=devices.dtype_vae))[0].cpu() for i in range(samples_ddim.size(0))]\r\n  File \"D:\\tools\\Stable-Diffusion-web-UI\\modules\\processing.py\", line 444, in decode_first_stage\r\n    x = model.decode_first_stage(x)\r\n  File \"D:\\tools\\Stable-Diffusion-web-UI\\modules\\sd_hijack_utils.py\", line 17, in <lambda>\r\n    setattr(resolved_obj, func_path[-1], lambda *args, **kwargs: self(*args, **kwargs))\r\n  File \"D:\\tools\\Stable-Diffusion-web-UI\\modules\\sd_hijack_utils.py\", line 28, in __call__\r\n    return self.__orig_func(*args, **kwargs)\r\n  File \"D:\\anaconda3\\lib\\site-packages\\torch\\utils\\_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"D:\\tools\\Stable-Diffusion-web-UI\\repositories\\stable-diffusion-stability-ai\\ldm\\models\\diffusion\\ddpm.py\", line 826, in decode_first_stage\r\n    return self.first_stage_model.decode(z)\r\n  File \"D:\\tools\\Stable-Diffusion-web-UI\\repositories\\stable-diffusion-stability-ai\\ldm\\models\\autoencoder.py\", line 90, in decode\r\n    dec = self.decoder(z)\r\n  File \"D:\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"D:\\tools\\Stable-Diffusion-web-UI\\repositories\\stable-diffusion-stability-ai\\ldm\\modules\\diffusionmodules\\model.py\", line 631, in forward\r\n    h = self.mid.attn_1(h)\r\n  File \"D:\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"D:\\tools\\Stable-Diffusion-web-UI\\modules\\sd_hijack_optimizations.py\", line 490, in sdp_attnblock_forward\r\n    out = torch.nn.functional.scaled_dot_product_attention(q, k, v, dropout_p=0.0, is_causal=False)\r\nRuntimeError: Expected query, key, and value to have the same dtype, but got query.dtype: float key.dtype: float and value.dtype: struct c10::Half instead.\r\n```\n\n### Steps to reproduce the problem\n\n1. Add --opt-sdp-attention or --opt-sdp-no-mem-attention to COMMANDLINE_ARGS (in webui-user.bat)\r\n2. Launch webui-user.bat.\r\n3. Try to generate any image using txt2img.\r\n\r\nNo problems without SDPA.\n\n### What should have happened?\n\nBoth SDPA options should work (txt2img should be able to generate images).\n\n### Commit where the problem happens\n\nhttps://github.com/AUTOMATIC1111/stable-diffusion-webui/commit/72cd27a13587c9579942577e9e3880778be195f6\n\n### What platforms do you use to access the UI ?\n\nWindows\n\n### What browsers do you use to access the UI ?\n\nMicrosoft Edge\n\n### Command Line Arguments\n\n```Shell\n--opt-sdp-attention\n```\n\n\n### List of extensions\n\nNo\n\n### Console logs\n\n```Shell\n(base) D:\\tools\\Stable-Diffusion-web-UI>webui-user-sdpa.bat\r\nPython 3.10.10 | packaged by Anaconda, Inc. | (main, Mar 21 2023, 18:39:17) [MSC v.1916 64 bit (AMD64)]\r\nCommit hash: 72cd27a13587c9579942577e9e3880778be195f6\r\nInstalling requirements\r\nLaunching Web UI with arguments: --opt-sdp-attention\r\nNo module 'xformers'. Proceeding without it.\r\n*** \"Disable all extensions\" option was set, will only load built-in extensions ***\r\nLoading weights [009eed2ef1] from D:\\tools\\Stable-Diffusion-web-UI\\models\\Stable-diffusion\\v1-5-pruned.safetensors\r\nCreating model from config: D:\\tools\\Stable-Diffusion-web-UI\\configs\\v1-inference.yaml\r\nLatentDiffusion: Running in eps-prediction mode\r\nDiffusionWrapper has 859.52 M params.\r\nApplying scaled dot product cross attention optimization.\r\nTextual inversion embeddings loaded(17): bad-artist, bad-artist-anime, bad-hands-5, bad-image-9600, bad-image-v2-11000, bad-image-v2-27000, bad-image-v2-39000, bad_prompt, bad_prompt_version2, bad_quality, boring_e621, EasyNegative, EasyNegativeV2, ng_deepnegative_v1_32t, ng_deepnegative_v1_64t, ng_deepnegative_v1_75t, pureerosface_v1\r\nTextual inversion embeddings skipped(15): GTA768, InkPunk768, InkPunkHeavy768, InkPunkLandscapes768, InkPunkLite768, nartfixer, Neg_Facelift768, nfixer, nrealfixer, PaintStyle4, pinup768, rev2-badprompt, SCG768-Euphoria, SCG768-Nebula, SDA768\r\nModel loaded in 2.4s (load weights from disk: 0.3s, create model: 0.3s, apply weights to model: 0.3s, apply half(): 0.3s, move model to device: 0.3s, load textual inversion embeddings: 0.8s).\r\nRunning on local URL:  http://127.0.0.1:7860\r\n\r\nTo create a public link, set `share=True` in `launch()`.\r\nStartup time: 14.4s (import torch: 3.6s, import gradio: 0.9s, import ldm: 0.4s, other imports: 0.8s, list SD models: 0.4s, load scripts: 1.0s, load SD checkpoint: 2.6s, create ui: 2.5s, gradio launch: 2.2s).\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [00:02<00:00,  9.08it/s]\r\nError completing request\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f       | 19/20 [00:00<00:00, 22.13it/s]\r\nArguments: ('task(djamhlamkhsdoh9)', 'whatever', '', [], 20, 0, False, False, 1, 1, 7, 1.0, -1.0, 0, 0, 0, False, 512, 512, False, 0.7, 2, 'Latent', 0, 0, 0, [], 0, False, False, 'positive', 'comma', 0, False, False, '', 1, '', [], 0, '', [], 0, '', [], True, False, False, False, 0) {}\r\nTraceback (most recent call last):\r\n  File \"D:\\tools\\Stable-Diffusion-web-UI\\modules\\call_queue.py\", line 57, in f\r\n    res = list(func(*args, **kwargs))\r\n  File \"D:\\tools\\Stable-Diffusion-web-UI\\modules\\call_queue.py\", line 37, in f\r\n    res = func(*args, **kwargs)\r\n  File \"D:\\tools\\Stable-Diffusion-web-UI\\modules\\txt2img.py\", line 56, in txt2img\r\n    processed = process_images(p)\r\n  File \"D:\\tools\\Stable-Diffusion-web-UI\\modules\\processing.py\", line 515, in process_images\r\n    res = process_images_inner(p)\r\n  File \"D:\\tools\\Stable-Diffusion-web-UI\\modules\\processing.py\", line 671, in process_images_inner\r\n    x_samples_ddim = [decode_first_stage(p.sd_model, samples_ddim[i:i+1].to(dtype=devices.dtype_vae))[0].cpu() for i in range(samples_ddim.size(0))]\r\n  File \"D:\\tools\\Stable-Diffusion-web-UI\\modules\\processing.py\", line 671, in <listcomp>\r\n    x_samples_ddim = [decode_first_stage(p.sd_model, samples_ddim[i:i+1].to(dtype=devices.dtype_vae))[0].cpu() for i in range(samples_ddim.size(0))]\r\n  File \"D:\\tools\\Stable-Diffusion-web-UI\\modules\\processing.py\", line 444, in decode_first_stage\r\n    x = model.decode_first_stage(x)\r\n  File \"D:\\tools\\Stable-Diffusion-web-UI\\modules\\sd_hijack_utils.py\", line 17, in <lambda>\r\n    setattr(resolved_obj, func_path[-1], lambda *args, **kwargs: self(*args, **kwargs))\r\n  File \"D:\\tools\\Stable-Diffusion-web-UI\\modules\\sd_hijack_utils.py\", line 28, in __call__\r\n    return self.__orig_func(*args, **kwargs)\r\n  File \"D:\\anaconda3\\lib\\site-packages\\torch\\utils\\_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"D:\\tools\\Stable-Diffusion-web-UI\\repositories\\stable-diffusion-stability-ai\\ldm\\models\\diffusion\\ddpm.py\", line 826, in decode_first_stage\r\n    return self.first_stage_model.decode(z)\r\n  File \"D:\\tools\\Stable-Diffusion-web-UI\\repositories\\stable-diffusion-stability-ai\\ldm\\models\\autoencoder.py\", line 90, in decode\r\n    dec = self.decoder(z)\r\n  File \"D:\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"D:\\tools\\Stable-Diffusion-web-UI\\repositories\\stable-diffusion-stability-ai\\ldm\\modules\\diffusionmodules\\model.py\", line 631, in forward\r\n    h = self.mid.attn_1(h)\r\n  File \"D:\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"D:\\tools\\Stable-Diffusion-web-UI\\modules\\sd_hijack_optimizations.py\", line 490, in sdp_attnblock_forward\r\n    out = torch.nn.functional.scaled_dot_product_attention(q, k, v, dropout_p=0.0, is_causal=False)\r\nRuntimeError: Expected query, key, and value to have the same dtype, but got query.dtype: float key.dtype: float and value.dtype: struct c10::Half instead.\r\n\r\n\r\nTotal progress: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [00:18<00:00, 22.13it/s]\n```\n\n\n### Additional information\n\n_No response_\n", "hints_text": "What's the graphics card you're using?\n@ClashSAN RTX 4080\nPS I am not sure if it matters (it didn't cause any issues w/o SDPA), but as there is something about types in the error message I will mention it, just in case - I am using txt2img with fp16 models (converted using sd-webui-model-converter).\nI also find this error, the same use SDPA, when i use xformers can finished\u3002\r\n\r\nRXT4090\r\n\r\n2.0.0+cu118 autocast  half\r\nStable Diffusion: [cf1d67a] 2023-03-25\r\nTaming Transformers: [2426893] 2022-01-13\r\nCodeFormer: [c5b4593] 2022-09-09\r\nBLIP: [48211a1] 2022-06-07\r\nk_diffusion: [5b3af03] 2022-11-23\r\n\ngetting this same error since update earlier today\r\n`RuntimeError: Expected query, key, and value to have the same dtype,`\r\n\r\n3080, using `COMMANDLINE_ARGS=--opt-sdp-no-mem-attention --api`\r\n\r\nit was fine yesterday\nI'm getting this error today after pulling 1.3.0\r\n\r\n```\r\n Expected query, key, and value to have the same dtype,\r\nbut got query.dtype: float key.dtype: float and value.dtype: struct c10::Half instead\r\n```\nDoes it work if you turn off Upcast in settings->Stable Diffusion?\nI got it fixed by any of following\r\n1. Turning off xformers in A111 setting of Cross Attention Optimization.\r\n2. Turning off 32 bit upcasting.\r\n3. Keeping 32 upcasting with Doggetx Cross Attention Optimization.", "created_at": "2023-06-06T20:48:35Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 11048, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-11048", "issue_numbers": ["10873"], "base_commit": "a009fe15fd98b44aede07c47bf7938fb4280924b", "patch": "diff --git a/webui.sh b/webui.sh\nindex 6c48e9699a9..5c8d977cb2d 100755\n--- a/webui.sh\n+++ b/webui.sh\n@@ -114,7 +114,22 @@ fi\n # Check prerequisites\n gpu_info=$(lspci 2>/dev/null | grep -E \"VGA|Display\")\n case \"$gpu_info\" in\n-    *\"Navi 1\"*|*\"Navi 2\"*) export HSA_OVERRIDE_GFX_VERSION=10.3.0\n+    *\"Navi 1\"*)\n+        export HSA_OVERRIDE_GFX_VERSION=10.3.0\n+        if [[ -z \"${TORCH_COMMAND}\" ]]\n+        then\n+            pyv=\"$(${python_cmd} -c 'import sys; print(\".\".join(map(str, sys.version_info[0:2])))')\"\n+            if [[ $(bc <<< \"$pyv <= 3.10\") -eq 1 ]] \n+            then\n+                # Navi users will still use torch 1.13 because 2.0 does not seem to work.\n+                export TORCH_COMMAND=\"pip install torch==1.13.1+rocm5.2 torchvision==0.14.1+rocm5.2 --index-url https://download.pytorch.org/whl/rocm5.2\"\n+            else\n+                printf \"\\e[1m\\e[31mERROR: RX 5000 series GPUs must be using at max python 3.10, aborting...\\e[0m\"\n+                exit 1\n+            fi\n+        fi\n+    ;;\n+    *\"Navi 2\"*) export HSA_OVERRIDE_GFX_VERSION=10.3.0\n     ;;\n     *\"Renoir\"*) export HSA_OVERRIDE_GFX_VERSION=9.0.0\n         printf \"\\n%s\\n\" \"${delimiter}\"\n", "test_patch": "", "problem_statement": "[Bug]: gfx906 ROCM won't work with  torch: 2.0.1+rocm5.4.2 but works with other AIs\n### Is there an existing issue for this?\n\n- [X] I have searched the existing issues and checked the recent builds/commits\n\n### What happened?\n\nTrying to make webui work with pytourch 2.0.1 + rocm 5.4.2 but It won't work.\n\n### Steps to reproduce the problem\n\n1. Download last dev.\r\n2. Extract it\r\n3. Run webui.sh\r\n4. Generate.\r\n5. terminal fill with it.\n\n### What should have happened?\n\nIt should be generate like normal\n\n### Commit where the problem happens\n\n b957dcf\n\n### What Python version are you running on ?\n\nPython 3.10.x\n\n### What platforms do you use to access the UI ?\n\nLinux\n\n### What device are you running WebUI on?\n\nAMD GPUs (RX 6000 above), AMD GPUs (RX 5000 below)\n\n### What browsers do you use to access the UI ?\n\nMozilla Firefox\n\n### Command Line Arguments\n\n```Shell\nDirectly webui.sh give me that terminal errors.\r\nBut if I run with this\r\n--no-half --disable-nan-check\r\nIt render black.\n```\n\n\n### List of extensions\n\nNo extra extensions directly from github\n\n### Console logs\n\n```Shell\nCalculating sha256 for /media/bcansin/ai/ai/mem/stable-diffusion-webui-dev/stable-diffusion-webui/models/Stable-diffusion/v1-5-pruned-emaonly.safetensors: Running on local URL:  http://127.0.0.1:7860\r\n\r\nTo create a public link, set `share=True` in `launch()`.\r\nStartup time: 807.5s (import torch: 5.4s, import gradio: 3.2s, import ldm: 3.7s, other imports: 3.0s, setup codeformer: 0.3s, list SD models: 781.1s, load scripts: 9.8s, create ui: 0.7s, gradio launch: 0.2s).\r\n6ce0161689b3853acaa03779ec93eafe75a02f4ced659bee03f50797806fa2fa\r\nLoading weights [6ce0161689] from /media/bcansin/ai/ai/mem/stable-diffusion-webui-dev/stable-diffusion-webui/models/Stable-diffusion/v1-5-pruned-emaonly.safetensors\r\nCreating model from config: /media/bcansin/ai/ai/mem/stable-diffusion-webui-dev/stable-diffusion-webui/configs/v1-inference.yaml\r\nLatentDiffusion: Running in eps-prediction mode\r\nDiffusionWrapper has 859.52 M params.\r\nApplying optimization: sdp-no-mem... done.\r\nTextual inversion embeddings loaded(0): \r\nModel loaded in 41.0s (calculate hash: 14.1s, load weights from disk: 0.3s, create model: 1.6s, apply weights to model: 18.6s, apply half(): 1.9s, load VAE: 2.8s, move model to device: 0.8s, load textual inversion embeddings: 0.9s).\r\n  0%|                                                    | 0/20 [00:02<?, ?it/s]\r\nError completing request\r\nArguments: ('task(i3jff5ltdmmulfl)', 'miku', '', [], 20, 0, False, False, 1, 1, 7, -1.0, -1.0, 0, 0, 0, False, 512, 512, False, 0.7, 2, 'Latent', 0, 0, 0, 0, '', '', [], 0, False, False, 'positive', 'comma', 0, False, False, '', 1, '', [], 0, '', [], 0, '', [], True, False, False, False, 0) {}\r\nTraceback (most recent call last):\r\n  File \"/media/bcansin/ai/ai/mem/stable-diffusion-webui-dev/stable-diffusion-webui/modules/call_queue.py\", line 57, in f\r\n    res = list(func(*args, **kwargs))\r\n  File \"/media/bcansin/ai/ai/mem/stable-diffusion-webui-dev/stable-diffusion-webui/modules/call_queue.py\", line 37, in f\r\n    res = func(*args, **kwargs)\r\n  File \"/media/bcansin/ai/ai/mem/stable-diffusion-webui-dev/stable-diffusion-webui/modules/txt2img.py\", line 57, in txt2img\r\n    processed = processing.process_images(p)\r\n  File \"/media/bcansin/ai/ai/mem/stable-diffusion-webui-dev/stable-diffusion-webui/modules/processing.py\", line 611, in process_images\r\n    res = process_images_inner(p)\r\n  File \"/media/bcansin/ai/ai/mem/stable-diffusion-webui-dev/stable-diffusion-webui/modules/processing.py\", line 729, in process_images_inner\r\n    samples_ddim = p.sample(conditioning=p.c, unconditional_conditioning=p.uc, seeds=p.seeds, subseeds=p.subseeds, subseed_strength=p.subseed_strength, prompts=p.prompts)\r\n  File \"/media/bcansin/ai/ai/mem/stable-diffusion-webui-dev/stable-diffusion-webui/modules/processing.py\", line 977, in sample\r\n    samples = self.sampler.sample(self, x, conditioning, unconditional_conditioning, image_conditioning=self.txt2img_image_conditioning(x))\r\n  File \"/media/bcansin/ai/ai/mem/stable-diffusion-webui-dev/stable-diffusion-webui/modules/sd_samplers_kdiffusion.py\", line 383, in sample\r\n    samples = self.launch_sampling(steps, lambda: self.func(self.model_wrap_cfg, x, extra_args={\r\n  File \"/media/bcansin/ai/ai/mem/stable-diffusion-webui-dev/stable-diffusion-webui/modules/sd_samplers_kdiffusion.py\", line 257, in launch_sampling\r\n    return func()\r\n  File \"/media/bcansin/ai/ai/mem/stable-diffusion-webui-dev/stable-diffusion-webui/modules/sd_samplers_kdiffusion.py\", line 383, in <lambda>\r\n    samples = self.launch_sampling(steps, lambda: self.func(self.model_wrap_cfg, x, extra_args={\r\n  File \"/media/bcansin/ai/ai/mem/stable-diffusion-webui-dev/stable-diffusion-webui/venv/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/media/bcansin/ai/ai/mem/stable-diffusion-webui-dev/stable-diffusion-webui/repositories/k-diffusion/k_diffusion/sampling.py\", line 145, in sample_euler_ancestral\r\n    denoised = model(x, sigmas[i] * s_in, **extra_args)\r\n  File \"/media/bcansin/ai/ai/mem/stable-diffusion-webui-dev/stable-diffusion-webui/venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/media/bcansin/ai/ai/mem/stable-diffusion-webui-dev/stable-diffusion-webui/modules/sd_samplers_kdiffusion.py\", line 169, in forward\r\n    devices.test_for_nans(x_out, \"unet\")\r\n  File \"/media/bcansin/ai/ai/mem/stable-diffusion-webui-dev/stable-diffusion-webui/modules/devices.py\", line 156, in test_for_nans\r\n    raise NansException(message)\r\nmodules.devices.NansException: A tensor with all NaNs was produced in Unet. This could be either because there's not enough precision to represent the picture, or because your video card does not support half type. Try setting the \"Upcast cross attention layer to float32\" option in Settings > Stable Diffusion or using the --no-half commandline argument to fix this. Use --disable-nan-check commandline argument to disable this check.\r\n\r\n\r\n\r\n\r\nAlso with --no-half --disable-nan-check but it render black.\r\n\r\n\r\n\r\nCreating model from config: /media/bcansin/ai/ai/mem/stable-diffusion-webui-dev/stable-diffusion-webui/configs/v1-inference.yaml\r\nLatentDiffusion: Running in eps-prediction mode\r\nRunning on local URL:  http://127.0.0.1:7860\r\n\r\nTo create a public link, set `share=True` in `launch()`.\r\nStartup time: 24.9s (import torch: 5.1s, import gradio: 2.7s, import ldm: 3.7s, other imports: 2.5s, setup codeformer: 0.2s, load scripts: 9.6s, create ui: 0.7s, gradio launch: 0.3s).\r\nDiffusionWrapper has 859.52 M params.\r\nApplying optimization: sdp-no-mem... done.\r\nTextual inversion embeddings loaded(0): \r\nModel loaded in 5.7s (load weights from disk: 0.9s, create model: 1.3s, apply weights to model: 1.7s, load VAE: 0.3s, move model to device: 1.3s, load textual inversion embeddings: 0.1s).\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [00:05<00:00,  3.42it/s]\r\nTotal progress: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [00:04<00:00,  4.26it/s]\r\nTotal progress: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [00:04<00:00,  4.25it/s]\n```\n\n\n### Additional information\n\nI got AMD Radeon\u2122 VII ( GFX9 GPUs  gfx906 Vega 20 ) and installed ROCM 5.5\r\nAnd I only got problem with webui other AI works perfecly with \r\neven other AI in same system I use for webui work direcly with this:\r\npip3 install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/rocm5.4.2\r\n\r\nAlso I read this too. This is why I typing.\r\nhttps://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/10465\r\n\r\nPlease fix it I really want to get last pythourch and rocm version I Stuck with this\r\npip install torch==1.13.0+rocm5.2 torchvision==0.14.0+rocm5.2 torchaudio==0.13.0 --extra-index-url https://download.pytorch.org/whl/rocm5.2\r\n\r\nI begging for help at this point. Please help me. I gave my days to make it work but every time I try I fail.\n", "hints_text": "What do you mean for \"Other AIs\"? other UIs for StableDiffusion? or something different, like oobabooga's  text-generation-webui?", "created_at": "2023-06-05T23:22:19Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 10783, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-10783", "issue_numbers": ["10566"], "base_commit": "881de0df38c1fa6d0d61f7bc6fc93c100a9f35d0", "patch": "diff --git a/requirements.txt b/requirements.txt\nindex a464447bcfa..3142085eaf4 100644\n--- a/requirements.txt\n+++ b/requirements.txt\n@@ -1,32 +1,32 @@\n-astunparse\r\n-blendmodes\r\n+GitPython\r\n+Pillow\r\n accelerate\r\n+\r\n basicsr\r\n+blendmodes\r\n+clean-fid\r\n+einops\r\n gfpgan\r\n gradio==3.32.0\r\n+inflection\r\n+jsonmerge\r\n+kornia\r\n+lark\r\n numpy\r\n omegaconf\r\n-opencv-contrib-python\r\n-requests\r\n+\r\n piexif\r\n-Pillow\r\n-pytorch_lightning==1.7.7\r\n+psutil\r\n+pytorch_lightning\r\n realesrgan\r\n+requests\r\n+resize-right\r\n+\r\n+safetensors\r\n scikit-image>=0.19\r\n-timm==0.4.12\r\n-transformers==4.25.1\r\n+timm\r\n+tomesd\r\n torch\r\n-einops\r\n-jsonmerge\r\n-clean-fid\r\n-resize-right\r\n torchdiffeq\r\n-kornia\r\n-lark\r\n-inflection\r\n-GitPython\r\n torchsde\r\n-safetensors\r\n-psutil\r\n-rich\r\n-tomesd\r\n+transformers==4.25.1\r\ndiff --git a/requirements_versions.txt b/requirements_versions.txt\nindex b2a3a3276e4..f71b9d6c555 100644\n--- a/requirements_versions.txt\n+++ b/requirements_versions.txt\n@@ -1,29 +1,30 @@\n-blendmodes==2022\r\n-transformers==4.29.2\r\n+GitPython==3.1.30\r\n+Pillow==9.5.0\r\n accelerate==0.18.0\r\n basicsr==1.4.2\r\n+blendmodes==2022\r\n+clean-fid==0.1.35\r\n+einops==0.4.1\r\n+fastapi==0.94.0\r\n gfpgan==1.3.8\r\n gradio==3.32.0\r\n+httpcore<=0.15\r\n+inflection==0.5.1\r\n+jsonmerge==1.8.0\r\n+kornia==0.6.7\r\n+lark==1.1.2\r\n numpy==1.23.5\r\n-Pillow==9.5.0\r\n-realesrgan==0.3.0\r\n-torch\r\n omegaconf==2.2.3\r\n+piexif==1.1.3\r\n+psutil~=5.9.5\r\n pytorch_lightning==1.9.4\r\n+realesrgan==0.3.0\r\n+resize-right==0.0.2\r\n+safetensors==0.3.1\r\n scikit-image==0.20.0\r\n timm==0.6.7\r\n-piexif==1.1.3\r\n-einops==0.4.1\r\n-jsonmerge==1.8.0\r\n-clean-fid==0.1.35\r\n-resize-right==0.0.2\r\n+tomesd==0.1.2\r\n+torch\r\n torchdiffeq==0.2.3\r\n-kornia==0.6.7\r\n-lark==1.1.2\r\n-inflection==0.5.1\r\n-GitPython==3.1.30\r\n torchsde==0.2.5\r\n-safetensors==0.3.1\r\n-httpcore<=0.15\r\n-fastapi==0.94.0\r\n-tomesd==0.1.2\r\n+transformers==4.25.1\r\n", "test_patch": "", "problem_statement": "[Bug]: timm versions are different in requirments.txt and requirements_versions.txt\n### Is there an existing issue for this?\n\n- [X] I have searched the existing issues and checked the recent builds/commits\n\n### What happened?\n\ntimm versions are different in requirments.txt and requirements_versions.txt, and pip check shows \"blip-ci 0.0.3 has requirement timm==0.4.12, but you have timm 0.6.7.\"\r\n\r\n![image](https://github.com/AUTOMATIC1111/stable-diffusion-webui/assets/99337435/9a146874-de81-4f75-959c-fe8428a39b53)\r\n\r\n![image](https://github.com/AUTOMATIC1111/stable-diffusion-webui/assets/99337435/a5a103bd-e95d-4567-b9cf-deb0c1df3174)\r\n\r\n\n\n### Steps to reproduce the problem\n\n1. Activate the virtual environment\r\n2. pip check\n\n### What should have happened?\n\nI don't know \n\n### Commit where the problem happens\n\nversion: v1.2.1 \u2000\n\n### What platforms do you use to access the UI ?\n\n_No response_\n\n### What browsers do you use to access the UI ?\n\n_No response_\n\n### Command Line Arguments\n\n```Shell\nset COMMANDLINE_ARGS=--xformers --no-gradio-queue --vae-path \"D:\\stable-diffusion-webui\\models\\VAE\\vae-ft-mse-840000-ema-pruned.safetensors\"\n```\n\n\n### List of extensions\n\na1111-sd-webui-tagcomplete\t\r\nclip-interrogator-ext\t\r\nsd-extension-system-info\t\r\nsd-webui-controlnet\r\n\r\n\r\n\r\n\r\n\r\n\n\n### Console logs\n\n```Shell\nvenv \"D:\\stable-diffusion-webui\\venv\\Scripts\\Python.exe\"\r\nPython 3.10.9 | packaged by conda-forge | (main, Jan 11 2023, 15:15:40) [MSC v.1916 64 bit (AMD64)]\r\nVersion: v1.2.1\r\nCommit hash: 89f9faa63388756314e8a1d96cf86bf5e0663045\r\nInstalling requirements\r\n\r\n\r\n\r\nLaunching Web UI with arguments: --xformers --no-gradio-queue --vae-path D:\\stable-diffusion-webui\\models\\VAE\\vae-ft-mse-840000-ema-pruned.safetensors\r\nLoading weights [e6415c4892] from D:\\stable-diffusion-webui\\models\\Stable-diffusion\\realisticVisionV20_v20.safetensors\r\nCreating model from config: D:\\stable-diffusion-webui\\configs\\v1-inference.yaml\r\nLatentDiffusion: Running in eps-prediction mode\r\nDiffusionWrapper has 859.52 M params.\r\nLoading VAE weights from commandline argument: D:\\stable-diffusion-webui\\models\\VAE\\vae-ft-mse-840000-ema-pruned.safetensors\r\nApplying xformers cross attention optimization.\r\nTextual inversion embeddings loaded(1): PlanIt\r\nModel loaded in 3.3s (load weights from disk: 0.4s, create model: 0.5s, apply weights to model: 0.5s, apply half(): 0.5s, load VAE: 0.1s, move model to device: 0.5s, load textual inversion embeddings: 0.8s).\r\nControlNet v1.1.181\r\nControlNet v1.1.181\r\nRunning on local URL:  http://127.0.0.1:7860\r\n\r\nTo create a public link, set `share=True` in `launch()`.\r\nStartup time: 13.0s (import torch: 2.3s, import gradio: 1.3s, import ldm: 0.6s, other imports: 1.1s, load scripts: 5.8s, create ui: 1.7s, gradio launch: 0.1s).\n```\n\n\n### Additional information\n\nafter started webui-user.bat, timm version will always be 0.6.7, i can still draw, and i don't know the impact\r\n\n", "hints_text": "[`blip-ci`](https://pypi.org/project/blip-ci/) is not one of the webui's requirements, so it has likely been installed by another extension.\r\n\r\nIt indeed [has a hard requirement on](https://github.com/pharmapsychotic/BLIP/blob/main/requirements.txt#L1) `timm==0.4.12`, which produces that warning (not an error).", "created_at": "2023-05-28T12:41:26Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 10780, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-10780", "issue_numbers": ["10778"], "base_commit": "b957dcfece29c84ac0cfcd5a69475ff8684c531f", "patch": "diff --git a/modules/textual_inversion/image_embedding.py b/modules/textual_inversion/image_embedding.py\nindex 5858a55f57c..81cff7bf17c 100644\n--- a/modules/textual_inversion/image_embedding.py\n+++ b/modules/textual_inversion/image_embedding.py\n@@ -1,8 +1,10 @@\n import base64\r\n import json\r\n+import warnings\r\n+\r\n import numpy as np\r\n import zlib\r\n-from PIL import Image, ImageDraw, ImageFont\r\n+from PIL import Image, ImageDraw\r\n import torch\r\n \r\n \r\n@@ -129,14 +131,17 @@ def extract_image_data_embed(image):\n \r\n \r\n def caption_image_overlay(srcimage, title, footerLeft, footerMid, footerRight, textfont=None):\r\n+    from modules.images import get_font\r\n+    if textfont:\r\n+        warnings.warn(\r\n+            'passing in a textfont to caption_image_overlay is deprecated and does nothing',\r\n+            DeprecationWarning,\r\n+            stacklevel=2,\r\n+        )\r\n     from math import cos\r\n \r\n     image = srcimage.copy()\r\n     fontsize = 32\r\n-    if textfont is None:\r\n-        from modules.images import get_font\r\n-        textfont = get_font(fontsize)\r\n-\r\n     factor = 1.5\r\n     gradient = Image.new('RGBA', (1, image.size[1]), color=(0, 0, 0, 0))\r\n     for y in range(image.size[1]):\r\n@@ -147,12 +152,12 @@ def caption_image_overlay(srcimage, title, footerLeft, footerMid, footerRight, t\n \r\n     draw = ImageDraw.Draw(image)\r\n \r\n-    font = ImageFont.truetype(textfont, fontsize)\r\n+    font = get_font(fontsize)\r\n     padding = 10\r\n \r\n     _, _, w, h = draw.textbbox((0, 0), title, font=font)\r\n     fontsize = min(int(fontsize * (((image.size[0]*0.75)-(padding*4))/w)), 72)\r\n-    font = ImageFont.truetype(textfont, fontsize)\r\n+    font = get_font(fontsize)\r\n     _, _, w, h = draw.textbbox((0, 0), title, font=font)\r\n     draw.text((padding, padding), title, anchor='lt', font=font, fill=(255, 255, 255, 230))\r\n \r\n@@ -163,7 +168,7 @@ def caption_image_overlay(srcimage, title, footerLeft, footerMid, footerRight, t\n     _, _, w, h = draw.textbbox((0, 0), footerRight, font=font)\r\n     fontsize_right = min(int(fontsize * (((image.size[0]/3)-(padding))/w)), 72)\r\n \r\n-    font = ImageFont.truetype(textfont, min(fontsize_left, fontsize_mid, fontsize_right))\r\n+    font = get_font(min(fontsize_left, fontsize_mid, fontsize_right))\r\n \r\n     draw.text((padding, image.size[1]-padding),               footerLeft, anchor='ls', font=font, fill=(255, 255, 255, 230))\r\n     draw.text((image.size[0]/2, image.size[1]-padding),       footerMid, anchor='ms', font=font, fill=(255, 255, 255, 230))\r\n", "test_patch": "", "problem_statement": "[Bug]: Training embeddings stop after 1 epoch\n### Is there an existing issue for this?\n\n- [X] I have searched the existing issues and checked the recent builds/commits\n\n### What happened?\n\nTraining embeddings suddenly stopped working after the 1st epoch.\r\n\r\nAfter some digging, I  narrowed it down to the \"image_embeddings\", and perhaps a font issue. I was able to solve the issue (train embeds through completion) by unchecking \"Save images with embedding in PNG chunks\" \n\n### Steps to reproduce the problem\n\nTrain embeddings as usual, but they stop after the first epoch (error logs below)\n\n### What should have happened?\n\nTraining should've completed as per normal \n\n### Commit where the problem happens\n\n20ae71f\n\n### What Python version are you running on ?\n\nPython 3.10.x\n\n### What platforms do you use to access the UI ?\n\nLinux, Other/Cloud\n\n### What device are you running WebUI on?\n\nAMD GPUs (RX 6000 above)\n\n### What browsers do you use to access the UI ?\n\nGoogle Chrome\n\n### Command Line Arguments\n\n```Shell\nNo\n```\n\n\n### List of extensions\n\nAuto-Photoshop-StableDiffusion-Plugin\r\nSadTalker\r\nadetailer\r\nclip-interrogator-ext deforum-for-automatic1111-webui\r\ngif2gif openOutpaint-webUl-extension sd-dynamic-prompts\r\nsd-webui-controlnet\r\nsd-webui-infinite-image-browsing sd-webui-text2video\r\nsd_civitai_extension sd_web_ui_preset_utils stable-diffusion-webui-state\r\nultimate-upscale-for-automatic1111\r\nunprompted\r\nLDSR\r\nLora\r\nScuNET\r\nSwinIR\r\nprompt-bracket-checker\n\n### Console logs\n\n```Shell\nTraceback (most recent call last):\r\n  File \"/notebooks/sd/stable-diffusion-webui/modules/textual_inversion/textual_inversion.py\", line 612, in train_embedding\r\n    captioned_image = caption_image_overlay(image, title, footer_left, footer_mid, footer_right)\r\n  File \"/notebooks/sd/stable-diffusion-webui/modules/textual_inversion/image_embedding.py\", line 150, in caption_image_overlay\r\n    font = ImageFont.truetype(textfont, fontsize)\r\n  File \"/usr/local/lib/python3.9/dist-packages/PIL/ImageFont.py\", line 976, in truetype\r\n    return freetype(font)\r\n  File \"/usr/local/lib/python3.9/dist-packages/PIL/ImageFont.py\", line 973, in freetype\r\n    return FreeTypeFont(font, size, index, encoding, layout_engine)\r\n  File \"/usr/local/lib/python3.9/dist-packages/PIL/ImageFont.py\", line 253, in __init__\r\n    load_from_bytes(font)\r\n  File \"/usr/local/lib/python3.9/dist-packages/PIL/ImageFont.py\", line 233, in load_from_bytes\r\n    self.font_bytes = f.read()\r\nAttributeError: 'FreeTypeFont' object has no attribute 'read'\n```\n\n\n### Additional information\n\n_No response_\n", "hints_text": "> > What Python version are you running on ?\r\n>\r\n> Python 3.10.x\r\n\r\nand yet your traceback quite plainly says `/usr/local/lib/python3.9/`...\r\n\r\n\r\n\nAnyway, yeah, I found the bug, it's related to my changes in df7070eca22278b25c921ef72d3f97a221d66242. Will fix.", "created_at": "2023-05-28T11:50:24Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 10635, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-10635", "issue_numbers": ["10625"], "base_commit": "cc2f6e3b7b8d720a8e0fb2732751e34b7f41c2e9", "patch": "diff --git a/modules/api/api.py b/modules/api/api.py\nindex 9bb95dfd1a4..bfeec3856ba 100644\n--- a/modules/api/api.py\n+++ b/modules/api/api.py\n@@ -682,4 +682,4 @@ def get_memory(self):\n \n     def launch(self, server_name, port):\n         self.app.include_router(self.router)\n-        uvicorn.run(self.app, host=server_name, port=port)\n+        uvicorn.run(self.app, host=server_name, port=port, timeout_keep_alive=0)\n", "test_patch": "", "problem_statement": "[Bug]: h11._util.LocalProtocolError: Can't send data when our state is ERROR\n### Is there an existing issue for this?\r\n\r\n- [X] I have searched the existing issues and checked the recent builds/commits\r\n\r\n### What happened?\r\n\r\nIt very strong that when use webui , it could not see the problem, but when use API, it usually release this issue \r\n\r\nException in callback H11Protocol.timeout_keep_alive_handler()\r\nhandle: <TimerHandle when=30411.431255625 H11Protocol.timeout_keep_alive_handler()>\r\n\r\n  File \"/aml/stable-diffusion-webui/venv/lib/python3.10/site-packages/h11/_connection.py\", line 483, in send_with_data_passthrough\r\n    raise LocalProtocolError(\"Can't send data when our state is ERROR\")\r\n\r\n\r\n### Steps to reproduce the problem\r\n\r\nJust common call the API\r\n\r\n\r\n### What should have happened?\r\n\r\nIt is very stange, sometimes it will happen, but in some times , the same prompt, it could work good \r\n\r\n### Commit where the problem happens\r\n\r\nCommon call api\r\n\r\n### What platforms do you use to access the UI ?\r\n\r\nLinux\r\n\r\n### What browsers do you use to access the UI ?\r\n\r\nMicrosoft Edge\r\n\r\n### Command Line Arguments\r\n\r\n```Shell\r\n(no background:1.30)\uff0c(There is only 1 Africa black boy:1.4), (front_view, full_body:1.50), cartoon ,with a yellow Lakers basketball shirt and blue shorts standing with his hands in his pockets and his hair in the air\r\n, best quality, 8K,( extreme detail description:1.4), (sharp focus:1.4), <lora:zby-50k-000010:0.5>\r\n```\r\n\r\n\r\n### List of extensions\r\n\r\nNo extension\r\n\r\n### Console logs\r\n\r\n```Shell\r\n\r\nTraceback (most recent call last):\r\n  File \"/aml/stable-diffusion-webui/venv/lib/python3.10/site-packages/starlette/middleware/errors.py\", line 162, in __call__\r\n    await self.app(scope, receive, _send)\r\n  File \"/aml/stable-diffusion-webui/venv/lib/python3.10/site-packages/starlette/middleware/base.py\", line 109, in __call__\r\n    await response(scope, receive, send)\r\n  File \"/aml/stable-diffusion-webui/venv/lib/python3.10/site-packages/starlette/responses.py\", line 270, in __call__\r\n    async with anyio.create_task_group() as task_group:\r\n  File \"/aml/stable-diffusion-webui/venv/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 662, in __aexit__\r\n    raise exceptions[0]\r\n  File \"/aml/stable-diffusion-webui/venv/lib/python3.10/site-packages/starlette/responses.py\", line 273, in wrap\r\n    await func()\r\n  File \"/aml/stable-diffusion-webui/venv/lib/python3.10/site-packages/starlette/middleware/base.py\", line 134, in stream_response\r\n    return await super().stream_response(send)\r\n  File \"/aml/stable-diffusion-webui/venv/lib/python3.10/site-packages/starlette/responses.py\", line 255, in stream_response\r\n    await send(\r\n  File \"/aml/stable-diffusion-webui/venv/lib/python3.10/site-packages/starlette/middleware/errors.py\", line 159, in _send\r\n    await send(message)\r\n  File \"/aml/stable-diffusion-webui/venv/lib/python3.10/site-packages/uvicorn/protocols/http/h11_impl.py\", line 513, in send\r\n    output = self.conn.send(event)\r\n  File \"/aml/stable-diffusion-webui/venv/lib/python3.10/site-packages/h11/_connection.py\", line 468, in send\r\n    data_list = self.send_with_data_passthrough(event)\r\n  File \"/aml/stable-diffusion-webui/venv/lib/python3.10/site-packages/h11/_connection.py\", line 483, in send_with_data_passthrough\r\n    raise LocalProtocolError(\"Can't send data when our state is ERROR\")\r\nh11._util.LocalProtocolError: Can't send data when our state is ERROR\r\nERROR:    Exception in ASGI application\r\nTraceback (most recent call last):\r\n  File \"/aml/stable-diffusion-webui/venv/lib/python3.10/site-packages/uvicorn/protocols/http/h11_impl.py\", line 429, in run_asgi\r\n    result = await app(  # type: ignore[func-returns-value]\r\n  File \"/aml/stable-diffusion-webui/venv/lib/python3.10/site-packages/uvicorn/middleware/proxy_headers.py\", line 78, in __call__\r\n    return await self.app(scope, receive, send)\r\n  File \"/aml/stable-diffusion-webui/venv/lib/python3.10/site-packages/fastapi/applications.py\", line 273, in __call__\r\n    await super().__call__(scope, receive, send)\r\n  File \"/aml/stable-diffusion-webui/venv/lib/python3.10/site-packages/starlette/applications.py\", line 122, in __call__\r\n    await self.middleware_stack(scope, receive, send)\r\n  File \"/aml/stable-diffusion-webui/venv/lib/python3.10/site-packages/starlette/middleware/errors.py\", line 184, in __call__\r\n    raise exc\r\n  File \"/aml/stable-diffusion-webui/venv/lib/python3.10/site-packages/starlette/middleware/errors.py\", line 162, in __call__\r\n    await self.app(scope, receive, _send)\r\n  File \"/aml/stable-diffusion-webui/venv/lib/python3.10/site-packages/starlette/middleware/base.py\", line 109, in __call__\r\n    await response(scope, receive, send)\r\n  File \"/aml/stable-diffusion-webui/venv/lib/python3.10/site-packages/starlette/responses.py\", line 270, in __call__\r\n    async with anyio.create_task_group() as task_group:\r\n  File \"/aml/stable-diffusion-webui/venv/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 662, in __aexit__\r\n    raise exceptions[0]\r\n  File \"/aml/stable-diffusion-webui/venv/lib/python3.10/site-packages/starlette/responses.py\", line 273, in wrap\r\n    await func()\r\n  File \"/aml/stable-diffusion-webui/venv/lib/python3.10/site-packages/starlette/middleware/base.py\", line 134, in stream_response\r\n    return await super().stream_response(send)\r\n  File \"/aml/stable-diffusion-webui/venv/lib/python3.10/site-packages/starlette/responses.py\", line 255, in stream_response\r\n    await send(\r\n  File \"/aml/stable-diffusion-webui/venv/lib/python3.10/site-packages/starlette/middleware/errors.py\", line 159, in _send\r\n    await send(message)\r\n  File \"/aml/stable-diffusion-webui/venv/lib/python3.10/site-packages/uvicorn/protocols/http/h11_impl.py\", line 513, in send\r\n    output = self.conn.send(event)\r\n  File \"/aml/stable-diffusion-webui/venv/lib/python3.10/site-packages/h11/_connection.py\", line 468, in send\r\n    data_list = self.send_with_data_passthrough(event)\r\n  File \"/aml/stable-diffusion-webui/venv/lib/python3.10/site-packages/h11/_connection.py\", line 483, in send_with_data_passthrough\r\n    raise LocalProtocolError(\"Can't send data when our state is ERROR\")\r\nh11._util.LocalProtocolError: Can't send data when our state is ERROR\r\n```\r\n\r\n\r\n### Additional information\r\n\r\nI just check a lot of issue, but not see same as mine, any expert  please help me to fix the issue, Thanks\n", "hints_text": "I saw the problem should impact by ./stable-diffusion-webui/venv/lib/python3.10/site-packages/uvicorn/main.py\r\n\r\nThere is a configuration:\r\n\r\nchange it from 5 to 0\r\n\r\nwill resolve this issue\r\n\r\n![image](https://github.com/AUTOMATIC1111/stable-diffusion-webui/assets/15274284/1b4ae4d2-3376-43ea-a921-e58abc75489b)\r\n", "created_at": "2023-05-22T14:56:19Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 10534, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-10534", "issue_numbers": ["10488"], "base_commit": "a375acdd2635fdfeb3d77a0715b7df7e6350dd62", "patch": "diff --git a/javascript/dragdrop.js b/javascript/dragdrop.js\nindex e316a365011..398a33f9e9a 100644\n--- a/javascript/dragdrop.js\n+++ b/javascript/dragdrop.js\n@@ -81,7 +81,10 @@ window.addEventListener('paste', e => {\n     }\n \n     const visibleImageFields = [...gradioApp().querySelectorAll('[data-testid=\"image\"]')]\n-        .filter(el => uiElementIsVisible(el));\n+        .filter(el => uiElementIsVisible(el))\n+        .sort((a,b) => uiElementInSight(b) - uiElementInSight(a));\n+\n+\n     if (!visibleImageFields.length) {\n         return;\n     }\ndiff --git a/script.js b/script.js\nindex db4d915768d..f76127799c1 100644\n--- a/script.js\n+++ b/script.js\n@@ -92,19 +92,21 @@ document.addEventListener('keydown', function(e) {\n  * checks that a UI element is not in another hidden element or tab content\n  */\n function uiElementIsVisible(el) {\n-    let isVisible = !el.closest('.\\\\!hidden');\n-    if (!isVisible) {\n-        return false;\n+    if (el === document) {\n+        return true;\n     }\n \n-    while ((isVisible = el.closest('.tabitem')?.style.display) !== 'none') {\n-        if (!isVisible) {\n-            return false;\n-        } else if (el.parentElement) {\n-            el = el.parentElement;\n-        } else {\n-            break;\n-        }\n-    }\n-    return isVisible;\n+    const computedStyle = getComputedStyle(el);\n+    const isVisible = computedStyle.display !== 'none';\n+\n+    if (!isVisible) return false;\n+    return uiElementIsVisible(el.parentNode);\n+}\n+\n+function uiElementInSight(el) {\n+    const clRect = el.getBoundingClientRect();\n+    const windowHeight = window.innerHeight;\n+    const isOnScreen = clRect.bottom > 0 && clRect.top < windowHeight;\n+\n+    return isOnScreen;\n }\n", "test_patch": "", "problem_statement": "[Bug]: paste broken on t2i controlnet as of latest build\n### Is there an existing issue for this?\r\n\r\n- [X] I have searched the existing issues and checked the recent builds/commits\r\n\r\n### What happened?\r\n\r\nin 1.1 i could open the controlnet panel and hit Ctrl+V to past an image into the controlnet in t2i, in 1.2.1 this is no-longer the case (have not tested with 1.2)\r\n\r\n### Steps to reproduce the problem\r\n\r\n1. Go to t2i\r\n2. open up a controlnet unit\r\n3. Press Ctrl+V\r\n\r\n\r\n### What should have happened?\r\n\r\nThe image should end up inside the controlnet unit. This is **NOT** a problem with the controlnet extension, as i have not updated it and it was previously working.\r\n\r\n### Commit where the problem happens\r\n \r\n 89f9faa\r\n\r\n### What platforms do you use to access the UI ?\r\n\r\nWindows\r\n\r\n### What browsers do you use to access the UI ?\r\n\r\n_No response_\r\n\r\n### Command Line Arguments\r\n\r\n```Shell\r\n--listen\r\n```\r\n\r\n\r\n### List of extensions\r\n\r\ncontrolnet\n", "hints_text": "@Sakura-Luna please explain why you have changed this from `bug-report` to `not-an-issue`, this is clearly a regression in auto1111 core. The functionality works in 72cd27a and is broken in b08500c and 89f9faa. It's extremely annoying not to be able to paste controlnet images.\r\n", "created_at": "2023-05-18T23:25:17Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 10524, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-10524", "issue_numbers": ["10521"], "base_commit": "3d959f5b494f010300a3a4607dc1acf2a6e2fc16", "patch": "diff --git a/scripts/xyz_grid.py b/scripts/xyz_grid.py\nindex a725d74a3b3..db768fd2aa6 100644\n--- a/scripts/xyz_grid.py\n+++ b/scripts/xyz_grid.py\n@@ -86,7 +86,7 @@ def apply_checkpoint(p, x, xs):\n     info = modules.sd_models.get_closet_checkpoint_match(x)\r\n     if info is None:\r\n         raise RuntimeError(f\"Unknown checkpoint: {x}\")\r\n-    p.override_settings['sd_model_checkpoint'] = info.hash\r\n+    p.override_settings['sd_model_checkpoint'] = info.name\r\n \r\n \r\n def confirm_checkpoints(p, xs):\r\n", "test_patch": "", "problem_statement": "[Bug]: X/Y/Z plot load different model if model names are similar\n### Is there an existing issue for this?\n\n- [X] I have searched the existing issues and checked the recent builds/commits\n\n### What happened?\n\nAfter last update [9aba26abdf] X/Y/Z plot load different model than picked in settings.\r\n\r\nThis is generation from X/Y/Z plot\r\n\r\n![xyz_grid-0012-1969839337](https://github.com/AUTOMATIC1111/stable-diffusion-webui/assets/4281421/be06cf88-4c1a-411d-a92f-0b8935c7e1ae)\r\n\r\nThose from normal txt2img generated one by one\r\n\r\ndeliberate_v2\r\n![00441-1969839337](https://github.com/AUTOMATIC1111/stable-diffusion-webui/assets/4281421/492c836d-e9f4-4152-93fe-89244af424c7)\r\n\r\nhands_new2_with_multinoise\r\n![00445-1969839337](https://github.com/AUTOMATIC1111/stable-diffusion-webui/assets/4281421/c452c09e-9140-4024-8eaa-3762095b5f98)\r\n\r\nhands_new2_with_noise\r\n![00446-1969839337](https://github.com/AUTOMATIC1111/stable-diffusion-webui/assets/4281421/ad9bde06-d20b-4350-817b-c9ce3b7da7e1)\r\n\r\nAs you can see image from hands_new2_with_noise is different than others when generated separatly but it's exactly the same as hands_new2_with_multinoise when generated with X/Y/Z plot\n\n### Steps to reproduce the problem\n\n1. Go to txt2img\r\n2. Choose X/Y/Z plot\r\n3. Pick models with similar names (same prefix?)\r\n\n\n### What should have happened?\n\nEach image should be different\n\n### Commit where the problem happens\n\n9aba26abdf\n\n### What platforms do you use to access the UI ?\n\nWindows\n\n### What browsers do you use to access the UI ?\n\nGoogle Chrome\n\n### Command Line Arguments\n\n```Shell\nNo\n```\n\n\n### List of extensions\n\nSD-CN-Animation\r\nclip-interrogator-ext\r\nddetailer\r\nloopback_scaler\r\nmultidiffusion-upscaler-for-automatic1111\r\nsd-dynamic-thresholding\r\nsd-infinity-grid-generator-script\r\nsd-webui-controlnet\r\nsd-webui-cutoff\r\nsd-webui-regional-prompter\r\nsd-webui-supermerger\r\nsd-webui-text2video\r\nsd-webui-tome\r\nsd_webui_SAG\r\nseed_travel\r\nLDSR\r\nLora\r\nScuNET\r\nSwinIR\r\nprompt-bracket-checker\n\n### Console logs\n\n```Shell\nX/Y/Z plot will create 3 images on 1 3x1 grid. (Total steps to process: 60)\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 40/40 [00:40<00:00,  1.31it/s]\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [00:16<00:00,  1.24it/s]\r\nLoading weights [9aba26abdf] from E:\\stable-diffusion-webui\\models\\Stable-diffusion\\deliberate_v2.safetensors 1.30it/s]\r\nLoading VAE weights specified in settings: E:\\stable-diffusion-webui\\models\\VAE\\vae-ft-mse-840000-ema-pruned.ckpt\r\nApplying cross attention optimization (Doggettx).\r\nWeights loaded in 2.3s (load weights from disk: 0.2s, apply weights to model: 0.5s, load VAE: 0.3s, move model to device100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [00:16<00:00,  1.24it/s]\r\nLoading weights [7d13449163] from E:\\stable-diffusion-webui\\models\\Stable-diffusion\\hands_new2_with_multinoise.safetensors\r\nLoading VAE weights specified in settings: E:\\stable-diffusion-webui\\models\\VAE\\vae-ft-mse-840000-ema-pruned.ckpt\r\nApplying cross attention optimization (Doggettx).\r\nWeights loaded in 2.7s (load weights from disk: 0.6s, apply weights to model: 0.5s, load VAE: 0.3s, move model to device: 1.4s).\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [00:16<00:00,  1.25it/s]\r\nTotal progress: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 60/60 [00:58<00:00,  1.03it/s]\n```\n\n\n### Additional information\n\n_No response_\n", "hints_text": "", "created_at": "2023-05-18T19:52:40Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 10473, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-10473", "issue_numbers": ["10460"], "base_commit": "f8ca37b9035dc8cb09e15afc5ade6976b927e923", "patch": "diff --git a/modules/extras.py b/modules/extras.py\nindex bdf9b3b7a26..aabb3b2623b 100644\n--- a/modules/extras.py\n+++ b/modules/extras.py\n@@ -242,9 +242,10 @@ def filename_nothing():\n     shared.state.textinfo = \"Saving\"\r\n     print(f\"Saving to {output_modelname}...\")\r\n \r\n-    metadata = {\"format\": \"pt\", \"sd_merge_models\": {}, \"sd_merge_recipe\": None}\r\n+    metadata = None\r\n \r\n     if save_metadata:\r\n+        metadata = {\"format\": \"pt\", \"sd_merge_models\": {}}\r\n         merge_recipe = {\r\n             \"type\": \"webui\", # indicate this model was merged with webui's built-in merger\r\n             \"primary_model_hash\": primary_model_info.sha256,\r\n", "test_patch": "", "problem_statement": "[Bug]: Checkpoint merger error\n### Is there an existing issue for this?\n\n- [X] I have searched the existing issues and checked the recent builds/commits\n\n### What happened?\n\nError when trying to merge checkpoint\r\n\r\n```\r\nLoading C:\\Automatic1111\\models\\Stable-diffusion\\realisticVisionV20_v20_full.safetensors...\r\nLoading C:\\Automatic1111\\models\\Stable-diffusion\\geetesha\\oira666-02-v1.5_7000.safetensors...\r\nMerging...\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1131/1131 [00:02<00:00, 449.38it/s]\r\nSaving to C:\\Automatic1111\\models\\Stable-diffusion\\oira666_7000_realisticview.safetensors...\r\nError loading/saving model file:\r\nTraceback (most recent call last):\r\n  File \"C:\\Automatic1111\\modules\\ui.py\", line 1722, in modelmerger\r\n    results = modules.extras.run_modelmerger(*args)\r\n  File \"C:\\Automatic1111\\modules\\extras.py\", line 285, in run_modelmerger\r\n    safetensors.torch.save_file(theta_0, output_modelname, metadata=metadata)\r\n  File \"C:\\Automatic1111\\venv\\lib\\site-packages\\safetensors\\torch.py\", line 232, in save_file\r\n    serialize_file(_flatten(tensors), filename, metadata=metadata)\r\nTypeError: argument 'metadata': 'dict' object cannot be converted to 'PyString'\r\n```\n\n### Steps to reproduce the problem\n\n1. Go to `Checkpoint merger` tab\r\n2. Set up two models (I tried a lot of different combinations with the same result)\r\n3. Set up params: m 0.15, Weighted sum, safetensors, A, B or C, Bake in vae: none;\r\n4. Press `Merge`\r\n5. Wait for a few seconds \r\n6. Receive the error\r\n\r\n\n\n### What should have happened?\n\nShould have merged with no errors\n\n### Commit where the problem happens\n\nhttps://github.com/AUTOMATIC1111/stable-diffusion-webui/commit/89f9faa63388756314e8a1d96cf86bf5e0663045\n\n### What platforms do you use to access the UI ?\n\nWindows\n\n### What browsers do you use to access the UI ?\n\nGoogle Chrome\n\n### Command Line Arguments\n\n```Shell\n--xformers --api\n```\n\n\n### List of extensions\n\nAuto-Photoshop-StableDiffusion-Plugin\r\ndeforum-for-automatic1111-webui\r\nsd-webui-controlnet\r\nsd_dreambooth_extension\r\nLDSR\r\nLora\r\nScuNET\r\nSwinIR\r\nprompt-bracket-checker\n\n### Console logs\n\n```Shell\nLoading C:\\Automatic1111\\models\\Stable-diffusion\\realisticVisionV20_v20_full.safetensors...\r\nLoading C:\\Automatic1111\\models\\Stable-diffusion\\geetesha\\oira666-02-v1.5_7000.safetensors...\r\nMerging...\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1131/1131 [00:02<00:00, 449.38it/s]\r\nSaving to C:\\Automatic1111\\models\\Stable-diffusion\\oira666_7000_realisticview.safetensors...\r\nError loading/saving model file:\r\nTraceback (most recent call last):\r\n  File \"C:\\Automatic1111\\modules\\ui.py\", line 1722, in modelmerger\r\n    results = modules.extras.run_modelmerger(*args)\r\n  File \"C:\\Automatic1111\\modules\\extras.py\", line 285, in run_modelmerger\r\n    safetensors.torch.save_file(theta_0, output_modelname, metadata=metadata)\r\n  File \"C:\\Automatic1111\\venv\\lib\\site-packages\\safetensors\\torch.py\", line 232, in save_file\r\n    serialize_file(_flatten(tensors), filename, metadata=metadata)\r\nTypeError: argument 'metadata': 'dict' object cannot be converted to 'PyString'\n```\n\n\n### Additional information\n\n_No response_\n", "hints_text": "", "created_at": "2023-05-17T13:29:57Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 10461, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-10461", "issue_numbers": ["10416", "10416"], "base_commit": "9ac85b8b73e180154453609f10b044a475289e24", "patch": "diff --git a/modules/processing.py b/modules/processing.py\nindex 678c4468f4b..cd63b9a604f 100644\n--- a/modules/processing.py\n+++ b/modules/processing.py\n@@ -316,6 +316,7 @@ def __init__(self, p: StableDiffusionProcessing, images_list, seed=-1, info=\"\",\n         self.s_tmin = p.s_tmin\r\n         self.s_tmax = p.s_tmax\r\n         self.s_noise = p.s_noise\r\n+        self.s_min_uncond = p.s_min_uncond\r\n         self.sampler_noise_scheduler_override = p.sampler_noise_scheduler_override\r\n         self.prompt = self.prompt if type(self.prompt) != list else self.prompt[0]\r\n         self.negative_prompt = self.negative_prompt if type(self.negative_prompt) != list else self.negative_prompt[0]\r\n", "test_patch": "", "problem_statement": "[Bug]: \"Processed\" has no attribute 's_min_uncond'\n### Is there an existing issue for this?\r\n\r\n- [X] I have searched the existing issues and checked the recent builds/commits\r\n\r\n### What happened?\r\n\r\nUpon generating, python throws: `AttributeError: 'Processed' object has no attribute 's_min_uncond'`\r\n\r\n### Steps to reproduce the problem\r\n\r\n1. Install 'Depth Map extensions` \u2013 https://github.com/thygate/stable-diffusion-webui-depthmap-script/tree/main\r\n2. Generate Depth Map\r\n\r\n### What should have happened?\r\n\r\nIt should work as expected \u2013 see fix in \"Additional Information\".\r\n\r\n### Commit where the problem happens\r\n\r\n1.2.1\r\n\r\n### What platforms do you use to access the UI ?\r\n\r\nWindows\r\n\r\n### What browsers do you use to access the UI ?\r\n\r\nGoogle Chrome\r\n\r\n### Command Line Arguments\r\n\r\n```Shell\r\nNone\r\n```\r\n\r\n\r\n### List of extensions\r\n\r\n'Depth Map extensions' \u2013 https://github.com/thygate/stable-diffusion-webui-depthmap-script/tree/main\r\n\r\n### Console logs\r\n\r\n```Shell\r\nTraceback (most recent call last):\r\n  File \"[path]\\stable-diffusion-webui-master\\modules\\call_queue.py\", line 57, in f\r\n    res = list(func(*args, **kwargs))\r\n  File \"[path]\\stable-diffusion-webui-master\\modules\\call_queue.py\", line 37, in f\r\n    res = func(*args, **kwargs)\r\n  File \"[path]\\stable-diffusion-webui-master\\modules\\img2img.py\", line 180, in img2img\r\n    processed = modules.scripts.scripts_img2img.run(p, *args)\r\n  File \"[path]\\stable-diffusion-webui-master\\modules\\scripts.py\", line 408, in run\r\n    processed = script.run(p, *script_args)\r\n  File \"[path]\\stable-diffusion-webui-master\\extensions\\stable-diffusion-webui-depthmap-script\\scripts\\depthmap.py\", line 282, in run\r\n    newmaps, mesh_fi, meshsimple_fi = run_depthmap(processed, p.outpath_samples, inputimages, None,\r\n  File \"[path]\\stable-diffusion-webui-master\\extensions\\stable-diffusion-webui-depthmap-script\\scripts\\depthmap.py\", line 601, in run_depthmap\r\n    info = create_infotext(processed, processed.all_prompts, processed.all_seeds, processed.all_subseeds, \"\", 0, count)\r\n  File \"[path]\\stable-diffusion-webui-master\\modules\\processing.py\", line 495, in create_infotext\r\n    \"NGMS\": None if p.s_min_uncond == 0 else p.s_min_uncond,\r\nAttributeError: 'Processed' object has no attribute 's_min_uncond'\r\n```\r\n\r\n\r\n### Additional information\r\n\r\nFile `modules/processing.py`:\r\nError at Line 917, `def save_intermediate(image, index):` is missing `self` attribute.\r\nAdding `self` as an attribute fixes the issue \u2013 `def save_intermediate(self, image, index):`\n[Bug]: \"Processed\" has no attribute 's_min_uncond'\n### Is there an existing issue for this?\r\n\r\n- [X] I have searched the existing issues and checked the recent builds/commits\r\n\r\n### What happened?\r\n\r\nUpon generating, python throws: `AttributeError: 'Processed' object has no attribute 's_min_uncond'`\r\n\r\n### Steps to reproduce the problem\r\n\r\n1. Install 'Depth Map extensions` \u2013 https://github.com/thygate/stable-diffusion-webui-depthmap-script/tree/main\r\n2. Generate Depth Map\r\n\r\n### What should have happened?\r\n\r\nIt should work as expected \u2013 see fix in \"Additional Information\".\r\n\r\n### Commit where the problem happens\r\n\r\n1.2.1\r\n\r\n### What platforms do you use to access the UI ?\r\n\r\nWindows\r\n\r\n### What browsers do you use to access the UI ?\r\n\r\nGoogle Chrome\r\n\r\n### Command Line Arguments\r\n\r\n```Shell\r\nNone\r\n```\r\n\r\n\r\n### List of extensions\r\n\r\n'Depth Map extensions' \u2013 https://github.com/thygate/stable-diffusion-webui-depthmap-script/tree/main\r\n\r\n### Console logs\r\n\r\n```Shell\r\nTraceback (most recent call last):\r\n  File \"[path]\\stable-diffusion-webui-master\\modules\\call_queue.py\", line 57, in f\r\n    res = list(func(*args, **kwargs))\r\n  File \"[path]\\stable-diffusion-webui-master\\modules\\call_queue.py\", line 37, in f\r\n    res = func(*args, **kwargs)\r\n  File \"[path]\\stable-diffusion-webui-master\\modules\\img2img.py\", line 180, in img2img\r\n    processed = modules.scripts.scripts_img2img.run(p, *args)\r\n  File \"[path]\\stable-diffusion-webui-master\\modules\\scripts.py\", line 408, in run\r\n    processed = script.run(p, *script_args)\r\n  File \"[path]\\stable-diffusion-webui-master\\extensions\\stable-diffusion-webui-depthmap-script\\scripts\\depthmap.py\", line 282, in run\r\n    newmaps, mesh_fi, meshsimple_fi = run_depthmap(processed, p.outpath_samples, inputimages, None,\r\n  File \"[path]\\stable-diffusion-webui-master\\extensions\\stable-diffusion-webui-depthmap-script\\scripts\\depthmap.py\", line 601, in run_depthmap\r\n    info = create_infotext(processed, processed.all_prompts, processed.all_seeds, processed.all_subseeds, \"\", 0, count)\r\n  File \"[path]\\stable-diffusion-webui-master\\modules\\processing.py\", line 495, in create_infotext\r\n    \"NGMS\": None if p.s_min_uncond == 0 else p.s_min_uncond,\r\nAttributeError: 'Processed' object has no attribute 's_min_uncond'\r\n```\r\n\r\n\r\n### Additional information\r\n\r\nFile `modules/processing.py`:\r\nError at Line 917, `def save_intermediate(image, index):` is missing `self` attribute.\r\nAdding `self` as an attribute fixes the issue \u2013 `def save_intermediate(self, image, index):`\n", "hints_text": "There's an issue for this at https://github.com/thygate/stable-diffusion-webui-depthmap-script/issues/204\nThe error originates from 'modules/processing.py', which is part of this repo though.\nThere's an issue for this at https://github.com/thygate/stable-diffusion-webui-depthmap-script/issues/204\nThe error originates from 'modules/processing.py', which is part of this repo though.", "created_at": "2023-05-17T07:28:16Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 10201, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-10201", "issue_numbers": ["8555"], "base_commit": "2b96a7b694d3392f76940dfe5df895a2833400fb", "patch": "diff --git a/modules/mac_specific.py b/modules/mac_specific.py\nindex 6fe8dea0726..40ce2101764 100644\n--- a/modules/mac_specific.py\n+++ b/modules/mac_specific.py\n@@ -54,6 +54,11 @@ def cumsum_fix(input, cumsum_func, *args, **kwargs):\n         CondFunc('torch.cumsum', cumsum_fix_func, None)\n         CondFunc('torch.Tensor.cumsum', cumsum_fix_func, None)\n         CondFunc('torch.narrow', lambda orig_func, *args, **kwargs: orig_func(*args, **kwargs).clone(), None)\n-    if version.parse(torch.__version__) == version.parse(\"2.0\"):\n+\n         # MPS workaround for https://github.com/pytorch/pytorch/issues/96113\n-        CondFunc('torch.nn.functional.layer_norm', lambda orig_func, x, normalized_shape, weight, bias, eps, **kwargs: orig_func(x.float(), normalized_shape, weight.float() if weight is not None else None, bias.float() if bias is not None else bias, eps).to(x.dtype), lambda *args, **kwargs: len(args) == 6)\n+        CondFunc('torch.nn.functional.layer_norm', lambda orig_func, x, normalized_shape, weight, bias, eps, **kwargs: orig_func(x.float(), normalized_shape, weight.float() if weight is not None else None, bias.float() if bias is not None else bias, eps).to(x.dtype), lambda _, input, *args, **kwargs: len(args) == 4 and input.device.type == 'mps')\n+\n+        # MPS workaround for https://github.com/pytorch/pytorch/issues/92311\n+        if platform.processor() == 'i386':\n+            for funcName in ['torch.argmax', 'torch.Tensor.argmax']:\n+                CondFunc(funcName, lambda _, input, *args, **kwargs: torch.max(input.float() if input.dtype == torch.int64 else input, *args, **kwargs)[1], lambda _, input, *args, **kwargs: input.device.type == 'mps')\n\\ No newline at end of file\ndiff --git a/modules/sd_hijack_optimizations.py b/modules/sd_hijack_optimizations.py\nindex 372555ffaf4..f10865cd1e7 100644\n--- a/modules/sd_hijack_optimizations.py\n+++ b/modules/sd_hijack_optimizations.py\n@@ -256,6 +256,9 @@ def sub_quad_attention_forward(self, x, context=None, mask=None):\n     k = k.unflatten(-1, (h, -1)).transpose(1,2).flatten(end_dim=1)\r\n     v = v.unflatten(-1, (h, -1)).transpose(1,2).flatten(end_dim=1)\r\n \r\n+    if q.device.type == 'mps':\r\n+        q, k, v = q.contiguous(), k.contiguous(), v.contiguous()\r\n+\r\n     dtype = q.dtype\r\n     if shared.opts.upcast_attn:\r\n         q, k = q.float(), k.float()\r\n", "test_patch": "", "problem_statement": "[Bug]: Remove of --no-half cause errors under MacOS with any Torch version, but almost all samplers produce only noise with it and latest nightly builds\n### Is there an existing issue for this?\n\n- [X] I have searched the existing issues and checked the recent builds/commits\n\n### What happened?\n\nI've tested different versions of Torch to possibly find one that works with _--no-half_ but no luck.\r\n\r\n1.14.0.dev20221025 I'm currently using works fine but throws errors without the _--ho-half_ argument. The latest nightly version 2.1.0.dev20230312 seems to work with this argument and gives a really noticeable performance boost, but almost all samplers break on it.\r\n\r\n**My results**\r\n\r\nWith --no-half:\r\nThere are no errors, but all samplers apart from DDIM and PLMS produce only noise as final results, these two gives out normal pictures. Also new UniPC produce something that looks like a bit less then noise, but still really messy.\r\n\r\nWithout --no-half: \r\nErrors while using everything except DDIM and PLMS. They also works around 40% faster then with _--no-half_.\r\n\r\nWithout --no-half and with --disable-nan-check:\r\nJust black images instead of noise.\n\n### Steps to reproduce the problem\n\nI was just changing startup arguments\n\n### What should have happened?\n\nOther samplers should work too, I guess\n\n### Commit where the problem happens\n\n3c922d98\n\n### What platforms do you use to access the UI ?\n\nMacOS\n\n### What browsers do you use to access the UI ?\n\nMozilla Firefox\n\n### Command Line Arguments\n\n```Shell\n--opt-sub-quad-attention --skip-torch-cuda-test --upcast-sampling --use-cpu interrogate --no-half\n```\n\n\n### List of extensions\n\nNo\n\n### Console logs\n\n```Shell\nError completing request\r\nArguments: ('task(9rr6te8wtxyte2o)', 'watermelon', '', [], 20, 16, False, False, 1, 1, 7, -1.0, -1.0, 0, 0, 0, False, 512, 512, False, 0.7, 2, 'Latent', 0, 0, 0, [], 0, False, False, 'positive', 'comma', 0, False, False, '', 1, '', 0, '', 0, '', True, False, False, False, 0) {}\r\nTraceback (most recent call last):\r\n  File \"/stable-diffusion-webui/modules/call_queue.py\", line 56, in f\r\n    res = list(func(*args, **kwargs))\r\n  File \"/stable-diffusion-webui/modules/call_queue.py\", line 37, in f\r\n    res = func(*args, **kwargs)\r\n  File \"/stable-diffusion-webui/modules/txt2img.py\", line 56, in txt2img\r\n    processed = process_images(p)\r\n  File \"/stable-diffusion-webui/modules/processing.py\", line 486, in process_images\r\n    res = process_images_inner(p)\r\n  File \"/stable-diffusion-webui/modules/processing.py\", line 635, in process_images_inner\r\n    samples_ddim = p.sample(conditioning=c, unconditional_conditioning=uc, seeds=seeds, subseeds=subseeds, subseed_strength=p.subseed_strength, prompts=prompts)\r\n  File \"/stable-diffusion-webui/modules/processing.py\", line 835, in sample\r\n    samples = self.sampler.sample(self, x, conditioning, unconditional_conditioning, image_conditioning=self.txt2img_image_conditioning(x))\r\n  File \"/stable-diffusion-webui/modules/sd_samplers_kdiffusion.py\", line 351, in sample\r\n    samples = self.launch_sampling(steps, lambda: self.func(self.model_wrap_cfg, x, extra_args={\r\n  File \"/stable-diffusion-webui/modules/sd_samplers_kdiffusion.py\", line 227, in launch_sampling\r\n    return func()\r\n  File \"/stable-diffusion-webui/modules/sd_samplers_kdiffusion.py\", line 351, in <lambda>\r\n    samples = self.launch_sampling(steps, lambda: self.func(self.model_wrap_cfg, x, extra_args={\r\n  File \"/stable-diffusion-webui/venv/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/stable-diffusion-webui/repositories/k-diffusion/k_diffusion/sampling.py\", line 553, in sample_dpmpp_sde\r\n    denoised = model(x, sigmas[i] * s_in, **extra_args)\r\n  File \"/stable-diffusion-webui/venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/stable-diffusion-webui/modules/sd_samplers_kdiffusion.py\", line 145, in forward\r\n    devices.test_for_nans(x_out, \"unet\")\r\n  File \"/stable-diffusion-webui/modules/devices.py\", line 152, in test_for_nans\r\n    raise NansException(message)\r\n\r\nmodules.devices.NansException: A tensor with all NaNs was produced in Unet. This could be either because there's not enough precision to represent the picture, or because your video card does not support half type. Try setting the \"Upcast cross attention layer to float32\" option in Settings > Stable Diffusion or using the --no-half commandline argument to fix this. Use --disable-nan-check commandline argument to disable this check.\n```\n\n\n### Additional information\n\nIntel Mac with RX 6600XT, MacOS 13.2.1\n", "hints_text": "I'm getting similar on my 32GB M1 Max - MacOS 13.2. \r\nTorch v1.12.1\r\nCommit a9fed7c\r\n\r\nDefault Command Line Arguments:  --upcast-sampling --no-half-vae --use-cpu interrogate\r\n\r\nProblem is with v2-1_768 (2.0, 1.5 & 1.4 work fine). With SD2.1 it errors at 0% with all sampling methods (except DDIM, PLMS & UniPC):\r\n\r\n```\r\nError completing request\r\nArguments: ('task(ou61msdjo5m7nj1)', 'photo of a man', '', [], 10, 15, False, False, 1, 1, 7, -1.0, -1.0, 0, 0, 0, False, 768, 768, False, 0.7, 2, 'Latent', 0, 0, 0, [], 0, False, False, 'positive', 'comma', 0, False, False, '', 1, '', 0, '', 0, '', True, False, False, False, 0) {}\r\nTraceback (most recent call last):\r\n  File \"/Users/js/stable-diffusion-webui/modules/call_queue.py\", line 56, in f\r\n    res = list(func(*args, **kwargs))\r\n  File \"/Users/js/stable-diffusion-webui/modules/call_queue.py\", line 37, in f\r\n    res = func(*args, **kwargs)\r\n  File \"/Users/js/stable-diffusion-webui/modules/txt2img.py\", line 56, in txt2img\r\n    processed = process_images(p)\r\n  File \"/Users/js/stable-diffusion-webui/modules/processing.py\", line 486, in process_images\r\n    res = process_images_inner(p)\r\n  File \"/Users/js/stable-diffusion-webui/modules/processing.py\", line 636, in process_images_inner\r\n    samples_ddim = p.sample(conditioning=c, unconditional_conditioning=uc, seeds=seeds, subseeds=subseeds, subseed_strength=p.subseed_strength, prompts=prompts)\r\n  File \"/Users/js/stable-diffusion-webui/modules/processing.py\", line 836, in sample\r\n    samples = self.sampler.sample(self, x, conditioning, unconditional_conditioning, image_conditioning=self.txt2img_image_conditioning(x))\r\n  File \"/Users/js/stable-diffusion-webui/modules/sd_samplers_kdiffusion.py\", line 351, in sample\r\n    samples = self.launch_sampling(steps, lambda: self.func(self.model_wrap_cfg, x, extra_args={\r\n  File \"/Users/js/stable-diffusion-webui/modules/sd_samplers_kdiffusion.py\", line 227, in launch_sampling\r\n    return func()\r\n  File \"/Users/js/stable-diffusion-webui/modules/sd_samplers_kdiffusion.py\", line 351, in <lambda>\r\n    samples = self.launch_sampling(steps, lambda: self.func(self.model_wrap_cfg, x, extra_args={\r\n  File \"/Users/js/stable-diffusion-webui/venv/lib/python3.10/site-packages/torch/autograd/grad_mode.py\", line 27, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/Users/js/stable-diffusion-webui/repositories/k-diffusion/k_diffusion/sampling.py\", line 594, in sample_dpmpp_2m\r\n    denoised = model(x, sigmas[i] * s_in, **extra_args)\r\n  File \"/Users/js/stable-diffusion-webui/venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\r\n    return forward_call(*input, **kwargs)\r\n  File \"/Users/js/stable-diffusion-webui/modules/sd_samplers_kdiffusion.py\", line 145, in forward\r\n    devices.test_for_nans(x_out, \"unet\")\r\n  File \"/Users/js/stable-diffusion-webui/modules/devices.py\", line 152, in test_for_nans\r\n    raise NansException(message)\r\nmodules.devices.NansException: A tensor with all NaNs was produced in Unet. This could be either because there's not enough precision to represent the picture, or because your video card does not support half type. Try setting the \"Upcast cross attention layer to float32\" option in Settings > Stable Diffusion or using the --no-half commandline argument to fix this. Use --disable-nan-check commandline argument to disable this check.\r\n```\r\n\r\nWith DDIM, PLMS & UniPC it gets to 100% with black square and no saved image:\r\n\r\n```\r\nError completing request:41,  1.36s/it]\r\nArguments: ('task(ao35ifugi48be7m)', 'photo of a man', '', [], 10, 19, False, False, 1, 1, 7, -1.0, -1.0, 0, 0, 0, False, 768, 768, False, 0.7, 2, 'Latent', 0, 0, 0, [], 0, False, False, 'positive', 'comma', 0, False, False, '', 1, '', 0, '', 0, '', True, False, False, False, 0) {}\r\nTraceback (most recent call last):\r\n  File \"/Users/js/stable-diffusion-webui/modules/call_queue.py\", line 56, in f\r\n    res = list(func(*args, **kwargs))\r\n  File \"/Users/js/stable-diffusion-webui/modules/call_queue.py\", line 37, in f\r\n    res = func(*args, **kwargs)\r\n  File \"/Users/js/stable-diffusion-webui/modules/txt2img.py\", line 56, in txt2img\r\n    processed = process_images(p)\r\n  File \"/Users/js/stable-diffusion-webui/modules/processing.py\", line 486, in process_images\r\n    res = process_images_inner(p)\r\n  File \"/Users/js/stable-diffusion-webui/modules/processing.py\", line 640, in process_images_inner\r\n    devices.test_for_nans(x, \"vae\")\r\n  File \"/Users/js/stable-diffusion-webui/modules/devices.py\", line 152, in test_for_nans\r\n    raise NansException(message)\r\nmodules.devices.NansException: A tensor with all NaNs was produced in VAE. Use --disable-nan-check commandline argument to disable this check.\r\n```\r\nHave tried with all extensions off and have deleted /venv directory and get same results.\r\n\r\nHave been absorbed in controlnet with SD1.5 for a while so not sure how long has been a problem.\r\n\r\nThanks.\r\n\r\n\n> I'm getting similar on my 32GB M1 Max - MacOS 13.2. Torch v1.12.1 Commit [a9fed7c](https://github.com/AUTOMATIC1111/stable-diffusion-webui/commit/a9fed7c364061ae6efb37f797b6b522cb3cf7aa2)\r\n> \r\n> Default Command Line Arguments: --upcast-sampling --no-half-vae --use-cpu interrogate\r\n> \r\n> Problem is with v2-1_768 (2.0, 1.5 & 1.4 work fine). With SD2.1 it errors at 0% with all sampling methods (except DDIM, PLMS & UniPC):\r\n> \r\n> ```\r\n> Error completing request\r\n> Arguments: ('task(ou61msdjo5m7nj1)', 'photo of a man', '', [], 10, 15, False, False, 1, 1, 7, -1.0, -1.0, 0, 0, 0, False, 768, 768, False, 0.7, 2, 'Latent', 0, 0, 0, [], 0, False, False, 'positive', 'comma', 0, False, False, '', 1, '', 0, '', 0, '', True, False, False, False, 0) {}\r\n> Traceback (most recent call last):\r\n>   File \"/Users/js/stable-diffusion-webui/modules/call_queue.py\", line 56, in f\r\n>     res = list(func(*args, **kwargs))\r\n>   File \"/Users/js/stable-diffusion-webui/modules/call_queue.py\", line 37, in f\r\n>     res = func(*args, **kwargs)\r\n>   File \"/Users/js/stable-diffusion-webui/modules/txt2img.py\", line 56, in txt2img\r\n>     processed = process_images(p)\r\n>   File \"/Users/js/stable-diffusion-webui/modules/processing.py\", line 486, in process_images\r\n>     res = process_images_inner(p)\r\n>   File \"/Users/js/stable-diffusion-webui/modules/processing.py\", line 636, in process_images_inner\r\n>     samples_ddim = p.sample(conditioning=c, unconditional_conditioning=uc, seeds=seeds, subseeds=subseeds, subseed_strength=p.subseed_strength, prompts=prompts)\r\n>   File \"/Users/js/stable-diffusion-webui/modules/processing.py\", line 836, in sample\r\n>     samples = self.sampler.sample(self, x, conditioning, unconditional_conditioning, image_conditioning=self.txt2img_image_conditioning(x))\r\n>   File \"/Users/js/stable-diffusion-webui/modules/sd_samplers_kdiffusion.py\", line 351, in sample\r\n>     samples = self.launch_sampling(steps, lambda: self.func(self.model_wrap_cfg, x, extra_args={\r\n>   File \"/Users/js/stable-diffusion-webui/modules/sd_samplers_kdiffusion.py\", line 227, in launch_sampling\r\n>     return func()\r\n>   File \"/Users/js/stable-diffusion-webui/modules/sd_samplers_kdiffusion.py\", line 351, in <lambda>\r\n>     samples = self.launch_sampling(steps, lambda: self.func(self.model_wrap_cfg, x, extra_args={\r\n>   File \"/Users/js/stable-diffusion-webui/venv/lib/python3.10/site-packages/torch/autograd/grad_mode.py\", line 27, in decorate_context\r\n>     return func(*args, **kwargs)\r\n>   File \"/Users/js/stable-diffusion-webui/repositories/k-diffusion/k_diffusion/sampling.py\", line 594, in sample_dpmpp_2m\r\n>     denoised = model(x, sigmas[i] * s_in, **extra_args)\r\n>   File \"/Users/js/stable-diffusion-webui/venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\r\n>     return forward_call(*input, **kwargs)\r\n>   File \"/Users/js/stable-diffusion-webui/modules/sd_samplers_kdiffusion.py\", line 145, in forward\r\n>     devices.test_for_nans(x_out, \"unet\")\r\n>   File \"/Users/js/stable-diffusion-webui/modules/devices.py\", line 152, in test_for_nans\r\n>     raise NansException(message)\r\n> modules.devices.NansException: A tensor with all NaNs was produced in Unet. This could be either because there's not enough precision to represent the picture, or because your video card does not support half type. Try setting the \"Upcast cross attention layer to float32\" option in Settings > Stable Diffusion or using the --no-half commandline argument to fix this. Use --disable-nan-check commandline argument to disable this check.\r\n> ```\r\n> \r\n> With DDIM, PLMS & UniPC it gets to 100% with black square and no saved image:\r\n> \r\n> ```\r\n> Error completing request:41,  1.36s/it]\r\n> Arguments: ('task(ao35ifugi48be7m)', 'photo of a man', '', [], 10, 19, False, False, 1, 1, 7, -1.0, -1.0, 0, 0, 0, False, 768, 768, False, 0.7, 2, 'Latent', 0, 0, 0, [], 0, False, False, 'positive', 'comma', 0, False, False, '', 1, '', 0, '', 0, '', True, False, False, False, 0) {}\r\n> Traceback (most recent call last):\r\n>   File \"/Users/js/stable-diffusion-webui/modules/call_queue.py\", line 56, in f\r\n>     res = list(func(*args, **kwargs))\r\n>   File \"/Users/js/stable-diffusion-webui/modules/call_queue.py\", line 37, in f\r\n>     res = func(*args, **kwargs)\r\n>   File \"/Users/js/stable-diffusion-webui/modules/txt2img.py\", line 56, in txt2img\r\n>     processed = process_images(p)\r\n>   File \"/Users/js/stable-diffusion-webui/modules/processing.py\", line 486, in process_images\r\n>     res = process_images_inner(p)\r\n>   File \"/Users/js/stable-diffusion-webui/modules/processing.py\", line 640, in process_images_inner\r\n>     devices.test_for_nans(x, \"vae\")\r\n>   File \"/Users/js/stable-diffusion-webui/modules/devices.py\", line 152, in test_for_nans\r\n>     raise NansException(message)\r\n> modules.devices.NansException: A tensor with all NaNs was produced in VAE. Use --disable-nan-check commandline argument to disable this check.\r\n> ```\r\n> \r\n> Have tried with all extensions off and have deleted /venv directory and get same results.\r\n> \r\n> Have been absorbed in controlnet with SD1.5 for a while so not sure how long has been a problem.\r\n> \r\n> Thanks.\r\n\r\nTotally , the same issue on my device.\nI have the same problem after installing the fresh torch 2.0 GA, without `--no-halfs` I get the \r\n`input types 'tensor<1x77x1xf16>' and 'tensor<1xf32>' are not broadcast compatible`\r\nand with `--no-halfs` most of the samplers just produce some gibberish images with noise.\r\nThis is on an non M1 Mac running macOS 12.6.3. \r\nBefore the torch upgrade everything worked quite well aside from some random MPS problems regarding memory watermarks.\n> 1.14.0.dev20221025 I'm currently using works fine but throws errors without the _--ho-half_ argument.\r\n\r\n~~So all samplers work correctly with that build? Would you be able to test builds from the dates in between that and 2.1.0.dev20230312 to determine the latest build that works correctly? I haven\u2019t been able to reproduce this issue but if I knew the exact date of the last working PyTorch build then I could likely determine more about this issue and make a workaround.~~\r\n\r\nEdit: Actually, if you could give me the traceback from the error you get from using 1.14.0.dev20221025 without `--no-half`, that may give me a better idea of what is going wrong.\n@brkirch I'm not entirely sure how to use it with 1.14.0 now because it's installed in my Conda environment and the WebUI always tries to create its own venv and install dependencies there. Maybe I can somehow force WebUI to skip creating another venv? Before that I used the old version, which I deleted\nSo just tested some more torch versions from nightly and the earliest versio (torch-2.0.0.dev20230128 torchvision-0.15.0.dev20230128) showed the same effects for me, as the latest one (torch-2.1.0.dev20230327 torchvision-0.16.0.dev2023032). \r\nWithout --no-halfs I get for the latest nightly: `RuntimeError: \"upsample_nearest2d_channels_last\" not implemented for 'Half'` and with --no-halfs only noise is generated as image e.g. when using the euler sampler.\n@brkirch\r\nSame for me, just pulled master branch and updated dependencies.\r\nwebui config is default\r\n2.1_768 model, stable torch 2.0, M1 Max.\r\n\r\nFor DPM++ 2M/SDE Karras/non-Karras:\r\n```\r\n  |  File \"/Users/username/Documents/stable-diffusion-webui/modules/txt2img.py\", line 56, in txt2img\r\n  |    processed = process_images(p)\r\n  |  File \"/Users/username/Documents/stable-diffusion-webui/modules/processing.py\", line 503, in process_images\r\n  |    res = process_images_inner(p)\r\n  |  File \"/Users/username/Documents/stable-diffusion-webui/modules/processing.py\", line 653, in process_images_inner\r\n  |    samples_ddim = p.sample(conditioning=c, unconditional_conditioning=uc, seeds=seeds, subseeds=subseeds, subseed_strength=p.subseed_strength, prompts=prompts)\r\n  |  File \"/Users/username/Documents/stable-diffusion-webui/modules/processing.py\", line 869, in sample\r\n  |    samples = self.sampler.sample(self, x, conditioning, unconditional_conditioning, image_conditioning=self.txt2img_image_conditioning(x))\r\n  |  File \"/Users/username/Documents/stable-diffusion-webui/modules/sd_samplers_kdiffusion.py\", line 358, in sample\r\n  |    samples = self.launch_sampling(steps, lambda: self.func(self.model_wrap_cfg, x, extra_args={\r\n  |  File \"/Users/username/Documents/stable-diffusion-webui/modules/sd_samplers_kdiffusion.py\", line 234, in launch_sampling\r\n  |    return func()\r\n  |  File \"/Users/username/Documents/stable-diffusion-webui/modules/sd_samplers_kdiffusion.py\", line 358, in <lambda>\r\n  |    samples = self.launch_sampling(steps, lambda: self.func(self.model_wrap_cfg, x, extra_args={\r\n  |  File \"/Users/username/Documents/stable-diffusion-webui/venv/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n  |    return func(*args, **kwargs)\r\n  |  File \"/Users/username/Documents/stable-diffusion-webui/repositories/k-diffusion/k_diffusion/sampling.py\", line 128, in sample_euler\r\n  |    denoised = model(x, sigma_hat * s_in, **extra_args)\r\n  |  File \"/Users/username/Documents/stable-diffusion-webui/venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n  |    return forward_call(*args, **kwargs)\r\n  |  File \"/Users/username/Documents/stable-diffusion-webui/modules/sd_samplers_kdiffusion.py\", line 152, in forward\r\n  |    devices.test_for_nans(x_out, \"unet\")\r\n  |  File \"/Users/username/Documents/stable-diffusion-webui/modules/devices.py\", line 152, in test_for_nans\r\n  |    raise NansException(message)\r\n  |modules.devices.NansException: A tensor with all NaNs was produced in Unet. This could be either because there's not enough precision to represent the picture, or because your video card does not support half type. Try setting the \"Upcast cross attention layer to float32\" option in Settings > Stable Diffusion or using the --no-half commandline argument to fix this. Use --disable-nan-check commandline argument to disable this check.\r\n```\r\n\r\nFor DDIM:\r\n```\r\n  File \"/Users/username/Documents/stable-diffusion-webui/modules/txt2img.py\", line 56, in txt2img\r\n    processed = process_images(p)\r\n  File \"/Users/username/Documents/stable-diffusion-webui/modules/processing.py\", line 503, in process_images\r\n    res = process_images_inner(p)\r\n  File \"/Users/username/Documents/stable-diffusion-webui/modules/processing.py\", line 657, in process_images_inner\r\n    devices.test_for_nans(x, \"vae\")\r\n  File \"/Users/username/Documents/stable-diffusion-webui/modules/devices.py\", line 152, in test_for_nans\r\n    raise NansException(message)\r\nmodules.devices.NansException: A tensor with all NaNs was produced in VAE. Use --disable-nan-check commandline argument to disable this check.\r\n```\n> I have the same problem after installing the fresh torch 2.0 GA, without `--no-halfs` I get the `input types 'tensor<1x77x1xf16>' and 'tensor<1xf32>' are not broadcast compatible` and with `--no-halfs` most of the samplers just produce some gibberish images with noise. This is on an non M1 Mac running macOS 12.6.3. Before the torch upgrade everything worked quite well aside from some random MPS problems regarding memory watermarks.\r\n\r\nme too\n> So just tested some more torch versions from nightly and the earliest versio (torch-2.0.0.dev20230128 torchvision-0.15.0.dev20230128) showed the same effects for me, as the latest one (torch-2.1.0.dev20230327 torchvision-0.16.0.dev2023032). Without --no-halfs I get for the latest nightly: `RuntimeError: \"upsample_nearest2d_channels_last\" not implemented for 'Half'` and with --no-halfs only noise is generated as image e.g. when using the euler sampler.\r\n\r\nhave you solved this  ? i also have the problem\n> RuntimeError\r\n\r\nI also met the same problem, can you tell me you solved? thanks\r\nMacos M1\n> > So just tested some more torch versions from nightly and the earliest versio (torch-2.0.0.dev20230128 torchvision-0.15.0.dev20230128) showed the same effects for me, as the latest one (torch-2.1.0.dev20230327 torchvision-0.16.0.dev2023032). Without --no-halfs I get for the latest nightly: `RuntimeError: \"upsample_nearest2d_channels_last\" not implemented for 'Half'` and with --no-halfs only noise is generated as image e.g. when using the euler sampler.\r\n> \r\n> have you solved this ? i also have the problem\r\n\r\nme too!! how to solve this?\n```\r\nError completing request\r\nArguments: ('task(vim8g0n4kh0utdt)', 'create a classic woman with the pearl necklace', '', [], 20, 0, False, False, 1, 1, 7, -1.0, -1.0, 0, 0, 0, False, 512, 512, False, 0.7, 2, 'Latent', 0, 0, 0, [], 0, False, False, 'positive', 'comma', 0, False, False, '', 1, '', 0, '', 0, '', True, False, False, False, 0) {}\r\nTraceback (most recent call last):\r\n  File \"/Users/hstk/code_ground/source_code/stable-diffusion-webui/modules/call_queue.py\", line 56, in f\r\n    res = list(func(*args, **kwargs))\r\n  File \"/Users/hstk/code_ground/source_code/stable-diffusion-webui/modules/call_queue.py\", line 37, in f\r\n    res = func(*args, **kwargs)\r\n  File \"/Users/hstk/code_ground/source_code/stable-diffusion-webui/modules/txt2img.py\", line 56, in txt2img\r\n    processed = process_images(p)\r\n  File \"/Users/hstk/code_ground/source_code/stable-diffusion-webui/modules/processing.py\", line 503, in process_images\r\n    res = process_images_inner(p)\r\n  File \"/Users/hstk/code_ground/source_code/stable-diffusion-webui/modules/processing.py\", line 653, in process_images_inner\r\n    samples_ddim = p.sample(conditioning=c, unconditional_conditioning=uc, seeds=seeds, subseeds=subseeds, subseed_strength=p.subseed_strength, prompts=prompts)\r\n  File \"/Users/hstk/code_ground/source_code/stable-diffusion-webui/modules/processing.py\", line 869, in sample\r\n    samples = self.sampler.sample(self, x, conditioning, unconditional_conditioning, image_conditioning=self.txt2img_image_conditioning(x))\r\n  File \"/Users/hstk/code_ground/source_code/stable-diffusion-webui/modules/sd_samplers_kdiffusion.py\", line 358, in sample\r\n    samples = self.launch_sampling(steps, lambda: self.func(self.model_wrap_cfg, x, extra_args={\r\n  File \"/Users/hstk/code_ground/source_code/stable-diffusion-webui/modules/sd_samplers_kdiffusion.py\", line 234, in launch_sampling\r\n    return func()\r\n  File \"/Users/hstk/code_ground/source_code/stable-diffusion-webui/modules/sd_samplers_kdiffusion.py\", line 358, in <lambda>\r\n    samples = self.launch_sampling(steps, lambda: self.func(self.model_wrap_cfg, x, extra_args={\r\n  File \"/Users/hstk/code_ground/source_code/stable-diffusion-webui/venv/lib/python3.10/site-packages/torch/autograd/grad_mode.py\", line 27, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/Users/hstk/code_ground/source_code/stable-diffusion-webui/repositories/k-diffusion/k_diffusion/sampling.py\", line 145, in sample_euler_ancestral\r\n    denoised = model(x, sigmas[i] * s_in, **extra_args)\r\n  File \"/Users/hstk/code_ground/source_code/stable-diffusion-webui/venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\r\n    return forward_call(*input, **kwargs)\r\n  File \"/Users/hstk/code_ground/source_code/stable-diffusion-webui/modules/sd_samplers_kdiffusion.py\", line 152, in forward\r\n    devices.test_for_nans(x_out, \"unet\")\r\n  File \"/Users/hstk/code_ground/source_code/stable-diffusion-webui/modules/devices.py\", line 152, in test_for_nans\r\n    raise NansException(message)\r\nmodules.devices.NansException: A tensor with all NaNs was produced in Unet. This could be either because there's not enough precision to represent the picture, or because your video card does not support half type. Try setting the \"Upcast cross attention layer to float32\" option in Settings > Stable Diffusion or using the --no-half commandline argument to fix this. Use --disable-nan-check commandline argument to disable this check.\r\n```\r\n\r\nMe too! In my Apple M1 Pro 16G.\nTry torch 1.12.1\r\nSomeone said that the 1.12.1 is the only version can work on Mac, other version all have issue.\n> Try torch 1.12.1 Someone said that the 1.12.1 is the only version can work on Mac, other version all have issue.\r\n\r\nStill not work for me.", "created_at": "2023-05-08T21:53:37Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 10041, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-10041", "issue_numbers": ["9185"], "base_commit": "335428c2c8139dfe07ba096a6defa75036660244", "patch": "diff --git a/modules/img2img.py b/modules/img2img.py\nindex 56c846d62ec..9fc3a698e88 100644\n--- a/modules/img2img.py\n+++ b/modules/img2img.py\n@@ -48,7 +48,8 @@ def process_batch(p, input_dir, output_dir, inpaint_mask_dir, args):\n \r\n         try:\r\n             img = Image.open(image)\r\n-        except UnidentifiedImageError:\r\n+        except UnidentifiedImageError as e:\r\n+            print(e)\r\n             continue\r\n         # Use the EXIF orientation of photos taken by smartphones.\r\n         img = ImageOps.exif_transpose(img)\r\n", "test_patch": "", "problem_statement": "[Feature Request]: img2img batch should ignore non-image files\n### Is there an existing issue for this?\n\n- [X] I have searched the existing issues and checked the recent builds/commits\n\n### What would your feature do ?\n\nI may place some parameter files along the image files in the same folder.\r\nCurrently an exception is throw for non-image files when do \"Image.open()\".\n\n### Proposed workflow\n\n1. Place a txt file along with the png file in folder A\r\n2. Set batch input folder to A\r\n3. Press generate\r\n\n\n### Additional information\n\n_No response_\n", "hints_text": "@liudibo \r\nIn [img2img.py](https://github.com/AUTOMATIC1111/stable-diffusion-webui/blob/master/modules/img2img.py) you can see that for the batch (`process_batch()`) the contents of directory are retrieved with `shared.listfiles()`. In [shared.py](https://github.com/AUTOMATIC1111/stable-diffusion-webui/blob/master/modules/shared.py), you can see that `listfiles()` actually reads all files except those starting with a dot (`.` like Linux hidden files).\r\n\r\nSo you should start your parameter files with a dot (example: `.parameter.txt`) to avoid this. If you're on Windows it doesn't even hide the files.\r\n\r\nIt seems also that `listfiles()` is only used to retrieve images in a folder, so maybe it should be corrected to grab only the files with supported image extensions.\nIf we want to modify `listfile()`, it's probably as simple as adding lines similar to the following before the `return` statement:\r\n```\r\n    imagefilenames = []\r\n    for extension in [\".png\", \"jpg\"]:\r\n        imagefilenames += [file for file in filenames if file.lower().endswith(extension)]\r\n```\r\nHowever file extensions do not guarantee that the file is actually an image, same as no extension does not mean that the file is not an image. And you could also have an image with a unrelated extension.\r\nAlso the extension list has to represent all cases of possible image extensions that are supported.\r\n\r\nProbably a better way to handle this is to catch the exception `PIL.UnidentifiedImageError` being thrown when using `Image.open()` in `process_batch()`. For instance:\r\n\r\n```\r\n        try:\r\n            img = Image.open(image)\r\n        except PIL.UnidentifiedImageError:\r\n            continue\r\n```\n> @liudibo In [img2img.py](https://github.com/AUTOMATIC1111/stable-diffusion-webui/blob/master/modules/img2img.py) you can see that for the batch (`process_batch()`) the contents of directory are retrieved with `shared.listfiles()`. In [shared.py](https://github.com/AUTOMATIC1111/stable-diffusion-webui/blob/master/modules/shared.py), you can see that `listfiles()` actually reads all files except those starting with a dot (`.` like Linux hidden files).\r\n> \r\n> So you should start your parameter files with a dot (example: `.parameter.txt`) to avoid this. If you're on Windows it doesn't even hide the files.\r\n> \r\n> It seems also that `listfiles()` is only used to retrieve images in a folder, so maybe it should be corrected to grab only the files with supported image extensions.\r\n\r\nThanks for the reply an fix. Using \".\" as the file prefix seems a proper solution.\r\nBy the way, the try/except fix is more helpful for the users who are not familiar with the code base.", "created_at": "2023-05-03T05:37:26Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 9865, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-9865", "issue_numbers": ["7028"], "base_commit": "4499bead4c1a20cac94f5411f7ba343f663bd415", "patch": "diff --git a/modules/cmd_args.py b/modules/cmd_args.py\nindex f47c21bb479..d906a5717ee 100644\n--- a/modules/cmd_args.py\n+++ b/modules/cmd_args.py\n@@ -102,3 +102,4 @@\n parser.add_argument(\"--skip-version-check\", action='store_true', help=\"Do not check versions of torch and xformers\")\r\n parser.add_argument(\"--no-hashing\", action='store_true', help=\"disable sha256 hashing of checkpoints to help loading performance\", default=False)\r\n parser.add_argument(\"--no-download-sd-model\", action='store_true', help=\"don't download SD1.5 model even if no model is found in --ckpt-dir\", default=False)\r\n+parser.add_argument('--subpath', type=str, help='customize the subpath for gradio, use with reverse proxy')\n\\ No newline at end of file\ndiff --git a/webui.py b/webui.py\nindex 357bf4c1979..d4ccc1b0ed2 100644\n--- a/webui.py\n+++ b/webui.py\n@@ -350,6 +350,11 @@ def webui():\n \r\n         print(f\"Startup time: {startup_timer.summary()}.\")\r\n \r\n+        if cmd_opts.subpath:\r\n+            redirector = FastAPI()\r\n+            redirector.get(\"/\")\r\n+            mounted_app = gradio.mount_gradio_app(redirector, shared.demo, path=f\"/{cmd_opts.subpath}\")\r\n+\r\n         wait_on_server(shared.demo)\r\n         print('Restarting UI...')\r\n \r\ndiff --git a/webui.sh b/webui.sh\nold mode 100755\nnew mode 100644\n", "test_patch": "", "problem_statement": "[Bug]: Gradio issue, \"Value Error: File cannot be fetched: <... extensions/...the file...>\"\n### Is there an existing issue for this?\n\n- [x] I have searched the existing issues and checked the recent builds/commits\n\n### What happened?\n\nThe following error is logged:\r\n```\r\nValueError: File cannot be fetched: /home/valconius/Library-Remote/C30 Documents/C3080 Stable Diffusion/Interfaces/nsd--atagen/nix-stable-diffusion--no-nixlib-in-flake/stable-diffusion-webui/extensions/stable-diffusion-webui-wd14-tagger/javascript/tagger.js. All files must contained within the Gradio python app working directory, or be a temp file created by the Gradio python app\r\n```\r\nHowever, using the extension in question, \"stable diffusion webui wd14 tagger\", works just fine. Not all extensions have this issue, but many do.\n\n### Steps to reproduce the problem\n\n1. Go to .... \r\n2. Press ....\r\n3. ...\r\n\n\n### What should have happened?\n\nMy extensions and many other files are symlinked to locations outside the webui file tree. This issue did not occur with a git pull from a couple of days ago but now occurs.\n\n### Commit where the problem happens\n\n216f67ec7cdec45bcfa9db40aa6bcac480e3637e\n\n### What platforms do you use to access UI ?\n\nLinux\n\n### What browsers do you use to access the UI ?\n\nMozilla Firefox\n\n### Command Line Arguments\n\n```Shell\npytho launch.py --api\n```\n\n\n### Additional information, context and logs\n\n_No response_\n", "hints_text": "Same problem? https://github.com/AUTOMATIC1111/stable-diffusion-webui/issues/6939 Scenarios are a bit different, i'll leave this one open.\nConfirmed, but the ISSUE opener did focus on the wrong commit.\r\n\r\nThe REAL commit where the problem started is https://github.com/AUTOMATIC1111/stable-diffusion-webui/commit/50059ea661b63967b217e687819cf7a9081e4a0c#commitcomment-97757826\r\n\r\nYou can check it by reverting the webui to the previous commit by `git reset --hard 17af0fb95574068a1d5032ae96879dab145e173a` then the problem vanish.\r\nNow go to the commit `git reset --hard 50059ea661b63967b217e687819cf7a9081e4a0c` and the BUG is there.\r\n\r\n\r\nThe BUG is still there as of 97ba01a2136f0482774824fc812c8faf36fd9658 (HEAD at the time of writing)\r\n\r\n@AUTOMATIC1111 \nIs there a loss of functionality or just an error message?\n> ### Is there an existing issue for this?\r\n> * [x]  I have searched the existing issues and checked the recent builds/commits\r\n> \r\n> ### What happened?\r\n> The following error is logged:\r\n> \r\n> ```\r\n> ValueError: File cannot be fetched: /home/valconius/Library-Remote/C30 Documents/C3080 Stable Diffusion/Interfaces/nsd--atagen/nix-stable-diffusion--no-nixlib-in-flake/stable-diffusion-webui/extensions/stable-diffusion-webui-wd14-tagger/javascript/tagger.js. All files must contained within the Gradio python app working directory, or be a temp file created by the Gradio python app\r\n> ```\r\n> \r\n> However, using the extension in question, \"stable diffusion webui wd14 tagger\", works just fine. Not all extensions have this issue, but many do.\r\n> \r\n> ### Steps to reproduce the problem\r\n> 1. Go to ....\r\n> 2. Press ....\r\n> 3. ...\r\n> \r\n> ### What should have happened?\r\n> My extensions and many other files are symlinked to locations outside the webui file tree. This issue did not occur with a git pull from a couple of days ago but now occurs.\r\n> \r\n> ### Commit where the problem happens\r\n> [216f67e](https://github.com/AUTOMATIC1111/stable-diffusion-webui/commit/216f67ec7cdec45bcfa9db40aa6bcac480e3637e)\r\n> \r\n> ### What platforms do you use to access UI ?\r\n> Linux\r\n> \r\n> ### What browsers do you use to access the UI ?\r\n> Mozilla Firefox\r\n> \r\n> ### Command Line Arguments\r\n> ```shell\r\n> pytho launch.py --api\r\n> ```\r\n> \r\n> ### Additional information, context and logs\r\n> _No response_\r\n\r\nSorry for the inconvenience.  Currently the documentation requires copying them into the extensions directory, but I'm checking with the author on what should be supported: https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Extensions \r\nTry copying them directly and see if that doesn't fix the problem.\n> Is there a loss of functionality or just an error message?\r\n\r\nYes, total loss of functionality: none of the plugins works anymore.\r\nBy the way, I'l running stable diffusion in a dockerized environment with mounted individual folders => if this is the problem, please understand it never have been one prior ths commit I referred earlier.\n@martinobettucci Gradio has been making changes around the way it deals with symlinks: https://github.com/gradio-app/gradio/pull/3037 I'm keeping an eye on it.  In the meantime, in your Dockerfile, you can always mount the extensions to a separate file and then `cp -r` to the extensions directory, that should be a workaround.\nI found a workaround @jjtolton, just mount the '/extension' folder as a volume in docker instead of symlinking it to you external SDD\n@valconius Which version of python you are using? I have python 3.10.7 and had no issues related to gradio.\nI was able to get around this via:\r\n```bash\r\nsed -i 's/in_app_dir = .*/in_app_dir = True/g' /usr/local/lib/python3.10/dist-packages/gradio/routes.py\r\n```\r\nNot sure what other implications this has.", "created_at": "2023-04-25T14:28:44Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 9760, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-9760", "issue_numbers": ["8162"], "base_commit": "b3a44385b182496964a969e66f7f3b35e25b98b1", "patch": "diff --git a/modules/generation_parameters_copypaste.py b/modules/generation_parameters_copypaste.py\nindex 6df76858f51..ae855627d61 100644\n--- a/modules/generation_parameters_copypaste.py\n+++ b/modules/generation_parameters_copypaste.py\n@@ -59,6 +59,7 @@ def image_from_url_text(filedata):\n         is_in_right_dir = ui_tempdir.check_tmp_file(shared.demo, filename)\r\n         assert is_in_right_dir, 'trying to open image file outside of allowed directories'\r\n \r\n+        filename = filename.rsplit('?', 1)[0]\r\n         return Image.open(filename)\r\n \r\n     if type(filedata) == list:\r\ndiff --git a/modules/ui_tempdir.py b/modules/ui_tempdir.py\nindex 21945235ef3..42a85d3b82e 100644\n--- a/modules/ui_tempdir.py\n+++ b/modules/ui_tempdir.py\n@@ -2,6 +2,7 @@\n import tempfile\r\n from collections import namedtuple\r\n from pathlib import Path\r\n+from time import time\r\n \r\n import gradio as gr\r\n \r\n@@ -34,6 +35,7 @@ def check_tmp_file(gradio, filename):\n def save_pil_to_file(pil_image, dir=None):\r\n     already_saved_as = getattr(pil_image, 'already_saved_as', None)\r\n     if already_saved_as and os.path.isfile(already_saved_as):\r\n+        already_saved_as += f'?{int(time())}'\r\n         register_tmp_file(shared.demo, already_saved_as)\r\n \r\n         file_obj = Savedfile(already_saved_as)\r\n", "test_patch": "", "problem_statement": "[Bug]: Web shows the wrong result\n### Is there an existing issue for this?\r\n\r\n- [X] I have searched the existing issues and checked the recent builds/commits\r\n\r\n### What happened?\r\n\r\nAfter using txt2img many times, I found that the Result window after execution is often not refreshed or shows old results, so I have to open the output path to view the real results. This problem is more likely to occur when executing a new task after using interrupt or skip.\r\n\r\n### Steps to reproduce the problem\r\n\r\n1. Execute txt2img\r\n2. Repeat multiple times with different inputs and configs\r\n3. Observation results\r\n\r\n### What should have happened?\r\n\r\nEach execution displays the corresponding result.\r\n\r\n### Commit where the problem happens\r\n\r\n0cc0ee1bcb4c24a8c9715f66cede06601bfc00c8\r\n\r\n### What platforms do you use to access the UI ?\r\n\r\nWindows\r\n\r\n### What browsers do you use to access the UI ?\r\n\r\nGoogle Chrome\r\n\r\n### Command Line Arguments\r\n\r\n```Shell\r\n--no-half-vae --skip-version-check\r\n```\r\n\r\n\r\n### List of extensions\r\n\r\nNo\r\n\r\n### Console logs\r\n\r\n```Shell\r\nWithout any warnings and errors.\r\n```\r\n\r\n\r\n### Additional information\r\n\r\n_No response_\n", "hints_text": "+1", "created_at": "2023-04-20T06:59:57Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 9734, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-9734", "issue_numbers": ["9613"], "base_commit": "09069918e8e6e4e25a804d34c803abc3b76ae84a", "patch": "diff --git a/modules/devices.py b/modules/devices.py\nindex 52c3e7cd773..3bc86a6a9d1 100644\n--- a/modules/devices.py\n+++ b/modules/devices.py\n@@ -92,14 +92,18 @@ def cond_cast_float(input):\n \n \n def randn(seed, shape):\n+    from modules.shared import opts\n+\n     torch.manual_seed(seed)\n-    if device.type == 'mps':\n+    if opts.use_cpu_randn or device.type == 'mps':\n         return torch.randn(shape, device=cpu).to(device)\n     return torch.randn(shape, device=device)\n \n \n def randn_without_seed(shape):\n-    if device.type == 'mps':\n+    from modules.shared import opts\n+\n+    if opts.use_cpu_randn or device.type == 'mps':\n         return torch.randn(shape, device=cpu).to(device)\n     return torch.randn(shape, device=device)\n \ndiff --git a/modules/sd_samplers_common.py b/modules/sd_samplers_common.py\nindex a1aac7cf0aa..e6a372d52f8 100644\n--- a/modules/sd_samplers_common.py\n+++ b/modules/sd_samplers_common.py\n@@ -60,3 +60,12 @@ def store_latent(decoded):\n \r\n class InterruptedException(BaseException):\r\n     pass\r\n+\r\n+if opts.use_cpu_randn:\r\n+    import torchsde._brownian.brownian_interval\r\n+\r\n+    def torchsde_randn(size, dtype, device, seed):\r\n+        generator = torch.Generator(devices.cpu).manual_seed(int(seed))\r\n+        return torch.randn(size, dtype=dtype, device=devices.cpu, generator=generator).to(device)\r\n+\r\n+    torchsde._brownian.brownian_interval._randn = torchsde_randn\r\ndiff --git a/modules/sd_samplers_kdiffusion.py b/modules/sd_samplers_kdiffusion.py\nindex e9f08518fdc..13f4567aecb 100644\n--- a/modules/sd_samplers_kdiffusion.py\n+++ b/modules/sd_samplers_kdiffusion.py\n@@ -190,7 +190,7 @@ def randn_like(self, x):\n             if noise.shape == x.shape:\r\n                 return noise\r\n \r\n-        if x.device.type == 'mps':\r\n+        if opts.use_cpu_randn or x.device.type == 'mps':\r\n             return torch.randn_like(x, device=devices.cpu).to(x.device)\r\n         else:\r\n             return torch.randn_like(x)\r\ndiff --git a/modules/shared.py b/modules/shared.py\nindex 5fd0eecbd1f..59b037d5897 100644\n--- a/modules/shared.py\n+++ b/modules/shared.py\n@@ -331,6 +331,7 @@ def list_samplers():\n     \"comma_padding_backtrack\": OptionInfo(20, \"Increase coherency by padding from the last comma within n tokens when using more than 75 tokens\", gr.Slider, {\"minimum\": 0, \"maximum\": 74, \"step\": 1 }),\r\n     \"CLIP_stop_at_last_layers\": OptionInfo(1, \"Clip skip\", gr.Slider, {\"minimum\": 1, \"maximum\": 12, \"step\": 1}),\r\n     \"upcast_attn\": OptionInfo(False, \"Upcast cross attention layer to float32\"),\r\n+    \"use_cpu_randn\": OptionInfo(False, \"Use CPU for random number generation to make manual seeds generate the same image across platforms. This may change existing seeds.\"),\r\n }))\r\n \r\n options_templates.update(options_section(('compatibility', \"Compatibility\"), {\r\n", "test_patch": "", "problem_statement": "[Feature Request]: Seed compatibility across platforms\n### Is there an existing issue for this?\n\n- [X] I have searched the existing issues and checked the recent builds/commits\n\n### What would your feature do ?\n\nIt'd be nice to have seed compatibility between Mac OS and Nvidia/Windows/Linux.\r\nThere's an existing issue here, but raised as a bug report rather than feature request:\r\nhttps://github.com/AUTOMATIC1111/stable-diffusion-webui/issues/7673\r\n\r\n\n\n### Proposed workflow\n\n1. use seed on nvidia platforms\r\n2. reuse same seed on apple silicon / CPU-only\r\n3. get the same result\n\n### Additional information\n\nIf anyone's wondering what the purpose is - i use Win and Linux machines for training, and test inference or just explore on an M2 mac at the same time. Being able to reproduce results between the three machines would be very useful.\n", "hints_text": "", "created_at": "2023-04-19T05:48:02Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 9723, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-9723", "issue_numbers": ["9895"], "base_commit": "38f1c8183b4dee0e3dec0bef1b506b6cdda369aa", "patch": "diff --git a/extensions-builtin/Lora/extra_networks_lora.py b/extensions-builtin/Lora/extra_networks_lora.py\nindex 6be6ef73c66..45f899fc4ed 100644\n--- a/extensions-builtin/Lora/extra_networks_lora.py\n+++ b/extensions-builtin/Lora/extra_networks_lora.py\n@@ -8,7 +8,7 @@ def __init__(self):\n     def activate(self, p, params_list):\r\n         additional = shared.opts.sd_lora\r\n \r\n-        if additional != \"\" and additional in lora.available_loras and len([x for x in params_list if x.items[0] == additional]) == 0:\r\n+        if additional != \"None\" and additional in lora.available_loras and len([x for x in params_list if x.items[0] == additional]) == 0:\r\n             p.all_prompts = [x + f\"<lora:{additional}:{shared.opts.extra_networks_default_multiplier}>\" for x in p.all_prompts]\r\n             params_list.append(extra_networks.ExtraNetworkParams(items=[additional, shared.opts.extra_networks_default_multiplier]))\r\n \r\ndiff --git a/extensions-builtin/Lora/scripts/lora_script.py b/extensions-builtin/Lora/scripts/lora_script.py\nindex 0adab2254d2..3fc38ab9d52 100644\n--- a/extensions-builtin/Lora/scripts/lora_script.py\n+++ b/extensions-builtin/Lora/scripts/lora_script.py\n@@ -52,5 +52,5 @@ def before_ui():\n \r\n \r\n shared.options_templates.update(shared.options_section(('extra_networks', \"Extra Networks\"), {\r\n-    \"sd_lora\": shared.OptionInfo(\"None\", \"Add Lora to prompt\", gr.Dropdown, lambda: {\"choices\": [\"\"] + [x for x in lora.available_loras]}, refresh=lora.list_available_loras),\r\n+    \"sd_lora\": shared.OptionInfo(\"None\", \"Add Lora to prompt\", gr.Dropdown, lambda: {\"choices\": [\"None\"] + [x for x in lora.available_loras]}, refresh=lora.list_available_loras),\r\n }))\r\ndiff --git a/modules/extra_networks_hypernet.py b/modules/extra_networks_hypernet.py\nindex d3a4d7adcb7..33d100dd950 100644\n--- a/modules/extra_networks_hypernet.py\n+++ b/modules/extra_networks_hypernet.py\n@@ -9,7 +9,7 @@ def __init__(self):\n     def activate(self, p, params_list):\r\n         additional = shared.opts.sd_hypernetwork\r\n \r\n-        if additional != \"\" and additional in shared.hypernetworks and len([x for x in params_list if x.items[0] == additional]) == 0:\r\n+        if additional != \"None\" and additional in shared.hypernetworks and len([x for x in params_list if x.items[0] == additional]) == 0:\r\n             p.all_prompts = [x + f\"<hypernet:{additional}:{shared.opts.extra_networks_default_multiplier}>\" for x in p.all_prompts]\r\n             params_list.append(extra_networks.ExtraNetworkParams(items=[additional, shared.opts.extra_networks_default_multiplier]))\r\n \r\ndiff --git a/modules/shared.py b/modules/shared.py\nindex 5fd0eecbd1f..5a5fbae61cc 100644\n--- a/modules/shared.py\n+++ b/modules/shared.py\n@@ -361,7 +361,7 @@ def list_samplers():\n     \"extra_networks_card_width\": OptionInfo(0, \"Card width for Extra Networks (px)\"),\r\n     \"extra_networks_card_height\": OptionInfo(0, \"Card height for Extra Networks (px)\"),\r\n     \"extra_networks_add_text_separator\": OptionInfo(\" \", \"Extra text to add before <...> when adding extra network to prompt\"),\r\n-    \"sd_hypernetwork\": OptionInfo(\"None\", \"Add hypernetwork to prompt\", gr.Dropdown, lambda: {\"choices\": [\"\"] + [x for x in hypernetworks.keys()]}, refresh=reload_hypernetworks),\r\n+    \"sd_hypernetwork\": OptionInfo(\"None\", \"Add hypernetwork to prompt\", gr.Dropdown, lambda: {\"choices\": [\"None\"] + [x for x in hypernetworks.keys()]}, refresh=reload_hypernetworks),\r\n }))\r\n \r\n options_templates.update(options_section(('ui', \"User interface\"), {\r\n", "test_patch": "", "problem_statement": "[Bug]: 'Add Extra Networks to prompt' Tab has no 'None' in it\n### Is there an existing issue for this?\n\n- [X] I have searched the existing issues and checked the recent builds/commits\n\n### What happened?\n\n'Add Extra Networks to prompt' Tab has no 'None' in it\r\nThis causes the webui to select at least one\r\n22bcc7be\n\n### Steps to reproduce the problem\n\n1.Select a ExtraNetwork such as hypernetwork or lora with tab in settings or quick select tab\r\n2.Then you cannot cancel using a hypernet or lora anymore.\n\n### What should have happened?\n\n1.Select a ExtraNetwork such as hypernetwork or lora with tab in settings or quick select tab\r\n2.To cancel using them just click 'None' in the tab\n\n### Commit where the problem happens\n\n22bcc7be\n\n### What platforms do you use to access the UI ?\n\nWindows\n\n### What browsers do you use to access the UI ?\n\nMicrosoft Edge\n\n### Command Line Arguments\n\n```Shell\nnothing special\n```\n\n\n### List of extensions\n\nbuilt-in\n\n### Console logs\n\n```Shell\nnothing\n```\n\n\n### Additional information\n\n_No response_\n", "hints_text": "", "created_at": "2023-04-18T23:15:34Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 9513, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-9513", "issue_numbers": ["8952"], "base_commit": "7fc10e04456878ac4e0881dbfe56c8966e22f40d", "patch": "diff --git a/modules/postprocessing.py b/modules/postprocessing.py\nindex ff055aae967..9cb809577da 100644\n--- a/modules/postprocessing.py\n+++ b/modules/postprocessing.py\n@@ -1,4 +1,6 @@\n import os\r\n+import tempfile\r\n+from typing import List\r\n \r\n from PIL import Image\r\n \r\n@@ -6,7 +8,7 @@\n from modules.shared import opts\r\n \r\n \r\n-def run_postprocessing(extras_mode, image, image_folder, input_dir, output_dir, show_extras_results, *args, save_output: bool = True):\r\n+def run_postprocessing(extras_mode, image, image_folder: List[tempfile.NamedTemporaryFile], input_dir, output_dir, show_extras_results, *args, save_output: bool = True):\r\n     devices.torch_gc()\r\n \r\n     shared.state.begin()\r\n@@ -22,7 +24,7 @@ def run_postprocessing(extras_mode, image, image_folder, input_dir, output_dir,\n                 image = img\r\n                 fn = ''\r\n             else:\r\n-                image = Image.open(img)\r\n+                image = Image.open(os.path.abspath(img.name))\r\n                 fn = os.path.splitext(img.orig_name)[0]\r\n \r\n             image_data.append(image)\r\ndiff --git a/modules/ui_postprocessing.py b/modules/ui_postprocessing.py\nindex b418d955305..d278e1b60ed 100644\n--- a/modules/ui_postprocessing.py\n+++ b/modules/ui_postprocessing.py\n@@ -13,7 +13,7 @@ def create_ui():\n                     extras_image = gr.Image(label=\"Source\", source=\"upload\", interactive=True, type=\"pil\", elem_id=\"extras_image\")\r\n \r\n                 with gr.TabItem('Batch Process', elem_id=\"extras_batch_process_tab\") as tab_batch:\r\n-                    image_batch = gr.File(label=\"Batch Process\", file_count=\"multiple\", interactive=True, type=\"file\", elem_id=\"extras_image_batch\")\r\n+                    image_batch = gr.Files(label=\"Batch Process\", interactive=True, elem_id=\"extras_image_batch\")\r\n \r\n                 with gr.TabItem('Batch from Directory', elem_id=\"extras_batch_directory_tab\") as tab_batch_dir:\r\n                     extras_batch_input_dir = gr.Textbox(label=\"Input directory\", **shared.hide_dirs, placeholder=\"A directory on the same machine where the server is running.\", elem_id=\"extras_batch_input_dir\")\r\n", "test_patch": "", "problem_statement": "[Bug]: UnidentifiedImageError when using \"Batch Process\" in \"Extras\"\n### Is there an existing issue for this?\n\n- [X] I have searched the existing issues and checked the recent builds/commits\n\n### What happened?\n\nHello,\r\n\r\nI'm trying to upscale a bunch of images in the \"Extras\" Tab. When I pick an image in the \"Single Image\" tab it works perfectly fine. But when I pick the exact same imagefile in the \"Batch Process\" and hit \"Generate\" I instantly get this error message in the UI:\r\n\r\nUnidentifiedImageError: cannot identify image file <tempfile._TemporaryFileWrapper object at 0x000001F589DBF1F0>\r\nTime taken: 0.00sTorch active/reserved: 1/2 MiB, Sys VRAM: 1277/4096 MiB (31.18%)\r\n\r\nIn the CMD it looks like this:\r\nError completing request\r\nArguments: (1, <PIL.Image.Image image mode=RGB size=960x1200 at 0x1F589DBF160>, [<tempfile._TemporaryFileWrapper object at 0x000001F589DBF1F0>], '', '', True, 0, 4, 512, 512, True, 'SwinIR_4x', 'None', 0, 0, 0, 0) {}\r\nTraceback (most recent call last):\r\n  File \"C:\\AI\\stable-diffusion-webui\\modules\\call_queue.py\", line 56, in f\r\n    res = list(func(*args, **kwargs))\r\n  File \"C:\\AI\\stable-diffusion-webui\\modules\\call_queue.py\", line 37, in f\r\n    res = func(*args, **kwargs)\r\n  File \"C:\\AI\\stable-diffusion-webui\\modules\\postprocessing.py\", line 21, in run_postprocessing\r\n    image = Image.open(img)\r\n  File \"C:\\AI\\stable-diffusion-webui\\venv\\lib\\site-packages\\PIL\\Image.py\", line 3283, in open\r\n    raise UnidentifiedImageError(msg)\r\nPIL.UnidentifiedImageError: cannot identify image file <tempfile._TemporaryFileWrapper object at 0x000001F589DBF1F0>\r\n\r\n\r\n\n\n### Steps to reproduce the problem\n\n1. Go to .... Extras, Batch Process\r\n2. Press .... Import to add the image, and press Generate\r\n\n\n### What should have happened?\n\nIt should create the upscaled image, just like in the Single Image tab.\n\n### Commit where the problem happens\n\na0d07fb5\n\n### What platforms do you use to access the UI ?\n\nWindows\n\n### What browsers do you use to access the UI ?\n\nGoogle Chrome\n\n### Command Line Arguments\n\n```Shell\ngit pull\r\npause \r\n\r\n --xformers --medvram\n```\n\n\n### List of extensions\n\nno\n\n### Console logs\n\n```Shell\nC:\\AI\\stable-diffusion-webui>git pull\r\nAlready up to date.\r\n\r\nC:\\AI\\stable-diffusion-webui>pause\r\nDr\u00fccken Sie eine beliebige Taste . . .\r\nvenv \"C:\\AI\\stable-diffusion-webui\\venv\\Scripts\\Python.exe\"\r\nPython 3.10.10 (tags/v3.10.10:aad5f6a, Feb  7 2023, 17:20:36) [MSC v.1929 64 bit (AMD64)]\r\nCommit hash: a0d07fb5807ad55c8ccfdfc9a6d9ae3c62b9d211\r\nInstalling requirements for Web UI\r\nLaunching Web UI with arguments: --xformers --medvram\r\nLoading weights [cc6cb27103] from C:\\AI\\stable-diffusion-webui\\models\\Stable-diffusion\\model.ckpt\r\nCreating model from config: C:\\AI\\stable-diffusion-webui\\configs\\v1-inference.yaml\r\nLatentDiffusion: Running in eps-prediction mode\r\nDiffusionWrapper has 859.52 M params.\r\nApplying cross attention optimization (Doggettx).\r\nTextual inversion embeddings loaded(0):\r\nModel loaded in 3.2s (load weights from disk: 1.6s, create model: 0.5s, apply weights to model: 0.5s, apply half(): 0.6s).\r\nRunning on local URL:  http://127.0.0.1:7860\r\n\r\nTo create a public link, set `share=True` in `launch()`.\r\nStartup time: 12.3s (import torch: 2.7s, import gradio: 1.5s, import ldm: 0.4s, other imports: 1.3s, setup codeformer: 0.1s, load scripts: 2.2s, load SD checkpoint: 3.5s, create ui: 0.3s, gradio launch: 0.1s).\r\nError completing request\r\nArguments: (1, None, [<tempfile._TemporaryFileWrapper object at 0x000001E5841F11E0>], '', '', True, 0, 4, 512, 512, True, 'None', 'None', 0, 0, 0, 0) {}\r\nTraceback (most recent call last):\r\n  File \"C:\\AI\\stable-diffusion-webui\\modules\\call_queue.py\", line 56, in f\r\n    res = list(func(*args, **kwargs))\r\n  File \"C:\\AI\\stable-diffusion-webui\\modules\\call_queue.py\", line 37, in f\r\n    res = func(*args, **kwargs)\r\n  File \"C:\\AI\\stable-diffusion-webui\\modules\\postprocessing.py\", line 21, in run_postprocessing\r\n    image = Image.open(img)\r\n  File \"C:\\AI\\stable-diffusion-webui\\venv\\lib\\site-packages\\PIL\\Image.py\", line 3283, in open\r\n    raise UnidentifiedImageError(msg)\r\nPIL.UnidentifiedImageError: cannot identify image file <tempfile._TemporaryFileWrapper object at 0x000001E5841F11E0>\n```\n\n\n### Additional information\n\n_No response_\n", "hints_text": "I just got the exact same error when trying to upscale images\n> I just got the exact same error when trying to upscale images\r\n\r\nyes. I got the same, when trying to inpaint for only masked\nI am also having this issue, I did notice if I load images from the directory then it works fine\nSame here :-( Is there any update onm this issue yet?\nAFAIK this problem happens on https://github.com/AUTOMATIC1111/stable-diffusion-webui/commit/8ea8e712c43e493a9c96dcec7dfbc036a8630c97 and after, but not on https://github.com/AUTOMATIC1111/stable-diffusion-webui/commit/6a04a7f20fcc4a992ae017b06723e9ceffe17b37.\n> AFAIK this problem happens on [8ea8e71](https://github.com/AUTOMATIC1111/stable-diffusion-webui/commit/8ea8e712c43e493a9c96dcec7dfbc036a8630c97) and after, but not on [6a04a7f](https://github.com/AUTOMATIC1111/stable-diffusion-webui/commit/6a04a7f20fcc4a992ae017b06723e9ceffe17b37).\r\n\r\nThank you!! Very helpful\nSo will this be fixed in the next update?\n#9400 same problem when i use scripts 'multi-frame-video-rendering', notice me pls if anyone know how to fix this issue.\n> #9400 same problem when i use scripts 'multi-frame-video-rendering', notice me pls if anyone know how to fix this issue.\r\n\r\nline28 of its .py file\r\n change to \r\nfile_count = \"directory\" \r\nfrom =\"multiple\"", "created_at": "2023-04-09T20:23:38Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 9404, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-9404", "issue_numbers": ["9402"], "base_commit": "fdac48683595699189aa84f8c1bea7b3bf718801", "patch": "diff --git a/webui.sh b/webui.sh\nindex 30beac5b825..35e7b74ecdb 100755\n--- a/webui.sh\n+++ b/webui.sh\n@@ -118,8 +118,9 @@ case \"$gpu_info\" in\n esac\n if echo \"$gpu_info\" | grep -q \"AMD\" && [[ -z \"${TORCH_COMMAND}\" ]]\n then\n-    export TORCH_COMMAND=\"pip install torch torchvision --extra-index-url https://download.pytorch.org/whl/rocm5.2\"\n-fi\n+    # AMD users will still use torch 1.13 because 2.0 does not seem to work.\n+    export TORCH_COMMAND=\"pip install torch==1.13.1+rocm5.2 torchvision==0.14.1+rocm5.2 --index-url https://download.pytorch.org/whl/rocm5.2\"\n+fi  \n \n for preq in \"${GIT}\" \"${python_cmd}\"\n do\n", "test_patch": "", "problem_statement": "[Bug]: AssertionError: Torch is not able to use GPU; add --skip-torch-cuda-test to COMMANDLINE_ARGS variable to disable this check\n### Is there an existing issue for this?\r\n\r\n- [X] I have searched the existing issues and checked the recent builds/commits\r\n\r\n### What happened?\r\n\r\nAfter following the \"Automatic Installation\" instructions from the wiki for AMD cards, I get the following error:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/basil/stable-diffusion-webui/launch.py\", line 355, in <module>\r\n    prepare_environment()\r\n  File \"/home/basil/stable-diffusion-webui/launch.py\", line 260, in prepare_environment\r\n    run_python(\"import torch; assert torch.cuda.is_available(), 'Torch is not able to use GPU; add --skip-torch-cuda-test to COMMANDLINE_ARGS variable to disable this check'\")\r\n  File \"/home/basil/stable-diffusion-webui/launch.py\", line 121, in run_python\r\n    return run(f'\"{python}\" -c \"{code}\"', desc, errdesc)\r\n  File \"/home/basil/stable-diffusion-webui/launch.py\", line 97, in run\r\n    raise RuntimeError(message)\r\nRuntimeError: Error running command.\r\nCommand: \"/home/basil/stable-diffusion-webui/venv/bin/python3\" -c \"import torch; assert torch.cuda.is_available(), 'Torch is not able to use GPU; add --skip-torch-cuda-test to COMMANDLINE_ARGS variable to disable this check'\"\r\nError code: 1\r\nstdout: <empty>\r\nstderr: Traceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\nAssertionError: Torch is not able to use GPU; add --skip-torch-cuda-test to COMMANDLINE_ARGS variable to disable this check\r\n```\r\n\r\nIf I pass `--skip-torch-cuda-test`, I then get the following error:\r\n```\r\nError completing request\r\nArguments: ('task(ry0zf9rah332ozo)', 'caaa', '(low quality, worst quality:1.4),(bad_prompt:0.8), (monochrome:1.1), (greyscale)', [], 28, 16, False, False, 1, 1, 7, -1.0, -1.0, 0, 0, 0, False, 768, 512, False, 0.7, 2, 'Latent', 0, 0, 0, [], 0, False, False, 'positive', 'comma', 0, False, False, '', 1, '', 0, '', 0, '', True, False, False, False, 0) {}\r\nTraceback (most recent call last):\r\n  File \"/home/basil/stable-diffusion-webui/modules/call_queue.py\", line 56, in f\r\n    res = list(func(*args, **kwargs))\r\n  File \"/home/basil/stable-diffusion-webui/modules/call_queue.py\", line 37, in f\r\n    res = func(*args, **kwargs)\r\n  File \"/home/basil/stable-diffusion-webui/modules/txt2img.py\", line 56, in txt2img\r\n    processed = process_images(p)\r\n  File \"/home/basil/stable-diffusion-webui/modules/processing.py\", line 503, in process_images\r\n    res = process_images_inner(p)\r\n  File \"/home/basil/stable-diffusion-webui/modules/processing.py\", line 642, in process_images_inner\r\n    uc = get_conds_with_caching(prompt_parser.get_learned_conditioning, negative_prompts, p.steps, cached_uc)\r\n  File \"/home/basil/stable-diffusion-webui/modules/processing.py\", line 587, in get_conds_with_caching\r\n    cache[1] = function(shared.sd_model, required_prompts, steps)\r\n  File \"/home/basil/stable-diffusion-webui/modules/prompt_parser.py\", line 140, in get_learned_conditioning\r\n    conds = model.get_learned_conditioning(texts)\r\n  File \"/home/basil/stable-diffusion-webui/repositories/stable-diffusion-stability-ai/ldm/models/diffusion/ddpm.py\", line 669, in get_learned_conditioning\r\n    c = self.cond_stage_model(c)\r\n  File \"/home/basil/stable-diffusion-webui/venv/lib64/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/basil/stable-diffusion-webui/modules/sd_hijack_clip.py\", line 229, in forward\r\n    z = self.process_tokens(tokens, multipliers)\r\n  File \"/home/basil/stable-diffusion-webui/modules/sd_hijack_clip.py\", line 254, in process_tokens\r\n    z = self.encode_with_transformers(tokens)\r\n  File \"/home/basil/stable-diffusion-webui/modules/sd_hijack_clip.py\", line 302, in encode_with_transformers\r\n    outputs = self.wrapped.transformer(input_ids=tokens, output_hidden_states=-opts.CLIP_stop_at_last_layers)\r\n  File \"/home/basil/stable-diffusion-webui/venv/lib64/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/basil/stable-diffusion-webui/venv/lib64/python3.10/site-packages/transformers/models/clip/modeling_clip.py\", line 811, in forward\r\n    return self.text_model(\r\n  File \"/home/basil/stable-diffusion-webui/venv/lib64/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/basil/stable-diffusion-webui/venv/lib64/python3.10/site-packages/transformers/models/clip/modeling_clip.py\", line 721, in forward\r\n    encoder_outputs = self.encoder(\r\n  File \"/home/basil/stable-diffusion-webui/venv/lib64/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/basil/stable-diffusion-webui/venv/lib64/python3.10/site-packages/transformers/models/clip/modeling_clip.py\", line 650, in forward\r\n    layer_outputs = encoder_layer(\r\n  File \"/home/basil/stable-diffusion-webui/venv/lib64/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/basil/stable-diffusion-webui/venv/lib64/python3.10/site-packages/transformers/models/clip/modeling_clip.py\", line 378, in forward\r\n    hidden_states = self.layer_norm1(hidden_states)\r\n  File \"/home/basil/stable-diffusion-webui/venv/lib64/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/basil/stable-diffusion-webui/venv/lib64/python3.10/site-packages/torch/nn/modules/normalization.py\", line 190, in forward\r\n    return F.layer_norm(\r\n  File \"/home/basil/stable-diffusion-webui/venv/lib64/python3.10/site-packages/torch/nn/functional.py\", line 2515, in layer_norm\r\n    return torch.layer_norm(input, normalized_shape, weight, bias, eps, torch.backends.cudnn.enabled)\r\nRuntimeError: \"LayerNormKernelImpl\" not implemented for 'Half'\r\n```\r\n\r\nIf I then pass `--skip-torch-cuda-test --precision full --no-half`, the UI begins using my CPU to generate images instead of using my GPU like it should.\r\n\r\nWith the Docker method, I get the same error about Torch not being able to use the GPU, add `--skip-torch-cuda`, etc.\r\n\r\n### Steps to reproduce the problem\r\n\r\n1. Follow the instructions [here](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Install-and-Run-on-AMD-GPUs).\r\n\r\n### What should have happened?\r\n\r\nThe UI should have ran normally and utilized my GPU to generate images.\r\n\r\n### Commit where the problem happens\r\n\r\nhttps://github.com/AUTOMATIC1111/stable-diffusion-webui/commit/22bcc7be428c94e9408f589966c2040187245d81\r\n\r\n### What platforms do you use to access the UI ?\r\n\r\nLinux\r\n\r\n### What browsers do you use to access the UI ?\r\n\r\nMozilla Firefox\r\n\r\n### Command Line Arguments\r\n\r\n```Shell\r\n./webui.sh\r\n./webui.sh --skip-torch-cuda-test\r\n./webui.sh --skip-torch-cuda-test --precision full --no-half\r\n```\r\n\r\n\r\n### List of extensions\r\n\r\nNo\r\n\r\n### Console logs\r\n\r\n```Shell\r\n./webui.sh \r\n\r\n################################################################\r\nInstall script for stable-diffusion + Web UI\r\nTested on Debian 11 (Bullseye)\r\n################################################################\r\n\r\n################################################################\r\nRunning on basil user\r\n################################################################\r\n\r\n################################################################\r\nRepo already cloned, using it as install directory\r\n################################################################\r\n\r\n################################################################\r\nCreate and activate python venv\r\n################################################################\r\n\r\n################################################################\r\nLaunching launch.py...\r\n################################################################\r\nPython 3.10.10 (main, Mar 01 2023, 21:10:14) [GCC]\r\nCommit hash: 22bcc7be428c94e9408f589966c2040187245d81\r\nInstalling torch and torchvision\r\nLooking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/rocm5.2\r\nCollecting torch\r\n  Using cached torch-2.0.0-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\r\nCollecting torchvision\r\n  Using cached torchvision-0.15.1-cp310-cp310-manylinux1_x86_64.whl (6.0 MB)\r\nCollecting nvidia-cudnn-cu11==8.5.0.96\r\n  Using cached nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\r\nCollecting nvidia-cufft-cu11==10.9.0.58\r\n  Using cached nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\r\nCollecting nvidia-cusolver-cu11==11.4.0.1\r\n  Using cached nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\r\nCollecting nvidia-nccl-cu11==2.14.3\r\n  Using cached nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\r\nCollecting typing-extensions\r\n  Using cached typing_extensions-4.5.0-py3-none-any.whl (27 kB)\r\nCollecting nvidia-nvtx-cu11==11.7.91\r\n  Using cached nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\r\nCollecting networkx\r\n  Using cached networkx-3.1-py3-none-any.whl (2.1 MB)\r\nCollecting nvidia-cuda-cupti-cu11==11.7.101\r\n  Using cached nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\r\nCollecting nvidia-curand-cu11==10.2.10.91\r\n  Using cached nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\r\nCollecting nvidia-cusparse-cu11==11.7.4.91\r\n  Using cached nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\r\nCollecting triton==2.0.0\r\n  Using cached https://download.pytorch.org/whl/triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\r\nCollecting filelock\r\n  Using cached filelock-3.10.7-py3-none-any.whl (10 kB)\r\nCollecting jinja2\r\n  Using cached https://download.pytorch.org/whl/Jinja2-3.1.2-py3-none-any.whl (133 kB)\r\nCollecting nvidia-cublas-cu11==11.10.3.66\r\n  Using cached nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\r\nCollecting sympy\r\n  Using cached https://download.pytorch.org/whl/sympy-1.11.1-py3-none-any.whl (6.5 MB)\r\nCollecting nvidia-cuda-runtime-cu11==11.7.99\r\n  Using cached nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\r\nCollecting nvidia-cuda-nvrtc-cu11==11.7.99\r\n  Using cached nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\r\nRequirement already satisfied: wheel in ./venv/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.40.0)\r\nRequirement already satisfied: setuptools in ./venv/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (65.5.0)\r\nCollecting cmake\r\n  Using cached cmake-3.26.1-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (24.0 MB)\r\nCollecting lit\r\n  Using cached lit-16.0.0-py3-none-any.whl\r\nCollecting requests\r\n  Using cached requests-2.28.2-py3-none-any.whl (62 kB)\r\nCollecting pillow!=8.3.*,>=5.3.0\r\n  Using cached Pillow-9.5.0-cp310-cp310-manylinux_2_28_x86_64.whl (3.4 MB)\r\nCollecting numpy\r\n  Using cached numpy-1.24.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\r\nCollecting MarkupSafe>=2.0\r\n  Using cached https://download.pytorch.org/whl/MarkupSafe-2.1.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\r\nCollecting charset-normalizer<4,>=2\r\n  Using cached charset_normalizer-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (199 kB)\r\nCollecting idna<4,>=2.5\r\n  Using cached https://download.pytorch.org/whl/idna-3.4-py3-none-any.whl (61 kB)\r\nCollecting urllib3<1.27,>=1.21.1\r\n  Using cached urllib3-1.26.15-py2.py3-none-any.whl (140 kB)\r\nCollecting certifi>=2017.4.17\r\n  Using cached https://download.pytorch.org/whl/certifi-2022.12.7-py3-none-any.whl (155 kB)\r\nCollecting mpmath>=0.19\r\n  Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\r\nInstalling collected packages: mpmath, lit, cmake, urllib3, typing-extensions, sympy, pillow, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, numpy, networkx, MarkupSafe, idna, filelock, charset-normalizer, certifi, requests, nvidia-cusolver-cu11, nvidia-cudnn-cu11, jinja2, triton, torch, torchvision\r\nSuccessfully installed MarkupSafe-2.1.2 certifi-2022.12.7 charset-normalizer-3.1.0 cmake-3.26.1 filelock-3.10.7 idna-3.4 jinja2-3.1.2 lit-16.0.0 mpmath-1.3.0 networkx-3.1 numpy-1.24.2 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 pillow-9.5.0 requests-2.28.2 sympy-1.11.1 torch-2.0.0 torchvision-0.15.1 triton-2.0.0 typing-extensions-4.5.0 urllib3-1.26.15\r\nTraceback (most recent call last):\r\n  File \"/home/basil/stable-diffusion-webui/launch.py\", line 355, in <module>\r\n    prepare_environment()\r\n  File \"/home/basil/stable-diffusion-webui/launch.py\", line 260, in prepare_environment\r\n    run_python(\"import torch; assert torch.cuda.is_available(), 'Torch is not able to use GPU; add --skip-torch-cuda-test to COMMANDLINE_ARGS variable to disable this check'\")\r\n  File \"/home/basil/stable-diffusion-webui/launch.py\", line 121, in run_python\r\n    return run(f'\"{python}\" -c \"{code}\"', desc, errdesc)\r\n  File \"/home/basil/stable-diffusion-webui/launch.py\", line 97, in run\r\n    raise RuntimeError(message)\r\nRuntimeError: Error running command.\r\nCommand: \"/home/basil/stable-diffusion-webui/venv/bin/python3\" -c \"import torch; assert torch.cuda.is_available(), 'Torch is not able to use GPU; add --skip-torch-cuda-test to COMMANDLINE_ARGS variable to disable this check'\"\r\nError code: 1\r\nstdout: <empty>\r\nstderr: Traceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\nAssertionError: Torch is not able to use GPU; add --skip-torch-cuda-test to COMMANDLINE_ARGS variable to disable this check\r\n```\r\n\r\n\r\n### Additional information\r\n\r\nWhen I was on Arch Linux (about a month ago), the \"Automatic Installation\" instructions worked fine; the UI would install pytorch+rocm normally and the UI would generate images normally using my GPU.\r\n\r\nI am now on openSUSE Tumbleweed. My GPU is a 6700 XT.\r\n\r\nAnother thing worth noting: at the bottom of the UI, the PyTorch version is `torch: 2.0.0+cu117`.\n", "hints_text": "Install the correct Pytorch. `pip install torch --index-url https://download.pytorch.org/whl/rocm5.4.2` (may need to add `--force-reinstall`)\r\nor open the webui-user.sh file and under \"Install command for Torch\", add `export TORCH_COMMAND=\"pip install torch --index-url https://download.pytorch.org/whl/rocm5.4.2\"`", "created_at": "2023-04-05T23:48:15Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 9319, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-9319", "issue_numbers": ["9046"], "base_commit": "840d1854cdafb9aec72f3461bc5ce1cd316c1fa9", "patch": "diff --git a/webui.py b/webui.py\nindex b570895fb2c..8927aa330f8 100644\n--- a/webui.py\n+++ b/webui.py\n@@ -5,6 +5,7 @@\n import signal\r\n import re\r\n import warnings\r\n+import asyncio\r\n from fastapi import FastAPI\r\n from fastapi.middleware.cors import CORSMiddleware\r\n from fastapi.middleware.gzip import GZipMiddleware\r\n@@ -66,6 +67,46 @@\n else:\r\n     server_name = \"0.0.0.0\" if cmd_opts.listen else None\r\n \r\n+if sys.platform == \"win32\" and hasattr(asyncio, \"WindowsSelectorEventLoopPolicy\"):\r\n+    # \"Any thread\" and \"selector\" should be orthogonal, but there's not a clean\r\n+    # interface for composing policies so pick the right base.\r\n+    _BasePolicy = asyncio.WindowsSelectorEventLoopPolicy  # type: ignore\r\n+else:\r\n+    _BasePolicy = asyncio.DefaultEventLoopPolicy\r\n+\r\n+\r\n+class AnyThreadEventLoopPolicy(_BasePolicy):  # type: ignore\r\n+    \"\"\"Event loop policy that allows loop creation on any thread.\r\n+\r\n+    The default `asyncio` event loop policy only automatically creates\r\n+    event loops in the main threads. Other threads must create event\r\n+    loops explicitly or `asyncio.get_event_loop` (and therefore\r\n+    `.IOLoop.current`) will fail. Installing this policy allows event\r\n+    loops to be created automatically on any thread, matching the\r\n+    behavior of Tornado versions prior to 5.0 (or 5.0 on Python 2).\r\n+\r\n+    Usage::\r\n+\r\n+        asyncio.set_event_loop_policy(AnyThreadEventLoopPolicy())\r\n+\r\n+    .. versionadded:: 5.0\r\n+\r\n+    \"\"\"\r\n+\r\n+    def get_event_loop(self) -> asyncio.AbstractEventLoop:\r\n+        try:\r\n+            return super().get_event_loop()\r\n+        except (RuntimeError, AssertionError):\r\n+            # This was an AssertionError in python 3.4.2 (which ships with debian jessie)\r\n+            # and changed to a RuntimeError in 3.4.3.\r\n+            # \"There is no current event loop in thread %r\"\r\n+            loop = self.new_event_loop()\r\n+            self.set_event_loop(loop)\r\n+            return loop\r\n+\r\n+\r\n+asyncio.set_event_loop_policy(AnyThreadEventLoopPolicy())\r\n+\r\n \r\n def check_versions():\r\n     if shared.cmd_opts.skip_version_check:\r\n", "test_patch": "", "problem_statement": "[Bug]: /sdapi/v1/txt2img endpoint not working\n### Is there an existing issue for this?\n\n- [X] I have searched the existing issues and checked the recent builds/commits\n\n### What happened?\n\nToday when i tried to use my discord bot to make some requests to the endpoint /sdapi/v1/txt2img, i kept getting the error: \"There is no current event loop in thread 'AnyIO worker thread'.\" Last successful generations were around 48h ago.\n\n### Steps to reproduce the problem\n\n1. Start fast-stable-diffusion colab version by TheLastBen [colab](https://colab.research.google.com/github/TheLastBen/fast-stable-diffusion/blob/main/fast_stable_diffusion_AUTOMATIC1111.ipynb)\r\n2. . Make a post request using to the endpoint /sdapi/v1/txt2img (or using any other tool like Postman)\r\n<img width=\"883\" alt=\"1234\" src=\"https://user-images.githubusercontent.com/5587806/227983838-6ba7cbb5-c416-4120-bafe-0273e065a5a7.png\">\r\n<img width=\"861\" alt=\"12345\" src=\"https://user-images.githubusercontent.com/5587806/227985350-b6d3f781-f28f-451b-9c95-748ddb574100.png\">\r\n\n\n### What should have happened?\n\nGenerate image and return success response json body with image path\n\n### Commit where the problem happens\n\nhttps://github.com/AUTOMATIC1111/stable-diffusion-webui/commit/955df7751eef11bb7697e2d77f6b8a6226b21e13\n\n### What platforms do you use to access the UI ?\n\nOther/Cloud\n\n### What browsers do you use to access the UI ?\n\nGoogle Chrome\n\n### Command Line Arguments\n\n```Shell\nno\n```\n\n\n### List of extensions\n\nno\n\n### Console logs\n\n```Shell\nAPI error: POST: https://41209b1e8d4c8c9c05.gradio.live/sdapi/v1/txt2img {'error': 'RuntimeError', 'detail': '', 'body': '', 'errors': \"There is no current event loop in thread 'AnyIO worker thread'.\"}\r\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Traceback (most recent call last) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\r\n\u2502 /usr/local/lib/python3.9/dist-packages/anyio/streams/memory.py:94 in receive \u2502\r\n\u2502                                                                              \u2502\r\n\u2502 /usr/local/lib/python3.9/dist-packages/anyio/streams/memory.py:89 in         \u2502\r\n\u2502 receive_nowait                                                               \u2502\r\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\r\nWouldBlock\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Traceback (most recent call last) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\r\n\u2502 /usr/local/lib/python3.9/dist-packages/starlette/middleware/base.py:43 in    \u2502\r\n\u2502 call_next                                                                    \u2502\r\n\u2502                                                                              \u2502\r\n\u2502 /usr/local/lib/python3.9/dist-packages/anyio/streams/memory.py:114 in        \u2502\r\n\u2502 receive                                                                      \u2502\r\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\r\nEndOfStream\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Traceback (most recent call last) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\r\n\u2502 /content/gdrive/MyDrive/sd/stable-diffusion-webui/modules/api/api.py:145 in  \u2502\r\n\u2502 exception_handling                                                           \u2502\r\n\u2502                                                                              \u2502\r\n\u2502   144 \u2502   \u2502   try:                                                           \u2502\r\n\u2502 \u2771 145 \u2502   \u2502   \u2502   return await call_next(request)                            \u2502\r\n\u2502   146 \u2502   \u2502   except Exception as e:                                         \u2502\r\n\u2502                                                                              \u2502\r\n\u2502 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 locals \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\r\n\u2502 \u2502        call_next = <function                                             \u2502 \u2502\r\n\u2502 \u2502                    BaseHTTPMiddleware.__call__.<locals>.call_next at     \u2502 \u2502\r\n\u2502 \u2502                    0x7f45ac0faf70>                                       \u2502 \u2502\r\n\u2502 \u2502                e = RuntimeError(\"There is no current event loop in       \u2502 \u2502\r\n\u2502 \u2502                    thread 'AnyIO worker thread'.\")                       \u2502 \u2502\r\n\u2502 \u2502 handle_exception = <function api_middleware.<locals>.handle_exception at \u2502 \u2502\r\n\u2502 \u2502                    0x7f45ac638dc0>                                       \u2502 \u2502\r\n\u2502 \u2502          request = <starlette.requests.Request object at 0x7f45ac0b2790> \u2502 \u2502\r\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\r\n\u2502                                                                              \u2502\r\n\u2502 /usr/local/lib/python3.9/dist-packages/starlette/middleware/base.py:46 in    \u2502\r\n\u2502 call_next                                                                    \u2502\r\n\u2502                                                                              \u2502\r\n\u2502                           ... 25 frames hidden ...                           \u2502\r\n\u2502                                                                              \u2502\r\n\u2502 /usr/lib/python3.9/asyncio/locks.py:81 in __init__                           \u2502\r\n\u2502                                                                              \u2502\r\n\u2502    80 \u2502   \u2502   if loop is None:                                               \u2502\r\n\u2502 \u2771  81 \u2502   \u2502   \u2502   self._loop = events.get_event_loop()                       \u2502\r\n\u2502    82 \u2502   \u2502   else:                                                          \u2502\r\n\u2502                                                                              \u2502\r\n\u2502 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 locals \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e          \u2502\r\n\u2502 \u2502 loop = None                                                     \u2502          \u2502\r\n\u2502 \u2502 self = <asyncio.locks.Lock object at 0x7f45ac06b0a0 [unlocked]> \u2502          \u2502\r\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f          \u2502\r\n\u2502                                                                              \u2502\r\n\u2502 /usr/lib/python3.9/asyncio/events.py:642 in get_event_loop                   \u2502\r\n\u2502                                                                              \u2502\r\n\u2502   641 \u2502   \u2502   if self._local._loop is None:                                  \u2502\r\n\u2502 \u2771 642 \u2502   \u2502   \u2502   raise RuntimeError('There is no current event loop in thre \u2502\r\n\u2502   643 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502      % threading.current_thread().name)      \u2502\r\n\u2502                                                                              \u2502\r\n\u2502 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 locals \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\r\n\u2502 \u2502 self = <asyncio.unix_events._UnixDefaultEventLoopPolicy object at        \u2502 \u2502\r\n\u2502 \u2502        0x7f45adf50820>                                                   \u2502 \u2502\r\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\r\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\r\nRuntimeError: There is no current event loop in thread 'AnyIO worker thread'.\n```\n\n\n### Additional information\n\n_No response_\n", "hints_text": "same here, must be a new thing\nCan confirm, just updated my webui installation and it broke the API\r\nI guess I have to downgrade then...\nSame\nsame\nI also encountered the same problem.\nSame error on Colab.\r\nOn local Windows PC - OK (just done fresh install).\nyep. same issue in colab\r\n\nIf this problem is on colab, it's a Linux problem I believe since I try to make the API work on my server.\r\nIsn't there any solution?\nCould anyone solve this?\nany solution for colab?\ni didnt find anything yet\nsry wrong button\nSame here. I'm using 'git checkout 9377092a892687a8ae43ba5f7df44f0929ab2997' as a temporary solution\nI have the same problem\nAnother workaround while keeping the latest commit is to edit the `modules/api/api.py`.\r\nI don't think this is a good solution though, so I don't want to make a PR.\r\n\r\n```diff\r\ndiff --git a/modules/api/api.py b/modules/api/api.py\r\nindex 518b2a61..a9cef928 100644\r\n--- a/modules/api/api.py\r\n+++ b/modules/api/api.py\r\n@@ -29,6 +29,7 @@ from modules import devices\r\n from typing import List\r\n import piexif\r\n import piexif.helper\r\n+import asyncio\r\n \r\n def upscaler_to_index(name: str):\r\n     try:\r\n@@ -276,6 +277,15 @@ class Api:\r\n         return script_args\r\n \r\n     def text2imgapi(self, txt2imgreq: StableDiffusionTxt2ImgProcessingAPI):\r\n+        try:\r\n+            loop = asyncio.get_event_loop()\r\n+        except RuntimeError as e:\r\n+            if str(e).startswith('There is no current event loop in thread'):\r\n+                loop = asyncio.new_event_loop()\r\n+                asyncio.set_event_loop(loop)\r\n+            else:\r\n+                raise\r\n+\r\n         script_runner = scripts.scripts_txt2img\r\n         if not script_runner.scripts:\r\n             script_runner.initialize_scripts(False)\r\n```\nsame\n@MeemeeLab solution worked... thanks for this.\nuntil some1 fix this issue, @MeemeeLab solution is working fine, thanks for the help.\nThe issue is in the source code.\r\n\r\n```\r\ndef get_event_loop(self):\r\n    \"\"\"Get the event loop.\r\n    This may be None or an instance of EventLoop.\r\n    \"\"\"\r\n    if (self._local._loop is None and\r\n            not self._local._set_called and\r\n            isinstance(threading.current_thread(), threading._MainThread)):\r\n        self.set_event_loop(self.new_event_loop())\r\n\r\n    if self._local._loop is None:\r\n        raise RuntimeError('There is no current event loop in thread %r.'\r\n                            % threading.current_thread().name)\r\n\r\n    return self._local._loop\r\n```\n> The issue is in the source code.\r\n> \r\n> ```\r\n> def get_event_loop(self):\r\n>     \"\"\"Get the event loop.\r\n>     This may be None or an instance of EventLoop.\r\n>     \"\"\"\r\n>     if (self._local._loop is None and\r\n>             not self._local._set_called and\r\n>             isinstance(threading.current_thread(), threading._MainThread)):\r\n>         self.set_event_loop(self.new_event_loop())\r\n> \r\n>     if self._local._loop is None:\r\n>         raise RuntimeError('There is no current event loop in thread %r.'\r\n>                             % threading.current_thread().name)\r\n> \r\n>     return self._local._loop\r\n> ```\r\n\r\nI belive it's internal code of asyncio, there's should be a mistake in the code, not in the library.", "created_at": "2023-04-03T09:06:30Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 9219, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-9219", "issue_numbers": ["9185"], "base_commit": "579e13df7caee2c744f7c90c3730494a8ca02cbd", "patch": "diff --git a/modules/img2img.py b/modules/img2img.py\nindex 953ac5d2d12..5ce408f4809 100644\n--- a/modules/img2img.py\n+++ b/modules/img2img.py\n@@ -4,7 +4,7 @@\n import traceback\r\n \r\n import numpy as np\r\n-from PIL import Image, ImageOps, ImageFilter, ImageEnhance, ImageChops\r\n+from PIL import Image, ImageOps, ImageFilter, ImageEnhance, ImageChops, UnidentifiedImageError\r\n \r\n from modules import devices, sd_samplers\r\n from modules.generation_parameters_copypaste import create_override_settings_dict\r\n@@ -46,7 +46,10 @@ def process_batch(p, input_dir, output_dir, inpaint_mask_dir, args):\n         if state.interrupted:\r\n             break\r\n \r\n-        img = Image.open(image)\r\n+        try:\r\n+            img = Image.open(image)\r\n+        except UnidentifiedImageError:\r\n+            continue\r\n         # Use the EXIF orientation of photos taken by smartphones.\r\n         img = ImageOps.exif_transpose(img)\r\n         p.init_images = [img] * p.batch_size\r\n", "test_patch": "", "problem_statement": "[Feature Request]: img2img batch should ignore non-image files\n### Is there an existing issue for this?\n\n- [X] I have searched the existing issues and checked the recent builds/commits\n\n### What would your feature do ?\n\nI may place some parameter files along the image files in the same folder.\r\nCurrently an exception is throw for non-image files when do \"Image.open()\".\n\n### Proposed workflow\n\n1. Place a txt file along with the png file in folder A\r\n2. Set batch input folder to A\r\n3. Press generate\r\n\n\n### Additional information\n\n_No response_\n", "hints_text": "@liudibo \r\nIn [img2img.py](https://github.com/AUTOMATIC1111/stable-diffusion-webui/blob/master/modules/img2img.py) you can see that for the batch (`process_batch()`) the contents of directory are retrieved with `shared.listfiles()`. In [shared.py](https://github.com/AUTOMATIC1111/stable-diffusion-webui/blob/master/modules/shared.py), you can see that `listfiles()` actually reads all files except those starting with a dot (`.` like Linux hidden files).\r\n\r\nSo you should start your parameter files with a dot (example: `.parameter.txt`) to avoid this. If you're on Windows it doesn't even hide the files.\r\n\r\nIt seems also that `listfiles()` is only used to retrieve images in a folder, so maybe it should be corrected to grab only the files with supported image extensions.\nIf we want to modify `listfile()`, it's probably as simple as adding lines similar to the following before the `return` statement:\r\n```\r\n    imagefilenames = []\r\n    for extension in [\".png\", \"jpg\"]:\r\n        imagefilenames += [file for file in filenames if file.lower().endswith(extension)]\r\n```\r\nHowever file extensions do not guarantee that the file is actually an image, same as no extension does not mean that the file is not an image. And you could also have an image with a unrelated extension.\r\nAlso the extension list has to represent all cases of possible image extensions that are supported.\r\n\r\nProbably a better way to handle this is to catch the exception `PIL.UnidentifiedImageError` being thrown when using `Image.open()` in `process_batch()`. For instance:\r\n\r\n```\r\n        try:\r\n            img = Image.open(image)\r\n        except PIL.UnidentifiedImageError:\r\n            continue\r\n```", "created_at": "2023-03-31T08:55:58Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 9130, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-9130", "issue_numbers": ["9107"], "base_commit": "1dc21d79509cd1437f4bf150872fefdeeaf54ab8", "patch": "diff --git a/modules/api/api.py b/modules/api/api.py\nindex 518b2a61f52..8c5cd185375 100644\n--- a/modules/api/api.py\n+++ b/modules/api/api.py\n@@ -272,7 +272,9 @@ def init_script_args(self, request, default_script_args, selectable_scripts, sel\n                     raise HTTPException(status_code=422, detail=f\"Cannot have a selectable script in the always on scripts params\")\n                 # always on script with no arg should always run so you don't really need to add them to the requests\n                 if \"args\" in request.alwayson_scripts[alwayson_script_name]:\n-                    script_args[alwayson_script.args_from:alwayson_script.args_to] = request.alwayson_scripts[alwayson_script_name][\"args\"]\n+                    # min between arg length in scriptrunner and arg length in the request\n+                    for idx in range(0, min((alwayson_script.args_to - alwayson_script.args_from), len(request.alwayson_scripts[alwayson_script_name][\"args\"]))):\n+                        script_args[alwayson_script.args_from + idx] = request.alwayson_scripts[alwayson_script_name][\"args\"][idx]\n         return script_args\n \n     def text2imgapi(self, txt2imgreq: StableDiffusionTxt2ImgProcessingAPI):\n", "test_patch": "", "problem_statement": "[Bug]: Cannot load ControlNet Module with API /sdapi/v1/img2img\n### Is there an existing issue for this?\n\n- [X] I have searched the existing issues and checked the recent builds/commits\n\n### What happened?\n\nCannot load ControlNet Module with API /sdapi/v1/img2img\n\n### Steps to reproduce the problem\n\n1. Call API /sdapi/v1/img2img with \"controlnet\" in \"alwayson_script\" as described in https://github.com/Mikubill/sd-webui-controlnet/wiki/API#web-api   \r\n2. Produces following error and continue to do img2img without controlNet module.\r\n\r\nError running process: E:\\Workspace\\stable-diffusion-webui\\extensions\\sd-webui-controlnet\\scripts\\controlnet.py\r\nTraceback (most recent call last):\r\n  File \"E:\\Workspace\\stable-diffusion-webui\\modules\\scripts.py\", line 417, in process\r\n    script.process(p, *script_args)\r\n  File \"E:\\Workspace\\stable-diffusion-webui\\extensions\\sd-webui-controlnet\\scripts\\controlnet.py\", line 628, in process\r\n    unit = self.parse_remote_call(p, unit, idx)\r\n  File \"E:\\Workspace\\stable-diffusion-webui\\extensions\\sd-webui-controlnet\\scripts\\controlnet.py\", line 540, in parse_remote_call\r\n    unit.enabled = selector(p, \"control_net_enabled\", unit.enabled, idx, strict=True)\r\nAttributeError: 'str' object has no attribute 'enabled'\n\n### What should have happened?\n\nLoad ControlNet Module in used with img2img\n\n### Commit where the problem happens\n\nc7daba71\n\n### What platforms do you use to access the UI ?\n\nWindows\n\n### What browsers do you use to access the UI ?\n\nGoogle Chrome\n\n### Command Line Arguments\n\n```Shell\n--xformers --api --listen\n```\n\n\n### List of extensions\n\nsd-webui-controlnet\n\n### Console logs\n\n```Shell\nError running process: E:\\Workspace\\stable-diffusion-webui\\extensions\\sd-webui-controlnet\\scripts\\controlnet.py\r\nTraceback (most recent call last):\r\n  File \"E:\\Workspace\\stable-diffusion-webui\\modules\\scripts.py\", line 417, in process\r\n    script.process(p, *script_args)\r\n  File \"E:\\Workspace\\stable-diffusion-webui\\extensions\\sd-webui-controlnet\\scripts\\controlnet.py\", line 628, in process\r\n    unit = self.parse_remote_call(p, unit, idx)\r\n  File \"E:\\Workspace\\stable-diffusion-webui\\extensions\\sd-webui-controlnet\\scripts\\controlnet.py\", line 540, in parse_remote_call\r\n    unit.enabled = selector(p, \"control_net_enabled\", unit.enabled, idx, strict=True)\r\nAttributeError: 'str' object has no attribute 'enabled'\n```\n\n\n### Additional information\n\nAfter revert commit c7daba71, the problem gone.\n", "hints_text": "To me, while this is linked to the recent PR, this seems to belong in sd-webui-controlnet repo issues unless you can replicate this type of error on other extensions. ~I'd open an issue on their repo and include the commit of the webui and of the commit used for the controlnet extension so they can have a look at how the changes affected them~. https://github.com/Mikubill/sd-webui-controlnet/issues/675\r\n\r\n That said, the API tests that I ran for controlnet passed after the refactor of the PR, but I'll take a look when time permits.\nThanks for the investigation. For your information, this issue is resolved once I revert your commit.  Also, this problem does not exist for txt2img.  So I guess it may be missing to pass the args unit to controlnet in this commit. ", "created_at": "2023-03-29T04:49:09Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 9052, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-9052", "issue_numbers": ["9049"], "base_commit": "955df7751eef11bb7697e2d77f6b8a6226b21e13", "patch": "diff --git a/javascript/extensions.js b/javascript/extensions.js\nindex c593cd2e570..72924a28cf9 100644\n--- a/javascript/extensions.js\n+++ b/javascript/extensions.js\n@@ -1,5 +1,5 @@\n \r\n-function extensions_apply(_, _){\r\n+function extensions_apply(_, _, disable_all){\r\n     var disable = []\r\n     var update = []\r\n \r\n@@ -13,10 +13,10 @@ function extensions_apply(_, _){\n \r\n     restart_reload()\r\n \r\n-    return [JSON.stringify(disable), JSON.stringify(update)]\r\n+    return [JSON.stringify(disable), JSON.stringify(update), disable_all]\r\n }\r\n \r\n-function extensions_check(){\r\n+function extensions_check(_, _){\r\n     var disable = []\r\n \r\n     gradioApp().querySelectorAll('#extensions input[type=\"checkbox\"]').forEach(function(x){\r\ndiff --git a/launch.py b/launch.py\nindex c41ae82d2f0..2e3cd4c4b26 100644\n--- a/launch.py\n+++ b/launch.py\n@@ -206,6 +206,10 @@ def list_extensions(settings_file):\n         print(e, file=sys.stderr)\r\n \r\n     disabled_extensions = set(settings.get('disabled_extensions', []))\r\n+    disable_all_extensions = settings.get('disable_all_extensions', 'none')\r\n+\r\n+    if disable_all_extensions != 'none':\r\n+        return []\r\n \r\n     return [x for x in os.listdir(extensions_dir) if x not in disabled_extensions]\r\n \r\ndiff --git a/modules/extensions.py b/modules/extensions.py\nindex 0d34b89abca..3a7a0372787 100644\n--- a/modules/extensions.py\n+++ b/modules/extensions.py\n@@ -15,7 +15,12 @@\n \r\n \r\n def active():\r\n-    return [x for x in extensions if x.enabled]\r\n+    if shared.opts.disable_all_extensions == \"all\":\r\n+        return []\r\n+    elif shared.opts.disable_all_extensions == \"extra\":\r\n+        return [x for x in extensions if x.enabled and x.is_builtin]\r\n+    else:\r\n+        return [x for x in extensions if x.enabled]\r\n \r\n \r\n class Extension:\r\n@@ -97,6 +102,11 @@ def list_extensions():\n     if not os.path.isdir(extensions_dir):\r\n         return\r\n \r\n+    if shared.opts.disable_all_extensions == \"all\":\r\n+        print(\"*** \\\"Disable all extensions\\\" option was set, will not load any extensions ***\")\r\n+    elif shared.opts.disable_all_extensions == \"extra\":\r\n+        print(\"*** \\\"Disable all extensions\\\" option was set, will only load built-in extensions ***\")\r\n+\r\n     extension_paths = []\r\n     for dirname in [extensions_dir, extensions_builtin_dir]:\r\n         if not os.path.isdir(dirname):\r\n@@ -112,4 +122,3 @@ def list_extensions():\n     for dirname, path, is_builtin in extension_paths:\r\n         extension = Extension(name=dirname, path=path, enabled=dirname not in shared.opts.disabled_extensions, is_builtin=is_builtin)\r\n         extensions.append(extension)\r\n-\r\ndiff --git a/modules/shared.py b/modules/shared.py\nindex 3ad0862b212..5fd0eecbd1f 100644\n--- a/modules/shared.py\n+++ b/modules/shared.py\n@@ -422,7 +422,8 @@ def list_samplers():\n }))\r\n \r\n options_templates.update(options_section((None, \"Hidden options\"), {\r\n-    \"disabled_extensions\": OptionInfo([], \"Disable those extensions\"),\r\n+    \"disabled_extensions\": OptionInfo([], \"Disable these extensions\"),\r\n+    \"disable_all_extensions\": OptionInfo(\"none\", \"Disable all extensions (preserves the list of disabled extensions)\", gr.Radio, {\"choices\": [\"none\", \"extra\", \"all\"]}),\r\n     \"sd_checkpoint_hash\": OptionInfo(\"\", \"SHA256 hash of the current checkpoint\"),\r\n }))\r\n \r\ndiff --git a/modules/ui_extensions.py b/modules/ui_extensions.py\nindex b4a0d6ecfad..efd6cda2889 100644\n--- a/modules/ui_extensions.py\n+++ b/modules/ui_extensions.py\n@@ -21,7 +21,7 @@ def check_access():\n     assert not shared.cmd_opts.disable_extension_access, \"extension access disabled because of command line flags\"\r\n \r\n \r\n-def apply_and_restart(disable_list, update_list):\r\n+def apply_and_restart(disable_list, update_list, disable_all):\r\n     check_access()\r\n \r\n     disabled = json.loads(disable_list)\r\n@@ -43,6 +43,7 @@ def apply_and_restart(disable_list, update_list):\n             print(traceback.format_exc(), file=sys.stderr)\r\n \r\n     shared.opts.disabled_extensions = disabled\r\n+    shared.opts.disable_all_extensions = disable_all\r\n     shared.opts.save(shared.config_filename)\r\n \r\n     shared.state.interrupt()\r\n@@ -99,9 +100,13 @@ def extension_table():\n         else:\r\n             ext_status = ext.status\r\n \r\n+        style = \"\"\r\n+        if shared.opts.disable_all_extensions == \"extra\" and not ext.is_builtin or shared.opts.disable_all_extensions == \"all\":\r\n+            style = ' style=\"color: var(--primary-400)\"'\r\n+\r\n         code += f\"\"\"\r\n             <tr>\r\n-                <td><label><input class=\"gr-check-radio gr-checkbox\" name=\"enable_{html.escape(ext.name)}\" type=\"checkbox\" {'checked=\"checked\"' if ext.enabled else ''}>{html.escape(ext.name)}</label></td>\r\n+                <td><label{style}><input class=\"gr-check-radio gr-checkbox\" name=\"enable_{html.escape(ext.name)}\" type=\"checkbox\" {'checked=\"checked\"' if ext.enabled else ''}>{html.escape(ext.name)}</label></td>\r\n                 <td>{remote}</td>\r\n                 <td>{ext.version}</td>\r\n                 <td{' class=\"extension_status\"' if ext.remote is not None else ''}>{ext_status}</td>\r\n@@ -294,16 +299,24 @@ def create_ui():\n                 with gr.Row(elem_id=\"extensions_installed_top\"):\r\n                     apply = gr.Button(value=\"Apply and restart UI\", variant=\"primary\")\r\n                     check = gr.Button(value=\"Check for updates\")\r\n+                    extensions_disable_all = gr.Radio(label=\"Disable all extensions\", choices=[\"none\", \"extra\", \"all\"], value=shared.opts.disable_all_extensions, elem_id=\"extensions_disable_all\")\r\n                     extensions_disabled_list = gr.Text(elem_id=\"extensions_disabled_list\", visible=False).style(container=False)\r\n                     extensions_update_list = gr.Text(elem_id=\"extensions_update_list\", visible=False).style(container=False)\r\n \r\n-                info = gr.HTML()\r\n+                html = \"\"\r\n+                if shared.opts.disable_all_extensions != \"none\":\r\n+                    html = \"\"\"\r\n+<span style=\"color: var(--primary-400);\">\r\n+    \"Disable all extensions\" was set, change it to \"none\" to load all extensions again\r\n+</span>\r\n+                    \"\"\"\r\n+                info = gr.HTML(html)\r\n                 extensions_table = gr.HTML(lambda: extension_table())\r\n \r\n                 apply.click(\r\n                     fn=apply_and_restart,\r\n                     _js=\"extensions_apply\",\r\n-                    inputs=[extensions_disabled_list, extensions_update_list],\r\n+                    inputs=[extensions_disabled_list, extensions_update_list, extensions_disable_all],\r\n                     outputs=[],\r\n                 )\r\n \r\n", "test_patch": "", "problem_statement": "[Feature Request]: Option to disable all extensions temporarily\n### Is there an existing issue for this?\n\n- [X] I have searched the existing issues and checked the recent builds/commits\n\n### What would your feature do ?\n\nSometimes when debugging issues or testing new base webui features, it's necessary to disable extensions to ensure there are no issues without them before turning them on again. However I have a lot of disabled extensions, and ones I turn on and off ocassionally, I have to remember which ones they were if I need to disable everything\n\n### Proposed workflow\n\nA checkbox `disable all extensions` which prevents anything in `extensions/` from loading, that also preserves the disabled extensions setting so you don't have to fix it when you want extensions again\n\n### Additional information\n\n_No response_\n", "hints_text": "", "created_at": "2023-03-27T16:49:36Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 8959, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-8959", "issue_numbers": ["8955"], "base_commit": "a0d07fb5807ad55c8ccfdfc9a6d9ae3c62b9d211", "patch": "diff --git a/style.css b/style.css\nindex 0dcc3e25d90..48973d5a359 100644\n--- a/style.css\n+++ b/style.css\n@@ -56,6 +56,9 @@ div.compact{\n \r\n .gradio-dropdown ul.options{\r\n     z-index: 3000;\r\n+    min-width: fit-content;\r\n+    max-width: inherit;\r\n+    white-space: nowrap;\r\n }\r\n \r\n .gradio-dropdown label span:not(.has-info),\r\n", "test_patch": "", "problem_statement": "Stable Diffusion checkpoint dropdown list in quick settings on top is broken\n### Is there an existing issue for this?\r\n\r\n- [X] I have searched the existing issues and checked the recent builds/commits\r\n\r\n### What happened?\r\n\r\nClicking the Stable Diffusion checkpoint is now broken. Moment it is clicked it shows the list from the first checkpoint present in the model directory and doesn't show the list in a way that it displays the currently used checkpoint on the shown part of the listing.\r\n\r\nSo you have to scroll down and find it, each and every single time. Also, the spacing is now borked. So it became much less readable, mainly because of the vertical space introduced between lines. Maybe it has something to do with horizontal space limits it now has, I dunno, it just became unusable.\r\n\r\n![image](https://user-images.githubusercontent.com/1836566/227749908-25d99364-c1ff-46be-8d07-43d66a8ac966.png)\r\n\r\n\r\n### Steps to reproduce the problem\r\n\r\n1. Have the Stable Diffusion checkpoint dropdown on the top\r\n2. CLick it\r\n3. Argh intensifies\r\n\r\n\r\n### What should have happened?\r\n\r\nShould have shown model, including the currently selected one, so user doesn't have to go fish for it each single time.\r\n\r\n### Commit where the problem happens\r\n\r\na0d07fb5807ad55c8ccfdfc9a6d9ae3c62b9d211\r\n\r\n### What platforms do you use to access the UI ?\r\n\r\nWindows\r\n\r\n### What browsers do you use to access the UI ?\r\n\r\nMicrosoft Edge\r\n\r\n### Command Line Arguments\r\n\r\n```Shell\r\n--xformers --opt-channelslast --vae-path D:\\WORK\\conda_envs\\automatic\\stable-diffusion-webui\\models\\VAE\\vae-ft-mse-840000-ema-pruned.pt --gfpgan-model GFPGANv1.4.pth\r\n```\r\n\r\n\r\n### List of extensions\r\n\r\n![image](https://user-images.githubusercontent.com/1836566/227750025-1a2eb381-0780-44ab-be32-61484e04e61b.png)\r\n\r\n\r\n### Console logs\r\n\r\n```Shell\r\nThis isn't relevant to this particular problem.\r\n```\r\n\r\n\r\n### Additional information\r\n\r\n_No response_\n", "hints_text": "Related https://github.com/AUTOMATIC1111/stable-diffusion-webui/issues/8511\r\n", "created_at": "2023-03-26T03:07:28Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 8940, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-8940", "issue_numbers": ["8916", "9009"], "base_commit": "a0d07fb5807ad55c8ccfdfc9a6d9ae3c62b9d211", "patch": "diff --git a/javascript/imageviewer.js b/javascript/imageviewer.js\nindex 7547e771113..d6483562242 100644\n--- a/javascript/imageviewer.js\n+++ b/javascript/imageviewer.js\n@@ -32,13 +32,7 @@ function negmod(n, m) {\n function updateOnBackgroundChange() {\n     const modalImage = gradioApp().getElementById(\"modalImage\")\n     if (modalImage && modalImage.offsetParent) {\n-        let allcurrentButtons = gradioApp().querySelectorAll(\".gallery-item.transition-all.\\\\!ring-2\")\n-        let currentButton = null\n-        allcurrentButtons.forEach(function(elem) {\n-            if (elem.parentElement.offsetParent) {\n-                currentButton = elem;\n-            }\n-        })\n+        let currentButton = selected_gallery_button();\n \n         if (currentButton?.children?.length > 0 && modalImage.src != currentButton.children[0].src) {\n             modalImage.src = currentButton.children[0].src;\n@@ -50,22 +44,10 @@ function updateOnBackgroundChange() {\n }\n \n function modalImageSwitch(offset) {\n-    var allgalleryButtons = gradioApp().querySelectorAll(\".gradio-gallery .thumbnail-item\")\n-    var galleryButtons = []\n-    allgalleryButtons.forEach(function(elem) {\n-        if (elem.parentElement.offsetParent) {\n-            galleryButtons.push(elem);\n-        }\n-    })\n+    var galleryButtons = all_gallery_buttons();\n \n     if (galleryButtons.length > 1) {\n-        var allcurrentButtons = gradioApp().querySelectorAll(\".gradio-gallery .thumbnail-item.selected\")\n-        var currentButton = null\n-        allcurrentButtons.forEach(function(elem) {\n-            if (elem.parentElement.offsetParent) {\n-                currentButton = elem;\n-            }\n-        })\n+        var currentButton = selected_gallery_button();\n \n         var result = -1\n         galleryButtons.forEach(function(v, i) {\ndiff --git a/javascript/ui.js b/javascript/ui.js\nindex fcaf5608a16..4a440193b81 100644\n--- a/javascript/ui.js\n+++ b/javascript/ui.js\n@@ -7,9 +7,31 @@ function set_theme(theme){\n     }\n }\n \n+function all_gallery_buttons() {\n+    var allGalleryButtons = gradioApp().querySelectorAll('[style=\"display: block;\"].tabitem div[id$=_gallery].gradio-gallery .thumbnails > .thumbnail-item.thumbnail-small');\n+    var visibleGalleryButtons = [];\n+    allGalleryButtons.forEach(function(elem) {\n+        if (elem.parentElement.offsetParent) {\n+            visibleGalleryButtons.push(elem);\n+        }\n+    })\n+    return visibleGalleryButtons;\n+}\n+\n+function selected_gallery_button() {\n+    var allCurrentButtons = gradioApp().querySelectorAll('[style=\"display: block;\"].tabitem div[id$=_gallery].gradio-gallery .thumbnail-item.thumbnail-small.selected');\n+    var visibleCurrentButton = null;\n+    allCurrentButtons.forEach(function(elem) {\n+        if (elem.parentElement.offsetParent) {\n+            visibleCurrentButton = elem;\n+        }\n+    })\n+    return visibleCurrentButton;\n+}\n+\n function selected_gallery_index(){\n-    var buttons = gradioApp().querySelectorAll('[style=\"display: block;\"].tabitem div[id$=_gallery] .gallery-item')\n-    var button = gradioApp().querySelector('[style=\"display: block;\"].tabitem div[id$=_gallery] .gallery-item.\\\\!ring-2')\n+    var buttons = all_gallery_buttons();\n+    var button = selected_gallery_button();\n \n     var result = -1\n     buttons.forEach(function(v, i){ if(v==button) { result = i } })\n@@ -18,14 +40,18 @@ function selected_gallery_index(){\n }\n \n function extract_image_from_gallery(gallery){\n-    if(gallery.length == 1){\n-        return [gallery[0]]\n+    if (gallery.length == 0){\n+        return [null];\n+    }\n+    if (gallery.length == 1){\n+        return [gallery[0]];\n     }\n \n     index = selected_gallery_index()\n \n     if (index < 0 || index >= gallery.length){\n-        return [null]\n+        // Use the first image in the gallery as the default\n+        index = 0;\n     }\n \n     return [gallery[index]];\n", "test_patch": "", "problem_statement": "[Bug]:  \"Send To xxx\" action fail with AttributeError: 'NoneType' object has no attribute 'startswith' when using batch mode\n### Is there an existing issue for this?\n\n- [X] I have searched the existing issues and checked the recent builds/commits\n\n### What happened?\n\nWhen I generate an image with just one result, the Send To actions works just fine. When I either increase the \"Batch count\" or \"Batch size\" and I click one of the images, then no image is shown in the Inpaint (for example)\r\n\r\nThe call stack I get in the console:\r\n```\r\nTraceback (most recent call last):\r\n  File \"S:\\auto1111\\venv\\lib\\site-packages\\gradio\\routes.py\", line 394, in run_predict\r\n    output = await app.get_blocks().process_api(\r\n  File \"S:\\auto1111\\venv\\lib\\site-packages\\gradio\\blocks.py\", line 1075, in process_api\r\n    result = await self.call_function(\r\n  File \"S:\\auto1111\\venv\\lib\\site-packages\\gradio\\blocks.py\", line 884, in call_function\r\n    prediction = await anyio.to_thread.run_sync(\r\n  File \"S:\\auto1111\\venv\\lib\\site-packages\\anyio\\to_thread.py\", line 31, in run_sync\r\n    return await get_asynclib().run_sync_in_worker_thread(\r\n  File \"S:\\auto1111\\venv\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 937, in run_sync_in_worker_thread\r\n    return await future\r\n  File \"S:\\auto1111\\venv\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 867, in run\r\n    result = context.run(func, *args)\r\n  File \"S:\\auto1111\\modules\\generation_parameters_copypaste.py\", line 79, in image_from_url_text\r\n    if filedata.startswith(\"data:image/png;base64,\"):\r\nAttributeError: 'NoneType' object has no attribute 'startswith'\r\n```\n\n### Steps to reproduce the problem\n\n1. Type any prompt\r\n2. Press Generate with either Batch setting set a value higher than 1\r\n3. Press a Send to button, for example \"Send to Inpaint\"\r\n4. Image is not transferred and a console error appears.\n\n### What should have happened?\n\nI expected the selected image to be transferred to the Inpaint UI.\n\n### Commit where the problem happens\n\naba5d639fb20df035243246cf5edb143eda94546\n\n### What platforms do you use to access the UI ?\n\nWindows\n\n### What browsers do you use to access the UI ?\n\nGoogle Chrome\n\n### Command Line Arguments\n\n```Shell\nNo\n```\n\n\n### List of extensions\n\nNo\n\n### Console logs\n\n```Shell\nvenv \"S:\\auto1111\\venv\\Scripts\\Python.exe\"\r\nPython 3.10.10 (tags/v3.10.10:aad5f6a, Feb  7 2023, 17:20:36) [MSC v.1929 64 bit (AMD64)]\r\nCommit hash: aba5d639fb20df035243246cf5edb143eda94546\r\nInstalling requirements for Web UI\r\nLaunching Web UI with arguments:\r\nNo module 'xformers'. Proceeding without it.\r\nLoading weights [6ce0161689] from S:\\auto1111\\models\\Stable-diffusion\\v1-5-pruned-emaonly.safetensors\r\nCreating model from config: S:\\auto1111\\configs\\v1-inference.yaml\r\nLatentDiffusion: Running in eps-prediction mode\r\nDiffusionWrapper has 859.52 M params.\r\nApplying cross attention optimization (Doggettx).\r\nTextual inversion embeddings loaded(0):\r\nModel loaded in 2.9s (load weights from disk: 0.2s, create model: 0.4s, apply weights to model: 0.5s, apply half(): 0.5s, move model to device: 0.5s, load textual inversion embeddings: 0.8s).\r\nRunning on local URL:  http://127.0.0.1:7860\r\n\r\nTo create a public link, set `share=True` in `launch()`.\r\nStartup time: 6.9s (import torch: 1.1s, import gradio: 0.8s, import ldm: 0.3s, other imports: 0.7s, load scripts: 0.6s, load SD checkpoint: 3.0s, create ui: 0.3s).\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [00:06<00:00,  2.99it/s]\r\nTotal progress: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [00:06<00:00,  3.27it/s]\r\nTraceback (most recent call last):\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [00:06<00:00,  3.51it/s]\r\n  File \"S:\\auto1111\\venv\\lib\\site-packages\\gradio\\routes.py\", line 394, in run_predict\r\n    output = await app.get_blocks().process_api(\r\n  File \"S:\\auto1111\\venv\\lib\\site-packages\\gradio\\blocks.py\", line 1075, in process_api\r\n    result = await self.call_function(\r\n  File \"S:\\auto1111\\venv\\lib\\site-packages\\gradio\\blocks.py\", line 884, in call_function\r\n    prediction = await anyio.to_thread.run_sync(\r\n  File \"S:\\auto1111\\venv\\lib\\site-packages\\anyio\\to_thread.py\", line 31, in run_sync\r\n    return await get_asynclib().run_sync_in_worker_thread(\r\n  File \"S:\\auto1111\\venv\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 937, in run_sync_in_worker_thread\r\n    return await future\r\n  File \"S:\\auto1111\\venv\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 867, in run\r\n    result = context.run(func, *args)\r\n  File \"S:\\auto1111\\modules\\generation_parameters_copypaste.py\", line 157, in send_image_and_dimensions\r\n    img = image_from_url_text(x)\r\n  File \"S:\\auto1111\\modules\\generation_parameters_copypaste.py\", line 70, in image_from_url_text\r\n    if filedata.startswith(\"data:image/png;base64,\"):\r\nAttributeError: 'NoneType' object has no attribute 'startswith'\n```\n\n\n### Additional information\n\n_No response_\n[Bug]: Sending image to img2img or inapint don't work.\n### Is there an existing issue for this?\n\n- [X] I have searched the existing issues and checked the recent builds/commits\n\n### What happened?\n\nTried to send image from output to inpaint.\n\n### Steps to reproduce the problem\n\n1. Go to img2img - make an image.\r\n2. Press send to inpaint\r\n\n\n### What should have happened?\n\nPicture should have shown up in inpaint.\n\n### Commit where the problem happens\n\n4c1ad743e3baf1246db0711aa0107debf036a12b\n\n### What platforms do you use to access the UI ?\n\nWindows\n\n### What browsers do you use to access the UI ?\n\nGoogle Chrome\n\n### Command Line Arguments\n\n```Shell\n--api --xformers\n```\n\n\n### List of extensions\n\nopenpose\r\ndreambooth\r\nopenOutpaint\n\n### Console logs\n\n```Shell\nFile \"C:\\Users\\andre\\Projects\\stable-diffusion-webui\\modules\\generation_parameters_copypaste.py\", line 157, in send_image_and_dimensions\r\n    img = image_from_url_text(x)\r\n  File \"C:\\Users\\andre\\Projects\\stable-diffusion-webui\\modules\\generation_parameters_copypaste.py\", line 70, in image_from_url_text\r\n    if filedata.startswith(\"data:image/png;base64,\"):\r\nAttributeError: 'NoneType' object has no attribute 'startswith'\n```\n\n\n### Additional information\n\nnothingim aware of\n", "hints_text": "same issue here.. any.. fixes yet?\r\n![Screenshot 2023-03-25 223102](https://user-images.githubusercontent.com/121852581/227726895-89b4425e-85f3-420a-8482-ea7a106dea74.png)\r\n\nIssue is auto did not update the function `selected_gallery_index()` in `ui.js`\r\n\r\nhttps://github.com/AUTOMATIC1111/stable-diffusion-webui/blob/91ae48fd7e20c60d6374f340cac0939f56d87048/javascript/ui.js#L10-L18\r\n\r\nThat `.gallery-item` class was changed to `.thumbnail-item`, so the JS does not pass any image to the backend and becomes `NoneType`\r\n\r\nCorrect version:\r\n\r\n```js\r\nfunction selected_gallery_index(){\r\n    var buttons = gradioApp().querySelectorAll('[style=\"display: block;\"].tabitem div[id$=_gallery] .thumbnails > .thumbnail-item')\r\n    var button = gradioApp().querySelector('[style=\"display: block;\"].tabitem div[id$=_gallery] .thumbnails > .thumbnail-item.selected')\r\n\r\n    var result = -1\r\n    buttons.forEach(function(v, i){ if(v==button) { result = i } })\r\n\r\n    return result\r\n}\r\n```\nAlso another bug\r\n\r\nThere currently has to be a selected image in the gallery for the Send to img2img button to work, if not same error appears. So it should default to sending the first gallery item if none is selected\nFix confirmed. Didn't even have to restart the server!\r\nThanks for a fast response!\r\n(only locally ofc)\n", "created_at": "2023-03-25T18:54:17Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 8938, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-8938", "issue_numbers": ["8910", "8966"], "base_commit": "a0d07fb5807ad55c8ccfdfc9a6d9ae3c62b9d211", "patch": "diff --git a/javascript/aspectRatioOverlay.js b/javascript/aspectRatioOverlay.js\nindex 0f164b82c1a..a8278cca209 100644\n--- a/javascript/aspectRatioOverlay.js\n+++ b/javascript/aspectRatioOverlay.js\n@@ -12,7 +12,7 @@ function dimensionChange(e, is_width, is_height){\n \t\tcurrentHeight = e.target.value*1.0\r\n \t}\r\n \r\n-\tvar inImg2img   = Boolean(gradioApp().querySelector(\"button.rounded-t-lg.border-gray-200\"))\r\n+\tvar inImg2img = gradioApp().querySelector(\"#tab_img2img\").style.display == \"block\";\r\n \r\n \tif(!inImg2img){\r\n \t\treturn;\r\n@@ -22,7 +22,7 @@ function dimensionChange(e, is_width, is_height){\n \r\n     var tabIndex = get_tab_index('mode_img2img')\r\n \tif(tabIndex == 0){ // img2img\r\n-\t\ttargetElement = gradioApp().querySelector('div[data-testid=image] img');\r\n+\t\ttargetElement = gradioApp().querySelector('#img2img_image div[data-testid=image] img');\r\n \t} else if(tabIndex == 1){ //Sketch\r\n \t\ttargetElement = gradioApp().querySelector('#img2img_sketch div[data-testid=image] img');\r\n \t} else if(tabIndex == 2){ // Inpaint\r\n@@ -30,7 +30,7 @@ function dimensionChange(e, is_width, is_height){\n \t} else if(tabIndex == 3){ // Inpaint sketch\r\n \t\ttargetElement = gradioApp().querySelector('#inpaint_sketch div[data-testid=image] img');\r\n \t}\r\n-\t\r\n+\r\n \r\n \tif(targetElement){\r\n \r\n@@ -38,7 +38,7 @@ function dimensionChange(e, is_width, is_height){\n \t\tif(!arPreviewRect){\r\n \t\t    arPreviewRect = document.createElement('div')\r\n \t\t    arPreviewRect.id = \"imageARPreview\";\r\n-\t\t    gradioApp().getRootNode().appendChild(arPreviewRect)\r\n+\t\t    gradioApp().appendChild(arPreviewRect)\r\n \t\t}\r\n \r\n \r\n@@ -91,23 +91,26 @@ onUiUpdate(function(){\n \tif(arPreviewRect){\r\n \t\tarPreviewRect.style.display = 'none';\r\n \t}\r\n-\tvar inImg2img   = Boolean(gradioApp().querySelector(\"button.rounded-t-lg.border-gray-200\"))\r\n-\tif(inImg2img){\r\n-\t\tlet inputs = gradioApp().querySelectorAll('input');\r\n-\t\tinputs.forEach(function(e){\r\n-\t\t    var is_width = e.parentElement.id == \"img2img_width\"\r\n-\t\t    var is_height = e.parentElement.id == \"img2img_height\"\r\n-\r\n-\t\t\tif((is_width || is_height) && !e.classList.contains('scrollwatch')){\r\n-\t\t\t\te.addEventListener('input', function(e){dimensionChange(e, is_width, is_height)} )\r\n-\t\t\t\te.classList.add('scrollwatch')\r\n-\t\t\t}\r\n-\t\t\tif(is_width){\r\n-\t\t\t\tcurrentWidth = e.value*1.0\r\n-\t\t\t}\r\n-\t\t\tif(is_height){\r\n-\t\t\t\tcurrentHeight = e.value*1.0\r\n-\t\t\t}\r\n-\t\t})\r\n-\t}\r\n+    var tabImg2img = gradioApp().querySelector(\"#tab_img2img\");\r\n+    if (tabImg2img) {\r\n+        var inImg2img = tabImg2img.style.display == \"block\";\r\n+        if(inImg2img){\r\n+            let inputs = gradioApp().querySelectorAll('input');\r\n+            inputs.forEach(function(e){\r\n+                var is_width = e.parentElement.id == \"img2img_width\"\r\n+                var is_height = e.parentElement.id == \"img2img_height\"\r\n+\r\n+                if((is_width || is_height) && !e.classList.contains('scrollwatch')){\r\n+                    e.addEventListener('input', function(e){dimensionChange(e, is_width, is_height)} )\r\n+                    e.classList.add('scrollwatch')\r\n+                }\r\n+                if(is_width){\r\n+                    currentWidth = e.value*1.0\r\n+                }\r\n+                if(is_height){\r\n+                    currentHeight = e.value*1.0\r\n+                }\r\n+            })\r\n+        }\r\n+    }\r\n });\r\ndiff --git a/style.css b/style.css\nindex 0dcc3e25d90..b252e64d389 100644\n--- a/style.css\n+++ b/style.css\n@@ -507,6 +507,17 @@ div.dimensions-tools{\n   background-color: rgba(0, 0, 0, 0.8);\r\n }\r\n \r\n+#imageARPreview {\r\n+    position: absolute;\r\n+    top: 0px;\r\n+    left: 0px;\r\n+    border: 2px solid red;\r\n+    background: rgba(255, 0, 0, 0.3);\r\n+    z-index: 900;\r\n+    pointer-events: none;\r\n+    display: none;\r\n+}\r\n+\r\n /* context menu (ie for the generate button) */\r\n \r\n #context-menu{\r\n", "test_patch": "", "problem_statement": "[Bug]: the outline of the image ratio no longer appears on the image when moving the height and width sliders\n### Is there an existing issue for this?\n\n- [X] I have searched the existing issues and checked the recent builds/commits\n\n### What happened?\n\nSince the latest commit, the outline of the image ratio no longer appears on the image when moving the height and width sliders\n\n### Steps to reproduce the problem\n\n1. Go to img2img\r\n2. load an image\r\n3. Move the Width or Height slider\r\n\n\n### What should have happened?\n\nWe should see the outline moving to show the image ratio. But nothing happen\n\n### Commit where the problem happens\n\naba5d639fb20df035243246cf5edb143eda94546\n\n### What platforms do you use to access the UI ?\n\nWindows\n\n### What browsers do you use to access the UI ?\n\nGoogle Chrome\n\n### Command Line Arguments\n\n```Shell\n--xformers --api --gradio-img2img-tool color-sketch\n```\n\n\n### List of extensions\n\nsd-webui-3d-open-pose-editor\r\nsd-webui-aspect-ratio-helper\r\nsd-webui-controlnet\n\n### Console logs\n\n```Shell\nvenv \"C:\\Users\\Jyce\\Desktop\\stable-diffusion-webui\\venv\\Scripts\\Python.exe\"\r\nPython 3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]\r\nCommit hash: aba5d639fb20df035243246cf5edb143eda94546\r\nInstalling requirements for Web UI\r\n\r\nLaunching Web UI with arguments: --xformers --api --gradio-img2img-tool color-sketch\r\nLoading weights [f93e6a50ac] from C:\\Users\\Jyce\\Desktop\\stable-diffusion-webui\\models\\Stable-diffusion\\uberRealisticPornMerge_urpmv13.safetensors\r\nCreating model from config: C:\\Users\\Jyce\\Desktop\\stable-diffusion-webui\\configs\\v1-inference.yaml\r\nLatentDiffusion: Running in eps-prediction mode\r\nDiffusionWrapper has 859.52 M params.\r\nApplying xformers cross attention optimization.\r\nTextual inversion embeddings loaded(0):\r\nModel loaded in 2.7s (load weights from disk: 0.1s, create model: 0.2s, apply weights to model: 0.6s, apply half(): 0.4s, move model to device: 0.5s, load textual inversion embeddings: 0.8s).\r\nRunning on local URL:  http://127.0.0.1:7860\r\n\r\nTo create a public link, set `share=True` in `launch()`.\r\nStartup time: 7.9s (import torch: 1.3s, import gradio: 0.9s, import ldm: 0.2s, other imports: 0.8s, list extensions: 0.4s, setup codeformer: 0.1s, load scripts: 0.8s, load SD checkpoint: 2.9s, create ui: 0.2s, scripts app_started_callback: 0.1s).\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [00:03<00:00,  5.59it/s]\r\nTotal progress: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [00:01<00:00, 11.27it/s]\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [00:03<00:00,  6.65it/s]\r\nTotal progress: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [00:02<00:00,  6.91it/s]\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [00:08<00:00,  2.26it/s]\r\nTotal progress: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [00:08<00:00,  2.28it/s]\r\nTotal progress: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [00:08<00:00,  2.30it/s]\n```\n\n\n### Additional information\n\n_No response_\n[Bug]: Aspect Ratio red square no longer appears in Img2Img tab\n### Is there an existing issue for this?\n\n- [X] I have searched the existing issues and checked the recent builds/commits\n\n### What happened?\n\nWhen you change the Aspect Ratio in the inpaint tab, normally there would be a red square so you can see how much you have to change it to match the ratio of the image that you are inpainting (In case you want to inpaint the whole image). \r\n\r\nThis no longer appears. Now I have to manually check the ratio of the image, then convert the largest side to my largest size of renderable resolution, and then scale the other side by the same %. This is annoying to say the least.\n\n### Steps to reproduce the problem\n\n1. Go to Img2Img inpaint tab\r\n2. Try to change Aspect Ratio\r\n3. No red square appears over the image.\r\n\n\n### What should have happened?\n\nA red square should have appeared at the current ratio, and change as I change the ratio.\n\n### Commit where the problem happens\n\n64da5c46ef0d68b9048747c2e0d46ce3495f9f29\n\n### What platforms do you use to access the UI ?\n\nWindows\n\n### What browsers do you use to access the UI ?\n\nMicrosoft Edge\n\n### Command Line Arguments\n\n```Shell\n--xformers --disable-safe-unpickle --cors-allow-origins=http://localhost:7860/ --api\n```\n\n\n### List of extensions\n\nControlnet, Lora/LoCon, Promptgen, Wildcards, UltimateUpscale\n\n### Console logs\n\n```Shell\nG:\\Workspace\\stable-diffusion-webui>set root=C:\\Users\\meeee\\anaconda3\r\n\r\nG:\\Workspace\\stable-diffusion-webui>call C:\\Users\\meeee\\anaconda3\\Scripts\\activate.bat C:\\Users\\meeee\\anaconda3\r\n\r\n(base) G:\\Workspace\\stable-diffusion-webui>call activate sdwebui\r\n\r\n(sdwebui) G:\\Workspace\\stable-diffusion-webui>git pull\r\nremote: Enumerating objects: 6, done.\r\nremote: Counting objects: 100% (6/6), done.\r\nremote: Compressing objects: 100% (5/5), done.\r\nremote: Total 6 (delta 1), reused 3 (delta 1), pack-reused 0\r\nUnpacking objects: 100% (6/6), 4.29 KiB | 68.00 KiB/s, done.\r\nFrom https://github.com/AUTOMATIC1111/stable-diffusion-webui\r\n   a0d07fb5..64da5c46  master     -> origin/master\r\nUpdating a0d07fb5..64da5c46\r\nFast-forward\r\n scripts/loopback.py | 15 ++++++---------\r\n 1 file changed, 6 insertions(+), 9 deletions(-)\r\n\r\n(sdwebui) G:\\Workspace\\stable-diffusion-webui>start microsoft-edge:http://127.0.0.1:7860/\r\n\r\n(sdwebui) G:\\Workspace\\stable-diffusion-webui>webui-user.bat\r\nvenv \"G:\\Workspace\\stable-diffusion-webui\\venv\\Scripts\\Python.exe\"\r\nPython 3.10.6 | packaged by conda-forge | (main, Oct 24 2022, 16:02:16) [MSC v.1916 64 bit (AMD64)]\r\nCommit hash: 64da5c46ef0d68b9048747c2e0d46ce3495f9f29\r\nInstalling requirements for Web UI\r\nInstalling requirements for CLIP Interrogator\r\n\r\ncurrent transparent-background 1.2.3\r\n\r\n\r\n\r\n\r\n\r\nLaunching Web UI with arguments: --xformers --disable-safe-unpickle --cors-allow-origins=http://localhost:7860/ --api\r\nCivitai Helper: Get Custom Model Folder\r\nCivitai Helper: Load setting from: G:\\Workspace\\stable-diffusion-webui\\extensions\\Stable-Diffusion-Webui-Civitai-Helper\\setting.json\r\nCivitai Helper: No setting file, use default\r\nAdditional Network extension not installed, Only hijack built-in lora\r\nLoCon Extension hijack built-in lora successfully\r\nLoading booru2prompt settings\r\nLoading weights [10642fd1d2] from G:\\Workspace\\stable-diffusion-webui\\models\\Stable-diffusion\\realisticVisionV13_v13-inpainting.safetensors\r\nCreating model from config: G:\\Workspace\\stable-diffusion-webui\\models\\Stable-diffusion\\realisticVisionV13_v13-inpainting.yaml\r\nLatentInpaintDiffusion: Running in eps-prediction mode\r\nDiffusionWrapper has 859.54 M params.\r\nLoading VAE weights specified in settings: G:\\Workspace\\stable-diffusion-webui\\models\\VAE\\vae-ft-mse-840000-ema-pruned.safetensors\r\nApplying xformers cross attention optimization.\r\nTextual inversion embeddings loaded(48): \r\nModel loaded in 22.8s (load weights from disk: 0.4s, create model: 0.6s, apply weights to model: 15.8s, apply half(): 1.0s, load VAE: 1.4s, move model to device: 0.7s, load textual inversion embeddings: 2.8s).\r\n2560 1440\r\n1390\r\nRunning on local URL:  http://127.0.0.1:7860\r\n\r\nTo create a public link, set `share=True` in `launch()`.\r\nStartup time: 49.2s (import torch: 4.1s, import gradio: 2.4s, import ldm: 0.4s, other imports: 2.5s, list extensions: 3.2s, list SD models: 0.2s, setup codeformer: 0.2s, load scripts: 4.8s, load SD checkpoint: 23.1s, create ui: 7.7s, gradio launch: 0.3s, scripts app_started_callback: 0.1s).\n```\n\n\n### Additional information\n\n_No response_\n", "hints_text": "+1 A very crooked merge, too bad no one bothered to run it and check it before merging\nNeeds some frontend fixes to be compatible with 3.23's DOM changes\r\n\r\nhttps://github.com/AUTOMATIC1111/stable-diffusion-webui/commit/729b915c753cdbfcd98629580ddb0b876c46725a\r\nhttps://github.com/AUTOMATIC1111/stable-diffusion-webui/commit/faf39bb1b62f3d81f7747d537f320cfa1b0f4ef3\r\nhttps://github.com/AUTOMATIC1111/stable-diffusion-webui/commit/fbc851a3bf6a4681c23e3887379a73bc9ece1e76\nThanks.\r\nJust need to wait these commits to be pushed on master.\n", "created_at": "2023-03-25T18:45:42Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 8936, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-8936", "issue_numbers": ["8933"], "base_commit": "91ae48fd7e20c60d6374f340cac0939f56d87048", "patch": "diff --git a/javascript/notification.js b/javascript/notification.js\nindex 5ae6df24df1..8ddd4c5d919 100644\n--- a/javascript/notification.js\n+++ b/javascript/notification.js\n@@ -15,7 +15,7 @@ onUiUpdate(function(){\n         }\n     }\n \n-    const galleryPreviews = gradioApp().querySelectorAll('div[id^=\"tab_\"][style*=\"display: block\"] div[id$=\"_results\"] img.h-full.w-full.overflow-hidden');\n+    const galleryPreviews = gradioApp().querySelectorAll('div[id^=\"tab_\"][style*=\"display: block\"] div[id$=\"_results\"] .thumbnail-item > img');\n \n     if (galleryPreviews == null) return;\n \n", "test_patch": "", "problem_statement": "[Bug]: Notifications broken with 3.23 merge\n### Is there an existing issue for this?\n\n- [X] I have searched the existing issues and checked the recent builds/commits\n\n### What happened?\n\nNotifications/sounds no longer trigger on successful generation\r\n\r\nThis is because auto did not update `notifications.js` with new element selectors\r\n\r\n```js\r\nconst galleryPreviews = gradioApp().querySelectorAll('div[id^=\"tab_\"][style*=\"display: block\"] div[id$=\"_results\"] img.h-full.w-full.overflow-hidden');\r\n```\r\n\r\nneeds to be\r\n\r\n```js\r\nconst galleryPreviews = gradioApp().querySelectorAll('div[id^=\"tab_\"][style*=\"display: block\"] div[id$=\"_results\"] .thumbnail-item > img');\r\n```\r\n\r\n(https://github.com/space-nuko/stable-diffusion-webui/commit/c115ff6a6d928eb84a8cf02621f881d1b79caff4)\n\n### Steps to reproduce the problem\n\nGenerate image\n\n### What should have happened?\n\nNotification does not fire\n\n### Commit where the problem happens\n\n91ae48f\n\n### What platforms do you use to access the UI ?\n\nWindows\n\n### What browsers do you use to access the UI ?\n\nGoogle Chrome\n\n### Command Line Arguments\n\n```Shell\nN/A\n```\n\n\n### List of extensions\n\nN/A\n\n### Console logs\n\n```Shell\nN/A\n```\n\n\n### Additional information\n\n_No response_\n", "hints_text": "", "created_at": "2023-03-25T18:30:04Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 8931, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-8931", "issue_numbers": ["8927"], "base_commit": "91ae48fd7e20c60d6374f340cac0939f56d87048", "patch": "diff --git a/scripts/loopback.py b/scripts/loopback.py\nindex 9c388aa8d74..d3065fe6b52 100644\n--- a/scripts/loopback.py\n+++ b/scripts/loopback.py\n@@ -54,15 +54,12 @@ def calculate_denoising_strength(loop):\n                 return strength\r\n \r\n             progress = loop / (loops - 1)\r\n-            match denoising_curve:\r\n-                case \"Aggressive\":\r\n-                    strength = math.sin((progress) * math.pi * 0.5)\r\n-\r\n-                case \"Lazy\":\r\n-                    strength = 1 - math.cos((progress) * math.pi * 0.5)\r\n-\r\n-                case _:\r\n-                    strength = progress\r\n+            if denoising_curve == \"Aggressive\":\r\n+                strength = math.sin((progress) * math.pi * 0.5)\r\n+            elif denoising_curve == \"Lazy\":\r\n+                strength = 1 - math.cos((progress) * math.pi * 0.5)\r\n+            else:\r\n+                strength = progress\r\n \r\n             change = (final_denoising_strength - initial_denoising_strength) * strength\r\n             return initial_denoising_strength + change\r\n", "test_patch": "", "problem_statement": "[Bug]: loopback.py match-case traceback on Google Colab\n### Is there an existing issue for this?\n\n- [X] I have searched the existing issues and checked the recent builds/commits\n\n### What happened?\n\nwhat happened? I don't really know.\r\nall of a sudden I'm getting tracebacks and the WebUI won't load properly, making it unuseable.\r\n![image](https://user-images.githubusercontent.com/38461122/227729893-f922a4af-b29c-4908-ba16-e26998e10f87.png)\r\n![image](https://user-images.githubusercontent.com/38461122/227729556-6bcbedd3-c702-4223-9928-811169447a9e.png)\r\n\n\n### Steps to reproduce the problem\n\n1. open [this colab notebook](https://colab.research.google.com/github/shirooo39/MiXLab/blob/master/MiXLab.ipynb#scrollTo=6_Sxep-D4HEf)\r\n2. change the runtime type to GPU\r\n3. run the cell: Initialize\r\n4. urn the cell: Download Diffusion Models\r\n5. run the cell: Start WebUI\r\n6. wait for error\n\n### What should have happened?\n\nit shouldn't've thrown those tracebacks and the WebUI loads properly.\n\n### Commit where the problem happens\n\n91ae48fd\n\n### What platforms do you use to access the UI ?\n\nOther/Cloud\n\n### What browsers do you use to access the UI ?\n\nMozilla Firefox\n\n### Command Line Arguments\n\n```Shell\n!COMMANDLINE_ARGS=\"--share --api --gradio-debug --gradio-queue --medvram --xformers --disable-safe-unpickle --enable-insecure-extension-access --no-half-vae --no-download-sd-model --disable-console-progressbars\" REQS_FILE=\"requirements.txt\" python launch.py\n```\n\n\n### List of extensions\n\nNo\n\n### Console logs\n\n```Shell\nhttps://rentry.org/a1111webui91ae48fd\n```\n\n\n### Additional information\n\nI'm the author of MiXLab notebook and I wrote my own simple Colab implementation of your WebUI.\r\nit was working fine last night as of 2023-03-24, around 11 PM UTC+07:00, I was able to launch the WebUI without getting any tracebacks.\r\n\r\nI was able to generate images at that time (the screenshot below is an image I generated on Colab and downloaded locally)\r\n![image](https://user-images.githubusercontent.com/38461122/227730668-18f6a70e-4f62-403d-8232-fe8be1f80d48.png)\r\n\r\nI'm constantly making changes to my notebook BUT I did not touch to the \"Initialize\" and \"Start WebUI\" cells.  \r\nI only touch to the model downloader cells.  \r\nso I don't think the issue is within my notebook, but I haven't tried TheLastBen's notebook either...\r\n\r\nColab still uses Python 3.7 and the match-case used in the loopback.py was only introduced in Python 3.10 and newer.  \r\nthat could be the reason why Colab is throwing out tracebacks.\n", "hints_text": "suggested solution: to replace match-case with if-else", "created_at": "2023-03-25T17:52:26Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 8780, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-8780", "issue_numbers": ["8244"], "base_commit": "a9fed7c364061ae6efb37f797b6b522cb3cf7aa2", "patch": "diff --git a/modules/api/api.py b/modules/api/api.py\nindex 35e17afc9da..f52f7feff78 100644\n--- a/modules/api/api.py\n+++ b/modules/api/api.py\n@@ -18,7 +18,7 @@\n from modules.textual_inversion.preprocess import preprocess\n from modules.hypernetworks.hypernetwork import create_hypernetwork, train_hypernetwork\n from PIL import PngImagePlugin,Image\n-from modules.sd_models import checkpoints_list\n+from modules.sd_models import checkpoints_list, unload_model_weights, reload_model_weights\n from modules.sd_models_config import find_checkpoint_config_near_filename\n from modules.realesrgan_model import get_realesrgan_models\n from modules import devices\n@@ -150,6 +150,8 @@ def __init__(self, app: FastAPI, queue_lock: Lock):\n         self.add_api_route(\"/sdapi/v1/train/embedding\", self.train_embedding, methods=[\"POST\"], response_model=TrainResponse)\n         self.add_api_route(\"/sdapi/v1/train/hypernetwork\", self.train_hypernetwork, methods=[\"POST\"], response_model=TrainResponse)\n         self.add_api_route(\"/sdapi/v1/memory\", self.get_memory, methods=[\"GET\"], response_model=MemoryResponse)\n+        self.add_api_route(\"/sdapi/v1/unload-checkpoint\", self.unloadapi, methods=[\"POST\"])\n+        self.add_api_route(\"/sdapi/v1/reload-checkpoint\", self.reloadapi, methods=[\"POST\"])\n         self.add_api_route(\"/sdapi/v1/scripts\", self.get_scripts_list, methods=[\"GET\"], response_model=ScriptsList)\n \n     def add_api_route(self, path: str, endpoint, **kwargs):\n@@ -412,6 +414,16 @@ def interruptapi(self):\n \n         return {}\n \n+    def unloadapi(self):\n+        unload_model_weights()\n+\n+        return {}\n+\n+    def reloadapi(self):\n+        reload_model_weights()\n+\n+        return {}\n+\n     def skip(self):\n         shared.state.skip()\n \ndiff --git a/modules/sd_models.py b/modules/sd_models.py\nindex f0cb124009e..f9dd0521bd6 100644\n--- a/modules/sd_models.py\n+++ b/modules/sd_models.py\n@@ -494,7 +494,7 @@ def reload_model_weights(sd_model=None, info=None):\n     if sd_model is None or checkpoint_config != sd_model.used_config:\r\n         del sd_model\r\n         checkpoints_loaded.clear()\r\n-        load_model(checkpoint_info, already_loaded_state_dict=state_dict, time_taken_to_load_state_dict=timer.records[\"load weights from disk\"])\r\n+        load_model(checkpoint_info, already_loaded_state_dict=state_dict)\r\n         return shared.sd_model\r\n \r\n     try:\r\n@@ -517,3 +517,23 @@ def reload_model_weights(sd_model=None, info=None):\n     print(f\"Weights loaded in {timer.summary()}.\")\r\n \r\n     return sd_model\r\n+\r\n+def unload_model_weights(sd_model=None, info=None):\r\n+    from modules import lowvram, devices, sd_hijack\r\n+    timer = Timer()\r\n+\r\n+    if shared.sd_model:\r\n+\r\n+        # shared.sd_model.cond_stage_model.to(devices.cpu)\r\n+        # shared.sd_model.first_stage_model.to(devices.cpu)\r\n+        shared.sd_model.to(devices.cpu)\r\n+        sd_hijack.model_hijack.undo_hijack(shared.sd_model)\r\n+        shared.sd_model = None\r\n+        sd_model = None\r\n+        gc.collect()\r\n+        devices.torch_gc()\r\n+        torch.cuda.empty_cache()\r\n+\r\n+    print(f\"Unloaded weights {timer.summary()}.\")\r\n+\r\n+    return sd_model\n\\ No newline at end of file\ndiff --git a/modules/ui.py b/modules/ui.py\nindex 7e603332796..d93ef134c58 100644\n--- a/modules/ui.py\n+++ b/modules/ui.py\n@@ -1491,11 +1491,33 @@ def run_settings_single(value, key):\n                 request_notifications = gr.Button(value='Request browser notifications', elem_id=\"request_notifications\")\r\n                 download_localization = gr.Button(value='Download localization template', elem_id=\"download_localization\")\r\n                 reload_script_bodies = gr.Button(value='Reload custom script bodies (No ui updates, No restart)', variant='secondary', elem_id=\"settings_reload_script_bodies\")\r\n+                with gr.Row():\r\n+                    unload_sd_model = gr.Button(value='Unload SD checkpoint to free VRAM', elem_id=\"sett_unload_sd_model\")\r\n+                    reload_sd_model = gr.Button(value='Reload the last SD checkpoint back into VRAM', elem_id=\"sett_reload_sd_model\")\r\n \r\n             with gr.TabItem(\"Licenses\"):\r\n                 gr.HTML(shared.html(\"licenses.html\"), elem_id=\"licenses\")\r\n \r\n             gr.Button(value=\"Show all pages\", elem_id=\"settings_show_all_pages\")\r\n+            \r\n+\r\n+        def unload_sd_weights():\r\n+            modules.sd_models.unload_model_weights()\r\n+\r\n+        def reload_sd_weights():\r\n+            modules.sd_models.reload_model_weights()\r\n+\r\n+        unload_sd_model.click(\r\n+            fn=unload_sd_weights,\r\n+            inputs=[],\r\n+            outputs=[]\r\n+        )\r\n+\r\n+        reload_sd_model.click(\r\n+            fn=reload_sd_weights,\r\n+            inputs=[],\r\n+            outputs=[]\r\n+        )\r\n \r\n         request_notifications.click(\r\n             fn=lambda: None,\r\n", "test_patch": "", "problem_statement": "[Feature Request]: add VRAM Memory recovery feature\n### Is there an existing issue for this?\n\n- [X] I have searched the existing issues and checked the recent builds/commits\n\n### What would your feature do ?\n\nEverytime I hit a CUDA out of memory problem, I try to turn down the resolution and other parameters. But I still can't generate images even if have created same image in the same parameters. \r\nI opened task manager, and noticed that the dedicated GPU memory was still full, even though nothing was running.  I have to reboot webui to solve this problem, but rebooting consumes lot of time.\r\nMaybe you can add a feature in setting, so that we can recover VRAM after \"CUDA out of memory error\".it will appreciated if you add this. \r\n\r\n\n\n### Proposed workflow\n\n1. Go to settings \r\n2. Press enable recover VRAM after an error \r\n3. VRAM get released \r\n\n\n### Additional information\n\nNope \n", "hints_text": "As soon as you click the generate button again GPU memory is being reset:\r\nhttps://github.com/AUTOMATIC1111/stable-diffusion-webui/blob/master/modules/processing.py#L510\r\n\r\nI also tested and it seem to work. I wonder what goes wrong at your PC? \n> As soon as you click the generate button again GPU memory is being reset: https://github.com/AUTOMATIC1111/stable-diffusion-webui/blob/master/modules/processing.py#L510\r\n> \r\n> I also tested and it seem to work. I wonder what goes wrong at your PC?\r\n\r\nMost memory is being reset, but not all memory is being reset.Thank you for your reply.\r\n\n> Most memory is being reset, but not all memory is being reset.\r\n\r\nCan confirm. Using AMD Radeon RX 7900XT with 20464MB (20GB) of VRAM. Generate 1 image and have to restart webui.bat every time I want to generate a new image. Attempting to use Command Line ARG --opt-split-attention-v1 did allow me to generate 2 512x512 images instead of only 1, but the issue still persisted after the second image was generated. \r\nFirst image brought me from around 6GB to 15GB. Second image took me from 15GB to around 18GB. Third image maxed out and never made any progress when monitoring it from the CMD window. Seeking other avenues to manually clear VRAM after each completed generation until this is resolved. \n> > Most memory is being reset, but not all memory is being reset.\n> \n> \n> \n> Can confirm. Using AMD Radeon RX 7900XT with 20464MB (20GB) of VRAM. Generate 1 image and have to restart webui.bat every time I want to generate a new image. Attempting to use Command Line ARG --opt-split-attention-v1 did allow me to generate 2 512x512 images instead of only 1, but the issue still persisted after the second image was generated. \n> \n> First image brought me from around 6GB to 15GB. Second image took me from 15GB to around 18GB. Third image maxed out and never made any progress when monitoring it from the CMD window. Seeking other avenues to manually clear VRAM after each completed generation until this is resolved. \n\nAMD gpus are a diff story here. \n> > As soon as you click the generate button again GPU memory is being reset: https://github.com/AUTOMATIC1111/stable-diffusion-webui/blob/master/modules/processing.py#L510\n> \n> > \n> \n> > I also tested and it seem to work. I wonder what goes wrong at your PC?\n> \n> \n> \n> Most memory is being reset, but not all memory is being reset.Thank you for your reply.\n> \n> \n\nGonna give real world examples?\n> AMD gpus are a diff story here.\r\n\r\nI figured as much, but thought it might help in troubleshooting at least. I don't see anywhere that OP specified what GPU is being used; so, might be facing the same issue I am if it's a problem specific to AMD GPUs. If I have to reset webui.bat every time, that's quite the tradeoff for having self-hosted SD. \r\n\r\n\n> > AMD gpus are a diff story here.\r\n> \r\n> I figured as much, but thought it might help in troubleshooting at least. I don't see anywhere that OP specified what GPU is being used; so, might be facing the same issue I am if it's a problem specific to AMD GPUs. If I have to reset webui.bat every time, that's quite the tradeoff for having self-hosted SD.\r\n\r\nI'll have a look to see what can be done for AMD GPUs, but since I don't have one it's gonna be... interesting.\n@saltyollpheist \r\n\r\nno, this user is using nvidia, see \"CUDA\"\r\nreports should be separated to the direct-ml fork as memory doesn't decrease after use for many ml tasks\r\nbut this isn't something controllable for a developer either. only direct-ml.\r\n\r\nThere is an extension that lets you unload the model with a button, \"supermerger\"\r\nthis concept is useful when loading clip model using the \"clip interrogator\" extension\nI am facing the same issue here. Unless I restart my WebUI(close from command line and restart the bat file), the server keeps on showing CUDA out of memory problem,\n> I am facing the same issue here. Unless I restart my WebUI(close from command line and restart the bat file), the server keeps on showing CUDA out of memory problem,\r\n\r\nhttps://github.com/AUTOMATIC1111/stable-diffusion-webui/issues/8394", "created_at": "2023-03-21T07:53:52Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 8669, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-8669", "issue_numbers": ["8666"], "base_commit": "a9fed7c364061ae6efb37f797b6b522cb3cf7aa2", "patch": "diff --git a/modules/api/api.py b/modules/api/api.py\nindex 35e17afc9da..596b20b223b 100644\n--- a/modules/api/api.py\n+++ b/modules/api/api.py\n@@ -3,6 +3,7 @@\n import time\n import datetime\n import uvicorn\n+import gradio as gr\n from threading import Lock\n from io import BytesIO\n from gradio.processing_utils import decode_base64_to_file\n@@ -152,6 +153,9 @@ def __init__(self, app: FastAPI, queue_lock: Lock):\n         self.add_api_route(\"/sdapi/v1/memory\", self.get_memory, methods=[\"GET\"], response_model=MemoryResponse)\n         self.add_api_route(\"/sdapi/v1/scripts\", self.get_scripts_list, methods=[\"GET\"], response_model=ScriptsList)\n \n+        self.default_script_arg_txt2img = []\n+        self.default_script_arg_img2img = []\n+\n     def add_api_route(self, path: str, endpoint, **kwargs):\n         if shared.cmd_opts.api_auth:\n             return self.app.add_api_route(path, endpoint, dependencies=[Depends(self.auth)], **kwargs)\n@@ -185,7 +189,7 @@ def get_script(self, script_name, script_runner):\n         script_idx = script_name_to_index(script_name, script_runner.scripts)\n         return script_runner.scripts[script_idx]\n \n-    def init_script_args(self, request, selectable_scripts, selectable_idx, script_runner):\n+    def init_default_script_args(self, script_runner):\n         #find max idx from the scripts in runner and generate a none array to init script_args\n         last_arg_index = 1\n         for script in script_runner.scripts:\n@@ -193,13 +197,24 @@ def init_script_args(self, request, selectable_scripts, selectable_idx, script_r\n                 last_arg_index = script.args_to\n         # None everywhere except position 0 to initialize script args\n         script_args = [None]*last_arg_index\n+        script_args[0] = 0\n+\n+        # get default values\n+        with gr.Blocks(): # will throw errors calling ui function without this\n+            for script in script_runner.scripts:\n+                if script.ui(script.is_img2img):\n+                    ui_default_values = []\n+                    for elem in script.ui(script.is_img2img):\n+                        ui_default_values.append(elem.value)\n+                    script_args[script.args_from:script.args_to] = ui_default_values\n+        return script_args\n+\n+    def init_script_args(self, request, default_script_args, selectable_scripts, selectable_idx, script_runner):\n+        script_args = default_script_args.copy()\n         # position 0 in script_arg is the idx+1 of the selectable script that is going to be run when using scripts.scripts_*2img.run()\n         if selectable_scripts:\n             script_args[selectable_scripts.args_from:selectable_scripts.args_to] = request.script_args\n             script_args[0] = selectable_idx + 1\n-        else:\n-            # when [0] = 0 no selectable script to run\n-            script_args[0] = 0\n \n         # Now check for always on scripts\n         if request.alwayson_scripts and (len(request.alwayson_scripts) > 0):\n@@ -220,6 +235,8 @@ def text2imgapi(self, txt2imgreq: StableDiffusionTxt2ImgProcessingAPI):\n         if not script_runner.scripts:\n             script_runner.initialize_scripts(False)\n             ui.create_ui()\n+        if not self.default_script_arg_txt2img:\n+            self.default_script_arg_txt2img = self.init_default_script_args(script_runner)\n         selectable_scripts, selectable_script_idx = self.get_selectable_script(txt2imgreq.script_name, script_runner)\n \n         populate = txt2imgreq.copy(update={  # Override __init__ params\n@@ -235,7 +252,7 @@ def text2imgapi(self, txt2imgreq: StableDiffusionTxt2ImgProcessingAPI):\n         args.pop('script_args', None) # will refeed them to the pipeline directly after initializing them\n         args.pop('alwayson_scripts', None)\n \n-        script_args = self.init_script_args(txt2imgreq, selectable_scripts, selectable_script_idx, script_runner)\n+        script_args = self.init_script_args(txt2imgreq, self.default_script_arg_txt2img, selectable_scripts, selectable_script_idx, script_runner)\n \n         send_images = args.pop('send_images', True)\n         args.pop('save_images', None)\n@@ -272,6 +289,8 @@ def img2imgapi(self, img2imgreq: StableDiffusionImg2ImgProcessingAPI):\n         if not script_runner.scripts:\n             script_runner.initialize_scripts(True)\n             ui.create_ui()\n+        if not self.default_script_arg_img2img:\n+            self.default_script_arg_img2img = self.init_default_script_args(script_runner)\n         selectable_scripts, selectable_script_idx = self.get_selectable_script(img2imgreq.script_name, script_runner)\n \n         populate = img2imgreq.copy(update={  # Override __init__ params\n@@ -289,7 +308,7 @@ def img2imgapi(self, img2imgreq: StableDiffusionImg2ImgProcessingAPI):\n         args.pop('script_args', None)  # will refeed them to the pipeline directly after initializing them\n         args.pop('alwayson_scripts', None)\n \n-        script_args = self.init_script_args(img2imgreq, selectable_scripts, selectable_script_idx, script_runner)\n+        script_args = self.init_script_args(img2imgreq, self.default_script_arg_img2img, selectable_scripts, selectable_script_idx, script_runner)\n \n         send_images = args.pop('send_images', True)\n         args.pop('save_images', None)\n", "test_patch": "", "problem_statement": "[Bug]: API image generation returns errors in console with some alwayson_scripts\n### Is there an existing issue for this?\r\n\r\n- [X] I have searched the existing issues and checked the recent builds/commits\r\n\r\n### What happened?\r\n\r\nwebUI's console suddenly started returning errors unexpectedly when using `/sdapi/v1/txt2img` or `../img2img` calls such as the following (for my base install):\r\n```\r\nAPI 2023-03-15 16:56:02.038435 200 http/1.1 OPTIONS /sdapi/v1/txt2img 127.0.0.1 0.0\r\nError running process: E:\\storage\\stable-diffusion-webui\\extensions\\stable-diffusion-webui-aesthetic-gradients\\scripts\\aesthetic.py\r\nTraceback (most recent call last):\r\n  File \"E:\\storage\\stable-diffusion-webui\\modules\\scripts.py\", line 409, in process\r\n    script.process(p, *script_args)\r\n  File \"E:\\storage\\stable-diffusion-webui\\extensions\\stable-diffusion-webui-aesthetic-gradients\\scripts\\aesthetic.py\", line 36, in process\r\n    aesthetic.set_aesthetic_params(p, float(aesthetic_lr), float(aesthetic_weight), int(aesthetic_steps), aesthetic_imgs, aesthetic_slerp, aesthetic_imgs_text, aesthetic_slerp_angle, aesthetic_text_negative)\r\nTypeError: float() argument must be a string or a real number, not 'NoneType'\r\n\r\nError running process: E:\\storage\\stable-diffusion-webui\\extensions\\stable-diffusion-webui-conditioning-highres-fix\\scripts\\conditioning-highres-fix.py\r\nTraceback (most recent call last):\r\n  File \"E:\\storage\\stable-diffusion-webui\\modules\\scripts.py\", line 409, in process\r\n    script.process(p, *script_args)\r\n  File \"E:\\storage\\stable-diffusion-webui\\extensions\\stable-diffusion-webui-conditioning-highres-fix\\scripts\\conditioning-highres-fix.py\", line 22, in process\r\n    if conditioning_highres_fix>0 and p.denoising_strength is not None:\r\nTypeError: '>' not supported between instances of 'NoneType' and 'int'\r\n\r\nError running process: E:\\storage\\stable-diffusion-webui\\extensions\\stable-diffusion-webui-daam\\scripts\\daam_script.py\r\nTraceback (most recent call last):\r\n  File \"E:\\storage\\stable-diffusion-webui\\modules\\scripts.py\", line 409, in process\r\n    script.process(p, *script_args)\r\n  File \"E:\\storage\\stable-diffusion-webui\\extensions\\stable-diffusion-webui-daam\\scripts\\daam_script.py\", line 99, in process\r\n    self.attentions = [s.strip() for s in attention_texts.split(\",\") if s.strip()]\r\nAttributeError: 'NoneType' object has no attribute 'split'\r\n\r\nData shape for DDIM sampling is (2, 4, 64, 64), eta 0\r\nRunning DDIM Sampling with 31 timesteps\r\nDDIM Sampler:   0%|                                                                                                                              | 0/31 [00:00<?, ?it/s]\r\n```\r\n\r\ni made a few local changes to both `modules/api/api.py` and `modules/scripts.py` to determine where this was occurring, and it seems to be `scripts.py:409` just simply running every script that has a visible UI element whether or not it was declared by way of adding \r\n```\r\n        else:\r\n            print(\"no alwayson scripts\")\r\n```\r\nin `api.py:216-217`, as well as a simple `print(script)` in `scripts.py:407`\r\n\r\ni installed a clean instance of webUI, added `--api` to commandline args, installed the [aesthetic gradients](https://github.com/AUTOMATIC1111/stable-diffusion-webui-aesthetic-gradients) extension, and confirmed via simple http://127.0.0.1:7860/docs POST test to the txt2img endpoint.  \r\n\r\nreverting to commit 94ffa9fc5386e51f20692ab46906135e8de33110 prevents the issue from occurring\r\n\r\n### Steps to reproduce the problem\r\n\r\n0. enable `--api` commandline arg and install aesthetic gradients extension (for example)\r\n1. Go to [local API docs](http://127.0.0.1:7860/docs)\r\n2. Scroll down to `POST sdapi/v1/txt2img`, open it, click \"try it out\"\r\n3. Paste the following into the input field\r\n```\r\n{\r\n\t\"batch_size\": 1,\r\n\t\"cfg_scale\": 7,\r\n\t\"enable_hr\": false,\r\n\t\"height\": 512,\r\n\t\"n_iter\": 1,\r\n\t\"negative_prompt\": \"test\",\r\n\t\"prompt\": \"test\",\r\n\t\"restore_faces\": false,\r\n\t\"sampler_index\": \"DDIM\",\r\n\t\"seed\": \"1\",\r\n\t\"steps\": 1,\r\n\t\"width\": 512\r\n}\r\n```\r\n4. press execute and receive an error most similar to the following in the console\r\n```\r\nError running process: E:\\storage\\webui-clean-2\\extensions\\stable-diffusion-webui-aesthetic-gradients\\scripts\\aesthetic.py\r\nTraceback (most recent call last):\r\n  File \"E:\\storage\\webui-clean-2\\modules\\scripts.py\", line 386, in process\r\n    script.process(p, *script_args)\r\n  File \"E:\\storage\\webui-clean-2\\extensions\\stable-diffusion-webui-aesthetic-gradients\\scripts\\aesthetic.py\", line 36, in process\r\n    aesthetic.set_aesthetic_params(p, float(aesthetic_lr), float(aesthetic_weight), int(aesthetic_steps), aesthetic_imgs, aesthetic_slerp, aesthetic_imgs_text, aesthetic_slerp_angle, aesthetic_text_negative)\r\nTypeError: float() argument must be a string or a real number, not 'NoneType'\r\n\r\nData shape for DDIM sampling is (1, 4, 64, 64), eta 0.0\r\nRunning DDIM Sampling with 1 timesteps\r\nDDIM Sampler: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  5.56it/s]\r\n```\r\n\r\n### What should have happened?\r\n\r\npreferably, no errors would be encountered and the console just chugs along without running scripts not defined in the request, but i suppose \"always on\" might be a hint there\r\n\r\n### Commit where the problem happens\r\n\r\n2174f58daee1e077eec1125e196d34cc93dbaf23\r\n\r\n### What platforms do you use to access the UI ?\r\n\r\nWindows\r\n\r\n### What browsers do you use to access the UI ?\r\n\r\nMozilla Firefox\r\n\r\n### Command Line Arguments\r\n\r\n```Shell\r\n--api --ckpt-dir \"E:\\storage\\stable diffusion models\" --vae-dir \"E:\\storage\\stable diffusion VAEs\"\r\n```\r\n\r\n\r\n### List of extensions\r\n\r\n![image](https://user-images.githubusercontent.com/1649724/225466958-ebb0be67-82e2-4616-93ce-5a8de5195856.png)\r\n\r\n\r\n### Console logs\r\n\r\n(collected via commit 2174f58daee1e077eec1125e196d34cc93dbaf23)\r\n```Shell\r\nE:\\storage\\webui-clean-2>webui-user.bat\r\nvenv \"E:\\storage\\webui-clean-2\\venv\\Scripts\\Python.exe\"\r\nPython 3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]\r\nCommit hash: 2174f58daee1e077eec1125e196d34cc93dbaf23\r\nInstalling requirements for Web UI\r\nLaunching Web UI with arguments: --api --ckpt-dir E:\\storage\\stable diffusion models --vae-dir E:\\storage\\stable diffusion VAEs\r\nNo module 'xformers'. Proceeding without it.\r\nLoading weights [e1441589a6] from E:\\storage\\stable diffusion models\\runwayML-sd-v1-5-pruned.ckpt\r\nCreating model from config: E:\\storage\\webui-clean-2\\configs\\v1-inference.yaml\r\nLatentDiffusion: Running in eps-prediction mode\r\nDiffusionWrapper has 859.52 M params.\r\nApplying cross attention optimization (Doggettx).\r\nTextual inversion embeddings loaded(0):\r\nModel loaded in 4.4s (load weights from disk: 1.7s, create model: 0.3s, apply weights to model: 0.5s, apply half(): 0.7s, move model to device: 0.6s, load textual inversion embeddings: 0.7s).\r\nRunning on local URL:  http://127.0.0.1:7860\r\n\r\nTo create a public link, set `share=True` in `launch()`.\r\nError running process: E:\\storage\\webui-clean-2\\extensions\\stable-diffusion-webui-aesthetic-gradients\\scripts\\aesthetic.py\r\nTraceback (most recent call last):\r\n  File \"E:\\storage\\webui-clean-2\\modules\\scripts.py\", line 386, in process\r\n    script.process(p, *script_args)\r\n  File \"E:\\storage\\webui-clean-2\\extensions\\stable-diffusion-webui-aesthetic-gradients\\scripts\\aesthetic.py\", line 36, in process\r\n    aesthetic.set_aesthetic_params(p, float(aesthetic_lr), float(aesthetic_weight), int(aesthetic_steps), aesthetic_imgs, aesthetic_slerp, aesthetic_imgs_text, aesthetic_slerp_angle, aesthetic_text_negative)\r\nTypeError: float() argument must be a string or a real number, not 'NoneType'\r\n\r\nData shape for DDIM sampling is (1, 4, 64, 64), eta 0.0\r\nRunning DDIM Sampling with 1 timesteps\r\nDDIM Sampler: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:06<00:00,  6.93s/it]\r\nError running process: E:\\storage\\webui-clean-2\\extensions\\stable-diffusion-webui-aesthetic-gradients\\scripts\\aesthetic.py                        | 0/1 [00:00<?, ?it/s]\r\nTraceback (most recent call last):\r\n  File \"E:\\storage\\webui-clean-2\\modules\\scripts.py\", line 386, in process\r\n    script.process(p, *script_args)\r\n  File \"E:\\storage\\webui-clean-2\\extensions\\stable-diffusion-webui-aesthetic-gradients\\scripts\\aesthetic.py\", line 36, in process\r\n    aesthetic.set_aesthetic_params(p, float(aesthetic_lr), float(aesthetic_weight), int(aesthetic_steps), aesthetic_imgs, aesthetic_slerp, aesthetic_imgs_text, aesthetic_slerp_angle, aesthetic_text_negative)\r\nTypeError: float() argument must be a string or a real number, not 'NoneType'\r\n\r\nData shape for DDIM sampling is (1, 4, 64, 64), eta 0.0\r\nRunning DDIM Sampling with 1 timesteps\r\nDDIM Sampler: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.89it/s]\r\nError running process: E:\\storage\\webui-clean-2\\extensions\\stable-diffusion-webui-aesthetic-gradients\\scripts\\aesthetic.py\r\nTraceback (most recent call last):\r\n  File \"E:\\storage\\webui-clean-2\\modules\\scripts.py\", line 386, in process\r\n    script.process(p, *script_args)\r\n  File \"E:\\storage\\webui-clean-2\\extensions\\stable-diffusion-webui-aesthetic-gradients\\scripts\\aesthetic.py\", line 36, in process\r\n    aesthetic.set_aesthetic_params(p, float(aesthetic_lr), float(aesthetic_weight), int(aesthetic_steps), aesthetic_imgs, aesthetic_slerp, aesthetic_imgs_text, aesthetic_slerp_angle, aesthetic_text_negative)\r\nTypeError: float() argument must be a string or a real number, not 'NoneType'\r\n\r\nData shape for DDIM sampling is (1, 4, 64, 64), eta 0.0\r\nRunning DDIM Sampling with 1 timesteps\r\nDDIM Sampler: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  5.56it/s]\r\nTotal progress: 3it [16:03, 283.56s/it]\r\n```\r\n\r\n\r\n### Additional information\r\ndon't mean to be that guy who asks someone specific for help, but @Vespinian? seems you might be the one to pester unfortunately\n", "hints_text": "Yes, it's an unfortunate side effect of initializing script_arg array to None so that I can then insert the args at the correct place. If the extension doesn't handle receiving None in it's arg it will throw an error and extensions that don't take args will run either way. I did attempt to change the scriptrunner to remove from the alwayson_script list the script that were not included with more or less success but I'll give it another shot.", "created_at": "2023-03-16T02:24:54Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 8187, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-8187", "issue_numbers": ["8253"], "base_commit": "27e319dc4f09a2f040043948e5c52965976f8491", "patch": "diff --git a/modules/api/api.py b/modules/api/api.py\nindex 376f7f048f3..35e17afc9da 100644\n--- a/modules/api/api.py\n+++ b/modules/api/api.py\n@@ -164,14 +164,10 @@ def auth(self, credentials: HTTPBasicCredentials = Depends(HTTPBasic())):\n \n         raise HTTPException(status_code=401, detail=\"Incorrect username or password\", headers={\"WWW-Authenticate\": \"Basic\"})\n \n-    def get_script(self, script_name, script_runner):\n-        if script_name is None:\n+    def get_selectable_script(self, script_name, script_runner):\n+        if script_name is None or script_name == \"\":\n             return None, None\n \n-        if not script_runner.scripts:\n-            script_runner.initialize_scripts(False)\n-            ui.create_ui()\n-\n         script_idx = script_name_to_index(script_name, script_runner.selectable_scripts)\n         script = script_runner.selectable_scripts[script_idx]\n         return script, script_idx\n@@ -182,8 +178,49 @@ def get_scripts_list(self):\n \n         return ScriptsList(txt2img = t2ilist, img2img = i2ilist)  \n \n+    def get_script(self, script_name, script_runner):\n+        if script_name is None or script_name == \"\":\n+            return None, None\n+        \n+        script_idx = script_name_to_index(script_name, script_runner.scripts)\n+        return script_runner.scripts[script_idx]\n+\n+    def init_script_args(self, request, selectable_scripts, selectable_idx, script_runner):\n+        #find max idx from the scripts in runner and generate a none array to init script_args\n+        last_arg_index = 1\n+        for script in script_runner.scripts:\n+            if last_arg_index < script.args_to:\n+                last_arg_index = script.args_to\n+        # None everywhere except position 0 to initialize script args\n+        script_args = [None]*last_arg_index\n+        # position 0 in script_arg is the idx+1 of the selectable script that is going to be run when using scripts.scripts_*2img.run()\n+        if selectable_scripts:\n+            script_args[selectable_scripts.args_from:selectable_scripts.args_to] = request.script_args\n+            script_args[0] = selectable_idx + 1\n+        else:\n+            # when [0] = 0 no selectable script to run\n+            script_args[0] = 0\n+\n+        # Now check for always on scripts\n+        if request.alwayson_scripts and (len(request.alwayson_scripts) > 0):\n+            for alwayson_script_name in request.alwayson_scripts.keys():\n+                alwayson_script = self.get_script(alwayson_script_name, script_runner)\n+                if alwayson_script == None:\n+                    raise HTTPException(status_code=422, detail=f\"always on script {alwayson_script_name} not found\")\n+                # Selectable script in always on script param check\n+                if alwayson_script.alwayson == False:\n+                    raise HTTPException(status_code=422, detail=f\"Cannot have a selectable script in the always on scripts params\")\n+                # always on script with no arg should always run so you don't really need to add them to the requests\n+                if \"args\" in request.alwayson_scripts[alwayson_script_name]:\n+                    script_args[alwayson_script.args_from:alwayson_script.args_to] = request.alwayson_scripts[alwayson_script_name][\"args\"]\n+        return script_args\n+\n     def text2imgapi(self, txt2imgreq: StableDiffusionTxt2ImgProcessingAPI):\n-        script, script_idx = self.get_script(txt2imgreq.script_name, scripts.scripts_txt2img)\n+        script_runner = scripts.scripts_txt2img\n+        if not script_runner.scripts:\n+            script_runner.initialize_scripts(False)\n+            ui.create_ui()\n+        selectable_scripts, selectable_script_idx = self.get_selectable_script(txt2imgreq.script_name, script_runner)\n \n         populate = txt2imgreq.copy(update={  # Override __init__ params\n             \"sampler_name\": validate_sampler_name(txt2imgreq.sampler_name or txt2imgreq.sampler_index),\n@@ -195,20 +232,26 @@ def text2imgapi(self, txt2imgreq: StableDiffusionTxt2ImgProcessingAPI):\n \n         args = vars(populate)\n         args.pop('script_name', None)\n+        args.pop('script_args', None) # will refeed them to the pipeline directly after initializing them\n+        args.pop('alwayson_scripts', None)\n+\n+        script_args = self.init_script_args(txt2imgreq, selectable_scripts, selectable_script_idx, script_runner)\n \n         send_images = args.pop('send_images', True)\n         args.pop('save_images', None)\n \n         with self.queue_lock:\n             p = StableDiffusionProcessingTxt2Img(sd_model=shared.sd_model, **args)\n+            p.scripts = script_runner\n             p.outpath_grids = opts.outdir_txt2img_grids\n             p.outpath_samples = opts.outdir_txt2img_samples\n \n             shared.state.begin()\n-            if script is not None:\n-                p.script_args = [script_idx + 1] + [None] * (script.args_from - 1) + p.script_args\n-                processed = scripts.scripts_txt2img.run(p, *p.script_args)\n+            if selectable_scripts != None:\n+                p.script_args = script_args\n+                processed = scripts.scripts_txt2img.run(p, *p.script_args) # Need to pass args as list here\n             else:\n+                p.script_args = tuple(script_args) # Need to pass args as tuple here\n                 processed = process_images(p)\n             shared.state.end()\n \n@@ -221,12 +264,16 @@ def img2imgapi(self, img2imgreq: StableDiffusionImg2ImgProcessingAPI):\n         if init_images is None:\n             raise HTTPException(status_code=404, detail=\"Init image not found\")\n \n-        script, script_idx = self.get_script(img2imgreq.script_name, scripts.scripts_img2img)\n-\n         mask = img2imgreq.mask\n         if mask:\n             mask = decode_base64_to_image(mask)\n \n+        script_runner = scripts.scripts_img2img\n+        if not script_runner.scripts:\n+            script_runner.initialize_scripts(True)\n+            ui.create_ui()\n+        selectable_scripts, selectable_script_idx = self.get_selectable_script(img2imgreq.script_name, script_runner)\n+\n         populate = img2imgreq.copy(update={  # Override __init__ params\n             \"sampler_name\": validate_sampler_name(img2imgreq.sampler_name or img2imgreq.sampler_index),\n             \"do_not_save_samples\": not img2imgreq.save_images,\n@@ -239,6 +286,10 @@ def img2imgapi(self, img2imgreq: StableDiffusionImg2ImgProcessingAPI):\n         args = vars(populate)\n         args.pop('include_init_images', None)  # this is meant to be done by \"exclude\": True in model, but it's for a reason that I cannot determine.\n         args.pop('script_name', None)\n+        args.pop('script_args', None)  # will refeed them to the pipeline directly after initializing them\n+        args.pop('alwayson_scripts', None)\n+\n+        script_args = self.init_script_args(img2imgreq, selectable_scripts, selectable_script_idx, script_runner)\n \n         send_images = args.pop('send_images', True)\n         args.pop('save_images', None)\n@@ -246,14 +297,16 @@ def img2imgapi(self, img2imgreq: StableDiffusionImg2ImgProcessingAPI):\n         with self.queue_lock:\n             p = StableDiffusionProcessingImg2Img(sd_model=shared.sd_model, **args)\n             p.init_images = [decode_base64_to_image(x) for x in init_images]\n+            p.scripts = script_runner\n             p.outpath_grids = opts.outdir_img2img_grids\n             p.outpath_samples = opts.outdir_img2img_samples\n \n             shared.state.begin()\n-            if script is not None:\n-                p.script_args = [script_idx + 1] + [None] * (script.args_from - 1) + p.script_args\n-                processed = scripts.scripts_img2img.run(p, *p.script_args)\n+            if selectable_scripts != None:\n+                p.script_args = script_args\n+                processed = scripts.scripts_img2img.run(p, *p.script_args) # Need to pass args as list here\n             else:\n+                p.script_args = tuple(script_args) # Need to pass args as tuple here\n                 processed = process_images(p)\n             shared.state.end()\n \ndiff --git a/modules/api/models.py b/modules/api/models.py\nindex fa1c40df61f..4a70f440c8f 100644\n--- a/modules/api/models.py\n+++ b/modules/api/models.py\n@@ -106,6 +106,7 @@ def generate_model(self):\n         {\"key\": \"script_args\", \"type\": list, \"default\": []},\n         {\"key\": \"send_images\", \"type\": bool, \"default\": True},\n         {\"key\": \"save_images\", \"type\": bool, \"default\": False},\n+        {\"key\": \"alwayson_scripts\", \"type\": dict, \"default\": {}},\n     ]\n ).generate_model()\n \n@@ -122,6 +123,7 @@ def generate_model(self):\n         {\"key\": \"script_args\", \"type\": list, \"default\": []},\n         {\"key\": \"send_images\", \"type\": bool, \"default\": True},\n         {\"key\": \"save_images\", \"type\": bool, \"default\": False},\n+        {\"key\": \"alwayson_scripts\", \"type\": dict, \"default\": {}},\n     ]\n ).generate_model()\n \n", "test_patch": "", "problem_statement": "[Feature Request]: use extensions in API call\n### Is there an existing issue for this?\n\n- [X] I have searched the existing issues and checked the recent builds/commits\n\n### What would your feature do ?\n\nI'd like to let API users use selected extensions (mostly prompt manipulation ones). \r\nThis is not currently possible, see also https://github.com/mix1009/sdwebuiapi/issues/21\n\n### Proposed workflow\n\nI'd like to make an API call using (for example) a `wildcard` as\r\n```python\r\nimport webuiapi\r\n\r\napi = webuiapi.WebUIApi()\r\nresult = api.txt2img(prompt=\"__mywildcard__\",\r\n                    negative_prompt=\"\",\r\n                    seed=123,\r\n                    cfg_scale=7)\r\n```\n\n### Additional information\n\n_No response_\n", "hints_text": "", "created_at": "2023-02-28T05:19:29Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 8175, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-8175", "issue_numbers": ["8164"], "base_commit": "0cc0ee1bcb4c24a8c9715f66cede06601bfc00c8", "patch": "diff --git a/modules/shared.py b/modules/shared.py\nindex 805f9cc19cf..ec08b7becd5 100644\n--- a/modules/shared.py\n+++ b/modules/shared.py\n@@ -330,6 +330,7 @@ def list_samplers():\n     \"export_for_4chan\": OptionInfo(True, \"If the saved image file size is above the limit, or its either width or height are above the limit, save a downscaled copy as JPG\"),\r\n     \"img_downscale_threshold\": OptionInfo(4.0, \"File size limit for the above option, MB\", gr.Number),\r\n     \"target_side_length\": OptionInfo(4000, \"Width/height limit for the above option, in pixels\", gr.Number),\r\n+    \"img_max_size_mp\": OptionInfo(200, \"Maximum image size, in megapixels\", gr.Number),\r\n \r\n     \"use_original_name_batch\": OptionInfo(True, \"Use original name for output filename during batch process in extras tab\"),\r\n     \"use_upscaler_name_as_suffix\": OptionInfo(False, \"Use upscaler name as filename suffix in the extras tab\"),\r\ndiff --git a/scripts/xyz_grid.py b/scripts/xyz_grid.py\nindex 53511b121a2..44f7374cbaa 100644\n--- a/scripts/xyz_grid.py\n+++ b/scripts/xyz_grid.py\n@@ -484,6 +484,11 @@ def process_axis(opt, vals):\n         z_opt = self.current_axis_options[z_type]\r\n         zs = process_axis(z_opt, z_values)\r\n \r\n+        # this could be moved to common code, but unlikely to be ever triggered anywhere else\r\n+        Image.MAX_IMAGE_PIXELS = opts.img_max_size_mp * 1.1 # allow 10% overhead for margins and legend\r\n+        grid_mp = round(len(xs) * len(ys) * len(zs) * p.width * p.height / 1000000)\r\n+        assert grid_mp < opts.img_max_size_mp, f'Error: Resulting grid would be too large ({grid_mp} MPixels) (max configured size is {opts.img_max_size_mp} MPixels)'\r\n+\r\n         def fix_axis_seeds(axis_opt, axis_list):\r\n             if axis_opt.label in ['Seed', 'Var. seed']:\r\n                 return [int(random.randrange(4294967294)) if val is None or val == '' or val == -1 else val for val in axis_list]\r\n", "test_patch": "", "problem_statement": "[Bug]: DecompressionBombError using X/Y/Z script\n### Is there an existing issue for this?\r\n\r\n- [X] I have searched the existing issues and checked the recent builds/commits\r\n\r\n### What happened?\r\n\r\nWhen you are get large amount of combinations in X/Y/Z script, you'll get an error like this:\r\n```python\r\nPIL.Image.DecompressionBombError: Image size (231258240 pixels) exceeds limit of 178956970 pixels, could be decompression bomb DOS attack.\r\n```\r\n\r\nThis is protection from underlying Pillow library and the reasons described here:\r\nhttps://pillow.readthedocs.io/en/latest/reference/Image.html#PIL.Image.open\r\n\r\n### Possible solutions:\r\n- Disable check by setting PIL.Image.MAX_IMAGE_PIXELS=None (cons that grid can generate huge unusable images)\r\n- Split large grids to chunks\r\n\r\n### Steps to reproduce the problem\r\n\r\n1. Go to txt2img tab\r\n2. Select X/Y/Z script\r\n3. Enter a large number of parameters, e.g \r\n- Steps: 1\r\n- Seed: 1-100000\r\n\r\n\r\n### What should have happened?\r\n\r\nGrid should be generated\r\n\r\n### Commit where the problem happens\r\n\r\n0cc0ee1bcb4c24a8c9715f66cede06601bfc00c8\r\n\r\n### What platforms do you use to access the UI ?\r\n\r\n_No response_\r\n\r\n### What browsers do you use to access the UI ?\r\n\r\n_No response_\r\n\r\n### Command Line Arguments\r\n\r\n```Shell\r\nAny\r\n```\r\n\r\n\r\n### List of extensions\r\n\r\nAny\r\n\r\n### Console logs\r\n\r\n```Python\r\nTraceback (most recent call last):\r\n  File \"/stable-diffusion-webui/modules/call_queue.py\", line 56, in f\r\n    res = list(func(*args, **kwargs))\r\n  File \"/stable-diffusion-webui/modules/call_queue.py\", line 37, in f\r\n    res = func(*args, **kwargs)\r\n  File \"/stable-diffusion-webui/modules/txt2img.py\", line 53, in txt2img\r\n    processed = modules.scripts.scripts_txt2img.run(p, *args)\r\n  File \"/stable-diffusion-webui/modules/scripts.py\", line 376, in run\r\n    processed = script.run(p, *script_args)\r\n  File \"/stable-diffusion-webui/scripts/xyz_grid.py\", line 596, in run\r\n    processed, sub_grids = draw_xyz_grid(\r\n  File \"/stable-diffusion-webui/scripts/xyz_grid.py\", line 309, in draw_xyz_grid\r\n    z_grid = images.draw_grid_annotations(z_grid, sub_grid_size[0], sub_grid_size[1], title_texts, [[images.GridAnnotation()]])\r\n  File \"/stable-diffusion-webui/modules/images.py\", line 206, in draw_grid_annotations\r\n    cell = im.crop((width * col, height * row, width * (col+1), height * (row+1)))\r\n  File \"/stable-diffusion-webui/venv/lib/python3.10/site-packages/PIL/Image.py\", line 1234, in crop\r\n    return self._new(self._crop(self.im, box))\r\n  File \"stable-diffusion-webui/venv/lib/python3.10/site-packages/PIL/Image.py\", line 1252, in _crop\r\n    _decompression_bomb_check(absolute_values)\r\n  File \"/stable-diffusion-webui/venv/lib/python3.10/site-packages/PIL/Image.py\", line 3164, in _decompression_bomb_check\r\n    raise DecompressionBombError(msg)\r\nPIL.Image.DecompressionBombError: Image size (231258240 pixels) exceeds limit of 178956970 pixels, could be decompression bomb DOS attack.\r\n```\r\n\r\n\r\n### Additional information\r\n\r\n_No response_\n", "hints_text": "[Connected discussion](https://github.com/AUTOMATIC1111/stable-diffusion-webui/discussions/8051)", "created_at": "2023-02-27T22:32:18Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 8118, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-8118", "issue_numbers": ["8116"], "base_commit": "0cc0ee1bcb4c24a8c9715f66cede06601bfc00c8", "patch": "diff --git a/modules/extensions.py b/modules/extensions.py\nindex 3eef9eaf65d..ed4b58fe39c 100644\n--- a/modules/extensions.py\n+++ b/modules/extensions.py\n@@ -66,7 +66,7 @@ def list_files(self, subdir, extension):\n \r\n     def check_updates(self):\r\n         repo = git.Repo(self.path)\r\n-        for fetch in repo.remote().fetch(\"--dry-run\"):\r\n+        for fetch in repo.remote().fetch(dry_run=True):\r\n             if fetch.flags != fetch.HEAD_UPTODATE:\r\n                 self.can_update = True\r\n                 self.status = \"behind\"\r\n@@ -79,8 +79,8 @@ def fetch_and_reset_hard(self):\n         repo = git.Repo(self.path)\r\n         # Fix: `error: Your local changes to the following files would be overwritten by merge`,\r\n         # because WSL2 Docker set 755 file permissions instead of 644, this results to the error.\r\n-        repo.git.fetch('--all')\r\n-        repo.git.reset('--hard', 'origin')\r\n+        repo.git.fetch(all=True)\r\n+        repo.git.reset('origin', hard=True)\r\n \r\n \r\n def list_extensions():\r\n", "test_patch": "", "problem_statement": "[Bug]: GitPython breaking API change in 3.1.30, breaks extension updates\n### Is there an existing issue for this?\n\n- [X] I have searched the existing issues and checked the recent builds/commits\n\n### What happened?\n\n[Per this PR](https://github.com/gitpython-developers/GitPython/pull/1518) and [the changelog](https://github.com/gitpython-developers/GitPython/pull/1518) you can no longer feed arbitrary arguments to prevent remote code execution.\r\n\r\nEasy fix, just use the built kwarg that's already there for it.\r\nhttps://github.com/AUTOMATIC1111/stable-diffusion-webui/blame/0cc0ee1bcb4c24a8c9715f66cede06601bfc00c8/modules/extensions.py#LL69C28-L69C28\r\n\r\nthere may be other places in the code as well, I'll take a peek\n\n### Steps to reproduce the problem\n\n1. `pip install 'gitpython>=3.1.30'\r\n2. run the web-ui and try to install/check for updates while watching terminal\n\n### What should have happened?\n\nShould have successfully run the git commands and updated  the git repos\n\n### Commit where the problem happens\n\n0cc0ee1b\n\n### What platforms do you use to access the UI ?\n\nWindows\n\n### What browsers do you use to access the UI ?\n\nGoogle Chrome\n\n### Command Line Arguments\n\n```Shell\nNo\n```\n\n\n### List of extensions\n\nNo\n\n### Console logs\n\n```Shell\nTraceback (most recent call last):\r\n  File \"/mnt/d/stable-diffusion/stable-diffusion-webui/modules/ui_extensions.py\", line 66, in check_updates\r\n    ext.check_updates()\r\n  File \"/mnt/d/stable-diffusion/stable-diffusion-webui/modules/extensions.py\", line 69, in check_updates\r\n    for fetch in repo.remote().fetch(\"--dry-run\"):\r\n  File \"/home/adam/.cache/pypoetry/virtualenvs/sd-deps-z4SYejYZ-py3.10/lib/python3.10/site-packages/git/remote.py\", line 1007, in fetch\r\n    res = self._get_fetch_info_from_stderr(proc, progress, kill_after_timeout=kill_after_timeout)\r\n  File \"/home/adam/.cache/pypoetry/virtualenvs/sd-deps-z4SYejYZ-py3.10/lib/python3.10/site-packages/git/remote.py\", line 848, in _get_fetch_info_from_stderr\r\n    proc.wait(stderr=stderr_text)\r\n  File \"/home/adam/.cache/pypoetry/virtualenvs/sd-deps-z4SYejYZ-py3.10/lib/python3.10/site-packages/git/cmd.py\", line 604, in wait\r\n    raise GitCommandError(remove_password_if_present(self.args), status, errstr)\r\ngit.exc.GitCommandError: Cmd('git') failed due to: exit code(128)\r\n  cmdline: git fetch -v -- origin --dry-run\r\n  stderr: 'fatal: couldn't find remote ref --dry-run'\n```\n\n\n### Additional information\n\n_No response_\n", "hints_text": "", "created_at": "2023-02-25T19:30:33Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 8112, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-8112", "issue_numbers": ["7869"], "base_commit": "0cc0ee1bcb4c24a8c9715f66cede06601bfc00c8", "patch": "diff --git a/javascript/progressbar.js b/javascript/progressbar.js\nindex ff6d757bae8..9ccc9da46d3 100644\n--- a/javascript/progressbar.js\n+++ b/javascript/progressbar.js\n@@ -139,7 +139,7 @@ function requestProgress(id_task, progressbarContainer, gallery, atEnd, onProgre\n \n     var divProgress = document.createElement('div')\n     divProgress.className='progressDiv'\n-    divProgress.style.display = opts.show_progressbar ? \"\" : \"none\"\n+    divProgress.style.display = opts.show_progressbar ? \"block\" : \"none\"\n     var divInner = document.createElement('div')\n     divInner.className='progress'\n \n", "test_patch": "", "problem_statement": "[Bug]: The latest version, txt2img progress bar cannot be displayed\n### Is there an existing issue for this?\n\n- [X] I have searched the existing issues and checked the recent builds/commits\n\n### What happened?\n\nI updated the latest code (February 17th, today).\r\n\r\nWhen I use txt2img, the correct picture will be generated, but the progress bar cannot be displayed.\r\nIt always shows interrupt and skip buttons and can not generate the next picture.\r\n\r\nAre there any bugs in the latest code?\n\n### Steps to reproduce the problem\n\n1. Go to txt2img, write some prompts.\r\n2. Press Generate\r\n\n\n### What should have happened?\n\nDisplay the normal progress bar, and the button returns to Generate\n\n### Commit where the problem happens\n\n3715ece\n\n### What platforms do you use to access the UI ?\n\nLinux\n\n### What browsers do you use to access the UI ?\n\nGoogle Chrome\n\n### Command Line Arguments\n\n```Shell\nfalse\n```\n\n\n### List of extensions\n\nfalse\n\n### Console logs\n\n```Shell\nLog OK\n```\n\n\n### Additional information\n\n_No response_\n", "hints_text": "Does it happen when you are doing multi tasks or switching apps? If I do these things, the progress bar sometimes just disappear but the calculation continues.", "created_at": "2023-02-25T17:58:08Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 8100, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-8100", "issue_numbers": ["8086"], "base_commit": "0cc0ee1bcb4c24a8c9715f66cede06601bfc00c8", "patch": "diff --git a/modules/images.py b/modules/images.py\nindex 5b80c23e11e..7df2b08c7de 100644\n--- a/modules/images.py\n+++ b/modules/images.py\n@@ -556,7 +556,7 @@ def _atomically_save_image(image_to_save, filename_without_extension, extension)\n             elif image_to_save.mode == 'I;16':\r\n                 image_to_save = image_to_save.point(lambda p: p * 0.0038910505836576).convert(\"RGB\" if extension.lower() == \".webp\" else \"L\")\r\n \r\n-            image_to_save.save(temp_file_path, format=image_format, quality=opts.jpeg_quality)\r\n+            image_to_save.save(temp_file_path, format=image_format, quality=opts.jpeg_quality, lossless=opts.webp_lossless)\r\n \r\n             if opts.enable_pnginfo and info is not None:\r\n                 exif_bytes = piexif.dump({\r\ndiff --git a/modules/shared.py b/modules/shared.py\nindex 805f9cc19cf..511019880c9 100644\n--- a/modules/shared.py\n+++ b/modules/shared.py\n@@ -327,6 +327,7 @@ def list_samplers():\n     \"save_images_before_highres_fix\": OptionInfo(False, \"Save a copy of image before applying highres fix.\"),\r\n     \"save_images_before_color_correction\": OptionInfo(False, \"Save a copy of image before applying color correction to img2img results\"),\r\n     \"jpeg_quality\": OptionInfo(80, \"Quality for saved jpeg images\", gr.Slider, {\"minimum\": 1, \"maximum\": 100, \"step\": 1}),\r\n+    \"webp_lossless\": OptionInfo(False, \"Use lossless compression for webp images\"),\r\n     \"export_for_4chan\": OptionInfo(True, \"If the saved image file size is above the limit, or its either width or height are above the limit, save a downscaled copy as JPG\"),\r\n     \"img_downscale_threshold\": OptionInfo(4.0, \"File size limit for the above option, MB\", gr.Number),\r\n     \"target_side_length\": OptionInfo(4000, \"Width/height limit for the above option, in pixels\", gr.Number),\r\n", "test_patch": "", "problem_statement": "[Feature Request]: add support for lossless WebP as output format\n### Is there an existing issue for this?\n\n- [X] I have searched the existing issues and checked the recent builds/commits\n\n### What would your feature do ?\n\nCurrently, WebP works for me if I set it in settings, but it's lossy WebP. Would be great to have an option for lossless WebP. Smaller file size than PNG, supported almost everywhere.\n\n### Proposed workflow\n\nSet file format for images & grids in options to webp to generate lossless WebP\n\n### Additional information\n\n_No response_\n", "hints_text": "", "created_at": "2023-02-25T04:13:50Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 8031, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-8031", "issue_numbers": ["8030"], "base_commit": "0cc0ee1bcb4c24a8c9715f66cede06601bfc00c8", "patch": "diff --git a/javascript/notification.js b/javascript/notification.js\nindex 040a3afac20..5ae6df24df1 100644\n--- a/javascript/notification.js\n+++ b/javascript/notification.js\n@@ -15,7 +15,7 @@ onUiUpdate(function(){\n         }\n     }\n \n-    const galleryPreviews = gradioApp().querySelectorAll('div[id^=\"tab_\"][style*=\"display: block\"] img.h-full.w-full.overflow-hidden');\n+    const galleryPreviews = gradioApp().querySelectorAll('div[id^=\"tab_\"][style*=\"display: block\"] div[id$=\"_results\"] img.h-full.w-full.overflow-hidden');\n \n     if (galleryPreviews == null) return;\n \n", "test_patch": "", "problem_statement": "[Bug]: Notifications do not work with StylePile extension enabled\n### Is there an existing issue for this?\r\n\r\n- [X] I have searched the existing issues and checked the recent builds/commits\r\n\r\n### What happened?\r\n\r\nWhen the [StylePile](https://github.com/some9000/StylePile) extension is enabled, notifications for the Automatic1111 webui (both sound and browser notifications) will only trigger when the webpage is first loaded. They will not trigger when a batch of images has finished generating in the txt2img or img2img tabs. Notifications _do_ work in the Extras tab.\r\n\r\n### Steps to reproduce the problem\r\n\r\n1. Install the [StylePile](https://github.com/some9000/StylePile) extension.\r\n2. Enable browser notifications.\r\n3. Reload the WebUI.\r\n4. If set up, notification.mp3 will play when the UI is first loaded.\r\n5. Generate a batch of images with txt2img.\r\n6. When finished, no notifications will occur.\r\n7. Same behavior will occur with img2img.\r\n\r\n### What should have happened?\r\n\r\nA sound should play and a browser notification should generate once all batches have finished.\r\n\r\n### Commit where the problem happens\r\n\r\n0cc0ee1b\r\n\r\n### What platforms do you use to access the UI ?\r\n\r\nWindows\r\n\r\n### What browsers do you use to access the UI ?\r\n\r\n- Firefox\r\n- Edge\r\n\r\n### Command Line Arguments\r\n\r\n```Shell\r\nset COMMANDLINE_ARGS=--medvram --xformers --no-half-vae\r\n```\r\n\r\n\r\n### List of extensions\r\n\r\nStylePile: \thttps://github.com/some9000/StylePile\r\nopenpose-editor: \thttps://github.com/fkunn1326/openpose-editor.git\r\nsd-webui-controlnet: \thttps://github.com/Mikubill/sd-webui-controlnet.git\r\nsd_grid_add_image_number: \thttps://github.com/AlUlkesh/sd_grid_add_image_number.git\r\na1111-sd-webui-tagcomplete: \thttps://github.com/DominikDoom/a1111-sd-webui-tagcomplete\r\n\r\n### Console logs\r\n\r\n```Shell\r\nhttps://pastes.io/lgoac3ui9s\r\n```\r\n\r\n\r\n### Additional information\r\n\r\n_No response_\n", "hints_text": "", "created_at": "2023-02-22T22:04:27Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 7954, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-7954", "issue_numbers": ["7621"], "base_commit": "0cc0ee1bcb4c24a8c9715f66cede06601bfc00c8", "patch": "diff --git a/scripts/xyz_grid.py b/scripts/xyz_grid.py\nindex 53511b121a2..d0ff5cb8601 100644\n--- a/scripts/xyz_grid.py\n+++ b/scripts/xyz_grid.py\n@@ -418,7 +418,7 @@ def process_axis(opt, vals):\n             if opt.label == 'Nothing':\r\n                 return [0]\r\n \r\n-            valslist = [x.strip() for x in chain.from_iterable(csv.reader(StringIO(vals)))]\r\n+            valslist = [x.strip() for x in chain.from_iterable(csv.reader(StringIO(vals))) if x]\r\n \r\n             if opt.type == int:\r\n                 valslist_ext = []\r\n", "test_patch": "", "problem_statement": "[Bug]: X/Y/Z Plot is adding an extra image between the generated ones\n### Is there an existing issue for this?\n\n- [X] I have searched the existing issues and checked the recent builds/commits\n\n### What happened?\n\nHello.\r\n\r\nit does not matter the size of the plot it always add an image between the generated ones. making the grid unnecessary bigger.\r\n\r\nEven with a large margin it adds an image\r\n\r\n![image](https://user-images.githubusercontent.com/116231978/217324678-20e1d917-f870-487f-892b-be8fe49c4f1d.png)\r\n\r\n![xyz_grid-0000-1655128259](https://user-images.githubusercontent.com/116231978/217325456-6ca85995-e44a-44ea-98df-4fa5e25762ac.jpg)\r\n\r\n![image](https://user-images.githubusercontent.com/116231978/217327594-d0a18e73-9b71-4586-b06a-245043768397.png)\r\n\r\n\r\n\r\n\n\n### Steps to reproduce the problem\n\nPrompt:\r\nsharp focus of a plant growing from the dirt, blutrry nature green background, highly defined, photorealism, reality picture,depth of field\r\n\r\nSteps: 60,\r\nSampler: Euler a,\r\nCFG scale: 7.5,\r\nSeed: 1655128259,\r\nSize: 512x768,\r\nModel hash: b5fcd73ddc,\r\nModel: dreambox-mix-A,\r\nScript: X/Y/Z plot,\r\nX Type: Checkpoint name,\r\nX Values: \"dreambox-mix-A.ckpt, SlimeX.safetensors,\",\r\nY Type: Sampler, Y Values: \"Euler a, LMS\"\r\n\r\n![image](https://user-images.githubusercontent.com/116231978/217327976-410b8a2a-08f2-4419-abf6-16fbc539fd59.png)\r\n\n\n### What should have happened?\n\ndisplay the grid without extra images\n\n### Commit where the problem happens\n\nea9bd9f\n\n### What platforms do you use to access the UI ?\n\nWindows\n\n### What browsers do you use to access the UI ?\n\nGoogle Chrome\n\n### Command Line Arguments\n\n```Shell\nset COMMANDLINE_ARGS=--listen --no-half --precision full --port 9999 --disable-safe-unpickle --deepdanbooru --reinstall-torch --reinstall-xformers --enable-insecure-extension-access\n```\n\n\n### List of extensions\n\ndeforum\r\nstable-diffusion-webui-instruct-pix2pix | https://github.com/Klace/stable-diffusion-webui-instruct-pix2pix.git\r\nLDSR | built-in | \u00a0\r\nLora | built-in | \u00a0\r\nScuNET | built-in | \u00a0\r\nSwinIR | built-in | \u00a0\r\nprompt-bracket-checker | built-in\r\n\r\n\n\n### Console logs\n\n```Shell\nX/Y/Z plot will create 8 images on 1 4x2 grid. (Total steps to process: 480)\r\n                                  Loading weights [b5fcd73ddc] from F:\\auto1111\\models\\Stable-diffusion\\dreambox-mix-A.ckptal progress: 0it [00:00, ?it/s]\r\nApplying cross attention optimization (Doggettx).\r\nWeights loaded in 7.5s (load weights from disk: 5.9s, apply weights to model: 0.5s, move model to device: 1.1s).\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 60/60 [00:31<00:00,  1.91it/s]\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 60/60 [00:31<00:00,  1.91it/s]\r\nLoading weights [d1004a4e24] from F:\\auto1111\\models\\Stable-diffusion\\mjstyle.ckpt   | 120/480 [01:12<03:08,  1.91it/s]\r\nApplying cross attention optimization (Doggettx).\r\nWeights loaded in 3.1s (load weights from disk: 1.6s, apply weights to model: 0.4s, move model to device: 1.1s).\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 60/60 [00:31<00:00,  1.90it/s]\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 60/60 [00:31<00:00,  1.91it/s]\r\nLoading weights [f22782eb52] from F:\\auto1111\\models\\Stable-diffusion\\SlimeX.safetensors40/480 [02:21<02:05,  1.91it/s]\r\nApplying cross attention optimization (Doggettx).\r\nWeights loaded in 2.9s (load weights from disk: 0.2s, apply weights to model: 1.8s, move model to device: 0.9s).\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 60/60 [00:31<00:00,  1.89it/s]\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 60/60 [00:31<00:00,  1.91it/s]\r\nLoading weights [d1004a4e24] from F:\\auto1111\\models\\Stable-diffusion\\mjstyle.ckpt   | 360/480 [03:29<01:02,  1.91it/s]\r\nApplying cross attention optimization (Doggettx).\r\nWeights loaded in 2.1s (load weights from disk: 0.8s, apply weights to model: 0.4s, move model to device: 0.9s).\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 60/60 [00:31<00:00,  1.91it/s]\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 60/60 [00:31<00:00,  1.91it/s]\r\nTotal progress: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 480/480 [04:38<00:00,  1.72it/s]\r\n{\"prompt\": \"sharp focus of a plant growing from the dirt, blutrry nature green background, highly defined, photorealism, reality picture,depth of field\", \"all_prompts\": [\"sharp focus of a plant growing from the dirt, blutrry nature green background, highly defined, photorealism, reality picture,depth of field\"], \"negative_prompt\": \"\", \"all_negative_prompts\": [\"\"], \"seed\": 1655128259, \"all_seeds\": [1655128259], \"subseed\": 1013077508, \"all_subseeds\": [1013077508], \"subseed_strength\": 0, \"width\": 512, \"height\": 768, \"sampler_name\": \"Euler a\", \"cfg_scale\": 7.5, \"steps\": 60, \"batch_size\": 1, \"restore_faces\": false, \"face_restoration_model\": null, \"sd_model_hash\": \"b5fcd73ddc\", \"seed_resize_from_w\": 0, \"seed_resize_from_h\": 0, \"denoising_strength\": null, \"extra_generation_params\": {}, \"index_of_first_image\": 0, \"infotexts\": [\"sharp focus of a plant growing from the dirt, blutrry nature green background, highly defined, photorealism, reality picture,depth of field\\nSteps: 60, Sampler: Euler a, CFG scale: 7.5, Seed: 1655128259, Size: 512x768, Model hash: b5fcd73ddc, Model: dreambox-mix-A\"], \"styles\": [], \"job_timestamp\": \"20230207125234\", \"clip_skip\": 1, \"is_using_inpainting_conditioning\": false}\n```\n\n\n### Additional information\n\n_No response_\n", "hints_text": "I faced the same when there was an extra comma or CR/LF (ex. Pressing <kbd>Enter</kbd> after checkpoint/sampler name) between names or after the last name. Seems to be the case here.\nYes the issue is that you have a bunch of newlines after your checkpoint names and those are getting accidentally interpreted. Do not use newlines, just comma space and then the next entry.\nThis should not be close, it should be fixed. is not like is something out of the ordinary use ENTER instead of space.\r\nBut I guess if anyone else have this issue they will know how to work around", "created_at": "2023-02-20T15:23:54Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 7936, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-7936", "issue_numbers": ["7874"], "base_commit": "70615448b2ef3285dba9bb1992974cb1eaf10995", "patch": "diff --git a/html/extra-networks-card.html b/html/extra-networks-card.html\nindex 1bf3fc30d24..ef4b613afcf 100644\n--- a/html/extra-networks-card.html\n+++ b/html/extra-networks-card.html\n@@ -1,4 +1,4 @@\n-<div class='card' {preview_html} onclick={card_clicked}>\n+<div class='card' style={style} onclick={card_clicked}>\n \t{metadata_button}\n \n \t<div class='actions'>\ndiff --git a/modules/shared.py b/modules/shared.py\nindex e2ca12c10ea..73ce77d43d7 100644\n--- a/modules/shared.py\n+++ b/modules/shared.py\n@@ -451,6 +451,8 @@ def list_samplers():\n options_templates.update(options_section(('extra_networks', \"Extra Networks\"), {\r\n     \"extra_networks_default_view\": OptionInfo(\"cards\", \"Default view for Extra Networks\", gr.Dropdown, {\"choices\": [\"cards\", \"thumbs\"]}),\r\n     \"extra_networks_default_multiplier\": OptionInfo(1.0, \"Multiplier for extra networks\", gr.Slider, {\"minimum\": 0.0, \"maximum\": 1.0, \"step\": 0.01}),\r\n+    \"extra_networks_card_width\": OptionInfo(0, \"Card width for Extra Networks (px)\"),\r\n+    \"extra_networks_card_height\": OptionInfo(0, \"Card height for Extra Networks (px)\"),\r\n     \"extra_networks_add_text_separator\": OptionInfo(\" \", \"Extra text to add before <...> when adding extra network to prompt\"),\r\n     \"sd_hypernetwork\": OptionInfo(\"None\", \"Add hypernetwork to prompt\", gr.Dropdown, lambda: {\"choices\": [\"\"] + [x for x in hypernetworks.keys()]}, refresh=reload_hypernetworks),\r\n }))\r\ndiff --git a/modules/ui_extra_networks.py b/modules/ui_extra_networks.py\nindex 3cf8fcb6955..08a69930e42 100644\n--- a/modules/ui_extra_networks.py\n+++ b/modules/ui_extra_networks.py\n@@ -147,13 +147,16 @@ def create_html_for_item(self, item, tabname):\n         if onclick is None:\r\n             onclick = '\"' + html.escape(f\"\"\"return cardClicked({json.dumps(tabname)}, {item[\"prompt\"]}, {\"true\" if self.allow_negative_prompt else \"false\"})\"\"\") + '\"'\r\n \r\n+        height = f\"height: {shared.opts.extra_networks_card_height}px;\" if shared.opts.extra_networks_card_height else ''\r\n+        width = f\"width: {shared.opts.extra_networks_card_width}px;\" if shared.opts.extra_networks_card_width else ''\r\n+        background_image = f\"background-image: url(\\\"{html.escape(preview)}\\\");\" if preview else ''\r\n         metadata_button = \"\"\r\n         metadata = item.get(\"metadata\")\r\n         if metadata:\r\n             metadata_button = f\"<div class='metadata-button' title='Show metadata' onclick='extraNetworksRequestMetadata({json.dumps(self.name)}, {json.dumps(item['name'])})'></div>\"\r\n \r\n         args = {\r\n-            \"preview_html\": \"style='background-image: url(\\\"\" + html.escape(preview) + \"\\\")'\" if preview else '',\r\n+            \"style\": f\"'{height}{width}{background_image}'\",\r\n             \"prompt\": item.get(\"prompt\", None),\r\n             \"tabname\": json.dumps(tabname),\r\n             \"local_preview\": json.dumps(item[\"local_preview\"]),\r\n", "test_patch": "", "problem_statement": "[Feature Request]:  Extra Networks cards size change\n### Is there an existing issue for this?\n\n- [X] I have searched the existing issues and checked the recent builds/commits\n\n### What would your feature do ?\n\nExtra Networks cards size change\r\n\r\nI confirmed that there is a function to change to a thumbs rather than a card in the settings.\r\nHowever, there is a problem that the size is too small when changed to thumbs\r\nCan't add a function to change the card size in settings?\r\n\n\n### Proposed workflow\n\n1. Go to Settings tab -> Extra Networks -> Default view for Extra Networks\n\n### Additional information\n\n_No response_\n", "hints_text": "", "created_at": "2023-02-19T19:45:27Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 7931, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-7931", "issue_numbers": ["7422"], "base_commit": "955df7751eef11bb7697e2d77f6b8a6226b21e13", "patch": "diff --git a/javascript/ui.js b/javascript/ui.js\nindex 4a440193b81..a73eeaa29fb 100644\n--- a/javascript/ui.js\n+++ b/javascript/ui.js\n@@ -132,7 +132,14 @@ function create_tab_index_args(tabId, args){\n \n function get_img2img_tab_index() {\n     let res = args_to_array(arguments)\n-    res.splice(-2)\n+    res.splice(-2) // gradio also sends outputs to the arguments, pop them off\n+    res[0] = get_tab_index('mode_img2img')\n+    return res\n+}\n+\n+function get_img2img_tab_index_for_res_preview() {\n+    let res = args_to_array(arguments)\n+    res.splice(-1) // gradio also sends outputs to the arguments, pop them off\n     res[0] = get_tab_index('mode_img2img')\n     return res\n }\n@@ -361,3 +368,16 @@ function selectCheckpoint(name){\n     desiredCheckpointName = name;\n     gradioApp().getElementById('change_checkpoint').click()\n }\n+\n+\n+function onCalcResolutionImg2Img(mode, scale, width, height, resize_mode, init_img, sketch, init_img_with_mask, inpaint_color_sketch, init_img_inpaint){\n+    i2iScale = gradioApp().getElementById('img2img_scale')\n+    i2iWidth = gradioApp().getElementById('img2img_width')\n+    i2iHeight = gradioApp().getElementById('img2img_height')\n+\n+    setInactive(i2iScale, scale == 1)\n+    setInactive(i2iWidth, scale > 1)\n+    setInactive(i2iHeight, scale > 1)\n+\n+    return [];\n+}\ndiff --git a/modules/generation_parameters_copypaste.py b/modules/generation_parameters_copypaste.py\nindex 6df76858f51..0ad2ad4f035 100644\n--- a/modules/generation_parameters_copypaste.py\n+++ b/modules/generation_parameters_copypaste.py\n@@ -282,6 +282,9 @@ def parse_generation_parameters(x: str):\n         res[\"Hires resize-1\"] = 0\r\n         res[\"Hires resize-2\"] = 0\r\n \r\n+    if \"Img2Img upscale\" not in res:\r\n+        res[\"Img2Img upscale\"] = 1\r\n+\r\n     restore_old_hires_fix_params(res)\r\n \r\n     return res\r\ndiff --git a/modules/img2img.py b/modules/img2img.py\nindex c973b7708dd..959dd96eade 100644\n--- a/modules/img2img.py\n+++ b/modules/img2img.py\n@@ -78,7 +78,7 @@ def process_batch(p, input_dir, output_dir, inpaint_mask_dir, args):\n                 processed_image.save(os.path.join(output_dir, filename))\r\n \r\n \r\n-def img2img(id_task: str, mode: int, prompt: str, negative_prompt: str, prompt_styles, init_img, sketch, init_img_with_mask, inpaint_color_sketch, inpaint_color_sketch_orig, init_img_inpaint, init_mask_inpaint, steps: int, sampler_index: int, mask_blur: int, mask_alpha: float, inpainting_fill: int, restore_faces: bool, tiling: bool, n_iter: int, batch_size: int, cfg_scale: float, image_cfg_scale: float, denoising_strength: float, seed: int, subseed: int, subseed_strength: float, seed_resize_from_h: int, seed_resize_from_w: int, seed_enable_extras: bool, height: int, width: int, resize_mode: int, inpaint_full_res: bool, inpaint_full_res_padding: int, inpainting_mask_invert: int, img2img_batch_input_dir: str, img2img_batch_output_dir: str, img2img_batch_inpaint_mask_dir: str, override_settings_texts, *args):\r\n+def img2img(id_task: str, mode: int, prompt: str, negative_prompt: str, prompt_styles, init_img, sketch, init_img_with_mask, inpaint_color_sketch, inpaint_color_sketch_orig, init_img_inpaint, init_mask_inpaint, steps: int, sampler_index: int, mask_blur: int, mask_alpha: float, inpainting_fill: int, restore_faces: bool, tiling: bool, n_iter: int, batch_size: int, cfg_scale: float, image_cfg_scale: float, denoising_strength: float, seed: int, subseed: int, subseed_strength: float, seed_resize_from_h: int, seed_resize_from_w: int, seed_enable_extras: bool, height: int, width: int, scale: float, upscaler: str, resize_mode: int, inpaint_full_res: bool, inpaint_full_res_padding: int, inpainting_mask_invert: int, img2img_batch_input_dir: str, img2img_batch_output_dir: str, img2img_batch_inpaint_mask_dir: str, override_settings_texts, *args):\r\n     override_settings = create_override_settings_dict(override_settings_texts)\r\n \r\n     is_batch = mode == 5\r\n@@ -149,6 +149,8 @@ def img2img(id_task: str, mode: int, prompt: str, negative_prompt: str, prompt_s\n         inpaint_full_res_padding=inpaint_full_res_padding,\r\n         inpainting_mask_invert=inpainting_mask_invert,\r\n         override_settings=override_settings,\r\n+        scale=scale,\r\n+        upscaler=upscaler,\r\n     )\r\n \r\n     p.scripts = modules.scripts.scripts_txt2img\r\ndiff --git a/modules/processing.py b/modules/processing.py\nindex 2e5a363f07a..afb8cfd1c9c 100644\n--- a/modules/processing.py\n+++ b/modules/processing.py\n@@ -929,7 +929,7 @@ def save_intermediate(image, index):\n class StableDiffusionProcessingImg2Img(StableDiffusionProcessing):\r\n     sampler = None\r\n \r\n-    def __init__(self, init_images: list = None, resize_mode: int = 0, denoising_strength: float = 0.75, image_cfg_scale: float = None, mask: Any = None, mask_blur: int = 4, inpainting_fill: int = 0, inpaint_full_res: bool = True, inpaint_full_res_padding: int = 0, inpainting_mask_invert: int = 0, initial_noise_multiplier: float = None, **kwargs):\r\n+    def __init__(self, init_images: Optional[list] = None, resize_mode: int = 0, denoising_strength: float = 0.75, image_cfg_scale: Optional[float] = None, mask: Any = None, mask_blur: int = 4, inpainting_fill: int = 0, inpaint_full_res: bool = True, inpaint_full_res_padding: int = 0, inpainting_mask_invert: int = 0, initial_noise_multiplier: Optional[float] = None, scale: float = 0, upscaler: Optional[str] = None, **kwargs):\r\n         super().__init__(**kwargs)\r\n \r\n         self.init_images = init_images\r\n@@ -949,11 +949,37 @@ def __init__(self, init_images: list = None, resize_mode: int = 0, denoising_str\n         self.mask = None\r\n         self.nmask = None\r\n         self.image_conditioning = None\r\n+        self.scale = scale\r\n+        self.upscaler = upscaler\r\n+\r\n+    def get_final_size(self):\r\n+        if self.scale > 1:\r\n+            img = self.init_images[0]\r\n+            width = int(img.width * self.scale)\r\n+            height = int(img.height * self.scale)\r\n+            return width, height\r\n+        else:\r\n+            return self.width, self.height\r\n+\r\n \r\n     def init(self, all_prompts, all_seeds, all_subseeds):\r\n         self.sampler = sd_samplers.create_sampler(self.sampler_name, self.sd_model)\r\n         crop_region = None\r\n \r\n+        if self.scale > 1:\r\n+            self.extra_generation_params[\"Img2Img upscale\"] = self.scale\r\n+\r\n+        # Non-latent upscalers are run before sampling\r\n+        # Latent upscalers are run during sampling\r\n+        init_upscaler = None\r\n+        if self.upscaler is not None:\r\n+            self.extra_generation_params[\"Img2Img upscaler\"] = self.upscaler\r\n+            if self.upscaler not in shared.latent_upscale_modes:\r\n+                assert len([x for x in shared.sd_upscalers if x.name == self.upscaler]) > 0, f\"could not find upscaler named {self.upscaler}\"\r\n+                init_upscaler = self.upscaler\r\n+\r\n+        self.width, self.height = self.get_final_size()\r\n+\r\n         image_mask = self.image_mask\r\n \r\n         if image_mask is not None:\r\n@@ -976,7 +1002,7 @@ def init(self, all_prompts, all_seeds, all_subseeds):\n                 image_mask = images.resize_image(2, mask, self.width, self.height)\r\n                 self.paste_to = (x1, y1, x2-x1, y2-y1)\r\n             else:\r\n-                image_mask = images.resize_image(self.resize_mode, image_mask, self.width, self.height)\r\n+                image_mask = images.resize_image(self.resize_mode, image_mask, self.width, self.height, init_upscaler)\r\n                 np_mask = np.array(image_mask)\r\n                 np_mask = np.clip((np_mask.astype(np.float32)) * 2, 0, 255).astype(np.uint8)\r\n                 self.mask_for_overlay = Image.fromarray(np_mask)\r\n@@ -993,7 +1019,7 @@ def init(self, all_prompts, all_seeds, all_subseeds):\n             image = images.flatten(img, opts.img2img_background_color)\r\n \r\n             if crop_region is None and self.resize_mode != 3:\r\n-                image = images.resize_image(self.resize_mode, image, self.width, self.height)\r\n+                image = images.resize_image(self.resize_mode, image, self.width, self.height, init_upscaler)\r\n \r\n             if image_mask is not None:\r\n                 image_masked = Image.new('RGBa', (image.width, image.height))\r\n@@ -1038,8 +1064,9 @@ def init(self, all_prompts, all_seeds, all_subseeds):\n \r\n         self.init_latent = self.sd_model.get_first_stage_encoding(self.sd_model.encode_first_stage(image))\r\n \r\n-        if self.resize_mode == 3:\r\n-            self.init_latent = torch.nn.functional.interpolate(self.init_latent, size=(self.height // opt_f, self.width // opt_f), mode=\"bilinear\")\r\n+        latent_scale_mode = shared.latent_upscale_modes.get(self.upscaler, None) if self.upscaler is not None else shared.latent_upscale_modes.get(shared.latent_upscale_default_mode, \"nearest\")\r\n+        if latent_scale_mode is not None:\r\n+            self.init_latent = torch.nn.functional.interpolate(self.init_latent, size=(self.height // opt_f, self.width // opt_f), mode=latent_scale_mode[\"mode\"], antialias=latent_scale_mode[\"antialias\"])\r\n \r\n         if image_mask is not None:\r\n             init_mask = latent_mask\r\ndiff --git a/modules/ui.py b/modules/ui.py\nindex af8546c2949..40dd76f25e0 100644\n--- a/modules/ui.py\n+++ b/modules/ui.py\n@@ -15,6 +15,7 @@\n import gradio as gr\r\n import gradio.routes\r\n import gradio.utils\r\n+from gradio.events import Releaseable\r\n import numpy as np\r\n from PIL import Image, PngImagePlugin\r\n from modules.call_queue import wrap_gradio_gpu_call, wrap_queued_call, wrap_gradio_call\r\n@@ -138,6 +139,26 @@ def calc_resolution_hires(enable, width, height, hr_scale, hr_resize_x, hr_resiz\n     return f\"resize: from <span class='resolution'>{p.width}x{p.height}</span> to <span class='resolution'>{p.hr_resize_x or p.hr_upscale_to_x}x{p.hr_resize_y or p.hr_upscale_to_y}</span>\"\r\n \r\n \r\n+def calc_resolution_img2img(mode, scale, resize_x, resize_y, resize_mode, *i2i_images):\r\n+    init_img = None\r\n+    if mode in {0, 1, 3, 4}:\r\n+        init_img = i2i_images[mode]\r\n+    elif mode == 2:\r\n+        init_img = i2i_images[mode][\"image\"]\r\n+\r\n+    if not init_img:\r\n+        return \"\"\r\n+\r\n+    if scale > 1:\r\n+        width = int(init_img.width * scale)\r\n+        height = int(init_img.height * scale)\r\n+    else:\r\n+        width = resize_x\r\n+        height = resize_y\r\n+\r\n+    return f\"resize: from <span class='resolution'>{init_img.width}x{init_img.height}</span> to <span class='resolution'>{width}x{height}</span>\"\r\n+\r\n+\r\n def apply_styles(prompt, prompt_neg, styles):\r\n     prompt = shared.prompt_styles.apply_styles_to_prompt(prompt, styles)\r\n     prompt_neg = shared.prompt_styles.apply_negative_styles_to_prompt(prompt_neg, styles)\r\n@@ -746,7 +767,7 @@ def copy_image(img):\n                     )\r\n \r\n                 with FormRow():\r\n-                    resize_mode = gr.Radio(label=\"Resize mode\", elem_id=\"resize_mode\", choices=[\"Just resize\", \"Crop and resize\", \"Resize and fill\", \"Just resize (latent upscale)\"], type=\"index\", value=\"Just resize\")\r\n+                    resize_mode = gr.Radio(label=\"Resize mode\", elem_id=\"resize_mode\", choices=[\"Just resize\", \"Crop and resize\", \"Resize and fill\"], type=\"index\", value=\"Just resize\")\r\n \r\n                 for category in ordered_ui_categories():\r\n                     if category == \"sampler\":\r\n@@ -755,8 +776,13 @@ def copy_image(img):\n                     elif category == \"dimensions\":\r\n                         with FormRow():\r\n                             with gr.Column(elem_id=\"img2img_column_size\", scale=4):\r\n-                                width = gr.Slider(minimum=64, maximum=2048, step=8, label=\"Width\", value=512, elem_id=\"img2img_width\")\r\n-                                height = gr.Slider(minimum=64, maximum=2048, step=8, label=\"Height\", value=512, elem_id=\"img2img_height\")\r\n+                                with FormRow(variant=\"compact\"):\r\n+                                    final_resolution = FormHTML(value=\"\", elem_id=\"img2img_finalres\", label=\"Upscaled resolution\", interactive=False)\r\n+                                with FormRow(variant=\"compact\"):\r\n+                                    scale = gr.Slider(minimum=1.0, maximum=4.0, step=0.05, label=\"Upscale by\", value=1.0, elem_id=\"img2img_scale\")\r\n+                                with FormRow(variant=\"compact\"):\r\n+                                    width = gr.Slider(minimum=64, maximum=2048, step=8, label=\"Width\", value=512, elem_id=\"img2img_width\")\r\n+                                    height = gr.Slider(minimum=64, maximum=2048, step=8, label=\"Height\", value=512, elem_id=\"img2img_height\")\r\n \r\n                             with gr.Column(elem_id=\"img2img_dimensions_row\", scale=1, elem_classes=\"dimensions-tools\"):\r\n                                 res_switch_btn = ToolButton(value=switch_values_symbol, elem_id=\"img2img_res_switch_btn\")\r\n@@ -771,7 +797,9 @@ def copy_image(img):\n                             with FormRow():\r\n                                 cfg_scale = gr.Slider(minimum=1.0, maximum=30.0, step=0.5, label='CFG Scale', value=7.0, elem_id=\"img2img_cfg_scale\")\r\n                                 image_cfg_scale = gr.Slider(minimum=0, maximum=3.0, step=0.05, label='Image CFG Scale', value=1.5, elem_id=\"img2img_image_cfg_scale\", visible=shared.sd_model and shared.sd_model.cond_stage_key == \"edit\")\r\n-                            denoising_strength = gr.Slider(minimum=0.0, maximum=1.0, step=0.01, label='Denoising strength', value=0.75, elem_id=\"img2img_denoising_strength\")\r\n+                            with FormRow():\r\n+                                upscaler = gr.Dropdown(label=\"Upscaler\", elem_id=\"img2img_upscaler\", choices=[*shared.latent_upscale_modes, *[x.name for x in shared.sd_upscalers]], value=shared.latent_upscale_default_mode)\r\n+                                denoising_strength = gr.Slider(minimum=0.0, maximum=1.0, step=0.01, label='Denoising strength', value=0.75, elem_id=\"img2img_denoising_strength\")\r\n \r\n                     elif category == \"seed\":\r\n                         seed, reuse_seed, subseed, reuse_subseed, subseed_strength, seed_resize_from_h, seed_resize_from_w, seed_checkbox = create_seed_inputs('img2img')\r\n@@ -824,6 +852,39 @@ def select_img2img_tab(tab):\n                                     outputs=[inpaint_controls, mask_alpha],\r\n                                 )\r\n \r\n+            img2img_resolution_preview_inputs = [dummy_component, # filled in by selected img2img tab index in _js\r\n+                                                 scale, width, height, resize_mode,\r\n+                                                 init_img, sketch, init_img_with_mask, inpaint_color_sketch, init_img_inpaint]\r\n+            for input in img2img_resolution_preview_inputs[1:]:\r\n+                if isinstance(input, Releaseable):\r\n+                    input.release(\r\n+                        fn=calc_resolution_img2img,\r\n+                        _js=\"get_img2img_tab_index_for_res_preview\",\r\n+                        inputs=img2img_resolution_preview_inputs,\r\n+                        outputs=[final_resolution],\r\n+                        show_progress=False,\r\n+                    ).success(\r\n+                        None,\r\n+                        _js=\"onCalcResolutionImg2Img\",\r\n+                        inputs=img2img_resolution_preview_inputs,\r\n+                        outputs=[],\r\n+                        show_progress=False,\r\n+                    )\r\n+                else:\r\n+                    input.change(\r\n+                        fn=calc_resolution_img2img,\r\n+                        _js=\"get_img2img_tab_index_for_res_preview\",\r\n+                        inputs=img2img_resolution_preview_inputs,\r\n+                        outputs=[final_resolution],\r\n+                        show_progress=False,\r\n+                    ).success(\r\n+                        None,\r\n+                        _js=\"onCalcResolutionImg2Img\",\r\n+                        inputs=img2img_resolution_preview_inputs,\r\n+                        outputs=[],\r\n+                        show_progress=False,\r\n+                    )\r\n+\r\n             img2img_gallery, generation_info, html_info, html_log = create_output_panel(\"img2img\", opts.outdir_img2img_samples)\r\n \r\n             connect_reuse_seed(seed, reuse_seed, generation_info, dummy_component, is_subseed=False)\r\n@@ -872,6 +933,8 @@ def select_img2img_tab(tab):\n                     subseed, subseed_strength, seed_resize_from_h, seed_resize_from_w, seed_checkbox,\r\n                     height,\r\n                     width,\r\n+                    scale,\r\n+                    upscaler,\r\n                     resize_mode,\r\n                     inpaint_full_res,\r\n                     inpaint_full_res_padding,\r\n@@ -957,6 +1020,8 @@ def select_img2img_tab(tab):\n                 (seed, \"Seed\"),\r\n                 (width, \"Size-1\"),\r\n                 (height, \"Size-2\"),\r\n+                (scale, \"Img2Img upscale\"),\r\n+                (upscaler, \"Img2Img upscaler\"),\r\n                 (batch_size, \"Batch size\"),\r\n                 (subseed, \"Variation seed\"),\r\n                 (subseed_strength, \"Variation seed strength\"),\r\ndiff --git a/scripts/xyz_grid.py b/scripts/xyz_grid.py\nindex 3895a795c30..3f6c1997d94 100644\n--- a/scripts/xyz_grid.py\n+++ b/scripts/xyz_grid.py\n@@ -220,6 +220,7 @@ def __init__(self, *args, **kwargs):\n     AxisOption(\"Clip skip\", int, apply_clip_skip),\r\n     AxisOption(\"Denoising\", float, apply_field(\"denoising_strength\")),\r\n     AxisOptionTxt2Img(\"Hires upscaler\", str, apply_field(\"hr_upscaler\"), choices=lambda: [*shared.latent_upscale_modes, *[x.name for x in shared.sd_upscalers]]),\r\n+    AxisOptionImg2Img(\"Upscaler\", str, apply_field(\"upscaler\"), choices=lambda: [*shared.latent_upscale_modes, *[x.name for x in shared.sd_upscalers]]),\r\n     AxisOptionImg2Img(\"Cond. Image Mask Weight\", float, apply_field(\"inpainting_mask_weight\")),\r\n     AxisOption(\"VAE\", str, apply_vae, cost=0.7, choices=lambda: list(sd_vae.vae_dict)),\r\n     AxisOption(\"Styles\", str, apply_styles, choices=lambda: list(shared.prompt_styles.styles)),\r\ndiff --git a/style.css b/style.css\nindex 5e8fb5330bf..75c26b4078e 100644\n--- a/style.css\n+++ b/style.css\n@@ -287,13 +287,13 @@ button.custom-button{\n     border-radius: 0 0.5rem 0.5rem 0;\r\n }\r\n \r\n-#txtimg_hr_finalres{\r\n+#txtimg_hr_finalres, #img2img_finalres {\r\n     min-height: 0 !important;\r\n     padding: .625rem .75rem;\r\n     margin-left: -0.75em\r\n }\r\n \r\n-#txtimg_hr_finalres .resolution{\r\n+#txtimg_hr_finalres .resolution, #img2img_finalres .resolution{\r\n     font-weight: bold;\r\n }\r\n \r\n", "test_patch": "", "problem_statement": "[Feature Request]: img2img UI consistency with new Highres. fix\n### Is there an existing issue for this?\r\n\r\n- [X] I have searched the existing issues and checked the recent builds/commits\r\n\r\n### What would your feature do ?\r\n\r\nThe new highres fix upscales an image based on a slider, and has an option to select an upscaler. From my understanding highres fix is basically supposed to be img2img but applied automatically on a txt2img output. However the img2img tab only allows you to scale by latent and use width/height sliders instead of scale factor. It would be nice for consistency's sake to mirror the new highres fix in img2img\r\n\r\nOf course some people don't like the new highres fix, I personally find it more convenient when I know I just want to scale up by 2x to not have to fiddle around with the sliders; it would probably be best to have a checkbox that shows/hides the scale slider and replaces them with the width/height sliders\r\n\r\nAnd the other thing is when I checked I didn't see an upscaler option in img2img like with highres fix, only \"Just resize (latent upscale)\", I assume this is equivalent to setting the upscaler to \"Latent\" in highres fix. It would be better to have the same upscaler dropdown in img2img too\r\n\r\nWhy is this important? I find myself using highres fix after img2img to regenerate the same seed with higher resolution, because of the aforementioned inconsistencies in img2img. This means the pre-highres-fix image has to be regenerated again which is wasted time. There is also a chance that while generating in txt2img you change the prompt/settings preemptively by force of habit and find a good txt2img output, then you want to go back to that output and highres fix it, but you would get different results because you accidentally changed the prompt. \"Send to img2img\" alleviates this because it sends the embedded infotext, ensuring the same exact prompt used for that image is intact\r\n\r\nBasically it would be nice to be able to run \"highres fix step 2\", a.k.a. img2img, without needing to generate the first image again and worry that you changed something in the prompt accidentally\r\n\r\n### Proposed workflow\r\n\r\n1. Generate txt2img at 512x512px. Pick out one or more to upscale\r\n2. Click \"Send to img2img\". The image is sent to img2img\r\n3. Put a slider in img2img to upscale 2x, and set the upscaler to \"remacri\" (or whatever)\r\n  a. Alternatively check a \"Width/Height\" checkbox and set width/height just like before in img2img\r\n4. Click generate, this is functionally equivalent to running the second half of highres. fix\r\n\r\n### Additional information\r\n\r\n_No response_\n", "hints_text": "", "created_at": "2023-02-19T11:51:05Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 7818, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-7818", "issue_numbers": ["7648"], "base_commit": "3715ece0adce7bf7c5e9c5ab3710b2fdc3848f39", "patch": "diff --git a/modules/generation_parameters_copypaste.py b/modules/generation_parameters_copypaste.py\nindex fc9e17aa29f..93d955dbee3 100644\n--- a/modules/generation_parameters_copypaste.py\n+++ b/modules/generation_parameters_copypaste.py\n@@ -23,13 +23,14 @@\n \r\n \r\n class ParamBinding:\r\n-    def __init__(self, paste_button, tabname, source_text_component=None, source_image_component=None, source_tabname=None, override_settings_component=None):\r\n+    def __init__(self, paste_button, tabname, source_text_component=None, source_image_component=None, source_tabname=None, override_settings_component=None, paste_field_names=[]):\r\n         self.paste_button = paste_button\r\n         self.tabname = tabname\r\n         self.source_text_component = source_text_component\r\n         self.source_image_component = source_image_component\r\n         self.source_tabname = source_tabname\r\n         self.override_settings_component = override_settings_component\r\n+        self.paste_field_names = paste_field_names\r\n \r\n \r\n def reset():\r\n@@ -133,7 +134,7 @@ def connect_paste_params_buttons():\n             connect_paste(binding.paste_button, fields, binding.source_text_component, binding.override_settings_component, binding.tabname)\r\n \r\n         if binding.source_tabname is not None and fields is not None:\r\n-            paste_field_names = ['Prompt', 'Negative prompt', 'Steps', 'Face restoration'] + ([\"Seed\"] if shared.opts.send_seed else [])\r\n+            paste_field_names = ['Prompt', 'Negative prompt', 'Steps', 'Face restoration'] + ([\"Seed\"] if shared.opts.send_seed else []) + binding.paste_field_names\r\n             binding.paste_button.click(\r\n                 fn=lambda *x: x,\r\n                 inputs=[field for field, name in paste_fields[binding.source_tabname][\"fields\"] if name in paste_field_names],\r\ndiff --git a/modules/scripts.py b/modules/scripts.py\nindex 24056a12f90..ac0785ce1f7 100644\n--- a/modules/scripts.py\n+++ b/modules/scripts.py\n@@ -33,6 +33,11 @@ class Script:\n     parsing infotext to set the value for the component; see ui.py's txt2img_paste_fields for an example\r\n     \"\"\"\r\n \r\n+    paste_field_names = None\r\n+    \"\"\"if set in ui(), this is a list of names of infotext fields; the fields will be sent through the\r\n+    various \"Send to <X>\" buttons when clicked\r\n+    \"\"\"\r\n+\r\n     def title(self):\r\n         \"\"\"this function should return the title of the script. This is what will be displayed in the dropdown menu.\"\"\"\r\n \r\n@@ -256,6 +261,7 @@ def __init__(self):\n         self.alwayson_scripts = []\r\n         self.titles = []\r\n         self.infotext_fields = []\r\n+        self.paste_field_names = []\r\n \r\n     def initialize_scripts(self, is_img2img):\r\n         from modules import scripts_auto_postprocessing\r\n@@ -304,6 +310,9 @@ def create_script_ui(script, inputs, inputs_alwayson):\n             if script.infotext_fields is not None:\r\n                 self.infotext_fields += script.infotext_fields\r\n \r\n+            if script.paste_field_names is not None:\r\n+                self.paste_field_names += script.paste_field_names\r\n+\r\n             inputs += controls\r\n             inputs_alwayson += [script.alwayson for _ in controls]\r\n             script.args_to = len(inputs)\r\ndiff --git a/modules/ui_common.py b/modules/ui_common.py\nindex fd047f31887..a12433d237d 100644\n--- a/modules/ui_common.py\n+++ b/modules/ui_common.py\n@@ -198,9 +198,16 @@ def open_folder(f):\n                 html_info = gr.HTML(elem_id=f'html_info_{tabname}')\r\n                 html_log = gr.HTML(elem_id=f'html_log_{tabname}')\r\n \r\n+            paste_field_names = []\r\n+            if tabname == \"txt2img\":\r\n+                paste_field_names = modules.scripts.scripts_txt2img.paste_field_names\r\n+            elif tabname == \"img2img\":\r\n+                paste_field_names = modules.scripts.scripts_img2img.paste_field_names\r\n+\r\n             for paste_tabname, paste_button in buttons.items():\r\n                 parameters_copypaste.register_paste_params_button(parameters_copypaste.ParamBinding(\r\n-                    paste_button=paste_button, tabname=paste_tabname, source_tabname=\"txt2img\" if tabname == \"txt2img\" else None, source_image_component=result_gallery\r\n+                    paste_button=paste_button, tabname=paste_tabname, source_tabname=\"txt2img\" if tabname == \"txt2img\" else None, source_image_component=result_gallery,\r\n+                    paste_field_names=paste_field_names\r\n                 ))\r\n \r\n             return result_gallery, generation_info if tabname != \"extras\" else html_info_x, html_info, html_log\r\n", "test_patch": "", "problem_statement": "[Feature Request]: Allow extensions to send paste params to other tabs\n### Is there an existing issue for this?\n\n- [X] I have searched the existing issues and checked the recent builds/commits\n\n### What would your feature do ?\n\nCurrently it is not possible for infotext fields added by extensions to hook into the `Send to <X>` feature. They can be parsed from the infotext, but the list of parameters that can be sent (`paste_field_names`) is hardcoded.\r\n\r\nKeeping in mind the various \"Allow sending X to img2img\" options that have been added thus far, it would be nice if there were some kind of uniformity for choosing what to send. There could later be an option that allows disabling one of these extension-added send to parameters.\n\n### Proposed workflow\n\n1. Generate an image with an always-on script extension parameter\r\n2. Click \"Send to img2img\"\r\n3. The extension parameters are copied to the other UI\n\n### Additional information\n\n_No response_\n", "hints_text": "Yo I'm curious if this is for an extension you are doing.  \r\nI faced something similar before.  \r\n\r\nSince my script is (was), being ran twice, once for txt2img and once for img2img, there are two instances in memory, but technically one class.  \r\n\r\nSet the data you want passed back and forth as class members, when your components change, have them push and pull from the class variables. This way, when they change tab, it's already populated.  \r\n\r\nYou may have to store a reference of self in a list as a class variable. This way one instance can grab the other instance from the list and update those components as the local ones are changed.  \r\n\r\nAnother way is to create some controllers is js, add event listeners that will set the other components.  \r\nYou can test it using some of these controllers. https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/7601/files#diff-f273af4b9b957263caf8afbdf69c1c5bfe7c53a62c1de4a5adc267d77cc8ebc5  \r\nJust copy and paste what you need. I'm trying to get my basic library included, while also implementing a feature, my feature uses about half of them, so it might be allowed.  \r\n\r\nAnyways, for the js controllers, since they are classes with a constructor, you call them using the 'new' keyword.  \r\nThese classes are built to take in an html element.  \r\nThe html element that you would use are the same elements that the element id gets created on when creating a gradio component.  \r\ni.e.- `gr.Text(elem_id=\"my_id\")`  \r\n\r\nThen in javascript you'll call it like this:\r\n```javascript\r\nlet myComponentController = new TextComponentController(gradioApp().getElementById(\"my_id\"));\r\n```  \r\n\r\nJust think of it like a python class, with a getter and setter that triggers the appropriate event handler that updates the front and backend. Like this.  \r\n```javascript\r\nmyComponentController.setVal(\"My cool Prompt\")\r\n```  \r\n\r\nSo then on your `myComponentController`, you can add an event listener that will set the val of the other text area.  \r\n```javascript\r\nmyComponentController.child.addEventListener(\"change\", (e) => {e.preventDefault(); myOtherComponent.setVal(myComponentController.getVal())})\r\n```\r\n\r\nThis way, when one changes, the other changes automatically, now they're in sync, front and backend.  \r\n\r\nOh, but there might be a circular call, so you might want to create a class and hold some value that can reference which one started the calls, so when it comes back around you can stop it. You can call this class, your AppController, or whatever makes sense.  \r\n\r\nYou can also say screw (I'm a carpenter so I can say screw it) making my own prompt box, I'm just going to hook into the regular one.  \r\n```javascript\r\nlet txt2imgPrompt = new TextComponentController(gradioApp().getElementById(\"txt2img_prompt\"));\r\ntxt2imgPrompt.setVal(\"FTW\");\r\n```", "created_at": "2023-02-14T11:58:06Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 7798, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-7798", "issue_numbers": ["7693"], "base_commit": "3715ece0adce7bf7c5e9c5ab3710b2fdc3848f39", "patch": "diff --git a/modules/extensions.py b/modules/extensions.py\nindex 5e12b1aaa28..1975fca1c51 100644\n--- a/modules/extensions.py\n+++ b/modules/extensions.py\n@@ -2,6 +2,7 @@\n import sys\r\n import traceback\r\n \r\n+import time\r\n import git\r\n \r\n from modules import paths, shared\r\n@@ -25,6 +26,7 @@ def __init__(self, name, path, enabled=True, is_builtin=False):\n         self.status = ''\r\n         self.can_update = False\r\n         self.is_builtin = is_builtin\r\n+        self.version = ''\r\n \r\n         repo = None\r\n         try:\r\n@@ -40,6 +42,10 @@ def __init__(self, name, path, enabled=True, is_builtin=False):\n             try:\r\n                 self.remote = next(repo.remote().urls, None)\r\n                 self.status = 'unknown'\r\n+                head = repo.head.commit\r\n+                ts = time.asctime(time.gmtime(repo.head.commit.committed_date))\r\n+                self.version = f'{head.hexsha[:7]} ({ts})'\r\n+\r\n             except Exception:\r\n                 self.remote = None\r\n \r\ndiff --git a/modules/ui_extensions.py b/modules/ui_extensions.py\nindex 37d30e1f260..bd4308ef028 100644\n--- a/modules/ui_extensions.py\n+++ b/modules/ui_extensions.py\n@@ -80,6 +80,7 @@ def extension_table():\n             <tr>\r\n                 <th><abbr title=\"Use checkbox to enable the extension; it will be enabled or disabled when you click apply button\">Extension</abbr></th>\r\n                 <th>URL</th>\r\n+                <th><abbr title=\"Extension version\">Version</abbr></th>\r\n                 <th><abbr title=\"Use checkbox to mark the extension for update; it will be updated when you click apply button\">Update</abbr></th>\r\n             </tr>\r\n         </thead>\r\n@@ -87,11 +88,7 @@ def extension_table():\n     \"\"\"\r\n \r\n     for ext in extensions.extensions:\r\n-        remote = \"\"\r\n-        if ext.is_builtin:\r\n-            remote = \"built-in\"\r\n-        elif ext.remote:\r\n-            remote = f\"\"\"<a href=\"{html.escape(ext.remote or '')}\" target=\"_blank\">{html.escape(\"built-in\" if ext.is_builtin else ext.remote or '')}</a>\"\"\"\r\n+        remote = f\"\"\"<a href=\"{html.escape(ext.remote or '')}\" target=\"_blank\">{html.escape(\"built-in\" if ext.is_builtin else ext.remote or '')}</a>\"\"\"\r\n \r\n         if ext.can_update:\r\n             ext_status = f\"\"\"<label><input class=\"gr-check-radio gr-checkbox\" name=\"update_{html.escape(ext.name)}\" checked=\"checked\" type=\"checkbox\">{html.escape(ext.status)}</label>\"\"\"\r\n@@ -102,6 +99,7 @@ def extension_table():\n             <tr>\r\n                 <td><label><input class=\"gr-check-radio gr-checkbox\" name=\"enable_{html.escape(ext.name)}\" type=\"checkbox\" {'checked=\"checked\"' if ext.enabled else ''}>{html.escape(ext.name)}</label></td>\r\n                 <td>{remote}</td>\r\n+                <td>{ext.version}</td>\r\n                 <td{' class=\"extension_status\"' if ext.remote is not None else ''}>{ext_status}</td>\r\n             </tr>\r\n     \"\"\"\r\n", "test_patch": "", "problem_statement": "[Feature Request]: Extensions - Show current and new versions\n### Is there an existing issue for this?\n\n- [X] I have searched the existing issues and checked the recent builds/commits\n\n### What would your feature do ?\n\nList current version and new version when checking for update.\n\n### Proposed workflow\n\nsee description\n\n### Additional information\n\n_No response_\n", "hints_text": "just to clarify, you want to see extensions current commit hash and latest hash in the extensions ui tab?\n> just to clarify, you want to see extensions current commit hash and latest hash in the extensions ui tab?\n\nYes please. One of the reasons is that this makes it a lot easier to file bug reports on the extension's repo, as I can point to a specific version having the problem.\n\nExtra credit would be to allow me to \"update\" to any arbitrary revision in order to figure out where a bug was introduced. But I think that's out of scope for this issue.", "created_at": "2023-02-13T16:07:08Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 7789, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-7789", "issue_numbers": ["7586"], "base_commit": "3715ece0adce7bf7c5e9c5ab3710b2fdc3848f39", "patch": "diff --git a/modules/processing.py b/modules/processing.py\nindex e1b53ac0a48..e4b989d4d99 100644\n--- a/modules/processing.py\n+++ b/modules/processing.py\n@@ -543,8 +543,6 @@ def infotext(iteration=0, position_in_batch=0):\n     if os.path.exists(cmd_opts.embeddings_dir) and not p.do_not_reload_embeddings:\r\n         model_hijack.embedding_db.load_textual_inversion_embeddings()\r\n \r\n-    _, extra_network_data = extra_networks.parse_prompts(p.all_prompts[0:1])\r\n-\r\n     if p.scripts is not None:\r\n         p.scripts.process(p)\r\n \r\n@@ -582,9 +580,6 @@ def get_conds_with_caching(function, required_prompts, steps, cache):\n             if shared.opts.live_previews_enable and opts.show_progress_type == \"Approx NN\":\r\n                 sd_vae_approx.model()\r\n \r\n-            if not p.disable_extra_networks:\r\n-                extra_networks.activate(p, extra_network_data)\r\n-\r\n         with open(os.path.join(paths.data_path, \"params.txt\"), \"w\", encoding=\"utf8\") as file:\r\n             processed = Processed(p, [], p.seed, \"\")\r\n             file.write(processed.infotext(p, 0))\r\n@@ -609,7 +604,11 @@ def get_conds_with_caching(function, required_prompts, steps, cache):\n             if len(prompts) == 0:\r\n                 break\r\n \r\n-            prompts, _ = extra_networks.parse_prompts(prompts)\r\n+            prompts, extra_network_data = extra_networks.parse_prompts(prompts)\r\n+\r\n+            if not p.disable_extra_networks:\r\n+                with devices.autocast():\r\n+                    extra_networks.activate(p, extra_network_data)\r\n \r\n             if p.scripts is not None:\r\n                 p.scripts.process_batch(p, batch_number=n, prompts=prompts, seeds=seeds, subseeds=subseeds)\r\n", "test_patch": "", "problem_statement": "[Feature Request]: Wildcard support for Loras\n### Is there an existing issue for this?\r\n\r\n- [X] I have searched the existing issues and checked the recent builds/commits\r\n\r\n### What would your feature do ?\r\n\r\n(Not sure if this would fit as a bug report instead,  to be honest)\r\nWould be useful if i want to batch generate various images, each one using a different Lora + their corresponding activation keywords , when specified, if any.\r\nThis would be easily done by making it so Wildcards (tried different wildcard extensions,  none worked) are able to use Loras.\r\n\r\nNormally when entering a Lora via a wildcard it won't be recognized and won't used when generating an image, this works for embeddings so it makes sense it if worked for loras too\r\n\r\n\r\n\r\n\r\n### Proposed workflow\r\n\r\n1.- User makes a wildcard file, each line may consist of a different lora.\r\nLike in this example:\r\n<lora:loraname1:1> , activation keyword1, keyword2, etc.\r\n<lora:loraname2:1> , activation keyword1, keyword2, etc.\r\nAnd so on.\r\n2.- Use the wildcard in the prompt and if there's a lora in the wildcard then it will be used (instead of it just placing \"<lora:loraname1:1>\" as plain text in the prompt)\r\n\r\n### Additional information\r\n\r\n_No response_\n", "hints_text": "", "created_at": "2023-02-13T11:36:33Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 7691, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-7691", "issue_numbers": ["6065", "7616"], "base_commit": "3715ece0adce7bf7c5e9c5ab3710b2fdc3848f39", "patch": "diff --git a/modules/images.py b/modules/images.py\nindex c2ca8849de8..b335502b4c6 100644\n--- a/modules/images.py\n+++ b/modules/images.py\n@@ -553,6 +553,8 @@ def _atomically_save_image(image_to_save, filename_without_extension, extension)\n         elif extension.lower() in (\".jpg\", \".jpeg\", \".webp\"):\r\n             if image_to_save.mode == 'RGBA':\r\n                 image_to_save = image_to_save.convert(\"RGB\")\r\n+            elif image_to_save.mode == 'I;16':\r\n+                image_to_save = image_to_save.point(lambda p: p * 0.0038910505836576).convert(\"RGB\" if extension.lower() == \".webp\" else \"L\")\r\n \r\n             image_to_save.save(temp_file_path, format=image_format, quality=opts.jpeg_quality)\r\n \r\n", "test_patch": "", "problem_statement": "[Bug]: OSError: cannot write mode I;16 as JPEG\n### Is there an existing issue for this?\n\n- [X] I have searched the existing issues and checked the recent builds/commits\n\n### What happened?\n\nI used `stable-diffusion-webui-depthmap-script` to generate depthmaps. They are supposed to be saved as 16-bit grayscale PNGs. However, `_atomically_save_image` may also try to export this image for 4chan, meaning saving it as JPEG. The saving then fails with an error, as JPEGs can not be 16-bit. This error may prevent a batch job from completing.\r\n\n\n### Steps to reproduce the problem\n\n1. Generate and save depthmap using `stable-diffusion-webui-depthmap-script`, make sure the result is larger than 4 megabytes. For this, try a large file as an input, like the picture attached\r\n2. Observe an error\r\n\r\n![example](https://user-images.githubusercontent.com/54073010/209736858-6a8b8314-d184-4a53-8a74-5676f2bd246b.jpg)\r\n![image](https://user-images.githubusercontent.com/54073010/209736822-0e7f7d66-bedb-4f27-ad8c-6cf0b0a27071.png)\r\n\n\n### What should have happened?\n\nEither no conversion to JPEG, or a successful conversion to grayscale JPEG.\n\n### Commit where the problem happens\n\n4af3ca5393151d61363c30eef4965e694eeac15e\n\n### What platforms do you use to access UI ?\n\nWindows\n\n### What browsers do you use to access the UI ?\n\nMozilla Firefox\n\n### Command Line Arguments\n\n```Shell\n--medvram --opt-split-attention --xformers\n```\n\n\n### Additional information, context and logs\n\n_No response_\nDo not convert to JPEG if not possible\nDo not export for 4chan if conversion to JPEG is not possible. This prevents an exception when trying to save large depthmaps when export_for_4chan setting is on. See [#6065](https://github.com/AUTOMATIC1111/stable-diffusion-webui/issues/6065)\r\n\n", "hints_text": "Still broken in the latest master commit\nMay I ask, what is this \"Export for 4chan\" thing? \n@Nacurutu It is an UI setting. If enabled, export every large/heavy image as a shrunk/smaller JPG. The setting has a \"4MB\" in its wording in the UI (sorry, I am not near a computer rn).\n> @Nacurutu It is an UI setting. If enabled, export every large/heavy image as a shrunk/smaller JPG. The setting has a \"4MB\" in its wording in the UI (sorry, I am not near a computer rn).\r\n\r\nthank you!", "created_at": "2023-02-09T23:50:14Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 7583, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-7583", "issue_numbers": ["7553"], "base_commit": "ea9bd9fc7409109adcd61b897abc2c8881161256", "patch": "diff --git a/modules/shared_items.py b/modules/shared_items.py\nindex 8b5ec96dcdf..b72b2bae068 100644\n--- a/modules/shared_items.py\n+++ b/modules/shared_items.py\n@@ -20,4 +20,4 @@ def sd_vae_items():\n def refresh_vae_list():\r\n     import modules.sd_vae\r\n \r\n-    return modules.sd_vae.refresh_vae_list\r\n+    return modules.sd_vae.refresh_vae_list()\r\n", "test_patch": "", "problem_statement": "[Bug]: vae does not appear when clicking refresh button in models/VAE\n### Is there an existing issue for this?\n\n- [X] I have searched the existing issues and checked the recent builds/commits\n\n### What happened?\n\nPressing the button to update the VAE list does not update the VAE list.\n\n### Steps to reproduce the problem\n\n1. Insert new VAE file to models/VAE\r\n2. Press buttion Refresh VAE list \n\n### What should have happened?\n\nApprear new VAE file in list\n\n### Commit where the problem happens\n\nLastest\n\n### What platforms do you use to access the UI ?\n\n_No response_\n\n### What browsers do you use to access the UI ?\n\n_No response_\n\n### Command Line Arguments\n\n```Shell\nNo\n```\n\n\n### List of extensions\n\nNo\n\n### Console logs\n\n```Shell\nNothing\n```\n\n\n### Additional information\n\n_No response_\n", "hints_text": "", "created_at": "2023-02-06T06:00:52Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 7556, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-7556", "issue_numbers": ["4779"], "base_commit": "3993aa43e95bb60b9df977946af988f119525a71", "patch": "diff --git a/modules/images.py b/modules/images.py\nindex f4b20b2817f..c2ca8849de8 100644\n--- a/modules/images.py\n+++ b/modules/images.py\n@@ -199,7 +199,7 @@ def draw_texts(drawing, draw_x, draw_y, lines, initial_fnt, initial_fontsize):\n \r\n     pad_top = 0 if sum(hor_text_heights) == 0 else max(hor_text_heights) + line_spacing * 2\r\n \r\n-    result = Image.new(\"RGB\", (im.width + pad_left + margin * (rows-1), im.height + pad_top + margin * (cols-1)), \"white\")\r\n+    result = Image.new(\"RGB\", (im.width + pad_left + margin * (cols-1), im.height + pad_top + margin * (rows-1)), \"white\")\r\n \r\n     for row in range(rows):\r\n         for col in range(cols):\r\n@@ -223,7 +223,7 @@ def draw_texts(drawing, draw_x, draw_y, lines, initial_fnt, initial_fontsize):\n     return result\r\n \r\n \r\n-def draw_prompt_matrix(im, width, height, all_prompts):\r\n+def draw_prompt_matrix(im, width, height, all_prompts, margin=0):\r\n     prompts = all_prompts[1:]\r\n     boundary = math.ceil(len(prompts) / 2)\r\n \r\n@@ -233,7 +233,7 @@ def draw_prompt_matrix(im, width, height, all_prompts):\n     hor_texts = [[GridAnnotation(x, is_active=pos & (1 << i) != 0) for i, x in enumerate(prompts_horiz)] for pos in range(1 << len(prompts_horiz))]\r\n     ver_texts = [[GridAnnotation(x, is_active=pos & (1 << i) != 0) for i, x in enumerate(prompts_vert)] for pos in range(1 << len(prompts_vert))]\r\n \r\n-    return draw_grid_annotations(im, width, height, hor_texts, ver_texts)\r\n+    return draw_grid_annotations(im, width, height, hor_texts, ver_texts, margin)\r\n \r\n \r\n def resize_image(resize_mode, im, width, height, upscaler_name=None):\r\ndiff --git a/scripts/prompt_matrix.py b/scripts/prompt_matrix.py\nindex de921ea84a5..3ee3cbe4c17 100644\n--- a/scripts/prompt_matrix.py\n+++ b/scripts/prompt_matrix.py\n@@ -48,23 +48,17 @@ def ui(self, is_img2img):\n         gr.HTML('<br />')\r\n         with gr.Row():\r\n             with gr.Column():\r\n-                put_at_start = gr.Checkbox(label='Put variable parts at start of prompt',\r\n-                                           value=False, elem_id=self.elem_id(\"put_at_start\"))\r\n+                put_at_start = gr.Checkbox(label='Put variable parts at start of prompt', value=False, elem_id=self.elem_id(\"put_at_start\"))\r\n+                different_seeds = gr.Checkbox(label='Use different seed for each picture', value=False, elem_id=self.elem_id(\"different_seeds\"))\r\n             with gr.Column():\r\n-                # Radio buttons for selecting the prompt between positive and negative\r\n-                prompt_type = gr.Radio([\"positive\", \"negative\"], label=\"Select prompt\",\r\n-                                       elem_id=self.elem_id(\"prompt_type\"), value=\"positive\")\r\n-        with gr.Row():\r\n-            with gr.Column():\r\n-                different_seeds = gr.Checkbox(\r\n-                    label='Use different seed for each picture', value=False, elem_id=self.elem_id(\"different_seeds\"))\r\n+                prompt_type = gr.Radio([\"positive\", \"negative\"], label=\"Select prompt\", elem_id=self.elem_id(\"prompt_type\"), value=\"positive\")\r\n+                variations_delimiter = gr.Radio([\"comma\", \"space\"], label=\"Select joining char\", elem_id=self.elem_id(\"variations_delimiter\"), value=\"comma\")\r\n             with gr.Column():\r\n-                # Radio buttons for selecting the delimiter to use in the resulting prompt\r\n-                variations_delimiter = gr.Radio([\"comma\", \"space\"], label=\"Select delimiter\", elem_id=self.elem_id(\r\n-                    \"variations_delimiter\"), value=\"comma\")\r\n-        return [put_at_start, different_seeds, prompt_type, variations_delimiter]\r\n+                margin_size = gr.Slider(label=\"Grid margins (px)\", min=0, max=500, value=0, step=2, elem_id=self.elem_id(\"margin_size\"))\r\n+\r\n+        return [put_at_start, different_seeds, prompt_type, variations_delimiter, margin_size]\r\n \r\n-    def run(self, p, put_at_start, different_seeds, prompt_type, variations_delimiter):\r\n+    def run(self, p, put_at_start, different_seeds, prompt_type, variations_delimiter, margin_size):\r\n         modules.processing.fix_seed(p)\r\n         # Raise error if promp type is not positive or negative\r\n         if prompt_type not in [\"positive\", \"negative\"]:\r\n@@ -106,7 +100,7 @@ def run(self, p, put_at_start, different_seeds, prompt_type, variations_delimite\n         processed = process_images(p)\r\n \r\n         grid = images.image_grid(processed.images, p.batch_size, rows=1 << ((len(prompt_matrix_parts) - 1) // 2))\r\n-        grid = images.draw_prompt_matrix(grid, p.width, p.height, prompt_matrix_parts)\r\n+        grid = images.draw_prompt_matrix(grid, p.width, p.height, prompt_matrix_parts, margin_size)\r\n         processed.images.insert(0, grid)\r\n         processed.index_of_first_image = 1\r\n         processed.infotexts.insert(0, processed.infotexts[0])\r\ndiff --git a/scripts/xyz_grid.py b/scripts/xyz_grid.py\nindex 3122f6f66db..5982cfbaa7a 100644\n--- a/scripts/xyz_grid.py\n+++ b/scripts/xyz_grid.py\n@@ -205,7 +205,7 @@ def __init__(self, *args, **kwargs):\n ]\r\n \r\n \r\n-def draw_xyz_grid(p, xs, ys, zs, x_labels, y_labels, z_labels, cell, draw_legend, include_lone_images, include_sub_grids, first_axes_processed, second_axes_processed):\r\n+def draw_xyz_grid(p, xs, ys, zs, x_labels, y_labels, z_labels, cell, draw_legend, include_lone_images, include_sub_grids, first_axes_processed, second_axes_processed, margin_size):\r\n     hor_texts = [[images.GridAnnotation(x)] for x in x_labels]\r\n     ver_texts = [[images.GridAnnotation(y)] for y in y_labels]\r\n     title_texts = [[images.GridAnnotation(z)] for z in z_labels]\r\n@@ -292,7 +292,7 @@ def index(ix, iy, iz):\n         end_index = start_index + len(xs) * len(ys)\r\n         grid = images.image_grid(image_cache[start_index:end_index], rows=len(ys))\r\n         if draw_legend:\r\n-            grid = images.draw_grid_annotations(grid, cell_size[0], cell_size[1], hor_texts, ver_texts)\r\n+            grid = images.draw_grid_annotations(grid, cell_size[0], cell_size[1], hor_texts, ver_texts, margin_size)\r\n         sub_grids[i] = grid\r\n         if include_sub_grids and len(zs) > 1:\r\n             processed_result.images.insert(i+1, grid)\r\n@@ -351,10 +351,16 @@ def ui(self, is_img2img):\n                     fill_z_button = ToolButton(value=fill_values_symbol, elem_id=\"xyz_grid_fill_z_tool_button\", visible=False)\r\n \r\n         with gr.Row(variant=\"compact\", elem_id=\"axis_options\"):\r\n-            draw_legend = gr.Checkbox(label='Draw legend', value=True, elem_id=self.elem_id(\"draw_legend\"))\r\n-            include_lone_images = gr.Checkbox(label='Include Sub Images', value=False, elem_id=self.elem_id(\"include_lone_images\"))\r\n-            include_sub_grids = gr.Checkbox(label='Include Sub Grids', value=False, elem_id=self.elem_id(\"include_sub_grids\"))\r\n-            no_fixed_seeds = gr.Checkbox(label='Keep -1 for seeds', value=False, elem_id=self.elem_id(\"no_fixed_seeds\"))\r\n+            with gr.Column():\r\n+                draw_legend = gr.Checkbox(label='Draw legend', value=True, elem_id=self.elem_id(\"draw_legend\"))\r\n+                no_fixed_seeds = gr.Checkbox(label='Keep -1 for seeds', value=False, elem_id=self.elem_id(\"no_fixed_seeds\"))\r\n+            with gr.Column():\r\n+                include_lone_images = gr.Checkbox(label='Include Sub Images', value=False, elem_id=self.elem_id(\"include_lone_images\"))\r\n+                include_sub_grids = gr.Checkbox(label='Include Sub Grids', value=False, elem_id=self.elem_id(\"include_sub_grids\"))\r\n+            with gr.Column():\r\n+                margin_size = gr.Slider(label=\"Grid margins (px)\", min=0, max=500, value=0, step=2, elem_id=self.elem_id(\"margin_size\"))\r\n+        \r\n+        with gr.Row(variant=\"compact\", elem_id=\"swap_axes\"):\r\n             swap_xy_axes_button = gr.Button(value=\"Swap X/Y axes\", elem_id=\"xy_grid_swap_axes_button\")\r\n             swap_yz_axes_button = gr.Button(value=\"Swap Y/Z axes\", elem_id=\"yz_grid_swap_axes_button\")\r\n             swap_xz_axes_button = gr.Button(value=\"Swap X/Z axes\", elem_id=\"xz_grid_swap_axes_button\")\r\n@@ -393,9 +399,9 @@ def select_axis(x_type):\n             (z_values, \"Z Values\"),\r\n         )\r\n \r\n-        return [x_type, x_values, y_type, y_values, z_type, z_values, draw_legend, include_lone_images, include_sub_grids, no_fixed_seeds]\r\n+        return [x_type, x_values, y_type, y_values, z_type, z_values, draw_legend, include_lone_images, include_sub_grids, no_fixed_seeds, margin_size]\r\n \r\n-    def run(self, p, x_type, x_values, y_type, y_values, z_type, z_values, draw_legend, include_lone_images, include_sub_grids, no_fixed_seeds):\r\n+    def run(self, p, x_type, x_values, y_type, y_values, z_type, z_values, draw_legend, include_lone_images, include_sub_grids, no_fixed_seeds, margin_size):\r\n         if not no_fixed_seeds:\r\n             modules.processing.fix_seed(p)\r\n \r\n@@ -590,7 +596,8 @@ def cell(x, y, z):\n                 include_lone_images=include_lone_images,\r\n                 include_sub_grids=include_sub_grids,\r\n                 first_axes_processed=first_axes_processed,\r\n-                second_axes_processed=second_axes_processed\r\n+                second_axes_processed=second_axes_processed,\r\n+                margin_size=margin_size\r\n             )\r\n \r\n         if opts.grid_save and len(sub_grids) > 1:\r\n", "test_patch": "", "problem_statement": "[Feature Request]: xy plot, \"dividing\" lines between batches\n### Is there an existing issue for this?\n\n- [X] I have searched the existing issues and checked the recent builds/commits\n\n### What would your feature do ?\n\nAdd a few px of space betwen different columns and rows. the bathes themselves that are from the same X and Y option, can stay together.\r\n\r\nCurrently it can be tedious in big grids to figure out which picture goes to which option when like 8+ images are created per option. especially for those on the edge.\n\n### Proposed workflow\n\nadd checkbox \"add margins\" or similar, and just add a few PX of black between batches\n\n### Additional information\n\n-\n", "hints_text": "", "created_at": "2023-02-05T08:47:56Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 7430, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-7430", "issue_numbers": ["6866"], "base_commit": "d99bd04b3f8c7753e31aa6dea6109785c4bb92c9", "patch": "diff --git a/scripts/prompt_matrix.py b/scripts/prompt_matrix.py\nindex 6340a7d9aff..b1c486d44e3 100644\n--- a/scripts/prompt_matrix.py\n+++ b/scripts/prompt_matrix.py\n@@ -99,8 +99,8 @@ def run(self, p, put_at_start, different_seeds, prompt_type, variations_delimite\n         p.prompt_for_display = positive_prompt\r\n         processed = process_images(p)\r\n \r\n-        grid = images.image_grid(processed.images, p.batch_size, rows=1 << ((len(prompt_matrix_parts) - 1) // 2))\r\n-        grid = images.draw_prompt_matrix(grid, p.width, p.height, prompt_matrix_parts, margin_size)\r\n+        grid = images.image_grid(processed.images, p.batch_size, rows=1 << ((len(prompt_matrix_parts) - 1) // 2)) \r\n+        grid = images.draw_prompt_matrix(grid, processed.images[0].width, processed.images[1].height, prompt_matrix_parts, margin_size)\r\n         processed.images.insert(0, grid)\r\n         processed.index_of_first_image = 1\r\n         processed.infotexts.insert(0, processed.infotexts[0])\r\n", "test_patch": "", "problem_statement": "[Bug]: \"AssertionError: bad number of vertical texts: 2; must be 3\" on hires and prompt matrix\n### Is there an existing issue for this?\n\n- [X] I have searched the existing issues and checked the recent builds/commits\n\n### What happened?\n\nPerforming a render using both **hires fix** and **prompt matrix** results in an error and a failure to render the image grid (and accompanying legend). The individual images still render and can be accessed in the txt2img output folder. The image preview pane remains blank and the error text _AssertionError: bad number of vertical texts: 2; must be 3_* shows below it.\r\n\r\n*These numbers might be different. Sometimes _vertical_ will instead say _horizontal_.\n\n### Steps to reproduce the problem\n\n1. In the txt2img tab, enter a base prompt with 2 or more variable prompts separated by the vertical line character. For example: **Painting of a man|Cowboy hat|beard**\r\n2. Toggle **Hires. fix** to on. \u2705\r\n3. Under **Script**, select **Prompt matrix**.\r\n4. Click generate.\n\n### What should have happened?\n\nOn completion, the generation should have produced a grid showing the array of possible images with the variable prompts toggled on or off. Instead, no grid is produced and an error appears instead. The images still render (and can be viewed from the output folder), but the image grid does not. Since the image grid is the most important part (allowing you to compare images very easily), this is a problem.\r\n\r\nNote that using prompt matrix or hires. fix individually still work just fine.\n\n### Commit where the problem happens\n\n3a0d6b77295162146d0a8d04278804334da6f1b4\n\n### What platforms do you use to access UI ?\n\nWindows\n\n### What browsers do you use to access the UI ?\n\nGoogle Chrome\n\n### Command Line Arguments\n\n_No response_\n\n### Additional information, context and logs\n\nThe following text is an example of what shows in my command window when the error occurs:\r\n\r\n```\r\nError completing request8:54,  1.83s/it]\r\nArguments: ('task(xe7gus9bhl4yqzh)', 'painting of a beautiful woman|cowboy hat|blindfold', '', 'None', 'None', 20, 1, False, False, 1, 1, 10, -1.0, -1.0, 0, 0, 0, False, 512, 512, True, 0.7, 2, 'Latent', 0, 0, 0, 1, False, False, False, False, '', 1, '', 0, '', True, False, False) {}\r\nTraceback (most recent call last):\r\n  File \"C:\\StableDiffusion\\stable-diffusion-webui\\modules\\call_queue.py\", line 56, in f\r\n    res = list(func(*args, **kwargs))\r\n  File \"C:\\StableDiffusion\\stable-diffusion-webui\\modules\\call_queue.py\", line 37, in f\r\n    res = func(*args, **kwargs)\r\n  File \"C:\\StableDiffusion\\stable-diffusion-webui\\modules\\txt2img.py\", line 49, in txt2img\r\n    processed = modules.scripts.scripts_txt2img.run(p, *args)\r\n  File \"C:\\StableDiffusion\\stable-diffusion-webui\\modules\\scripts.py\", line 337, in run\r\n    processed = script.run(p, *script_args)\r\n  File \"C:\\StableDiffusion\\stable-diffusion-webui\\scripts\\prompt_matrix.py\", line 82, in run\r\n    grid = images.draw_prompt_matrix(grid, p.width, p.height, prompt_matrix_parts)\r\n  File \"C:\\StableDiffusion\\stable-diffusion-webui\\modules\\images.py\", line 230, in draw_prompt_matrix\r\n    return draw_grid_annotations(im, width, height, hor_texts, ver_texts)\r\n  File \"C:\\StableDiffusion\\stable-diffusion-webui\\modules\\images.py\", line 175, in draw_grid_annotations\r\n    assert cols == len(hor_texts), f'bad number of horizontal texts: {len(hor_texts)}; must be {cols}'\r\nAssertionError: bad number of horizontal texts: 2; must be 4\r\n```\n", "hints_text": "I think the issue stems from here: https://github.com/AUTOMATIC1111/stable-diffusion-webui/blob/2c1bb46c7ad5b4536f6587d327a03f0ff7811c5d/modules/images.py#L174-L175\r\n\r\nThe call comes from here:\r\nhttps://github.com/AUTOMATIC1111/stable-diffusion-webui/blob/2c1bb46c7ad5b4536f6587d327a03f0ff7811c5d/scripts/prompt_matrix.py#L82\r\n\r\nHere,  `p.width` and `p.height` are the dimensions **before** upscaling, which defaults to 2x. Thus, the result is a factor of 2 increase in the computed `rows` and `cols`, which leads to the error.\r\n\r\nI've fixed it locally, by modifying the above rows to:\r\n```python\r\ngrid = images.draw_prompt_matrix(grid, max(p.width, p.hr_upscale_to_x), max(p.height, p.hr_upscale_to_y), prompt_matrix_parts)\r\n```\r\n\r\nI also need to round the cols/rows because sometimes the scaling is not exact:\r\n```python\r\ncols = round(im.width / width)\r\nrows = round(im.height / height)\r\n```\r\n\r\nThis seems to me more like an hack rather than a fix, and it may break something else along the way. So I'll check when I have more time and then maybe send a PR.", "created_at": "2023-01-31T18:23:03Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 7357, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-7357", "issue_numbers": ["7306", "7284"], "base_commit": "8d7382ab24756cdcc37e71406832814f4713c55e", "patch": "diff --git a/modules/ui.py b/modules/ui.py\nindex f119569204b..7e193240c86 100644\n--- a/modules/ui.py\n+++ b/modules/ui.py\n@@ -466,8 +466,8 @@ def create_ui():\n                                 width = gr.Slider(minimum=64, maximum=2048, step=8, label=\"Width\", value=512, elem_id=\"txt2img_width\")\r\n                                 height = gr.Slider(minimum=64, maximum=2048, step=8, label=\"Height\", value=512, elem_id=\"txt2img_height\")\r\n \r\n+                            res_switch_btn = ToolButton(value=switch_values_symbol, elem_id=\"txt2img_res_switch_btn\")\r\n                             if opts.dimensions_and_batch_together:\r\n-                                res_switch_btn = ToolButton(value=switch_values_symbol, elem_id=\"txt2img_res_switch_btn\")\r\n                                 with gr.Column(elem_id=\"txt2img_column_batch\"):\r\n                                     batch_count = gr.Slider(minimum=1, step=1, label='Batch count', value=1, elem_id=\"txt2img_batch_count\")\r\n                                     batch_size = gr.Slider(minimum=1, maximum=8, step=1, label='Batch size', value=1, elem_id=\"txt2img_batch_size\")\r\n@@ -737,8 +737,8 @@ def copy_image(img):\n                                 width = gr.Slider(minimum=64, maximum=2048, step=8, label=\"Width\", value=512, elem_id=\"img2img_width\")\r\n                                 height = gr.Slider(minimum=64, maximum=2048, step=8, label=\"Height\", value=512, elem_id=\"img2img_height\")\r\n \r\n+                            res_switch_btn = ToolButton(value=switch_values_symbol, elem_id=\"img2img_res_switch_btn\")\r\n                             if opts.dimensions_and_batch_together:\r\n-                                res_switch_btn = ToolButton(value=switch_values_symbol, elem_id=\"img2img_res_switch_btn\")\r\n                                 with gr.Column(elem_id=\"img2img_column_batch\"):\r\n                                     batch_count = gr.Slider(minimum=1, step=1, label='Batch count', value=1, elem_id=\"img2img_batch_count\")\r\n                                     batch_size = gr.Slider(minimum=1, maximum=8, step=1, label='Batch size', value=1, elem_id=\"img2img_batch_size\")\r\n", "test_patch": "", "problem_statement": "[Bug]: Error on launching - UnboundLocalError: local variable 'res_switch_btn' referenced before assignment\n### Is there an existing issue for this?\r\n\r\n- [X] I have searched the existing issues and checked the recent builds/commits\r\n\r\n### What happened?\r\n\r\nAn error occurs on launching.\r\n\r\n### Steps to reproduce the problem\r\n\r\nLaunch app.\r\n\r\nEdit: I've worked out that this occurs if the \"Show Width/Height and Batch sliders in same row\" setting is unchecked.\r\n\r\n### What should have happened?\r\n\r\nNo error.\r\n\r\n### Commit where the problem happens\r\n\r\ne8a41df49fadd2cf9f23b1f02d75a4947bec5646\r\n\r\n### What platforms do you use to access the UI ?\r\n\r\nWindows\r\n\r\n### What browsers do you use to access the UI ?\r\n\r\nMozilla Firefox\r\n\r\n### Command Line Arguments\r\n\r\n```Shell\r\nNone\r\n```\r\n\r\n\r\n### List of extensions\r\n\r\nNo\r\n\r\n### Console logs\r\n\r\n```Shell\r\nvenv \"C:\\Users\\<USER>\\Documents\\GitHub\\stable-diffusion-webui\\venv\\Scripts\\Python.exe\"\r\nPython 3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]\r\nCommit hash: e8a41df49fadd2cf9f23b1f02d75a4947bec5646\r\nInstalling requirements for Web UI\r\nLaunching Web UI with arguments:\r\nNo module 'xformers'. Proceeding without it.\r\nLoading weights [cc6cb27103] from C:\\Users\\<USER>\\Documents\\GitHub\\stable-diffusion-webui\\models\\Stable-diffusion\\StableDiffusion_v15.ckpt\r\nCreating model from config: C:\\Users\\<USER>\\Documents\\GitHub\\stable-diffusion-webui\\configs\\v1-inference.yaml\r\nLatentDiffusion: Running in eps-prediction mode\r\nDiffusionWrapper has 859.52 M params.\r\nApplying cross attention optimization (Doggettx).\r\nTextual inversion embeddings loaded(0):\r\nModel loaded in 3.0s (create model: 0.4s, apply weights to model: 0.7s, apply half(): 0.6s, move model to device: 0.4s, load textual inversion embeddings: 0.8s).\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\<USER>\\Documents\\GitHub\\stable-diffusion-webui\\launch.py\", line 360, in <module>\r\n    start()\r\n  File \"C:\\Users\\<USER>\\Documents\\GitHub\\stable-diffusion-webui\\launch.py\", line 355, in start\r\n    webui.webui()\r\n  File \"C:\\Users\\<USER>\\Documents\\GitHub\\stable-diffusion-webui\\webui.py\", line 196, in webui\r\n    shared.demo = modules.ui.create_ui()\r\n  File \"C:\\Users\\<USER>\\Documents\\GitHub\\stable-diffusion-webui\\modules\\ui.py\", line 572, in create_ui\r\n    res_switch_btn.click(lambda w, h: (h, w), inputs=[width, height], outputs=[width, height])\r\nUnboundLocalError: local variable 'res_switch_btn' referenced before assignment\r\nPress any key to continue . . .\r\n```\r\n\r\n\r\n### Additional information\r\n\r\n_No response_\nAdd button to switch width and height\n**Describe what this pull request is trying to achieve.**\r\n\r\nAdds a button to switch width and height, allowing quick and easy switching between landscape and portrait.\r\n\r\n**Additional notes and description of your changes**\r\n\r\nI'm not sure how to better adjust the position of the button, this was the best I could come up with - further suggestions are welcomed\r\n\r\n**Environment this was tested in**\r\n\r\n - OS: Windows 11 21H2 \r\n - Browser: Firefox, Chrome, Edge\r\n - Graphics card: RTX 3050 Ti\r\n\r\n**Screenshots or videos of your changes**\r\n\r\n![image](https://user-images.githubusercontent.com/73721238/215168280-aec51267-78c3-4238-aeee-343f34e5eb80.png)\r\n\n", "hints_text": "Also experiencing this issue.\nThis must be related to config as I've done a hard-reset and the problem has gone.\nTemporary fix, edit the \"dimensions_and_batch_together\" line in your config.json file so the value is true:\r\n\r\n`\"dimensions_and_batch_together\": true,`\nCannot confirm. Config.json is not being changed/properly read from the settings panel on a 100% stock install.\r\nEven changing the option manually in editor, changing any other value immediately reverts to the default invalid state.\r\n\r\nEdit: Did a full reinstall again. The **only** option I changed was to the show width/height in line option. And then saved. Settings said 7 options were changed, and the UI option was not applied. Immediately returns to invalid state.\r\n![firefox_Z0fFBO2Qu8](https://user-images.githubusercontent.com/7834910/215279792-6a899ef9-4f78-4a48-a2a7-d688c0b499ad.png)\nHaving same issue and can confirm setting \"dimensions_and_batch_together\": true, works as a workaround. I don't know the code but suspect this relates to the recent update that added a button for switching height/width. \nIt seems to be the case. As long as I do not change the width/height inline option, the settings appear to work.\nyou can do `a,b = b,a` in python", "created_at": "2023-01-29T08:40:10Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 7353, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-7353", "issue_numbers": ["7338"], "base_commit": "f6b7768f84a335d351ba8c0d4c34d78e59272339", "patch": "diff --git a/modules/ui_extra_networks_checkpoints.py b/modules/ui_extra_networks_checkpoints.py\nindex c66cb8307ad..5b471671a09 100644\n--- a/modules/ui_extra_networks_checkpoints.py\n+++ b/modules/ui_extra_networks_checkpoints.py\n@@ -34,5 +34,5 @@ def list_items(self):\n             }\r\n \r\n     def allowed_directories_for_previews(self):\r\n-        return [shared.cmd_opts.ckpt_dir, sd_models.model_path]\r\n+        return [v for v in [shared.cmd_opts.ckpt_dir, sd_models.model_path] if v is not None]\r\n \r\n", "test_patch": "", "problem_statement": "[Bug]: thumbnail cards are not loading the preview image\n### Is there an existing issue for this?\n\n- [X] I have searched the existing issues and checked the recent builds/commits\n\n### What happened?\n\njust getting black image, and if I try to update an image, it goes black too.\r\n\r\nIt was working before checkpoints were added, I don't know if that's related.\n\n### Steps to reproduce the problem\n\n1. Go to .... \r\n2. Press ....\r\n3. ...\r\n\n\n### What should have happened?\n\nshould see the preview images\n\n### Commit where the problem happens\n\n0a8515085ef258d4b76fdc000f7ed9d55751d6b8\n\n### What platforms do you use to access the UI ?\n\n_No response_\n\n### What browsers do you use to access the UI ?\n\n_No response_\n\n### Command Line Arguments\n\n```Shell\n--api --cors-allow-origins http://localhost:5173 --administrator --no-half-vae --no-half --disable-safe-unpickle --force-cpu --xformers\n```\n\n\n### List of extensions\n\nall of them\n\n### Console logs\n\n```Shell\nERROR:    Exception in ASGI application\r\nTraceback (most recent call last):\r\n  File \"D:\\stable-diffusion-webui\\venv\\lib\\site-packages\\anyio\\streams\\memory.py\", line 94, in receive\r\n    return self.receive_nowait()\r\n  File \"D:\\stable-diffusion-webui\\venv\\lib\\site-packages\\anyio\\streams\\memory.py\", line 89, in receive_nowait\r\n    raise WouldBlock\r\nanyio.WouldBlock\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"D:\\stable-diffusion-webui\\venv\\lib\\site-packages\\starlette\\middleware\\base.py\", line 77, in call_next\r\n    message = await recv_stream.receive()\r\n  File \"D:\\stable-diffusion-webui\\venv\\lib\\site-packages\\anyio\\streams\\memory.py\", line 114, in receive\r\n    raise EndOfStream\r\nanyio.EndOfStream\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"D:\\stable-diffusion-webui\\venv\\lib\\site-packages\\uvicorn\\protocols\\http\\h11_impl.py\", line 407, in run_asgi\r\n    result = await app(  # type: ignore[func-returns-value]\r\n  File \"D:\\stable-diffusion-webui\\venv\\lib\\site-packages\\uvicorn\\middleware\\proxy_headers.py\", line 78, in __call__\r\n    return await self.app(scope, receive, send)\r\n  File \"D:\\stable-diffusion-webui\\venv\\lib\\site-packages\\fastapi\\applications.py\", line 270, in __call__\r\n    await super().__call__(scope, receive, send)\r\n  File \"D:\\stable-diffusion-webui\\venv\\lib\\site-packages\\starlette\\applications.py\", line 124, in __call__\r\n    await self.middleware_stack(scope, receive, send)\r\n  File \"D:\\stable-diffusion-webui\\venv\\lib\\site-packages\\starlette\\middleware\\errors.py\", line 184, in __call__\r\n    raise exc\r\n  File \"D:\\stable-diffusion-webui\\venv\\lib\\site-packages\\starlette\\middleware\\errors.py\", line 162, in __call__\r\n    await self.app(scope, receive, _send)\r\n  File \"D:\\stable-diffusion-webui\\venv\\lib\\site-packages\\starlette\\middleware\\base.py\", line 106, in __call__\r\n    response = await self.dispatch_func(request, call_next)\r\n  File \"D:\\stable-diffusion-webui\\extensions\\auto-sd-paint-ext\\backend\\app.py\", line 391, in app_encryption_middleware\r\n    res: StreamingResponse = await call_next(req)\r\n  File \"D:\\stable-diffusion-webui\\venv\\lib\\site-packages\\starlette\\middleware\\base.py\", line 80, in call_next\r\n    raise app_exc\r\n  File \"D:\\stable-diffusion-webui\\venv\\lib\\site-packages\\starlette\\middleware\\base.py\", line 69, in coro\r\n    await self.app(scope, receive_or_disconnect, send_no_error)\r\n  File \"D:\\stable-diffusion-webui\\venv\\lib\\site-packages\\starlette\\middleware\\base.py\", line 106, in __call__\r\n    response = await self.dispatch_func(request, call_next)\r\n  File \"D:\\stable-diffusion-webui\\modules\\api\\api.py\", line 96, in log_and_time\r\n    res: Response = await call_next(req)\r\n  File \"D:\\stable-diffusion-webui\\venv\\lib\\site-packages\\starlette\\middleware\\base.py\", line 80, in call_next\r\n    raise app_exc\r\n  File \"D:\\stable-diffusion-webui\\venv\\lib\\site-packages\\starlette\\middleware\\base.py\", line 69, in coro\r\n    await self.app(scope, receive_or_disconnect, send_no_error)\r\n  File \"D:\\stable-diffusion-webui\\venv\\lib\\site-packages\\starlette\\middleware\\gzip.py\", line 24, in __call__\r\n    await responder(scope, receive, send)\r\n  File \"D:\\stable-diffusion-webui\\venv\\lib\\site-packages\\starlette\\middleware\\gzip.py\", line 43, in __call__\r\n    await self.app(scope, receive, self.send_with_gzip)\r\n  File \"D:\\stable-diffusion-webui\\venv\\lib\\site-packages\\starlette\\middleware\\cors.py\", line 84, in __call__\r\n    await self.app(scope, receive, send)\r\n  File \"D:\\stable-diffusion-webui\\venv\\lib\\site-packages\\starlette\\middleware\\exceptions.py\", line 79, in __call__\r\n    raise exc\r\n  File \"D:\\stable-diffusion-webui\\venv\\lib\\site-packages\\starlette\\middleware\\exceptions.py\", line 68, in __call__\r\n    await self.app(scope, receive, sender)\r\n  File \"D:\\stable-diffusion-webui\\venv\\lib\\site-packages\\fastapi\\middleware\\asyncexitstack.py\", line 21, in __call__\r\n    raise e\r\n  File \"D:\\stable-diffusion-webui\\venv\\lib\\site-packages\\fastapi\\middleware\\asyncexitstack.py\", line 18, in __call__\r\n    await self.app(scope, receive, send)\r\n  File \"D:\\stable-diffusion-webui\\venv\\lib\\site-packages\\starlette\\routing.py\", line 706, in __call__\r\n    await route.handle(scope, receive, send)\r\n  File \"D:\\stable-diffusion-webui\\venv\\lib\\site-packages\\starlette\\routing.py\", line 276, in handle\r\n    await self.app(scope, receive, send)\r\n  File \"D:\\stable-diffusion-webui\\venv\\lib\\site-packages\\starlette\\routing.py\", line 66, in app\r\n    response = await func(request)\r\n  File \"D:\\stable-diffusion-webui\\venv\\lib\\site-packages\\fastapi\\routing.py\", line 235, in app\r\n    raw_response = await run_endpoint_function(\r\n  File \"D:\\stable-diffusion-webui\\venv\\lib\\site-packages\\fastapi\\routing.py\", line 163, in run_endpoint_function\r\n    return await run_in_threadpool(dependant.call, **values)\r\n  File \"D:\\stable-diffusion-webui\\venv\\lib\\site-packages\\starlette\\concurrency.py\", line 41, in run_in_threadpool\r\n    return await anyio.to_thread.run_sync(func, *args)\r\n  File \"D:\\stable-diffusion-webui\\venv\\lib\\site-packages\\anyio\\to_thread.py\", line 31, in run_sync\r\n    return await get_asynclib().run_sync_in_worker_thread(\r\n  File \"D:\\stable-diffusion-webui\\venv\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 937, in run_sync_in_worker_thread\r\n    return await future\r\n  File \"D:\\stable-diffusion-webui\\venv\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 867, in run\r\n    result = context.run(func, *args)\r\n  File \"D:\\stable-diffusion-webui\\modules\\ui_extra_networks.py\", line 28, in fetch_file\r\n    if not any([Path(x).resolve() in Path(filename).resolve().parents for x in allowed_dirs]):\r\n  File \"D:\\stable-diffusion-webui\\modules\\ui_extra_networks.py\", line 28, in <listcomp>\r\n    if not any([Path(x).resolve() in Path(filename).resolve().parents for x in allowed_dirs]):\r\n  File \"D:\\Python\\Python310\\lib\\pathlib.py\", line 960, in __new__\r\n    self = cls._from_parts(args)\r\n  File \"D:\\Python\\Python310\\lib\\pathlib.py\", line 594, in _from_parts\r\n    drv, root, parts = self._parse_args(args)\r\n  File \"D:\\Python\\Python310\\lib\\pathlib.py\", line 578, in _parse_args\r\n    a = os.fspath(a)\r\nTypeError: expected str, bytes or os.PathLike object, not NoneType\n```\n\n\n### Additional information\n\n_No response_\n", "hints_text": "Same here. My thumbnails are all borked since this last update an hour ago. Empty.\nThis is the last working commit for thumbnails: 91c8d0dcfc9a5d46aec47fd3eb34c633c104b5bb\nThe same issue, I started having the same issue when the ui_extra_networks was added with SD-Model-Preview Extension. \nSame here, it just cycle spews the \"ERROR:    Exception in ASGI application\". No thumbnail gets loaded.\r\nI think it is throwing one error for each, but can't really tell because it went over my console buffer limit.\r\nIf I try to \"replace preview\" a thumbnail it throws:\r\n\r\nTraceback (most recent call last):\r\n  File \"d:\\WORK\\conda_envs\\automatic\\stable-diffusion-webui\\venv\\lib\\site-packages\\gradio\\routes.py\", line 337, in run_predict\r\n    output = await app.get_blocks().process_api(\r\n  File \"d:\\WORK\\conda_envs\\automatic\\stable-diffusion-webui\\venv\\lib\\site-packages\\gradio\\blocks.py\", line 1015, in process_api\r\n    result = await self.call_function(\r\n  File \"d:\\WORK\\conda_envs\\automatic\\stable-diffusion-webui\\venv\\lib\\site-packages\\gradio\\blocks.py\", line 833, in call_function\r\n    prediction = await anyio.to_thread.run_sync(\r\n  File \"d:\\WORK\\conda_envs\\automatic\\stable-diffusion-webui\\venv\\lib\\site-packages\\anyio\\to_thread.py\", line 31, in run_sync\r\n    return await get_asynclib().run_sync_in_worker_thread(\r\n  File \"d:\\WORK\\conda_envs\\automatic\\stable-diffusion-webui\\venv\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 937, in run_sync_in_worker_thread\r\n    return await future\r\n  File \"d:\\WORK\\conda_envs\\automatic\\stable-diffusion-webui\\venv\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 867, in run\r\n    result = context.run(func, *args)\r\n  File \"D:\\WORK\\conda_envs\\automatic\\stable-diffusion-webui\\modules\\ui_extra_networks.py\", line 192, in save_preview\r\n    if any([path_is_parent(x, filename) for x in extra_page.allowed_directories_for_previews()]):\r\n  File \"D:\\WORK\\conda_envs\\automatic\\stable-diffusion-webui\\modules\\ui_extra_networks.py\", line 192, in <listcomp>\r\n    if any([path_is_parent(x, filename) for x in extra_page.allowed_directories_for_previews()]):\r\n  File \"D:\\WORK\\conda_envs\\automatic\\stable-diffusion-webui\\modules\\ui_extra_networks.py\", line 171, in path_is_parent\r\n    parent_path = os.path.abspath(parent_path)\r\n  File \"C:\\Users\\Utilizador\\.conda\\envs\\automatic\\lib\\ntpath.py\", line 566, in abspath\r\n    return normpath(_getfullpathname(path))\r\nTypeError: _getfullpathname: path should be string, bytes or os.PathLike, not NoneType\nSame here. My thumbnails are all borked since this last update an hour ago. Empty too\n![Screenshot 2023-01-29 183424](https://user-images.githubusercontent.com/1563909/215307168-5db32677-3c12-472e-8a8d-af31feb77bdb.png)\r\nSame here\nYa'll can get thumbs back if you roll the head back till you see\r\n`91c8d0d Merge pull request #7231 from EllangoK/master`\r\n\r\nFor me I rolled back twice\r\n\r\nthis roll back didn't get thumbs:\r\n```powershell\r\nshane in stable-diffusion-webui in master\r\ngit reset HEAD~1 --hard\r\nHEAD is now at 1d8e06d add checkpoints tab for extra networks UI\r\n```\r\n\r\nSo rolled back once more to this:\r\n```powershell\r\nshane in stable-diffusion-webui in master\r\ngit reset HEAD~1 --hard\r\nHEAD is now at 91c8d0d Merge pull request #7231 from EllangoK/master\r\n```\r\n\r\n![image](https://user-images.githubusercontent.com/1563909/215307901-a3b24084-ca6e-4a9f-b50f-f23449523770.png)\r\n", "created_at": "2023-01-29T07:32:00Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 7334, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-7334", "issue_numbers": ["7320"], "base_commit": "0a8515085ef258d4b76fdc000f7ed9d55751d6b8", "patch": "diff --git a/scripts/xyz_grid.py b/scripts/xyz_grid.py\nindex 3df404834e6..3122f6f66db 100644\n--- a/scripts/xyz_grid.py\n+++ b/scripts/xyz_grid.py\n@@ -286,23 +286,24 @@ def index(ix, iy, iz):\n         print(\"Unexpected error: draw_xyz_grid failed to return even a single processed image\")\r\n         return Processed(p, [])\r\n \r\n-    grids = [None] * len(zs)\r\n+    sub_grids = [None] * len(zs)\r\n     for i in range(len(zs)):\r\n         start_index = i * len(xs) * len(ys)\r\n         end_index = start_index + len(xs) * len(ys)\r\n         grid = images.image_grid(image_cache[start_index:end_index], rows=len(ys))\r\n         if draw_legend:\r\n             grid = images.draw_grid_annotations(grid, cell_size[0], cell_size[1], hor_texts, ver_texts)\r\n-        \r\n-        grids[i] = grid        \r\n+        sub_grids[i] = grid\r\n         if include_sub_grids and len(zs) > 1:\r\n             processed_result.images.insert(i+1, grid)\r\n \r\n-    original_grid_size = grids[0].size\r\n-    grids = images.image_grid(grids, rows=1)\r\n-    processed_result.images[0] = images.draw_grid_annotations(grids, original_grid_size[0], original_grid_size[1], title_texts, [[images.GridAnnotation()]])\r\n+    sub_grid_size = sub_grids[0].size\r\n+    z_grid = images.image_grid(sub_grids, rows=1)\r\n+    if draw_legend:\r\n+        z_grid = images.draw_grid_annotations(z_grid, sub_grid_size[0], sub_grid_size[1], title_texts, [[images.GridAnnotation()]])\r\n+    processed_result.images[0] = z_grid\r\n \r\n-    return processed_result\r\n+    return processed_result, sub_grids\r\n \r\n \r\n class SharedSettingsStackHelper(object):\r\n@@ -576,7 +577,7 @@ def cell(x, y, z):\n             return res\r\n \r\n         with SharedSettingsStackHelper():\r\n-            processed = draw_xyz_grid(\r\n+            processed, sub_grids = draw_xyz_grid(\r\n                 p,\r\n                 xs=xs,\r\n                 ys=ys,\r\n@@ -592,6 +593,10 @@ def cell(x, y, z):\n                 second_axes_processed=second_axes_processed\r\n             )\r\n \r\n+        if opts.grid_save and len(sub_grids) > 1:\r\n+            for sub_grid in sub_grids:\r\n+                images.save_image(sub_grid, p.outpath_grids, \"xyz_grid\", info=grid_infotext[0], extension=opts.grid_format, prompt=p.prompt, seed=processed.seed, grid=True, p=p)\r\n+\r\n         if opts.grid_save:\r\n             images.save_image(processed.images[0], p.outpath_grids, \"xyz_grid\", info=grid_infotext[0], extension=opts.grid_format, prompt=p.prompt, seed=processed.seed, grid=True, p=p)\r\n \r\n", "test_patch": "", "problem_statement": "[Bug]: X/Y/Z plot script not saving Sub grids\n### Is there an existing issue for this?\n\n- [X] I have searched the existing issues and checked the recent builds/commits\n\n### What happened?\n\nI have used X/Y/Z plot script, after ticking both \"Include Sub Images\" and '\"Include Sub Grids\", the sub grids are created (I have a sub grid image visible in the UI.\r\n\r\nHowever these Sub Grids and not auto-saved in the regular `./output/txt2img-grids/` folder. Only the grid compounded from all Sub Grids is saved there.\r\n\r\nSee screenshot (yes, I know I made a 5x7x19 cube), where we can see that all are visible, but neither `./output/txt2img-grids/` no `./output/txt2img-images/` have the sub-grids saved. Only the first compunded grid is saved.\r\n\r\n![image](https://user-images.githubusercontent.com/123163912/215270196-62be778a-8aef-406a-ab42-bec16da15505.png)\r\n\r\nI have 'Always save all generated image grids\" and 'Do not save grids consisting of one picture' ticked in the option.\r\n\r\nMost likely related to PR #7146 from @EllangoK that changes X/Y plot to X/Y/Z.\r\n\n\n### Steps to reproduce the problem\n\n1. Go to txt2img tab,\r\n2. Select X/Y/Z plot script\r\n3. Select a type and at least 2 values for X, Y, Z\r\n4. Tick \"Include Sub Grids\"\r\n\n\n### What should have happened?\n\neach Sub Grid (one for each Z) should be auto-saved in `./output/txt2img-grids/`\n\n### Commit where the problem happens\n\n9beb794e0b0dc1a0f9e89d8e38bd789a8c608397\n\n### What platforms do you use to access the UI ?\n\nWindows\n\n### What browsers do you use to access the UI ?\n\nMozilla Firefox\n\n### Command Line Arguments\n\n```Shell\n--xformers\n```\n\n\n### List of extensions\n\nNo\n\n### Console logs\n\n```Shell\nNo useful information in console log, but see here:\r\nhttps://bin.infini.fr/?ed8e59234f9f7f1f#FKdiVPqjf3rLHeMis3npWvUgFRgDzxBQ9W38jiKPLUzt\r\n(expires in 6 day)\n```\n\n\n### Additional information\n\n_No response_\n", "hints_text": "", "created_at": "2023-01-28T20:42:44Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 7285, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-7285", "issue_numbers": ["7275"], "base_commit": "9beb794e0b0dc1a0f9e89d8e38bd789a8c608397", "patch": "diff --git a/README.md b/README.md\nindex c6bd6f27e5c..2149dcc511a 100644\n--- a/README.md\n+++ b/README.md\n@@ -17,7 +17,7 @@ A browser interface based on Gradio library for Stable Diffusion.\n     - a man in a (tuxedo:1.21) - alternative syntax\r\n     - select text and press ctrl+up or ctrl+down to automatically adjust attention to selected text (code contributed by anonymous user)\r\n - Loopback, run img2img processing multiple times\r\n-- X/Y plot, a way to draw a 2 dimensional plot of images with different parameters\r\n+- X/Y/Z plot, a way to draw a 3 dimensional plot of images with different parameters\r\n - Textual Inversion\r\n     - have as many embeddings as you want and use any names you like for them\r\n     - use multiple embeddings with different numbers of vectors per token\r\ndiff --git a/javascript/hints.js b/javascript/hints.js\nindex 3cf10e2034e..7b60b25e88e 100644\n--- a/javascript/hints.js\n+++ b/javascript/hints.js\n@@ -50,7 +50,7 @@ titles = {\n \n     \"None\": \"Do not do anything special\",\n     \"Prompt matrix\": \"Separate prompts into parts using vertical pipe character (|) and the script will create a picture for every combination of them (except for the first part, which will be present in all combinations)\",\n-    \"X/Y plot\": \"Create a grid where images will have different parameters. Use inputs below to specify which parameters will be shared by columns and rows\",\n+    \"X/Y/Z plot\": \"Create grid(s) where images will have different parameters. Use inputs below to specify which parameters will be shared by columns and rows\",\n     \"Custom code\": \"Run Python code. Advanced user only. Must run program with --allow-code for this to work\",\n \n     \"Prompt S/R\": \"Separate a list of words with commas, and the first word will be used as a keyword: script will search for this word in the prompt, and replace it with others\",\ndiff --git a/scripts/xyz_grid.py b/scripts/xyz_grid.py\nindex 828c2d1226a..f0116055974 100644\n--- a/scripts/xyz_grid.py\n+++ b/scripts/xyz_grid.py\n@@ -123,7 +123,7 @@ def apply_vae(p, x, xs):\n \r\n \r\n def apply_styles(p: StableDiffusionProcessingTxt2Img, x: str, _):\r\n-    p.styles = x.split(',')\r\n+    p.styles.extend(x.split(','))\r\n \r\n \r\n def format_value_add_label(p, opt, x):\r\n@@ -499,7 +499,7 @@ def fix_axis_seeds(axis_opt, axis_list):\n         image_cell_count = p.n_iter * p.batch_size\r\n         cell_console_text = f\"; {image_cell_count} images per cell\" if image_cell_count > 1 else \"\"\r\n         plural_s = 's' if len(zs) > 1 else ''\r\n-        print(f\"X/Y plot will create {len(xs) * len(ys) * len(zs) * image_cell_count} images on {len(zs)} {len(xs)}x{len(ys)} grid{plural_s}{cell_console_text}. (Total steps to process: {total_steps})\")\r\n+        print(f\"X/Y/Z plot will create {len(xs) * len(ys) * len(zs) * image_cell_count} images on {len(zs)} {len(xs)}x{len(ys)} grid{plural_s}{cell_console_text}. (Total steps to process: {total_steps})\")\r\n         shared.total_tqdm.updateTotal(total_steps)\r\n \r\n         grid_infotext = [None]\r\n@@ -533,6 +533,7 @@ def cell(x, y, z):\n                 return Processed(p, [], p.seed, \"\")\r\n \r\n             pc = copy(p)\r\n+            pc.styles = pc.styles[:]\r\n             x_opt.apply(pc, x, xs)\r\n             y_opt.apply(pc, y, ys)\r\n             z_opt.apply(pc, z, zs)\r\n", "test_patch": "", "problem_statement": "[Bug]: xyz script fails to update x dimension\n### Is there an existing issue for this?\n\n- [X] I have searched the existing issues and checked the recent builds/commits\n\n### What happened?\n\nI'm trying to compare checkpoints (z dimension) on a x (photo, 3d, anime) vs y (subjects) grid.\r\n\r\nI saved 3 styles and 3 subjects and called the script against 2 checkpoints\r\n\r\nThis is the result\r\n\r\n![xyz_grid-0001-1131235204-e6e99610c4-3840x1864](https://user-images.githubusercontent.com/4661737/215094307-e85569e4-6f58-47ae-ac7a-cd1d54a90f08.png)\r\n\r\nwhen switching x with y\r\n\r\n![xyz_grid-0002-1131235204-e6e99610c4-3840x2710](https://user-images.githubusercontent.com/4661737/215095803-17a48c32-aa90-403e-a5ea-fe84c0e59780.png)\r\n\r\nso y and z work fine, x doesn't\n\n### Steps to reproduce the problem\n\n1. Add 6 new styles: photo, 3d, anime, person, organic, urban defined as\r\n```\r\nphoto,\"DSLR RAW filmgrain photo, shot on iphone, photography, edited in Adobe Lightroom\",\"anime, illustration, 3d, octane render, drawing, cell shading, semi-realistic, cgi, sketch, cartoon\"\r\n3d,\"3d octane render, unreal engine raytraced screenshot, cgi, photorealistic\",\"photo, DSLR, cell shading, anime, illustration, drawing, sketch, cartoon\"\r\nanime,\"anime absurdres illustration, drawing, lineart\",\"photo, DSLR, 3d, cell shading, octane render, cgi, unreal engine\"\r\nperson,\"woman sleeping slumped over in the rain during the nighttime festival, wet hair, perfect anatomy\",\"ugly, old, nsfw, big breasts, bad hands\"\r\norganic,\"ancient giant tree in the deep jungle, birds in the branches, fog at dawn, beautiful scenery\",\"interior\"\r\nurban,\"modern skyscraper towering over an old abandoned medieval town, sheds and taverns, storm clouds, urban setting\",\"rural, portrait\"\r\n```\r\n2. setup xyz grid using person, organic and urban as style x and photo, 3d and anime as style y. For z pick two checkpoints\r\n3. launch generation\r\n4. have a look at the grid\n\n### What should have happened?\n\nI should get something like this where all the images are different, not the same across one dimension\r\n\r\n![](https://i.imgur.com/Nce9Asn.jpg)\n\n### Commit where the problem happens\n\n9beb794e0b0dc1a0f9e89d8e38bd789a8c608397\n\n### What platforms do you use to access UI ?\n\nLinux\n\n### What browsers do you use to access the UI ?\n\nMozilla Firefox\n\n### Command Line Arguments\n\n```Shell\n--share --enable-insecure-extension-access\n```\n\n\n### Additional information, context and logs\n\n_No response_\n", "hints_text": "I will see if I can modify the code but currently if you use two of the same axes (such as \"Styles\") in xyz grid script (or the old xy script for matter), only one of them gets applied.\r\n\r\n> ![](https://camo.githubusercontent.com/bb67c8406c51f8aead9c0f715c3eabe34aeaf5798865e9c0a2af5cb67abe39cc/68747470733a2f2f692e696d6775722e636f6d2f4e63653941736e2e6a7067)\r\n\r\nThis image you sent does not use two \"Styles\" axii, it uses a style axis, checkpoint axis and the top z axis is a Prompt S/R axis (otherwise the top axis would say \"Styles: Car\", \"Styles: Urban\", etc.)\r\n\nthanks for the quick reply!\r\n\r\nI see...\r\n\r\nhow do you switch between saved styles in prompt s/r? is there any keyword?\r\nsay something like `style:<my saved style name>` to be added inside the prompt\nJust as an added comment, it would be very useful to be able to use two of the same axes, especially for this case, as you could then see the effect of combining styles", "created_at": "2023-01-27T19:05:57Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 7116, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-7116", "issue_numbers": ["7051"], "base_commit": "e407d1af897a7896d8c81e32dc86e7eb753ce207", "patch": "diff --git a/modules/api/api.py b/modules/api/api.py\nindex 5d60fc0a084..b1dd14cc643 100644\n--- a/modules/api/api.py\n+++ b/modules/api/api.py\n@@ -22,6 +22,8 @@\n from modules.realesrgan_model import get_realesrgan_models\n from modules import devices\n from typing import List\n+import piexif\n+import piexif.helper\n \n def upscaler_to_index(name: str):\n     try:\n@@ -56,18 +58,30 @@ def decode_base64_to_image(encoding):\n def encode_pil_to_base64(image):\n     with io.BytesIO() as output_bytes:\n \n-        # Copy any text-only metadata\n-        use_metadata = False\n-        metadata = PngImagePlugin.PngInfo()\n-        for key, value in image.info.items():\n-            if isinstance(key, str) and isinstance(value, str):\n-                metadata.add_text(key, value)\n-                use_metadata = True\n+        if opts.samples_format.lower() == 'png':\n+            use_metadata = False\n+            metadata = PngImagePlugin.PngInfo()\n+            for key, value in image.info.items():\n+                if isinstance(key, str) and isinstance(value, str):\n+                    metadata.add_text(key, value)\n+                    use_metadata = True\n+            image.save(output_bytes, format=\"PNG\", pnginfo=(metadata if use_metadata else None), quality=opts.jpeg_quality)\n+\n+        elif opts.samples_format.lower() in (\"jpg\", \"jpeg\", \"webp\"):\n+            parameters = image.info.get('parameters', None)\n+            exif_bytes = piexif.dump({\n+                \"Exif\": { piexif.ExifIFD.UserComment: piexif.helper.UserComment.dump(parameters or \"\", encoding=\"unicode\") }\n+            })\n+            if opts.samples_format.lower() in (\"jpg\", \"jpeg\"):\n+                image.save(output_bytes, format=\"JPEG\", exif = exif_bytes, quality=opts.jpeg_quality)\n+            else:\n+                image.save(output_bytes, format=\"WEBP\", exif = exif_bytes, quality=opts.jpeg_quality)\n+\n+        else:\n+            raise HTTPException(status_code=500, detail=\"Invalid image format\")\n \n-        image.save(\n-            output_bytes, \"PNG\", pnginfo=(metadata if use_metadata else None)\n-        )\n         bytes_data = output_bytes.getvalue()\n+\n     return base64.b64encode(bytes_data)\n \n def api_middleware(app: FastAPI):\n", "test_patch": "", "problem_statement": "[Feature Request]: Add image format selection to API\n### Is there an existing issue for this?\n\n- [X] I have searched the existing issues and checked the recent builds/commits\n\n### What would your feature do ?\n\nAdd format selection to endpoints that return images.\r\nPng files can get very big and add considerable latency when using api over slow connections.\r\nJpg files are usually much smaller and don't lose too much quality.\n\n### Proposed workflow\n\n`requests.post('http://127.0.0.1:7860/sdapi/v1/txt2img' , json={'prompt': 'cat', 'format': 'jpg'})`\r\nShould return a picture of cat in jpg format instead of png.\n\n### Additional information\n\n_No response_\n", "hints_text": "you can already post to `sdapi/options` and set `samples_format` and `grid_format` to `jpg`\r\n\nI tried that, it still returns png.\r\n```\r\nrequests.get(\"http://127.0.0.1:7860/sdapi/v1/options\").json()[\"samples_format\"]\r\nOut[29]: 'jpg'\r\n\r\nrequests.get(\"http://127.0.0.1:7860/sdapi/v1/options\").json()[\"grid_format\"]\r\nOut[30]: 'jpg'\r\n\r\nr = requests.post(\"http://127.0.0.1:7860/sdapi/v1/txt2img\", json={\"prompt\": \"cat\"})\r\n\r\nbase64.decodebytes(r.json()[\"images\"][0].encode())[:10]\r\nOut[32]: b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00'\r\n```\nyou're right, i just took a look: `encode_pil_to_base64` in `modules/api.py` always forces png.  \r\npassing additional arg in api request would be messy as it would require to modify all endpoints.  \r\nbut it would be easy to add to one more `options`, something like `api_format` (like we already have `samples_format`) so its only used within encode function.  \r\n\nThat would be great, or just make samples_format also apply to api.\r\nDo I need to make a separate issue for this, or edit this one, or leave it as it is?\nleave it as-is, i'll do it...", "created_at": "2023-01-23T15:15:23Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 7093, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-7093", "issue_numbers": ["7048"], "base_commit": "c98cb0f8ecc904666f47684e238dd022039ca16f", "patch": "diff --git a/modules/ui.py b/modules/ui.py\nindex eb4b7e6b765..43bcb7e50a1 100644\n--- a/modules/ui.py\n+++ b/modules/ui.py\n@@ -1942,11 +1942,11 @@ def reload_javascript():\n     if cmd_opts.theme is not None:\r\n         inline += f\"set_theme('{cmd_opts.theme}');\"\r\n \r\n-    head += f'<script type=\"text/javascript\">{inline}</script>\\n'\r\n-\r\n     for script in modules.scripts.list_scripts(\"javascript\", \".js\"):\r\n         head += f'<script type=\"text/javascript\" src=\"file={script.path}\"></script>\\n'\r\n \r\n+    head += f'<script type=\"text/javascript\">{inline}</script>\\n'\r\n+\r\n     def template_response(*args, **kwargs):\r\n         res = shared.GradioTemplateResponseOriginal(*args, **kwargs)\r\n         res.body = res.body.replace(b'</head>', f'{head}</head>'.encode(\"utf8\"))\r\n", "test_patch": "", "problem_statement": "[Bug]: dark theme not working in recent commit\n### Is there an existing issue for this?\n\n- [X] I have searched the existing issues and checked the recent builds/commits\n\n### What happened?\n\nDark theme is broken, only light theme works\n\n### Steps to reproduce the problem\n\n1. check out any commit past 50059ea661b63967b217e687819cf7a9081e4a0c\r\n2. run with --theme=dark\n\n### What should have happened?\n\ntheme should be dark\n\n### Commit where the problem happens\n\n50059ea661b63967b217e687819cf7a9081e4a0c\n\n### What platforms do you use to access UI ?\n\nLinux\n\n### What browsers do you use to access the UI ?\n\nMozilla Firefox\n\n### Command Line Arguments\n\n```Shell\n--xformers --theme=dark\n```\n\n\n### Additional information, context and logs\n\n_No response_\n", "hints_text": "The '--theme dark' form does not work either\nSame here. Doesn't work. Light theme hurts my eyes.\nYou can use: http://127.0.0.1:7860/?__theme=dark", "created_at": "2023-01-23T08:57:11Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 6910, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-6910", "issue_numbers": ["6878"], "base_commit": "bb0978ecfd3177d0bfd7cacd1ac8796d7eec2d79", "patch": "diff --git a/modules/extras.py b/modules/extras.py\nindex 22668fcda32..88eea22eabb 100644\n--- a/modules/extras.py\n+++ b/modules/extras.py\n@@ -287,10 +287,19 @@ def get_difference(theta1, theta2):\n     def add_difference(theta0, theta1_2_diff, alpha):\r\n         return theta0 + (alpha * theta1_2_diff)\r\n \r\n+    if not primary_model_name:\r\n+        shared.state.textinfo = \"Failed: Merging requires a primary model.\"\r\n+        shared.state.end()\r\n+        return [\"Failed: Merging requires a primary model.\"] + [gr.Dropdown.update(choices=sd_models.checkpoint_tiles()) for _ in range(4)]\r\n+\r\n     primary_model_info = sd_models.checkpoints_list[primary_model_name]\r\n+\r\n+    if not secondary_model_name:\r\n+        shared.state.textinfo = \"Failed: Merging requires a secondary model.\"\r\n+        shared.state.end()\r\n+        return [\"Failed: Merging requires a secondary model.\"] + [gr.Dropdown.update(choices=sd_models.checkpoint_tiles()) for _ in range(4)]\r\n+    \r\n     secondary_model_info = sd_models.checkpoints_list[secondary_model_name]\r\n-    tertiary_model_info = sd_models.checkpoints_list.get(tertiary_model_name, None)\r\n-    result_is_inpainting_model = False\r\n \r\n     theta_funcs = {\r\n         \"Weighted sum\": (None, weighted_sum),\r\n@@ -298,10 +307,14 @@ def add_difference(theta0, theta1_2_diff, alpha):\n     }\r\n     theta_func1, theta_func2 = theta_funcs[interp_method]\r\n \r\n-    if theta_func1 and not tertiary_model_info:\r\n+    if theta_func1 and not tertiary_model_name:\r\n         shared.state.textinfo = \"Failed: Interpolation method requires a tertiary model.\"\r\n         shared.state.end()\r\n-        return [\"Failed: Interpolation method requires a tertiary model.\"] + [gr.Dropdown.update(choices=sd_models.checkpoint_tiles()) for _ in range(4)]\r\n+        return [f\"Failed: Interpolation method ({interp_method}) requires a tertiary model.\"] + [gr.Dropdown.update(choices=sd_models.checkpoint_tiles()) for _ in range(4)]\r\n+    \r\n+    tertiary_model_info = sd_models.checkpoints_list[tertiary_model_name] if theta_func1 else None\r\n+\r\n+    result_is_inpainting_model = False\r\n \r\n     shared.state.textinfo = f\"Loading {secondary_model_info.filename}...\"\r\n     print(f\"Loading {secondary_model_info.filename}...\")\r\n", "test_patch": "", "problem_statement": "[Bug]:   Error merging checkpoints: unhashable type: 'list'  \n### Is there an existing issue for this?\n\n- [X] I have searched the existing issues and checked the recent builds/commits\n\n### What happened?\n\nAS of today, receiving the following error trying to merge checkpoints.\r\nError merging checkpoints: unhashable type: 'list'\n\n### Steps to reproduce the problem\n\n1. Go to Checkpoint merger\r\n2. Select 2 models to merge\r\n3. Set weights\r\n4. click Merge\r\n\n\n### What should have happened?\n\nNew merged CPKT file should have been created.\n\n### Commit where the problem happens\n\nCommit hash: 6faae2323963f9b0e0086a85b9d0472a24fbaa73\n\n### What platforms do you use to access UI ?\n\nWindows\n\n### What browsers do you use to access the UI ?\n\nBrave\n\n### Command Line Arguments\n\n_No response_\n\n### Additional information, context and logs\n\n_No response_\n", "hints_text": "I'm getting the same issue.\nSame issue here!\nI have the same issue. Thanks. \r\n\r\nError loading/saving model file:\r\nTraceback (most recent call last):\r\n  File \"C:\\A1111\\stable-diffusion-webui\\modules\\ui.py\", line 1713, in modelmerger\r\n    results = modules.extras.run_modelmerger(*args)\r\n  File \"C:\\A1111\\stable-diffusion-webui\\modules\\extras.py\", line 292, in run_modelmerger\r\n    tertiary_model_info = sd_models.checkpoints_list.get(tertiary_model_name, None)\r\nTypeError: unhashable type: 'list'\r\n\nJust tried and it's not working right now; it was working last time I booted it up 10 hours ago,\ngit reset --hard dac59b9\r\n\r\nthis worked for me, just reverting changes, make sure not to git pull tho!\r\nill check for commits that fix this\nseems to be an issue where the checkpoint merger is demanding a third model even for Weighted sum.\nSame issue\ud83d\ude48\nI got this issue as well, seems the Tertiary drop down menu needs to have a model selected in order to merge 2 models. If the Tertiary menu is left blank it throws this error.\n> I got this issue as well, seems the Tertiary drop down menu needs to have a model selected in order to merge 2 models. If the Tertiary menu is left blank it throws this error.\r\n\r\nCan confirm.  Once I put an arbitrary model value into the Tertiary field the merge worked as expected.\nCan also confirm it's an issue with the merger expecting a third model in the C slot even for weighted sum, setting one makes it work as usual, C model is ignored for weighted sum.", "created_at": "2023-01-19T00:15:59Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 6895, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-6895", "issue_numbers": ["835"], "base_commit": "889b851a5260ce869a3286ad15d17d1bbb1da0a7", "patch": "diff --git a/javascript/ui.js b/javascript/ui.js\nindex 954beadda45..7d3d57a3c38 100644\n--- a/javascript/ui.js\n+++ b/javascript/ui.js\n@@ -109,6 +109,13 @@ function get_extras_tab_index(){\n     return [get_tab_index('mode_extras'), get_tab_index('extras_resize_mode'), ...args]\n }\n \n+function get_img2img_tab_index() {\n+    let res = args_to_array(arguments)\n+    res.splice(-2)\n+    res[0] = get_tab_index('mode_img2img')\n+    return res\n+}\n+\n function create_submit_args(args){\n     res = []\n     for(var i=0;i<args.length;i++){\ndiff --git a/modules/ui.py b/modules/ui.py\nindex 20b661658ed..78c0c92a483 100644\n--- a/modules/ui.py\n+++ b/modules/ui.py\n@@ -204,9 +204,31 @@ def apply_styles(prompt, prompt_neg, style1_name, style2_name):\n     return [gr.Textbox.update(value=prompt), gr.Textbox.update(value=prompt_neg), gr.Dropdown.update(value=\"None\"), gr.Dropdown.update(value=\"None\")]\r\n \r\n \r\n+def process_interrogate(interrogation_function, mode, ii_input_dir, ii_output_dir, *ii_singles):\r\n+    if mode in {0, 1, 3, 4}:\r\n+        return [interrogation_function(ii_singles[mode]), None]\r\n+    elif mode == 2:\r\n+        return [interrogation_function(ii_singles[mode][\"image\"]), None]\r\n+    elif mode == 5:\r\n+        assert not shared.cmd_opts.hide_ui_dir_config, \"Launched with --hide-ui-dir-config, batch img2img disabled\"\r\n+        images = shared.listfiles(ii_input_dir)\r\n+        print(f\"Will process {len(images)} images.\")\r\n+        if ii_output_dir != \"\":\r\n+            os.makedirs(ii_output_dir, exist_ok=True)\r\n+        else:\r\n+            ii_output_dir = ii_input_dir\r\n+\r\n+        for image in images:\r\n+            img = Image.open(image)\r\n+            filename = os.path.basename(image)\r\n+            left, _ = os.path.splitext(filename)\r\n+            print(interrogation_function(img), file=open(os.path.join(ii_output_dir, left + \".txt\"), 'a'))\r\n+\r\n+        return [gr_show(True), None]\r\n+\r\n+\r\n def interrogate(image):\r\n     prompt = shared.interrogator.interrogate(image.convert(\"RGB\"))\r\n-\r\n     return gr_show(True) if prompt is None else prompt\r\n \r\n \r\n@@ -983,19 +1005,33 @@ def select_img2img_tab(tab):\n                 show_progress=False,\r\n             )\r\n \r\n+            interrogate_args = dict(\r\n+                _js=\"get_img2img_tab_index\",\r\n+                inputs=[\r\n+                    dummy_component,\r\n+                    img2img_batch_input_dir,\r\n+                    img2img_batch_output_dir,\r\n+                    init_img,\r\n+                    sketch,\r\n+                    init_img_with_mask,\r\n+                    inpaint_color_sketch,\r\n+                    init_img_inpaint,\r\n+                ],\r\n+                outputs=[img2img_prompt, dummy_component],\r\n+                show_progress=False,\r\n+            )\r\n+\r\n             img2img_prompt.submit(**img2img_args)\r\n             submit.click(**img2img_args)\r\n \r\n             img2img_interrogate.click(\r\n-                fn=interrogate,\r\n-                inputs=[init_img],\r\n-                outputs=[img2img_prompt],\r\n+                fn=lambda *args : process_interrogate(interrogate, *args),\r\n+                **interrogate_args,\r\n             )\r\n \r\n             img2img_deepbooru.click(\r\n-                fn=interrogate_deepbooru,\r\n-                inputs=[init_img],\r\n-                outputs=[img2img_prompt],\r\n+                fn=lambda *args : process_interrogate(interrogate_deepbooru, *args),\r\n+                **interrogate_args,\r\n             )\r\n \r\n             prompts = [(txt2img_prompt, txt2img_negative_prompt), (img2img_prompt, img2img_negative_prompt)]\r\n", "test_patch": "", "problem_statement": "Interrogate not working [colab]\nUsing the interrogate function crashes the webui.\r\n\r\nI'm using the colab notebook to run it. \r\n\r\nThis has been happening since a week when I started using the webui.\r\n\r\nThis is the Error I get:\r\n```\r\nError interrogating\r\nTraceback (most recent call last):\r\n  File \"/content/stable-diffusion-webui/modules/interrogate.py\", line 136, in interrogate\r\n    caption = self.generate_caption(pil_image)\r\n  File \"/content/stable-diffusion-webui/modules/interrogate.py\", line 121, in generate_caption\r\n    caption = self.blip_model.generate(gpu_image, sample=False, num_beams=shared.opts.interrogate_clip_num_beams, min_length=shared.opts.interrogate_clip_min_length, max_length=shared.opts.interrogate_clip_max_length)\r\n  File \"/content/stable-diffusion-webui/repositories/BLIP/models/blip.py\", line 163, in generate\r\n    **model_kwargs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\", line 27, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/transformers/generation_utils.py\", line 1146, in generate\r\n    self._validate_model_kwargs(model_kwargs.copy())\r\n  File \"/usr/local/lib/python3.7/dist-packages/transformers/generation_utils.py\", line 862, in _validate_model_kwargs\r\n    f\"The following `model_kwargs` are not used by the model: {unused_model_args} (note: typos in the\"\r\nValueError: The following `model_kwargs` are not used by the model: ['encoder_hidden_states', 'encoder_attention_mask'] (note: typos in the generate arguments will also show up in this list)\r\n\r\nTraceback (most recent call last):\r\n  File \"/content/stable-diffusion-webui/modules/interrogate.py\", line 136, in interrogate\r\n    caption = self.generate_caption(pil_image)\r\n  File \"/content/stable-diffusion-webui/modules/interrogate.py\", line 121, in generate_caption\r\n    caption = self.blip_model.generate(gpu_image, sample=False, num_beams=shared.opts.interrogate_clip_num_beams, min_length=shared.opts.interrogate_clip_min_length, max_length=shared.opts.interrogate_clip_max_length)\r\n  File \"/content/stable-diffusion-webui/repositories/BLIP/models/blip.py\", line 163, in generate\r\n    **model_kwargs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\", line 27, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/transformers/generation_utils.py\", line 1146, in generate\r\n    self._validate_model_kwargs(model_kwargs.copy())\r\n  File \"/usr/local/lib/python3.7/dist-packages/transformers/generation_utils.py\", line 862, in _validate_model_kwargs\r\n    f\"The following `model_kwargs` are not used by the model: {unused_model_args} (note: typos in the\"\r\nValueError: The following `model_kwargs` are not used by the model: ['encoder_hidden_states', 'encoder_attention_mask'] (note: typos in the generate arguments will also show up in this list)\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.7/dist-packages/gradio/routes.py\", line 274, in run_predict\r\n    fn_index, raw_input, username, session_state, iterators\r\n  File \"/usr/local/lib/python3.7/dist-packages/gradio/blocks.py\", line 753, in process_api\r\n    result = await self.call_function(fn_index, inputs, iterator)\r\n  File \"/usr/local/lib/python3.7/dist-packages/gradio/blocks.py\", line 631, in call_function\r\n    block_fn.fn, *processed_input, limiter=self.limiter\r\n  File \"/usr/local/lib/python3.7/dist-packages/anyio/to_thread.py\", line 32, in run_sync\r\n    func, *args, cancellable=cancellable, limiter=limiter\r\n  File \"/usr/local/lib/python3.7/dist-packages/anyio/_backends/_asyncio.py\", line 937, in run_sync_in_worker_thread\r\n    return await future\r\n  File \"/usr/local/lib/python3.7/dist-packages/anyio/_backends/_asyncio.py\", line 867, in run\r\n    result = context.run(func, *args)\r\n  File \"/content/stable-diffusion-webui/modules/ui.py\", line 252, in interrogate\r\n    prompt = shared.interrogator.interrogate(image)\r\n  File \"/content/stable-diffusion-webui/modules/interrogate.py\", line 163, in interrogate\r\n    res += \"<error>\"\r\nTypeError: unsupported operand type(s) for +=: 'NoneType' and 'str'\r\n```\r\n\r\n\n", "hints_text": "I forgot to mention: After the webui crashes I'm not ablet to restart it anymore from within the same colab session. I have to delete/restart the whole runtime.", "created_at": "2023-01-18T17:51:11Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 6772, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-6772", "issue_numbers": ["6738"], "base_commit": "ebfdd7baeb660ec66f78e1a0e5e45442025f262d", "patch": "diff --git a/modules/hashes.py b/modules/hashes.py\nindex 14231771ca4..b85a75800ca 100644\n--- a/modules/hashes.py\n+++ b/modules/hashes.py\n@@ -34,9 +34,10 @@ def cache(subsection):\n \r\n def calculate_sha256(filename):\r\n     hash_sha256 = hashlib.sha256()\r\n+    blksize = 1024 * 1024\r\n \r\n     with open(filename, \"rb\") as f:\r\n-        for chunk in iter(lambda: f.read(4096), b\"\"):\r\n+        for chunk in iter(lambda: f.read(blksize), b\"\"):\r\n             hash_sha256.update(chunk)\r\n \r\n     return hash_sha256.hexdigest()\r\n", "test_patch": "", "problem_statement": "[Bug]: New SHA256 hash takes extremely long time up to a point of of model load being unusable\n### Is there an existing issue for this?\r\n\r\n- [X] I have searched the existing issues and checked the recent builds/commits\r\n\r\n### What happened?\r\n\r\nNewly added sha-256 hash takes extremely long time to calculate on model load up to a point where loading appears to hang (i've restarted server twice before i even let it run until completion)  \r\n\r\nPreviously switching to a new model was sub 10 sec, now switching to a new model (that does not have hash stored already) takes 100-150 sec (and this is a high end system)!\r\n\r\nAnd to make it worse, messages about hash calculation are only printed **after** it has been calculated, there is no progress info or anything to indicate system is actually doing anything for 2 min!\r\n\r\n\r\n### Steps to reproduce the problem\r\n\r\n1. Switch to a new model and wait for completion - it takes forever\r\n\r\n\r\n### What should have happened?\r\n\r\nModel load should **never** take over 2 minutes to complete.\r\n\r\n### Commit where the problem happens\r\n\r\nf8c512478568293155539f616dce26c5e4495055\r\n\r\n### What platforms do you use to access UI ?\r\n\r\nWindows, Linux\r\n\r\n### What browsers do you use to access the UI ?\r\n\r\nGoogle Chrome, Microsoft Edge\r\n\r\n### Command Line Arguments\r\n\r\n```Shell\r\n--api --xformers\r\n```\r\n\r\n\r\n### Additional information, context and logs\r\n\r\nConsole log showing model load taking 142 seconds!\r\n\r\n```text\r\nCalculating sha256 for /home/vlado/dev/automatic/models/Stable-diffusion/mood-beautyreal-v01.ckpt: bcc0afd3b264ea028928187f56f70840f8d87ccf283b020982beba35d9c7e4ef\r\nLoading weights [bcc0afd3b2] from /home/vlado/dev/automatic/models/Stable-diffusion/mood-beautyreal-v01.ckpt\r\nCouldn't find VAE named vae-ft-mse-840000-ema-pruned; using None instead\r\nApplying xformers cross attention optimization.\r\nWeights loaded in 142.6s.\r\n```\r\n\n", "hints_text": "Odd, how big is that model file? I'm getting nowhere near the time you do. On a 8GB file:\r\n```\r\nCalculating sha256 for C:\\tools\\Ai\\stable-diffusion-webui\\models\\Stable-diffusion\\protogenX58RebuiltSc_10.ckpt: 710fc74d4cf3245cf5f1664f25502da0c7750f80e8bbbb56929e710043d84efa\r\nLoading weights [710fc74d4c] from C:\\tools\\Ai\\stable-diffusion-webui\\models\\Stable-diffusion\\protogenX58RebuiltSc_10.ckpt\r\nCouldn't find VAE named vae-ft-mse-840000-ema-pruned.vae; using None instead\r\nApplying xformers cross attention optimization.\r\nModel loaded in 16.2s (0.5s create model, 15.6s load weights).\r\n```\r\n\r\nIs your model on ssd?\nI'm getting 80sec for 2gb model, 100sec for 4gb model and 140sec for 7gb model.\n\nDisk is as fast as possible, over 1. 5GBytes/sec.\n\nOnly difference is that disk is mounted over network loopback (shared between VMs), so maaaaaaybe sha method does some strange byterange access so it does full access on each range? But if it does, that's a broken file read inside sha method.\n\nAnd just to confirm - I was having sub 10sec times before the change - thats 10x slowdown!\n\nIt is taking longer for me too.. I also have the models on my ssd.. it's taking like 5 more times to load now...\r\n\r\nAlso, the hash number has disappeared from the model's name list? \n> Only difference is that disk is mounted over network loopback (shared between VMs), so maaaaaaybe sha method does some strange byterange access so it does full access on each range?\r\n\r\nCan you try changing this function in hashes.py from:\r\n```\r\ndef calculate_sha256(filename):\r\n    hash_sha256 = hashlib.sha256()\r\n\r\n    with open(filename, \"rb\") as f:\r\n        for chunk in iter(lambda: f.read(4096), b\"\"):\r\n            hash_sha256.update(chunk)\r\n\r\n    return hash_sha256.hexdigest()\r\n\r\n```\r\nto:\r\n```\r\ndef calculate_sha256(filename):\r\n    import mmap\r\n\r\n    hash_sha256 = hashlib.sha256()\r\n\r\n    with open(filename, \"rb\") as f:\r\n        mmapped = mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ)\r\n        while True:\r\n            chunk = mmapped.read(4096)\r\n            if not chunk:\r\n                break\r\n            hash_sha256.update(chunk)\r\n\r\n    return hash_sha256.hexdigest()\r\n\r\n```\r\nPerhaps memory-mapping can help in this case.\nI have some Error - \"Couldn't find VAE named vae-ft-mse-840000-ema-pruned; using None instead\"\r\nwhat is the problem?\nI figured it out, the next git pull in the settings reset the link to VAE simply\n@AlUlkesh \r\n\r\ni've tried your code with `mmap`, no change in performance.\r\nlooking at user vs sys time and actual io performance, 99% of time is spent inside python and my actual disk i/o is really slow at only 50mb/sec.\r\n\r\nthen i've switched to `byteranges` and omg its sooooo much faster - down from 100sec to 4sec!  \r\n(and my disk i/o went from 50mb/sec to 600mb/sec)\r\n\r\n```python\r\n    import io\r\n    chunk = bytearray(1024 * 1024)\r\n    with io.open(filename, \"rb\", 0) as f:\r\n        while True:\r\n            numread = f.readinto(chunk)\r\n            if not numread:\r\n                break\r\n            hash_sha256.update(chunk)\r\n```\r\n\r\ni'll create a pr for this.\nI did a fresh install and now loading models speed is normal again... but the hash is still gone\n@vladmandic Great, for me this cuts down the hash calculation from 9 to 4 seconds on a 8GB model.\r\n\r\nBut I seem to get a different hash then before (with model protogenX58RebuiltSc_10.ckpt)\r\n\r\nBefore I got 710fc74d4cf3245cf5f1664f25502da0c7750f80e8bbbb56929e710043d84efa\r\nNow it's 4d69515e3de52a4b0259ee5922c833f5e13deb8efb6c9ad500f1dda173c892d0\r\n\r\nChecking with an external tool, 710fc74d4cf3245cf5f1664f25502da0c7750f80e8bbbb56929e710043d84efa seems to be correct.\r\n\r\nAny idea what could be responsible for the change?\n@vladmandic Oh, I think I know what it is. The new version always uses the full 1024 * 1024 bytes to update the sha256. That means the last chunk will pass bytes that have not been read.\r\n\r\nSomething like this should solve the issue:\r\n`hash_sha256.update(chunk[:numread])`\r\n\n- [ ] @AlUlkesh \r\n\r\ni've started experimenting with \r\n\r\n- `open` + `read`\r\n- `open` + `mmap.read`\r\n- `io.open` + `read`\r\n- `io.open` + `readinto`\r\n\r\nand different blocksizes.\r\n\r\nthe fastest combination is simplest routine as-in, but with larger blocksize.\r\nbasically, with 4k chunks python is taking forerver in inner loop and actual io is not even close to saturate disk\r\nand with 1mb chunk, performance goes up 10x (my actual hash calc is down to ~3sec)\r\n\r\ni don't see why we would use 4kb chunks to start with? lets just do read with larger blksize and thats it.\r\n", "created_at": "2023-01-15T14:46:57Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 6685, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-6685", "issue_numbers": ["6640"], "base_commit": "d7aec59c4eb02f723b3d55c6f927a42e97acd679", "patch": "diff --git a/modules/generation_parameters_copypaste.py b/modules/generation_parameters_copypaste.py\nindex 620aa606997..593d99ef553 100644\n--- a/modules/generation_parameters_copypaste.py\n+++ b/modules/generation_parameters_copypaste.py\n@@ -7,7 +7,7 @@\n \r\n import gradio as gr\r\n from modules.shared import script_path\r\n-from modules import shared, ui_tempdir\r\n+from modules import shared, ui_tempdir, script_callbacks\r\n import tempfile\r\n from PIL import Image\r\n \r\n@@ -298,6 +298,7 @@ def paste_func(prompt):\n                     prompt = file.read()\r\n \r\n         params = parse_generation_parameters(prompt)\r\n+        script_callbacks.infotext_pasted_callback(prompt, params)\r\n         res = []\r\n \r\n         for output, key in paste_fields:\r\ndiff --git a/modules/script_callbacks.py b/modules/script_callbacks.py\nindex 608c5300ecf..a9e192369e2 100644\n--- a/modules/script_callbacks.py\n+++ b/modules/script_callbacks.py\n@@ -2,7 +2,7 @@\n import traceback\r\n from collections import namedtuple\r\n import inspect\r\n-from typing import Optional\r\n+from typing import Optional, Dict, Any\r\n \r\n from fastapi import FastAPI\r\n from gradio import Blocks\r\n@@ -71,6 +71,7 @@ def __init__(self, imgs, cols, rows):\n     callbacks_before_component=[],\r\n     callbacks_after_component=[],\r\n     callbacks_image_grid=[],\r\n+    callbacks_infotext_pasted=[],\r\n     callbacks_script_unloaded=[],\r\n )\r\n \r\n@@ -172,6 +173,14 @@ def image_grid_callback(params: ImageGridLoopParams):\n             report_exception(c, 'image_grid')\r\n \r\n \r\n+def infotext_pasted_callback(infotext: str, params: Dict[str, Any]):\r\n+    for c in callback_map['callbacks_infotext_pasted']:\r\n+        try:\r\n+            c.callback(infotext, params)\r\n+        except Exception:\r\n+            report_exception(c, 'infotext_pasted')\r\n+\r\n+\r\n def script_unloaded_callback():\r\n     for c in reversed(callback_map['callbacks_script_unloaded']):\r\n         try:\r\n@@ -290,6 +299,15 @@ def on_image_grid(callback):\n     add_callback(callback_map['callbacks_image_grid'], callback)\r\n \r\n \r\n+def on_infotext_pasted(callback):\r\n+    \"\"\"register a function to be called before applying an infotext.\r\n+    The callback is called with two arguments:\r\n+       - infotext: str - raw infotext.\r\n+       - result: Dict[str, any] - parsed infotext parameters.\r\n+    \"\"\"\r\n+    add_callback(callback_map['callbacks_infotext_pasted'], callback)\r\n+\r\n+\r\n def on_script_unloaded(callback):\r\n     \"\"\"register a function to be called before the script is unloaded. Any hooks/hijacks/monkeying about that\r\n     the script did should be reverted here\"\"\"\r\n", "test_patch": "", "problem_statement": "[Feature Request]: Script API for modifying pasted infotext (in case of missing parameters, etc.)\n### Is there an existing issue for this?\r\n\r\n- [X] I have searched the existing issues and checked the recent builds/commits\r\n\r\n### What would your feature do ?\r\n\r\nThe webui code has some special logic in `parse_generation_parameters` for fixing up the infotext if some of the fields are missing, like restoring the default Clip Skip/hypernet strength values. Extensions can add their own infotext, but currently there is no way for them to run their own custom \"fix infotext\" code in case they need the same\r\n\r\nFor example, the additional_networks extension attaches up to 5 models/weights to the infotext, omitting any where the model is None or the weight is 0. When the infotext is pasted, if an infotext parameter is missing a default `gr.update()` is passed, so in that case the unused sliders for those parameters are not cleared from the previous settings\r\n\r\n### Proposed workflow\r\n\r\nAdd a `Script.fix_generation_parameters()` or similar method to the scripting interface for modifying the infotext when a paste is detected.\r\n\r\n### Additional information\r\n\r\n_No response_\n", "hints_text": "", "created_at": "2023-01-12T21:52:36Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 6635, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-6635", "issue_numbers": ["6475"], "base_commit": "1a23dc32ac5e16fac10115cafd0b841abd06e59f", "patch": "diff --git a/javascript/imageviewer.js b/javascript/imageviewer.js\nindex b7bc2fe132f..1f29ad7bb67 100644\n--- a/javascript/imageviewer.js\n+++ b/javascript/imageviewer.js\n@@ -151,6 +151,7 @@ function showGalleryImage() {\n                     e.addEventListener('mousedown', function (evt) {\n                         if(!opts.js_modal_lightbox || evt.button != 0) return;\n                         modalZoomSet(gradioApp().getElementById('modalImage'), opts.js_modal_lightbox_initially_zoomed)\n+                        evt.preventDefault()\n                         showModal(evt)\n                     }, true);\n                 }\n", "test_patch": "", "problem_statement": "[Bug]: Keyboard input is now ignored immediately after an image is maximized in the UI\n### Is there an existing issue for this?\r\n\r\n- [X] I have searched the existing issues and checked the recent builds/commits\r\n\r\n### What happened?\r\n\r\nIn txt2img, for example, create 3 images of anything.  When they finish, click on one of the images to maximize it in the UI.  \r\n\r\nPreviously, the UI would take keyboard input at this point as expected.  The ESC key would close the image preview and the keyboard arrow keys would go forward/back.\r\n\r\nNow the behavior is that keyboard input is ignored UNTIL the user first manually clicks (with the mouse) one of the on-screen arrow keys.  Only after this step can the user go back to using the keyboard arrow keys or ESC to close.\r\n\r\nUPDATE:  The keyboard behavior appears to have been broken in Chrome via this commit:\r\nhttps://github.com/AUTOMATIC1111/stable-diffusion-webui/commit/b663ee2cff6831354e1b5326800c8d1bf300cafe\r\n\r\n### Steps to reproduce the problem\r\n\r\n1. In txt2img\r\n2. Create 3 images of anything\r\n3. Click on one image to maximize it in the UI\r\n4. Notice that keyboard input is ignored (arrow keys and ESC do nothing)\r\n5. [Get frustrated]\r\n6. Now click one of on-screen arrows on either side of the image\r\n7. Notice that the keyboard input is now valid (arrow keys move through images and ESC closes)\r\n\r\n\r\n### What should have happened?\r\n\r\nDESIRED BEHAVIOR\r\nWhen an image from a group is selected and maximized, keyboard input should be immediately valid, as it had always been.  This change in desired behavior is very recent. \r\n\r\n### Commit where the problem happens\r\n\r\n151233399c4b79934bdbb7c12a97eeb6499572fb\r\n\r\n### What platforms do you use to access UI ?\r\n\r\nWindows\r\n\r\n### What browsers do you use to access the UI ?\r\n\r\nGoogle Chrome\r\n\r\n### Command Line Arguments\r\n\r\n_No response_\r\n\r\n### Additional information, context and logs\r\n\r\n_No response_\n", "hints_text": "> ### Is there an existing issue for this?\r\n> * [x]  I have searched the existing issues and checked the recent builds/commits\r\n> \r\n> ### What happened?\r\n> In txt2img, for example, create 3 images of anything. When they finish, click on one of the images to maximize it in the UI.\r\n> \r\n> Previously, the UI would take keyboard input at this point as expected. The ESC key would close the image preview and the keyboard arrow keys would go forward/back.\r\n> \r\n> Now the behavior is that keyboard input is ignored UNTIL the user first manually clicks (with the mouse) one of the on-screen arrow keys. Only after this step can the user go back to using the keyboard arrow keys or ESC to close.\r\n> \r\n> UPDATE: The keyboard behavior appears to have been broken in Chrome via this commit: [b663ee2](https://github.com/AUTOMATIC1111/stable-diffusion-webui/commit/b663ee2cff6831354e1b5326800c8d1bf300cafe)\r\n> \r\n> ### Steps to reproduce the problem\r\n> 1. In txt2img\r\n> 2. Create 3 images of anything\r\n> 3. Click on one image to maximize it in the UI\r\n> 4. Notice that keyboard input is ignored (arrow keys and ESC do nothing)\r\n> 5. [Get frustrated]\r\n> 6. Now click one of on-screen arrows on either side of the image\r\n> 7. Notice that the keyboard input is now valid (arrow keys move through images and ESC closes)\r\n> \r\n> ### What should have happened?\r\n> DESIRED BEHAVIOR When an image from a group is selected and maximized, keyboard input should be immediately valid, as it had always been. This change in desired behavior is very recent.\r\n> \r\n> ### Commit where the problem happens\r\n> [1512333](https://github.com/AUTOMATIC1111/stable-diffusion-webui/commit/151233399c4b79934bdbb7c12a97eeb6499572fb)\r\n> \r\n> ### What platforms do you use to access UI ?\r\n> Windows\r\n> \r\n> ### What browsers do you use to access the UI ?\r\n> Google Chrome\r\n> \r\n> ### Command Line Arguments\r\n> _No response_\r\n> \r\n> ### Additional information, context and logs\r\n> _No response_\r\n\r\nJust revert to an older commit, it's a waste of time for chrome users ", "created_at": "2023-01-11T08:47:41Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 6612, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-6612", "issue_numbers": ["6608"], "base_commit": "50fb20cedc8dcbf64f86aed6d6e89595d655e638", "patch": "diff --git a/webui.bat b/webui.bat\nindex d4d626e2365..3a3e310ab57 100644\n--- a/webui.bat\n+++ b/webui.bat\n@@ -1,7 +1,7 @@\n @echo off\r\n \r\n if not defined PYTHON (set PYTHON=python)\r\n-if not defined VENV_DIR (set VENV_DIR=venv)\r\n+if not defined VENV_DIR (set VENV_DIR=%~dp0\\venv)\r\n \r\n set ERROR_REPORTING=FALSE\r\n \r\n@@ -26,7 +26,7 @@ echo Unable to create venv in directory %VENV_DIR%\n goto :show_stdout_stderr\r\n \r\n :activate_venv\r\n-set PYTHON=\"%~dp0%VENV_DIR%\\Scripts\\Python.exe\"\r\n+set PYTHON=\"%VENV_DIR%\\Scripts\\Python.exe\"\r\n echo venv %PYTHON%\r\n if [%ACCELERATE%] == [\"True\"] goto :accelerate\r\n goto :launch\r\n@@ -35,7 +35,7 @@ goto :launch\n \r\n :accelerate\r\n echo \"Checking for accelerate\"\r\n-set ACCELERATE=\"%~dp0%VENV_DIR%\\Scripts\\accelerate.exe\"\r\n+set ACCELERATE=\"%VENV_DIR%\\Scripts\\accelerate.exe\"\r\n if EXIST %ACCELERATE% goto :accelerate_launch\r\n \r\n :launch\r\n", "test_patch": "", "problem_statement": "[Bug]: VENV cannot be initialized\n### Is there an existing issue for this?\r\n\r\n- [x] I have searched the existing issues and checked the recent builds/commits\r\n\r\n### What happened?\r\n\r\n1. Updated `webui_user.bat` with actual exec paths:\r\n```bat\r\n@echo off\r\n\r\nset \"PYTHON=%PYTHON_HOME%\\python.exe\"\r\nset \"GIT=%GIT_HOME%\\bin\\git.exe\"\r\nset \"VENV_DIR=D:\\Programs\\StableDiffusionWebUI\\venv\"\r\nset COMMANDLINE_ARGS=\r\n\r\ncall webui.bat\r\n```\r\n2. Open CMD terminal and run the `webui.bat`:\r\n```\r\nvenv \"D:\\Programs\\StableDiffusionWebUI\\D:\\Programs\\StableDiffusionWebUI\\venv\\Scripts\\Python.exe\"\r\nThe filename, directory name, or volume label syntax is incorrect.\r\nPress any key to continue . . .\r\n```\r\n\r\n### Steps to reproduce the problem\r\n\r\n1. Clone the repo ( latest )\r\n2. Update `webui.bat`\r\n3. Add GAN* models\r\n4. Run `webui.bat`\r\n\r\n\r\n### What should have happened?\r\n\r\nPython VENV to be created\r\n\r\n### Commit where the problem happens\r\n\r\na0ef416aa769022ce9e97dcc87f88a0ae9e6cc58\r\n\r\n### What platforms do you use to access UI ?\r\n\r\nWindows\r\n\r\n### What browsers do you use to access the UI ?\r\n\r\nMozilla Firefox\r\n\r\n### Command Line Arguments\r\n\r\n```Shell\r\nset \"PYTHON=%PYTHON_HOME%\\python.exe\" :: D:\\Programs\\Python\r\nset \"GIT=%GIT_HOME%\\bin\\git.exe\"      :: C:\\Program Files\\Git\\bin\r\nset \"VENV_DIR=D:\\Programs\\StableDiffusionWebUI\\venv\"\r\n```\r\n\r\n### Additional information, context and logs\r\nPython is added to path\r\n```\r\nC:\\Users\\DVD>python\r\nPython 3.11.1 (tags/v3.11.1:a7a450f, Dec  6 2022, 19:58:39) [MSC v.1934 64 bit (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>>\r\n```\n", "hints_text": "Doesnt this path seem suspicious to you? `\"D:\\Programs\\StableDiffusionWebUI\\D:\\Programs\\StableDiffusionWebUI\\venv\\Scripts\\Python.exe\"`\r\nNo need to specify venv if it is already inside your folder\r\n~~And did you run `webui.bat` , and not `webui-user.bat`? You should run the latter one.~~\nReproduced your error by setting full path to venv, looks like the problem is here:\r\nhttps://github.com/AUTOMATIC1111/stable-diffusion-webui/blob/master/webui.bat#L28-L33\r\nFor some reason we can only set venv path relative to webui.bat. I don't know if that's deliberate, been like this from 2 Semptember, https://github.com/AUTOMATIC1111/stable-diffusion-webui/commit/efa0a6483c9dfbdde6717bec202942c6036e72ff\r\n\r\nAs i said, it's not needed at all in your case.\nYes, it really does, the first thing I noticed, but removing it gives:\r\n```\r\nvenv \"D:\\Programs\\StableDiffusionWebUI\\venv\\Scripts\\Python.exe\"\r\nNo Python at '\"D:\\Python-3-10\\python.exe'\r\nPress any key to continue . . .\r\n```\r\nMy Python is located elsewhere also. Why does it assume the python is there. I am on a fresh Win11 installation on dedicated computer. Here I see the `venv` is aromatically set to: `if not defined VENV_DIR (set VENV_DIR=venv)`\nFound it. Will just delete `venv` and try again\r\n![image](https://user-images.githubusercontent.com/9112662/211632782-b51abd1d-6742-407e-99c2-aac7d5159478.png)\r\n\r\n\nthat's scuffed, shouldnt be like that. isnt this file created automatically when creating venv, and its pointing towards base installation?\nhttps://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Command-Line-Arguments-and-Settings#webui-user\r\n\r\naccording to wiki, what you did was correct, and somehow looking at the code i cant figure out how this might work. But i have no expertise on batch files or managing environments\nHmm, tought so too, deleted all unchecked folder and tried again. Different error this time:\r\n```\r\nCreating venv in directory venv using python \"D:\\Programs\\Python\\python.exe\"\r\nvenv \"D:\\Programs\\StableDiffusionWebUI\\venv\\Scripts\\Python.exe\"\r\nPython 3.11.1 (tags/v3.11.1:a7a450f, Dec  6 2022, 19:58:39) [MSC v.1934 64 bit (AMD64)]\r\nCommit hash: <none>\r\nInstalling torch and torchvision\r\nTraceback (most recent call last):\r\n  File \"D:\\Programs\\StableDiffusionWebUI\\launch.py\", line 307, in <module>\r\n    prepare_environment()\r\n  File \"D:\\Programs\\StableDiffusionWebUI\\launch.py\", line 218, in prepare_environment\r\n    run(f'\"{python}\" -m {torch_command}', \"Installing torch and torchvision\", \"Couldn't install torch\")\r\n  File \"D:\\Programs\\StableDiffusionWebUI\\launch.py\", line 64, in run\r\n    raise RuntimeError(message)\r\nRuntimeError: Couldn't install torch.\r\nCommand: \"D:\\Programs\\StableDiffusionWebUI\\venv\\Scripts\\python.exe\" -m pip install torch==1.12.1+cu113 torchvision==0.13.1+cu113 --extra-index-url https://download.pytorch.org/whl/cu113\r\nError code: 1\r\nstdout: Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu113\r\n\r\nstderr: ERROR: Could not find a version that satisfies the requirement torch==1.12.1+cu113 (from versions: none)\r\nERROR: No matching distribution found for torch==1.12.1+cu113\r\n\r\nPress any key to continue . . .\r\n```\n> Python 3.11.1\r\n\r\nThat is expected\nHmm. The project requires `Install [Python 3.10.6](https://www.python.org/downloads/windows/), checking \"Add Python to PATH\"`. The thing is I have different python versions and currently use 3.11 for other projects. It seems the most possible outcome is to use `set \"PYTHON=D:\\Python3-10-6\\python.exe\"` and yell the project to use that version instead. Specific for it.\nWell, it's required for a reason, as there was no build of cuda+torch for python 3.11 needed for repo to be functional. Something's coming with pytorch 2.0, but it's still a draft PR. I use 3.11 too, just installed 3.10.6 for this project separately.\nit simply cannot work as advertised on wiki, \r\nhttps://github.com/AUTOMATIC1111/stable-diffusion-webui/blob/50fb20cedc8dcbf64f86aed6d6e89595d655e638/webui.bat#L29\r\nconcatenation of path to webui.bat, path to venv you provided and \\Scripts\\Python.exe It will always result in something like \r\n`E:\\sdwebui\\E:\\1\\2\\3\\myvenv\\Scripts\\Python.exe` which always will not exist. I cannot believe it was there for 4 months and no one noticed. Either that, or we're mistaken somehow.\r\nPath to custom python is functional, you just have to install it.\nI've installed  3.10.6 in a separate folder and no wants cuda as I am on AMD GPU will skip it via:\r\n```bat\r\nset \"PYTHON=D:\\Programs\\Python\\3-10-6\\python.exe\"\r\nset \"GIT=%GIT_HOME%\\bin\\git.exe\"\r\nset \"VENV_DIR=\"\r\nset \"COMMANDLINE_ARGS=--skip-torch-cuda-test\"\r\n```\nYou're on WINDOWS and AMD GPU? that's highly experimental, i believe there's only one untested guide for that case on wiki. Link in the first sentence here.\r\nhttps://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Install-and-Run-on-AMD-GPUs\n```\r\nError completing request\r\nArguments: ('dog , Fernando Botero', '', 'None', 'None', 20, 0, False, False, 1, 1, 7, -1.0, -1.0, 0, 0, 0, False, 512, 512, False, 0.7, 2, 'Latent', 0, 0, 0, 0, False, False, False, False, '', 1, '', 0, '', True, False, False) {}\r\nTraceback (most recent call last):\r\n  File \"D:\\Programs\\StableDiffusionWebUI\\modules\\call_queue.py\", line 45, in f\r\n    res = list(func(*args, **kwargs))\r\n  File \"D:\\Programs\\StableDiffusionWebUI\\modules\\call_queue.py\", line 28, in f\r\n    res = func(*args, **kwargs)\r\n  File \"D:\\Programs\\StableDiffusionWebUI\\modules\\txt2img.py\", line 52, in txt2img\r\n    processed = process_images(p)\r\n  File \"D:\\Programs\\StableDiffusionWebUI\\modules\\processing.py\", line 479, in process_images\r\n    res = process_images_inner(p)\r\n  File \"D:\\Programs\\StableDiffusionWebUI\\modules\\processing.py\", line 597, in process_images_inner\r\n    uc = get_conds_with_caching(prompt_parser.get_learned_conditioning, negative_prompts, p.steps, cached_uc)\r\n  File \"D:\\Programs\\StableDiffusionWebUI\\modules\\processing.py\", line 565, in get_conds_with_caching\r\n    cache[1] = function(shared.sd_model, required_prompts, steps)\r\n  File \"D:\\Programs\\StableDiffusionWebUI\\modules\\prompt_parser.py\", line 138, in get_learned_conditioning\r\n    conds = model.get_learned_conditioning(texts)\r\n  File \"D:\\Programs\\StableDiffusionWebUI\\repositories\\stable-diffusion-stability-ai\\ldm\\models\\diffusion\\ddpm.py\", line 669, in get_learned_conditioning\r\n    c = self.cond_stage_model(c)\r\n  File \"D:\\Programs\\StableDiffusionWebUI\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1130, in _call_impl\r\n    return forward_call(*input, **kwargs)\r\n  File \"D:\\Programs\\StableDiffusionWebUI\\modules\\sd_hijack_clip.py\", line 220, in forward\r\n    z = self.process_tokens(tokens, multipliers)\r\n  File \"D:\\Programs\\StableDiffusionWebUI\\modules\\sd_hijack_clip.py\", line 245, in process_tokens\r\n    z = self.encode_with_transformers(tokens)\r\n  File \"D:\\Programs\\StableDiffusionWebUI\\modules\\sd_hijack_clip.py\", line 293, in encode_with_transformers\r\n    outputs = self.wrapped.transformer(input_ids=tokens, output_hidden_states=-opts.CLIP_stop_at_last_layers)\r\n  File \"D:\\Programs\\StableDiffusionWebUI\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1130, in _call_impl\r\n    return forward_call(*input, **kwargs)\r\n  File \"D:\\Programs\\StableDiffusionWebUI\\venv\\lib\\site-packages\\transformers\\models\\clip\\modeling_clip.py\", line 722, in forward\r\n    return self.text_model(\r\n  File \"D:\\Programs\\StableDiffusionWebUI\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1130, in _call_impl\r\n    return forward_call(*input, **kwargs)\r\n  File \"D:\\Programs\\StableDiffusionWebUI\\venv\\lib\\site-packages\\transformers\\models\\clip\\modeling_clip.py\", line 643, in forward\r\n    encoder_outputs = self.encoder(\r\n  File \"D:\\Programs\\StableDiffusionWebUI\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1130, in _call_impl\r\n    return forward_call(*input, **kwargs)\r\n  File \"D:\\Programs\\StableDiffusionWebUI\\venv\\lib\\site-packages\\transformers\\models\\clip\\modeling_clip.py\", line 574, in forward\r\n    layer_outputs = encoder_layer(\r\n  File \"D:\\Programs\\StableDiffusionWebUI\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1130, in _call_impl\r\n    return forward_call(*input, **kwargs)\r\n  File \"D:\\Programs\\StableDiffusionWebUI\\venv\\lib\\site-packages\\transformers\\models\\clip\\modeling_clip.py\", line 316, in forward\r\n    hidden_states = self.layer_norm1(hidden_states)\r\n  File \"D:\\Programs\\StableDiffusionWebUI\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1130, in _call_impl\r\n    return forward_call(*input, **kwargs)\r\n  File \"D:\\Programs\\StableDiffusionWebUI\\venv\\lib\\site-packages\\torch\\nn\\modules\\normalization.py\", line 189, in forward\r\n    return F.layer_norm(\r\n  File \"D:\\Programs\\StableDiffusionWebUI\\venv\\lib\\site-packages\\torch\\nn\\functional.py\", line 2503, in layer_norm\r\n    return torch.layer_norm(input, normalized_shape, weight, bias, eps, torch.backends.cudnn.enabled)\r\nRuntimeError: \"LayerNormKernelImpl\" not implemented for 'Half'\r\n```", "created_at": "2023-01-10T19:41:31Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 6478, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-6478", "issue_numbers": ["6449"], "base_commit": "085427de0efc9e9e7a6e9a5aebc6b5a69f0365e7", "patch": "diff --git a/style.css b/style.css\nindex 76721756c70..d796cbe9920 100644\n--- a/style.css\n+++ b/style.css\n@@ -512,7 +512,7 @@ input[type=\"range\"]{\n     border: none;\r\n     background: none;\r\n     flex: unset;\r\n-    gap: 0.5em;\r\n+    gap: 1em;\r\n }\r\n \r\n #quicksettings > div > div{\r\n@@ -521,6 +521,17 @@ input[type=\"range\"]{\n     padding: 0;\r\n }\r\n \r\n+#quicksettings > div > div > div > div > label > span {\r\n+    position: relative;\r\n+    margin-right: 9em;\r\n+    margin-bottom: -1em;\r\n+}\r\n+\r\n+#quicksettings > div > div > label > span {\r\n+    position: relative;\r\n+    margin-bottom: -1em;\r\n+}\r\n+\r\n canvas[key=\"mask\"] {\r\n     z-index: 12 !important;\r\n     filter: invert();\r\n", "test_patch": "", "problem_statement": "[Bug]: Name of the quicksetting may block control\n### Is there an existing issue for this?\n\n- [X] I have searched the existing issues and checked the recent builds/commits\n\n### What happened?\n\nWhen adding settings to quicksettings, the full text of the setting is shown at the top of the UI, causing the setting name box to float over the controls. This make it very difficult to use certain quicksettings.\n\n### Steps to reproduce the problem\n\n1. Go to `settings` tab\r\n2. Add `show_progress_every_n_steps` to the `Quicksettings list`\r\n3. Refresh the UI\r\n4. See that the identifier for the progress quicksetting uses its full description (`Show image creation progress every N sampling steps. Set to 0 to disable. Set to -1 to show after completion of batch.`), obscuring the controls.\r\n\r\n![image](https://user-images.githubusercontent.com/2192097/211110276-9c8d1641-f729-4d42-85dc-bd61e7984e45.png)\n\n### What should have happened?\n\nQuicksettings should display a short name, to prevent it obscuring the controls.\n\n### Commit where the problem happens\n\n5a7f85d58a5c0d5a1f187e6dc961c6c9c995c4fd\n\n### What platforms do you use to access UI ?\n\nLinux\n\n### What browsers do you use to access the UI ?\n\nGoogle Chrome\n\n### Command Line Arguments\n\n_No response_\n\n### Additional information, context and logs\n\n_No response_\n", "hints_text": "Thanks for renaming the issue, I had to copy this across from another browser that wasn't letting me submit the issue and I missed that part.", "created_at": "2023-01-07T16:37:00Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 6432, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-6432", "issue_numbers": ["6391"], "base_commit": "3246a2d6b898da6a98fe9df4dc67944635a41bd3", "patch": "diff --git a/modules/shared.py b/modules/shared.py\nindex 57e489d0474..865c3c0708e 100644\n--- a/modules/shared.py\n+++ b/modules/shared.py\n@@ -430,7 +430,7 @@ def list_samplers():\n     \"samplers_in_dropdown\": OptionInfo(True, \"Use dropdown for sampler selection instead of radio group\"),\r\n     \"dimensions_and_batch_together\": OptionInfo(True, \"Show Witdth/Height and Batch sliders in same row\"),\r\n     'quicksettings': OptionInfo(\"sd_model_checkpoint\", \"Quicksettings list\"),\r\n-    'ui_reorder': OptionInfo(\", \".join(ui_reorder_categories), \"txt2img/ing2img UI item order\"),\r\n+    'ui_reorder': OptionInfo(\", \".join(ui_reorder_categories), \"txt2img/img2img UI item order\"),\r\n     'localization': OptionInfo(\"None\", \"Localization (requires restart)\", gr.Dropdown, lambda: {\"choices\": [\"None\"] + list(localization.localizations.keys())}, refresh=lambda: localization.list_localizations(cmd_opts.localizations_dir)),\r\n }))\r\n \r\n", "test_patch": "", "problem_statement": "[Bug]: Typo on Settings / User Interface / \"txt2img/ing2img UI item order\"\n### Is there an existing issue for this?\n\n- [X] I have searched the existing issues and checked the recent builds/commits\n\n### What happened?\n\nThres just a typo on -> Settings / User Interface / \"txt2img/ing2img UI item order\"\r\n\r\n> txt2img/ing2img UI item order\r\n\r\n![image](https://user-images.githubusercontent.com/18109442/210893686-2ee58343-9722-43fe-96a0-0944e75350b7.png)\r\n\r\n\r\nI dont know how to pull a fix, so if anyone knows, be my guest.\r\n\r\n\n\n### Steps to reproduce the problem\n\n1. Go to Settings / User Interface\r\n\r\ntxt2img/ing2img UI item order Label\r\n\n\n### What should have happened?\n\n> txt2img/img2img UI item order\n\n### Commit where the problem happens\n\n310b71f669e4f2cea11b023c47f7ffedd82ab464\n\n### What platforms do you use to access UI ?\n\nWindows\n\n### What browsers do you use to access the UI ?\n\nGoogle Chrome\n\n### Command Line Arguments\n\n```Shell\nNot relevant\n```\n\n\n### Additional information, context and logs\n\n...\n", "hints_text": "", "created_at": "2023-01-06T15:18:49Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 6285, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-6285", "issue_numbers": ["6263"], "base_commit": "3e22e294135ed0327ce9d9738655ff03c53df3c0", "patch": "diff --git a/modules/ui.py b/modules/ui.py\nindex d941cb5fa9e..bfc93634193 100644\n--- a/modules/ui.py\n+++ b/modules/ui.py\n@@ -635,6 +635,7 @@ def create_sampler_and_steps_selection(choices, tabname):\n     if opts.samplers_in_dropdown:\r\n         with FormRow(elem_id=f\"sampler_selection_{tabname}\"):\r\n             sampler_index = gr.Dropdown(label='Sampling method', elem_id=f\"{tabname}_sampling\", choices=[x.name for x in choices], value=choices[0].name, type=\"index\")\r\n+            sampler_index.save_to_config = True\r\n             steps = gr.Slider(minimum=1, maximum=150, step=1, elem_id=f\"{tabname}_steps\", label=\"Sampling Steps\", value=20)\r\n     else:\r\n         with FormGroup(elem_id=f\"sampler_selection_{tabname}\"):\r\n", "test_patch": "", "problem_statement": "[Bug]: Dropdown doesn't set defined Sampling Method from ui-config.json\n### Is there an existing issue for this?\n\n- [X] I have searched the existing issues and checked the recent builds/commits\n\n### What happened?\n\nSetting my prefferences in ui-config.json should be respected when staring new web ui\n\n### Steps to reproduce the problem\n\n1. Edit your ui.config.json and set sampling methods ie.:\r\n\"txt2img/Sampling method/value\": \"DPM++ SDE Karras\",\r\n \"img2img/Sampling method/value\": \"DPM++ SDE Karras\",\r\n\r\n2. Launch web ui\r\n3. See wrongly set drop boxes for sampling methoid to \"euler a\"\r\n\n\n### What should have happened?\n\ndrop boxes for sampling method set  to default value in ui-config.json ie \"DPM++ SDE Karras\" \n\n### Commit where the problem happens\n\ne9fb9bb0c25f59109a816fc53c385bed58965c24\n\n### What platforms do you use to access UI ?\n\nWindows\n\n### What browsers do you use to access the UI ?\n\nGoogle Chrome\n\n### Command Line Arguments\n\n_No response_\n\n### Additional information, context and logs\n\n_No response_\n", "hints_text": "Seems to be duplicate of #6180 ", "created_at": "2023-01-04T01:21:24Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 5999, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-5999", "issue_numbers": ["5866"], "base_commit": "c5bdba2089dc7060be2631bcbc83313b6358cbf2", "patch": "diff --git a/modules/api/api.py b/modules/api/api.py\nindex b43dd16bdaf..1ceba75d7d4 100644\n--- a/modules/api/api.py\n+++ b/modules/api/api.py\n@@ -10,13 +10,17 @@\n from secrets import compare_digest\n \n import modules.shared as shared\n-from modules import sd_samplers, deepbooru\n+from modules import sd_samplers, deepbooru, sd_hijack\n from modules.api.models import *\n from modules.processing import StableDiffusionProcessingTxt2Img, StableDiffusionProcessingImg2Img, process_images\n from modules.extras import run_extras, run_pnginfo\n+from modules.textual_inversion.textual_inversion import create_embedding, train_embedding\n+from modules.textual_inversion.preprocess import preprocess\n+from modules.hypernetworks.hypernetwork import create_hypernetwork, train_hypernetwork\n from PIL import PngImagePlugin,Image\n from modules.sd_models import checkpoints_list\n from modules.realesrgan_model import get_realesrgan_models\n+from modules import devices\n from typing import List\n \n def upscaler_to_index(name: str):\n@@ -97,6 +101,11 @@ def __init__(self, app: FastAPI, queue_lock: Lock):\n         self.add_api_route(\"/sdapi/v1/artist-categories\", self.get_artists_categories, methods=[\"GET\"], response_model=List[str])\n         self.add_api_route(\"/sdapi/v1/artists\", self.get_artists, methods=[\"GET\"], response_model=List[ArtistItem])\n         self.add_api_route(\"/sdapi/v1/refresh-checkpoints\", self.refresh_checkpoints, methods=[\"POST\"])\n+        self.add_api_route(\"/sdapi/v1/create/embedding\", self.create_embedding, methods=[\"POST\"], response_model=CreateResponse)\n+        self.add_api_route(\"/sdapi/v1/create/hypernetwork\", self.create_hypernetwork, methods=[\"POST\"], response_model=CreateResponse)\n+        self.add_api_route(\"/sdapi/v1/preprocess\", self.preprocess, methods=[\"POST\"], response_model=PreprocessResponse)\n+        self.add_api_route(\"/sdapi/v1/train/embedding\", self.train_embedding, methods=[\"POST\"], response_model=TrainResponse)\n+        self.add_api_route(\"/sdapi/v1/train/hypernetwork\", self.train_hypernetwork, methods=[\"POST\"], response_model=TrainResponse)\n \n     def add_api_route(self, path: str, endpoint, **kwargs):\n         if shared.cmd_opts.api_auth:\n@@ -326,6 +335,89 @@ def get_artists(self):\n     def refresh_checkpoints(self):\n         shared.refresh_checkpoints()\n \n+    def create_embedding(self, args: dict):\n+        try:\n+            shared.state.begin()\n+            filename = create_embedding(**args) # create empty embedding\n+            sd_hijack.model_hijack.embedding_db.load_textual_inversion_embeddings() # reload embeddings so new one can be immediately used\n+            shared.state.end()\n+            return CreateResponse(info = \"create embedding filename: {filename}\".format(filename = filename))\n+        except AssertionError as e:\n+            shared.state.end()\n+            return TrainResponse(info = \"create embedding error: {error}\".format(error = e))\n+\n+    def create_hypernetwork(self, args: dict):\n+        try:\n+            shared.state.begin()\n+            filename = create_hypernetwork(**args) # create empty embedding\n+            shared.state.end()\n+            return CreateResponse(info = \"create hypernetwork filename: {filename}\".format(filename = filename))\n+        except AssertionError as e:\n+            shared.state.end()\n+            return TrainResponse(info = \"create hypernetwork error: {error}\".format(error = e))\n+\n+    def preprocess(self, args: dict):\n+        try:\n+            shared.state.begin()\n+            preprocess(**args) # quick operation unless blip/booru interrogation is enabled\n+            shared.state.end()\n+            return PreprocessResponse(info = 'preprocess complete')\n+        except KeyError as e:\n+            shared.state.end()\n+            return PreprocessResponse(info = \"preprocess error: invalid token: {error}\".format(error = e))\n+        except AssertionError as e:\n+            shared.state.end()\n+            return PreprocessResponse(info = \"preprocess error: {error}\".format(error = e))\n+        except FileNotFoundError as e:\n+            shared.state.end()\n+            return PreprocessResponse(info = 'preprocess error: {error}'.format(error = e))\n+\n+    def train_embedding(self, args: dict):\n+        try:\n+            shared.state.begin()\n+            apply_optimizations = shared.opts.training_xattention_optimizations\n+            error = None\n+            filename = ''\n+            if not apply_optimizations:\n+                sd_hijack.undo_optimizations()\n+            try:\n+                embedding, filename = train_embedding(**args) # can take a long time to complete\n+            except Exception as e:\n+                error = e\n+            finally:\n+                if not apply_optimizations:\n+                    sd_hijack.apply_optimizations()\n+                shared.state.end()\n+            return TrainResponse(info = \"train embedding complete: filename: {filename} error: {error}\".format(filename = filename, error = error))\n+        except AssertionError as msg:\n+            shared.state.end()\n+            return TrainResponse(info = \"train embedding error: {msg}\".format(msg = msg))\n+\n+    def train_hypernetwork(self, args: dict):\n+        try:\n+            shared.state.begin()\n+            initial_hypernetwork = shared.loaded_hypernetwork\n+            apply_optimizations = shared.opts.training_xattention_optimizations\n+            error = None\n+            filename = ''\n+            if not apply_optimizations:\n+                sd_hijack.undo_optimizations()\n+            try:\n+                hypernetwork, filename = train_hypernetwork(*args)\n+            except Exception as e:\n+                error = e\n+            finally:\n+                shared.loaded_hypernetwork = initial_hypernetwork\n+                shared.sd_model.cond_stage_model.to(devices.device)\n+                shared.sd_model.first_stage_model.to(devices.device)\n+                if not apply_optimizations:\n+                    sd_hijack.apply_optimizations()\n+                shared.state.end()\n+            return TrainResponse(info = \"train embedding complete: filename: {filename} error: {error}\".format(filename = filename, error = error))\n+        except AssertionError as msg:\n+            shared.state.end()\n+            return TrainResponse(info = \"train embedding error: {error}\".format(error = error))\n+\n     def launch(self, server_name, port):\n         self.app.include_router(self.router)\n         uvicorn.run(self.app, host=server_name, port=port)\ndiff --git a/modules/api/models.py b/modules/api/models.py\nindex a22bc6b3bc0..c446ce7a6c8 100644\n--- a/modules/api/models.py\n+++ b/modules/api/models.py\n@@ -175,6 +175,15 @@ class InterrogateRequest(BaseModel):\n class InterrogateResponse(BaseModel):\n     caption: str = Field(default=None, title=\"Caption\", description=\"The generated caption for the image.\")\n \n+class TrainResponse(BaseModel):\n+    info: str = Field(title=\"Train info\", description=\"Response string from train embedding or hypernetwork task.\")\n+\n+class CreateResponse(BaseModel):\n+    info: str = Field(title=\"Create info\", description=\"Response string from create embedding or hypernetwork task.\")\n+\n+class PreprocessResponse(BaseModel):\n+    info: str = Field(title=\"Preprocess info\", description=\"Response string from preprocessing task.\")\n+\n fields = {}\n for key, metadata in opts.data_labels.items():\n     value = opts.data.get(key)\ndiff --git a/modules/hypernetworks/hypernetwork.py b/modules/hypernetworks/hypernetwork.py\nindex c406ffb379a..3182ff03c7c 100644\n--- a/modules/hypernetworks/hypernetwork.py\n+++ b/modules/hypernetworks/hypernetwork.py\n@@ -378,6 +378,32 @@ def report_statistics(loss_info:dict):\n             print(e)\r\n \r\n \r\n+def create_hypernetwork(name, enable_sizes, overwrite_old, layer_structure=None, activation_func=None, weight_init=None, add_layer_norm=False, use_dropout=False):\r\n+    # Remove illegal characters from name.\r\n+    name = \"\".join( x for x in name if (x.isalnum() or x in \"._- \"))\r\n+\r\n+    fn = os.path.join(shared.cmd_opts.hypernetwork_dir, f\"{name}.pt\")\r\n+    if not overwrite_old:\r\n+        assert not os.path.exists(fn), f\"file {fn} already exists\"\r\n+\r\n+    if type(layer_structure) == str:\r\n+        layer_structure = [float(x.strip()) for x in layer_structure.split(\",\")]\r\n+\r\n+    hypernet = modules.hypernetworks.hypernetwork.Hypernetwork(\r\n+        name=name,\r\n+        enable_sizes=[int(x) for x in enable_sizes],\r\n+        layer_structure=layer_structure,\r\n+        activation_func=activation_func,\r\n+        weight_init=weight_init,\r\n+        add_layer_norm=add_layer_norm,\r\n+        use_dropout=use_dropout,\r\n+    )\r\n+    hypernet.save(fn)\r\n+\r\n+    shared.reload_hypernetworks()\r\n+\r\n+    return fn\r\n+\r\n \r\n def train_hypernetwork(hypernetwork_name, learn_rate, batch_size, gradient_step, data_root, log_directory, training_width, training_height, steps, shuffle_tags, tag_drop_out, latent_sampling_method, create_image_every, save_hypernetwork_every, template_file, preview_from_txt2img, preview_prompt, preview_negative_prompt, preview_steps, preview_sampler_index, preview_cfg_scale, preview_seed, preview_width, preview_height):\r\n     # images allows training previews to have infotext. Importing it at the top causes a circular import problem.\r\ndiff --git a/modules/hypernetworks/ui.py b/modules/hypernetworks/ui.py\nindex c2d4b51c54c..e7f9e593a70 100644\n--- a/modules/hypernetworks/ui.py\n+++ b/modules/hypernetworks/ui.py\n@@ -3,39 +3,16 @@\n import re\r\n \r\n import gradio as gr\r\n-import modules.textual_inversion.preprocess\r\n-import modules.textual_inversion.textual_inversion\r\n+import modules.hypernetworks.hypernetwork\r\n from modules import devices, sd_hijack, shared\r\n-from modules.hypernetworks import hypernetwork\r\n \r\n not_available = [\"hardswish\", \"multiheadattention\"]\r\n-keys = list(x for x in hypernetwork.HypernetworkModule.activation_dict.keys() if x not in not_available)\r\n+keys = list(x for x in modules.hypernetworks.hypernetwork.HypernetworkModule.activation_dict.keys() if x not in not_available)\r\n \r\n def create_hypernetwork(name, enable_sizes, overwrite_old, layer_structure=None, activation_func=None, weight_init=None, add_layer_norm=False, use_dropout=False):\r\n-    # Remove illegal characters from name.\r\n-    name = \"\".join( x for x in name if (x.isalnum() or x in \"._- \"))\r\n+    filename = modules.hypernetworks.hypernetwork.create_hypernetwork(name, enable_sizes, overwrite_old, layer_structure, activation_func, weight_init, add_layer_norm, use_dropout)\r\n \r\n-    fn = os.path.join(shared.cmd_opts.hypernetwork_dir, f\"{name}.pt\")\r\n-    if not overwrite_old:\r\n-        assert not os.path.exists(fn), f\"file {fn} already exists\"\r\n-\r\n-    if type(layer_structure) == str:\r\n-        layer_structure = [float(x.strip()) for x in layer_structure.split(\",\")]\r\n-\r\n-    hypernet = modules.hypernetworks.hypernetwork.Hypernetwork(\r\n-        name=name,\r\n-        enable_sizes=[int(x) for x in enable_sizes],\r\n-        layer_structure=layer_structure,\r\n-        activation_func=activation_func,\r\n-        weight_init=weight_init,\r\n-        add_layer_norm=add_layer_norm,\r\n-        use_dropout=use_dropout,\r\n-    )\r\n-    hypernet.save(fn)\r\n-\r\n-    shared.reload_hypernetworks()\r\n-\r\n-    return gr.Dropdown.update(choices=sorted([x for x in shared.hypernetworks.keys()])), f\"Created: {fn}\", \"\"\r\n+    return gr.Dropdown.update(choices=sorted([x for x in shared.hypernetworks.keys()])), f\"Created: {filename}\", \"\"\r\n \r\n \r\n def train_hypernetwork(*args):\r\n", "test_patch": "", "problem_statement": "[Feature Request]: Add API for Training\n### Is there an existing issue for this?\n\n- [X] I have searched the existing issues and checked the recent builds/commits\n\n### What would your feature do ?\n\nadd api endpoints for built-in training functionality of automatic11111 (embedding, hypernetwork)\n\n### Proposed workflow\n\n```python\r\ntrain_options = { ... }\r\nreq = requests.post(url = f'http://127.0.0.1:7860/sdapi/v1/train', json = train_options)\r\n```\r\n\n\n### Additional information\n\n_No response_\n", "hints_text": "", "created_at": "2022-12-24T23:04:22Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 5907, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-5907", "issue_numbers": ["5893"], "base_commit": "4af3ca5393151d61363c30eef4965e694eeac15e", "patch": "diff --git a/modules/sd_models.py b/modules/sd_models.py\nindex ecdd91c5084..cd938656c87 100644\n--- a/modules/sd_models.py\n+++ b/modules/sd_models.py\n@@ -168,7 +168,10 @@ def get_state_dict_from_checkpoint(pl_sd):\n def read_state_dict(checkpoint_file, print_global_state=False, map_location=None):\r\n     _, extension = os.path.splitext(checkpoint_file)\r\n     if extension.lower() == \".safetensors\":\r\n-        pl_sd = safetensors.torch.load_file(checkpoint_file, device=map_location or shared.weight_load_location)\r\n+        device = map_location or shared.weight_load_location\r\n+        if device is None:\r\n+            device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\r\n+        pl_sd = safetensors.torch.load_file(checkpoint_file, device=device)\r\n     else:\r\n         pl_sd = torch.load(checkpoint_file, map_location=map_location or shared.weight_load_location)\r\n \r\ndiff --git a/requirements_versions.txt b/requirements_versions.txt\nindex c126c8c402f..52e988181c8 100644\n--- a/requirements_versions.txt\n+++ b/requirements_versions.txt\n@@ -26,5 +26,5 @@ lark==1.1.2\n inflection==0.5.1\r\n GitPython==3.1.27\r\n torchsde==0.2.5\r\n-safetensors==0.2.5\r\n+safetensors==0.2.7\r\n httpcore<=0.15\r\n", "test_patch": "", "problem_statement": "[Bug]:  Slow .safetensors loading\n### Is there an existing issue for this?\r\n\r\n- [X] I have searched the existing issues and checked the recent builds/commits\r\n\r\n### What happened?\r\n\r\nSystem specs: 5800X3D, RTX 3080 12GB, 64GB DDR4-3600, Windows 11 22H2 22621.963, latest Nvidia driver (527.56), latest BIOS update.\r\n\r\nThis is most likely a specific issue related only to my PC, but I've seen a couple of comments about it on other sites. safetensors load significantly slower than the same model in .ckpt, around 2-3 minutes to load a safetensors compared to .ckpt loading in less than 10 seconds.\r\n\r\nHowever, safetensors do load fast when doing merges, both with the inbuilt merger and with merge extensions. I think this is because they mostly only load into RAM and not fully into VRAM, but switching to either of the models used for a merge after the merge is done also makes them load instantly.\r\n\r\nTroubleshooting I've tried so far:\r\n-launching with optimizations and without optimizations\r\n-Fresh UI reinstall\r\n-deleted and rebuilt venv\r\n-set SAFETENSORS_FAST_GPU=1 and without the set parameter\r\n-Python 3.10.6, 3.10.8 and 3.10.9\r\n-No extensions\r\n-No antivirus\r\n-No GPU undervolt\r\n-setting python.exe to Max Performance in Nvidia panel\r\n-different browsers\r\n-browser HW acceleration on and off\r\n\r\nVideocard works as it should for everything else.\r\n\r\nThe most notable thing I've noticed is that when loading a .ckpt, both python.exe and System show reads in Resource Manager. When loading a .safetensors, only python.exe shows reads.\r\n\r\nckpt loading:\r\n![ckpt](https://user-images.githubusercontent.com/66507343/208746830-6cc6a8e8-ca53-42c8-83e4-bce66b0bcb9a.png)\r\nsafetensors loading:\r\n![safetensors](https://user-images.githubusercontent.com/66507343/208746828-893a666a-7a3c-4e62-8367-4d5325188db0.png)\r\n\r\n### Steps to reproduce the problem\r\n\r\n1. Go to Stable Diffusion checkpoint dropdown\r\n2. Load safetensors model\r\n3. Takes 2-3 minutes to load compared to the same model but in .ckpt\r\n\r\n\r\n### What should have happened?\r\n\r\n1. Go to Stable Diffusion checkpoint dropdown\r\n2. Load safetensors model\r\n3. Load in less than 10 seconds like the same model but in .ckpt does.\r\n\r\n### Commit where the problem happens\r\n\r\n 685f9631b56ff8bd43bce24ff5ce0f9a0e9af490\r\n\r\n### What platforms do you use to access UI ?\r\n\r\nWindows\r\n\r\n### What browsers do you use to access the UI ?\r\n\r\nBrave\r\n\r\n### Command Line Arguments\r\n\r\n```Shell\r\nOptimized .bat:\r\n\r\n--xformers --deepdanbooru --gradio-img2img-tool color-sketch --gradio-inpaint-tool=color-sketch --opt-split-attention --precision autocast --opt-channelslast --api\r\n\r\nNon optimized:\r\n--deepdanbooru \r\n\r\nBoth show the same issue.\r\n```\r\n\r\n\r\n### Additional information, context and logs\r\n\r\n_No response_\n", "hints_text": "have the same observation, thought it should load faster as advertised.\r\ncc @Narsil\nHi thanks for the ping, this definitely shouldn't happen.\r\n\r\nDo you mind sharing:\r\n\r\n- The actual files you're trying to load ? (Part of it could come from incorrect precision in the files, since you use `--precision autocast` it could cause some issues).\r\n   - I am guessing this : https://huggingface.co/riffusion/riffusion-model-v1/blob/main/riffusion-model-v1.ckpt\r\n   - And this : https://huggingface.co/runwayml/stable-diffusion-v1-5/blob/refs%2Fpr%2F46/v1-5-pruned-emaonly.safetensors\r\n- The exact PyTorch version\r\n- Can you confirm you're loading on the GPU correct ?\r\n- The type of Disk your are using (SSD, HDD).\r\n- Both files are stored on the same disk I guess ?\r\n- You don't have an antivirus that might be inspecting the file slowing down its load ?\r\n\r\n\r\nI need to try and reproduce, but so far I have failed (I must confess I'm only using cloud Windows since I don't own a Windows machine anymore).\r\n\r\n> they mostly only load into RAM and not fully into VRAM\r\n\r\nLoading on CPU is always going to be faster than loading onto GPU yes. \r\nHowever it's very surprising that the read speeds are vastly different for both files.\r\n\r\nCould you isolate the issue maybe ?\r\n\r\n\r\n```python\r\nimport torch\r\nimport datetime\r\n\r\nstart = datetime.datetime.now()\r\nweights = torch.load(\"riffusion-model-v1.ckpt\", device=\"cuda:0\")\r\nprint(f\"Loaded PT in {datetime.datetime.now() - start}\")\r\n```\r\n\r\n```python\r\nfrom safetensors.torch import load_file\r\nimport datetime\r\n\r\nstart = datetime.datetime.now()\r\nweights = load_file(\"v1-5-pruned-emaonly.safetensors\", device=\"cuda:0\")\r\nprint(f\"Loaded SF in {datetime.datetime.now() - start}\")\r\n```\r\n\r\nIf SF is indeed slower, we can also try this: https://gist.github.com/Narsil/3edeec2669a5e94e4707aa0f901d2282 to check if the slowness is indeed in the lib or not (It needs a bit adaptation to load the checkpoint you want onto GPU).\r\n\r\nAnd report if you see the same things (just trying to ignore webui if possible.\r\n\r\nIn the meantime, I will keep trying to reproduce.\r\n\r\nNB: The two files are not the same size it seems, but the SF one is smaller, so it should be faster to load.\r\n\n```\r\nLoading weights [3aafa6fe] from C:\\Users\\Administrator\\src\\stable-diffusion-webui\\models\\Stable-diffusion\\riffusion-model-v1.ckpt\r\nLoaded C:\\Users\\Administrator\\src\\stable-diffusion-webui\\models\\Stable-diffusion\\riffusion-model-v1.ckpt in 0:01:15.861647\r\nApplying cross attention optimization (Doggettx).\r\nWeights loaded. in 0:01:19.734654\r\nLoading weights [d7049739] from C:\\Users\\Administrator\\src\\stable-diffusion-webui\\models\\Stable-diffusion\\v1-5-pruned-emaonly.safetensors\r\nLoaded C:\\Users\\Administrator\\src\\stable-diffusion-webui\\models\\Stable-diffusion\\v1-5-pruned-emaonly.safetensors in 0:00:00.308988\r\nApplying cross attention optimization (Doggettx).\r\nWeights loaded. in 0:00:44.164628\r\n```\r\n\r\nHere is what I get on a brand new Windows (Server 2022) with cuda support.\r\nI merely added 2 timings within `sd-webui` one just after loading the file, and the other one where the classic prompt is\r\n\r\nSo it's indeed spending a bunch of time doing something AFTER loading the weights, instead of loading correctly the correct weights for SD-1.5 directly in a proper form.\r\n\r\nI'll investigate a bit more to see what's going on for SD-1.5\nOk, I was able to investigate. It seems `map_location or shared.weight_load_location` resolves to `None` meaning the tensors are loaded on CPU (which is extremely fast), but than are sent to VRAM with `load_state_dict` (which is painfully slow and I don't know why.\r\n\r\nhttps://github.com/AUTOMATIC1111/stable-diffusion-webui/blob/master/modules/sd_models.py#L170\r\n\r\nI changed that line and forced `device=\"cuda:0\"` and then I got those results: \r\n\r\n```\r\nLoaded C:\\Users\\Administrator\\src\\stable-diffusion-webui\\models\\Stable-diffusion\\v1-5-pruned-emaonly.safetensors in 0:00:05.154992\r\nUsing map_location None\r\nLoaded state dict 0:00:02.660000\r\nModel half 0:00:02.677993\r\nModel first stage to 0:00:02.696012\r\nApplying cross attention optimization (Doggettx).\r\nHIGHJACK stuff in 0:00:08.760007\r\ncallback stuff in 0:00:08.760007\r\nload on device cuda in 0:00:09.488031\r\nWeights loaded. in 0:00:09.488031\r\n```\r\n\r\nSo the load time is slower (it loads directly on GPU) but it seems faster overall (because there's no need to reallocate on GPU afterwards).\r\n\r\nCould that explain the issue you're having ?\r\n\r\nAs for a fix, I'm not sure what's the proper fix here. \r\nIMO the `device=` should always be properly set (both for PT and SF) instead of using `None` (the device is stored in the pickle themselves, but that's not really portable, since if some weights where saved on GPU they can't be loaded on a machine without GPU anymore).", "created_at": "2022-12-21T12:47:30Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 5894, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-5894", "issue_numbers": ["4833"], "base_commit": "685f9631b56ff8bd43bce24ff5ce0f9a0e9af490", "patch": "diff --git a/requirements_versions.txt b/requirements_versions.txt\nindex 185cd066ea3..7208aeba5c5 100644\n--- a/requirements_versions.txt\n+++ b/requirements_versions.txt\n@@ -26,3 +26,4 @@ inflection==0.5.1\n GitPython==3.1.27\r\n torchsde==0.2.5\r\n safetensors==0.2.5\r\n+httpcore<=0.15\r\n", "test_patch": "", "problem_statement": "[Bug]: AttributeError: module 'h11' has no attribute 'Event'\n### Is there an existing issue for this?\n\n- [X] I have searched the existing issues and checked the recent builds/commits\n\n### What happened?\n\nNo run colub\n\n### Steps to reproduce the problem\n\n1. Go to .... \r\n2. Press ....\r\n3. ...\r\n\n\n### What should have happened?\n\n  File \"launch.py\", line 256, in <module>\r\n    start()\r\n  File \"launch.py\", line 247, in start\r\n    import webui\r\n  File \"/content/stable-diffusion-webui/webui.py\", line 13, in <module>\r\n    from modules import devices, sd_samplers, upscaler, extensions, localization\r\n  File \"/content/stable-diffusion-webui/modules/sd_samplers.py\", line 11, in <module>\r\n    from modules import prompt_parser, devices, processing, images\r\n  File \"/content/stable-diffusion-webui/modules/processing.py\", line 14, in <module>\r\n    import modules.sd_hijack\r\n  File \"/content/stable-diffusion-webui/modules/sd_hijack.py\", line 10, in <module>\r\n    import modules.textual_inversion.textual_inversion\r\n  File \"/content/stable-diffusion-webui/modules/textual_inversion/textual_inversion.py\", line 13, in <module>\r\n    from modules import shared, devices, sd_hijack, processing, sd_models, images\r\n  File \"/content/stable-diffusion-webui/modules/shared.py\", line 8, in <module>\r\n    import gradio as gr\r\n  File \"/usr/local/lib/python3.8/site-packages/gradio/__init__.py\", line 3, in <module>\r\n    import gradio.components as components\r\n  File \"/usr/local/lib/python3.8/site-packages/gradio/components.py\", line 31, in <module>\r\n    from gradio import media_data, processing_utils, utils\r\n  File \"/usr/local/lib/python3.8/site-packages/gradio/processing_utils.py\", line 20, in <module>\r\n    from gradio import encryptor, utils\r\n  File \"/usr/local/lib/python3.8/site-packages/gradio/utils.py\", line 35, in <module>\r\n    import httpx\r\n  File \"/usr/local/lib/python3.8/site-packages/httpx/__init__.py\", line 2, in <module>\r\n    from ._api import delete, get, head, options, patch, post, put, request, stream\r\n  File \"/usr/local/lib/python3.8/site-packages/httpx/_api.py\", line 4, in <module>\r\n    from ._client import Client\r\n  File \"/usr/local/lib/python3.8/site-packages/httpx/_client.py\", line 29, in <module>\r\n    from ._transports.default import AsyncHTTPTransport, HTTPTransport\r\n  File \"/usr/local/lib/python3.8/site-packages/httpx/_transports/default.py\", line 30, in <module>\r\n    import httpcore\r\n  File \"/usr/local/lib/python3.8/site-packages/httpcore/__init__.py\", line 1, in <module>\r\n    from ._api import request, stream\r\n  File \"/usr/local/lib/python3.8/site-packages/httpcore/_api.py\", line 5, in <module>\r\n    from ._sync.connection_pool import ConnectionPool\r\n  File \"/usr/local/lib/python3.8/site-packages/httpcore/_sync/__init__.py\", line 1, in <module>\r\n    from .connection import HTTPConnection\r\n  File \"/usr/local/lib/python3.8/site-packages/httpcore/_sync/connection.py\", line 13, in <module>\r\n    from .http11 import HTTP11Connection\r\n  File \"/usr/local/lib/python3.8/site-packages/httpcore/_sync/http11.py\", line 44, in <module>\r\n    class HTTP11Connection(ConnectionInterface):\r\n  File \"/usr/local/lib/python3.8/site-packages/httpcore/_sync/http11.py\", line 140, in HTTP11Connection\r\n    self, event: h11.Event, timeout: Optional[float] = None\r\nAttributeError: module 'h11' has no attribute 'Event'\n\n### Commit where the problem happens\n\nStableDiffusionUI\n\n### What platforms do you use to access UI ?\n\nWindows\n\n### What browsers do you use to access the UI ?\n\nGoogle Chrome\n\n### Command Line Arguments\n\n_No response_\n\n### Additional information, context and logs\n\n_No response_\n", "hints_text": "link the colab\nNeed more info, but in any case you need to be running Python 3.10.x. The recommended build is 3.10.6. Several dependencies won't work with other major release versions of Python.\nlmao this is why we have 1.2k issues\nYes. I am facing the same issue. It used to work perfectly before but this issue crawled up suddenly.\npip install --force-reinstall httpcore==0.15\r\nworks as a temporary workaround until some other fix is found.\r\nThat or just append it to your requirement.txt\r\nThis comes from a recent update in httpcore and nothing related to this repository.\r\n\n> pip install --force-reinstall httpcore==0.15 works as a temporary workaround until some other fix is found. That or just append it to your requirement.txt This comes from a recent update in httpcore and nothing related to this repository.\r\n> \r\n> > lmao this is why we have 1.2k issues\r\n> \r\n> for real the dude didn't even title the issue\r\n\r\nthanks, that just fix this problem for me\npip install --force-reinstall httpcore==0.15 it's just on a computer, but how to register in a colab? https://colab.research.google.com/drive/1wriLAuWlY1xlfq_Zpq0Fk5WgFTRFyyOG?usp=sharing\n@TkonstAL \r\nadd a new code cell in colab and then run \r\n\r\n`!pip install --force-reinstall httpcore==0.15 `\nI also want to know how I can add this to the colab. https://huggingface.co/Gustavosta/MagicPrompt-Stable-Diffusion/tree/main \r\nIs it possible? I add - it used to work and now it doesn't?\njust manually install for now whenever you need to use it, hopefully this will be fixed soon and you don't have to do anything.\r\n\r\nalso @TkonstAL can you please update the title of this bug report from  [Bug]: #4833 to the below title please.\r\n`AttributeError: module 'h11' has no attribute 'Event'`\nI don't really know how to edit here. Where should I unpack it? https://www.reddit.com/r/StableDiffusion/comments/xvjm84/magicprompt_script_for_automatic1111_gui_let_the/ I read it but didn't fully understand.\r\nP.S i find this !pip install git+https://github.com/KichangKim/DeepDanbooru.git@edf73df4cdaeea2cf00e9ac08bd8a9026b7a7b26 but is non magicpromt\nin your colab notebook https://www.reddit.com/r/StableDiffusion/comments/xvjm84/magicprompt_script_for_automatic1111_gui_let_the/\r\nwhen you get the` AttributeError: module 'h11' has no attribute 'Event'`, scroll up and add a new cell by clicking the `+ code` button\r\n![image](https://user-images.githubusercontent.com/18790567/202858830-2914385c-ca84-463e-90d7-24f11faa4657.png)\r\nthe copy paste the below command and run the cell\r\n`!pip install --force-reinstall httpcore==0.15 `\n> pip install --force-reinstall httpcore==0.15 works as a temporary workaround until some other fix is found. That or just append it to your requirement.txt This comes from a recent update in httpcore and nothing related to this repository.\r\n> \r\n> > lmao this is why we have 1.2k issues\r\n> \r\n> for real the dude didn't even title the issue\r\n\r\nDoes not work for me still. I had to update python 3.8 to 3.10 to fix this issue. But its a hassle to update python. I thought that updating httpcore should work but it didn't\nGuys both reinstalling httpcore and python doesn't work for me and I still get the same error\n> \r\n\r\nI tried to force reinstall httpcore, it worked last week, but no more now.\r\ntry to replace the http11.py in \"../'env'(or usr/local/) /lib/python3.8/site-packages/httpcore/_async/\" or _sync\r\ncorresponding to the ERROR by the files from below:\r\n(better to backup the original file first)\r\n[httpcore.zip](https://github.com/AUTOMATIC1111/stable-diffusion-webui/files/10071668/httpcore.zip)\r\n\n> Guys both reinstalling httpcore and python doesn't work for me and I still get the same error\r\n\r\nHave you run it with the environment activated? \r\nif not, run \r\n`source venv/bin/activate `\r\nbefore reinstalling.\nJust wanted to note I saw this today. The above recommendations worked:\r\n`cd stable-diffusion-webui`\r\n`source venv/bin/activate`\r\n`pip install --force-reinstall httpcore==0.15`\nI am using Python 3.8 which seems to be the root of the problem. But if I update to Python 3.10, I get a whole problem that Torch cannot use my graphics card. \r\n\r\nIs there any way to resolve this without upgrading to 3.10? \r\n\r\nI tried: `pip install --force-reinstall httpcore==0.15`\r\n\r\nAnd I also tried using the files linked above by @pluieciel ... still getting the exact same error, `h11 has no attribute Event`\ncd stable-diffusion-webui\r\nsource venv/Scripts/activate\r\npip install --force-reinstall httpcore==0.15\r\n\r\nalso as am testing it on a pretty low spec laptop on file webui-user added:\r\nset COMMANDLINE_ARGS=--lowvram --precision full --no-half --skip-torch-cuda-test\r\n\r\n\nJust adding that I had to run the python.exe that was in the venv directory directly to get 2.0 to run on Windows 10\r\nstable-diffusion-webui\\venv\\scripts\\python.exe -m pip install --force-reinstall httpcore==0.15\nEDIT:\r\n\r\n> Just adding that I had to run the python.exe that was in the venv directory directly to get 2.0 to run on Windows 10 stable-diffusion-webui\\venv\\scripts\\python.exe -m pip install --force-reinstall httpcore==0.15\r\n\r\nThis worked!!!! THANK YOU!\n> Need more info, but in any case you need to be running Python 3.10.x. The recommended build is 3.10.6. Several dependencies won't work with other major release versions of Python.\r\n\r\nI just ran into this same issue because the pre-existing Python install on my PC was 3.9. I installed 3.10.8, it's working now.\r\n\r\nMaybe the startup scripts could check that the python version is correct before running venv? Since `Expected Python 3.10.x, got 3.9` would be a lot more informative than `AttributeError: module 'h11' has no attribute 'Event'`.\nI uninstalled python on chocolatey to install the correct version and now when i reinstall it doesnt work. Can anyone help?\nPreface: I'm running on Windows 10\r\n\r\nI was having the same issues. I finally got it to launch. I had installed Python 3.10, but apparently it installed for only my user. So when I launched, it used the system's Python 3.9.1 version. I uninstalled 3.10, then reinstalled for all users. But I was still getting the error. So, after seeing others in this thread talk about the Python.exe in the venv directory, I decided to nuke the whole `venv` directory. After that, I relaunched and, after having to sit through all the other installing again, I finally got past the Web UI portion. Then after about 10 minutes of it downloading other requirements, I got to the loading weights and the message saying it was ready.\r\n\r\nIt appears that, even after installing Python 3.10 globally, for some reason it was still using a copy (?) of Python 3.9 in the venv directory.\n> Preface: I'm running on Windows 10\r\n> \r\n> I was having the same issues. I finally got it to launch. I had installed Python 3.10, but apparently it installed for only my user. So when I launched, it used the system's Python 3.9.1 version. I uninstalled 3.10, then reinstalled for all users. But I was still getting the error. So, after seeing others in this thread talk about the Python.exe in the venv directory, I decided to nuke the whole `venv` directory. After that, I relaunched and, after having to sit through all the other installing again, I finally got past the Web UI portion. Then after about 10 minutes of it downloading other requirements, I got to the loading weights and the message saying it was ready.\r\n> \r\n> It appears that, even after installing Python 3.10 globally, for some reason it was still using a copy (?) of Python 3.9 in the venv directory.\r\n\r\nThis works! But what you really want to do is change the value in \\venv\\pyvenv.cfg to 310. That does the same thing, because nuking the whole folder for some reason used the old python version. This is how I fixed it.\n> Just wanted to note I saw this today. The above recommendations worked: `cd stable-diffusion-webui` `source venv/bin/activate` `pip install --force-reinstall httpcore==0.15`\r\n\r\nFor windows(using cmd.exe):\r\n\r\ncd stable-diffusion-webui\r\nvenv\\Scripts\\activate.bat\r\npip install --force-reinstall httpcore==0.15\r\n\nAfter trying with ubuntu 18 and 20, switched to ubuntu22.0.4 (WSL2) which come by default now with python3.10.6\r\n\r\nAlso had to install some missing stuff, here is the full list.\r\nsudo apt install python3-venv\r\nsudo apt install libgl1-mesa-dev\r\nsudo apt install build-essential\r\nsource venv/bin/activate\r\npip install xformers==0.0.12\r\npip install triton\r\n\r\n./webui.sh --xformers\r\n\r\nEDIT: The previous setup does not work for xformers... the fact is that xformers in WSL simply does not worth the investment. I surrender and resetup myself in plain windows....\nIf you are in WINDOWS environment make sure you are running your command prompt in elevated privilege \r\n**\"Run as administrator\".**", "created_at": "2022-12-20T19:21:45Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 5873, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-5873", "issue_numbers": ["5847"], "base_commit": "685f9631b56ff8bd43bce24ff5ce0f9a0e9af490", "patch": "diff --git a/modules/processing.py b/modules/processing.py\nindex 24c537d14ec..f7335da25bd 100644\n--- a/modules/processing.py\n+++ b/modules/processing.py\n@@ -77,7 +77,7 @@ class StableDiffusionProcessing():\n     \"\"\"\r\n     The first set of paramaters: sd_models -> do_not_reload_embeddings represent the minimum required to create a StableDiffusionProcessing\r\n     \"\"\"\r\n-    def __init__(self, sd_model=None, outpath_samples=None, outpath_grids=None, prompt: str = \"\", styles: List[str] = None, seed: int = -1, subseed: int = -1, subseed_strength: float = 0, seed_resize_from_h: int = -1, seed_resize_from_w: int = -1, seed_enable_extras: bool = True, sampler_name: str = None, batch_size: int = 1, n_iter: int = 1, steps: int = 50, cfg_scale: float = 7.0, width: int = 512, height: int = 512, restore_faces: bool = False, tiling: bool = False, do_not_save_samples: bool = False, do_not_save_grid: bool = False, extra_generation_params: Dict[Any, Any] = None, overlay_images: Any = None, negative_prompt: str = None, eta: float = None, do_not_reload_embeddings: bool = False, denoising_strength: float = 0, ddim_discretize: str = None, s_churn: float = 0.0, s_tmax: float = None, s_tmin: float = 0.0, s_noise: float = 1.0, override_settings: Dict[str, Any] = None, sampler_index: int = None):\r\n+    def __init__(self, sd_model=None, outpath_samples=None, outpath_grids=None, prompt: str = \"\", styles: List[str] = None, seed: int = -1, subseed: int = -1, subseed_strength: float = 0, seed_resize_from_h: int = -1, seed_resize_from_w: int = -1, seed_enable_extras: bool = True, sampler_name: str = None, batch_size: int = 1, n_iter: int = 1, steps: int = 50, cfg_scale: float = 7.0, width: int = 512, height: int = 512, restore_faces: bool = False, tiling: bool = False, do_not_save_samples: bool = False, do_not_save_grid: bool = False, extra_generation_params: Dict[Any, Any] = None, overlay_images: Any = None, negative_prompt: str = None, eta: float = None, do_not_reload_embeddings: bool = False, denoising_strength: float = 0, ddim_discretize: str = None, s_churn: float = 0.0, s_tmax: float = None, s_tmin: float = 0.0, s_noise: float = 1.0, override_settings: Dict[str, Any] = None, override_settings_restore_afterwards: bool = True, sampler_index: int = None):\r\n         if sampler_index is not None:\r\n             print(\"sampler_index argument for StableDiffusionProcessing does not do anything; use sampler_name\", file=sys.stderr)\r\n \r\n@@ -118,6 +118,7 @@ def __init__(self, sd_model=None, outpath_samples=None, outpath_grids=None, prom\n         self.s_tmax = s_tmax or float('inf')  # not representable as a standard ui option\r\n         self.s_noise = s_noise or opts.s_noise\r\n         self.override_settings = {k: v for k, v in (override_settings or {}).items() if k not in shared.restricted_opts}\r\n+        self.override_settings_restore_afterwards = override_settings_restore_afterwards\r\n         self.is_using_inpainting_conditioning = False\r\n \r\n         if not seed_enable_extras:\r\n@@ -147,11 +148,11 @@ def txt2img_image_conditioning(self, x, width=None, height=None):\n \r\n         # The \"masked-image\" in this case will just be all zeros since the entire image is masked.\r\n         image_conditioning = torch.zeros(x.shape[0], 3, height, width, device=x.device)\r\n-        image_conditioning = self.sd_model.get_first_stage_encoding(self.sd_model.encode_first_stage(image_conditioning)) \r\n+        image_conditioning = self.sd_model.get_first_stage_encoding(self.sd_model.encode_first_stage(image_conditioning))\r\n \r\n         # Add the fake full 1s mask to the first dimension.\r\n         image_conditioning = torch.nn.functional.pad(image_conditioning, (0, 0, 0, 0, 1, 0), value=1.0)\r\n-        image_conditioning = image_conditioning.to(x.dtype)            \r\n+        image_conditioning = image_conditioning.to(x.dtype)\r\n \r\n         return image_conditioning\r\n \r\n@@ -199,7 +200,7 @@ def inpainting_image_conditioning(self, source_image, latent_image, image_mask =\n             source_image * (1.0 - conditioning_mask),\r\n             getattr(self, \"inpainting_mask_weight\", shared.opts.inpainting_mask_weight)\r\n         )\r\n-        \r\n+\r\n         # Encode the new masked image using first stage of network.\r\n         conditioning_image = self.sd_model.get_first_stage_encoding(self.sd_model.encode_first_stage(conditioning_image))\r\n \r\n@@ -463,12 +464,14 @@ def process_images(p: StableDiffusionProcessing) -> Processed:\n \r\n         res = process_images_inner(p)\r\n \r\n-    finally:  # restore opts to original state\r\n-        for k, v in stored_opts.items():\r\n-            setattr(opts, k, v)\r\n-            if k == 'sd_hypernetwork': shared.reload_hypernetworks()\r\n-            if k == 'sd_model_checkpoint': sd_models.reload_model_weights()\r\n-            if k == 'sd_vae': sd_vae.reload_vae_weights()\r\n+    finally:\r\n+        # restore opts to original state\r\n+        if p.override_settings_restore_afterwards:\r\n+            for k, v in stored_opts.items():\r\n+                setattr(opts, k, v)\r\n+                if k == 'sd_hypernetwork': shared.reload_hypernetworks()\r\n+                if k == 'sd_model_checkpoint': sd_models.reload_model_weights()\r\n+                if k == 'sd_vae': sd_vae.reload_vae_weights()\r\n \r\n     return res\r\n \r\n@@ -537,7 +540,7 @@ def infotext(iteration=0, position_in_batch=0):\n         for n in range(p.n_iter):\r\n             if state.skipped:\r\n                 state.skipped = False\r\n-            \r\n+\r\n             if state.interrupted:\r\n                 break\r\n \r\n@@ -612,7 +615,7 @@ def infotext(iteration=0, position_in_batch=0):\n                     image.info[\"parameters\"] = text\r\n                 output_images.append(image)\r\n \r\n-            del x_samples_ddim \r\n+            del x_samples_ddim\r\n \r\n             devices.torch_gc()\r\n \r\n@@ -720,7 +723,7 @@ def save_intermediate(image, index):\n \r\n             samples = torch.nn.functional.interpolate(samples, size=(self.height // opt_f, self.width // opt_f), mode=\"bilinear\")\r\n \r\n-            # Avoid making the inpainting conditioning unless necessary as \r\n+            # Avoid making the inpainting conditioning unless necessary as\r\n             # this does need some extra compute to decode / encode the image again.\r\n             if getattr(self, \"inpainting_mask_weight\", shared.opts.inpainting_mask_weight) < 1.0:\r\n                 image_conditioning = self.img2img_image_conditioning(decode_first_stage(self.sd_model, samples), samples)\r\n", "test_patch": "", "problem_statement": "[Feature Request]: API method to queue a options change\n### Is there an existing issue for this?\n\n- [X] I have searched the existing issues and checked the recent builds/commits\n\n### What would your feature do ?\n\nAt present, there are two ways to change the model through the API:\r\n1) `POST /sdapi/v1/options` switches the model immediately, which can cause issues for generations in the queue\r\n2) `override_settings` in the generation request will temporarily change the model, but change it back to what it was before\r\n\r\nI would like to be able to set the model for upcoming generation requests, but without interfering with the requests already in the queue (ruling out option 1); additionally, I do not want to pay the cost of switching models back to the 'original' model if there's a run of generations with the same model (ruling out option 2).\r\n\r\nI'd like a third way that queues up the options change with respect to the existing queue, so that if you have\r\n\r\n`generation 1 (SD1.5) | generation 2 (SD1.5)` in the queue, you can issue an options change:\r\n`generation 1 (SD1.5) | generation 2 (SD1.5) | switch to SD2.1 | generation 3 (SD2.1) | generation 4 (SD2.1)`\r\n\r\nand generation 1 & 2 remains on SD1.5, while 3 & 4 will use SD2.1 without switching back to 1.5.\r\n\r\nThe simplest place to make this change would be to add a `queue` boolean parameter to `POST /sdapi/v1/options`, which adds the options switch to the queue instead of actioning it immediately.\n\n### Proposed workflow\n\n1. API user calls `POST /sdapi/v1/options {..., queue: true}`\r\n2. The desired options-change is added to the queue\r\n3. The options are not changed until that position in the queue is reached\n\n### Additional information\n\nI have multiple API clients using the same SD server (a Discord bot, and some experimental programs). I'd like them to be able to switch models and operate entirely independently without communicating with each other. Other API users have solved this by running their own internal queue, so that they can issue `POST /sdapi/v1/options` calls on demand, but I can't do this due to the multiple clients.\r\n\r\nAnother solution would be to offer a variant of `override_settings` that sets the settings without resetting (`change_settings`?) Either one would be sufficient for my use case (I *always* issue a model change before generating, anyway)\r\n\r\nSee also #4120 and #5191.\n", "hints_text": "cc @aliencaocao\nUnfortunately I do not have enough knowledge on the codes to implement the queue system that you described. As you said, all the API users currently implement the queue themselves. If you have multiple clients, you can also have a centralized middleware server that implements the queue, and only that server sends requests to the API endpoint.\r\n\r\nBecause a queue system already exists (just without awareness of potential duplicate model switching), if you request for a model change in the middle of a generation, it will actually get queued already and only make the switch after the generation is done. This only works if the generation is also done via the API, since the queue is only for API requests. I do not see why you cannot use the existing behaviour. If you are that conscious about unnecessary model switching, then just using the options endpoint should be your use case.\n> Unfortunately I do not have enough knowledge on the codes to implement the queue system that you described.\r\n\r\nAh, apologies; I wasn't suggesting you should implement it, I just wanted to bring it to your attention.\r\n\r\n> As you said, all the API users currently implement the queue themselves. If you have multiple clients, you can also have a centralized middleware server that implements the queue, and only that server sends requests to the API endpoint.\r\n\r\nYes, I've considered this as a solution - it's not the cleanest, though, as I'd have to write and run another server to implement a queue on top of an existing queue.\r\n\r\n> Because a queue system already exists (just without awareness of potential duplicate model switching), if you request for a model change in the middle of a generation, it will actually get queued already and only make the switch after the generation is done. This only works if the generation is also done via the API, since the queue is only for API requests. I do not see why you cannot use the existing behaviour. If you are that conscious about unnecessary model switching, then just using the options endpoint should be your use case.\r\n\r\nThe issue I've seen is that this will switch immediately after the current generation, not after all pending generations. With the example in the OP, if you had:\r\n\r\n`generation 1 (SD1.5, current) | generation 2 (SD1.5)` \r\nin the queue, and you issue a options change before issuing two SD2.1 generations, it'll end up doing this:\r\n`generation 1 (SD1.5, current) | switch to SD2.1 | generation 2 (SD2.1) | generation 3 (SD2.1) | generation 4 (SD2.1)` \r\ninstead of:\r\n`generation 1 (SD1.5, current) | generation 2 (SD1.5) | switch to SD2.1 | generation 3 (SD2.1) | generation 4 (SD2.1)`\n> switch immediately after the current generation, not after all pending generations\r\n\r\nI meant use this behaviour combined with your own queue server so you avoid unnecessary switching\nOh. Yeah, that's possible, but I'd like to avoid that. Might see if I can write a PR to implement this at some point", "created_at": "2022-12-20T09:40:11Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 5718, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-5718", "issue_numbers": ["4824"], "base_commit": "685f9631b56ff8bd43bce24ff5ce0f9a0e9af490", "patch": "diff --git a/modules/generation_parameters_copypaste.py b/modules/generation_parameters_copypaste.py\nindex 565e342df09..fbd91300550 100644\n--- a/modules/generation_parameters_copypaste.py\n+++ b/modules/generation_parameters_copypaste.py\n@@ -14,6 +14,7 @@\n re_param = re.compile(re_param_code)\r\n re_params = re.compile(r\"^(?:\" + re_param_code + \"){3,}$\")\r\n re_imagesize = re.compile(r\"^(\\d+)x(\\d+)$\")\r\n+re_hypernet_hash = re.compile(\"\\(([0-9a-f]+)\\)$\")\r\n type_of_gr_update = type(gr.update())\r\n paste_fields = {}\r\n bind_list = []\r\n@@ -139,6 +140,30 @@ def run_bind():\n             )\r\n \r\n \r\n+def find_hypernetwork_key(hypernet_name, hypernet_hash=None):\r\n+    \"\"\"Determines the config parameter name to use for the hypernet based on the parameters in the infotext.\r\n+\r\n+    Example: an infotext provides \"Hypernet: ke-ta\" and \"Hypernet hash: 1234abcd\". For the \"Hypernet\" config\r\n+    parameter this means there should be an entry that looks like \"ke-ta-10000(1234abcd)\" to set it to.\r\n+\r\n+    If the infotext has no hash, then a hypernet with the same name will be selected instead.\r\n+    \"\"\"\r\n+    hypernet_name = hypernet_name.lower()\r\n+    if hypernet_hash is not None:\r\n+        # Try to match the hash in the name\r\n+        for hypernet_key in shared.hypernetworks.keys():\r\n+            result = re_hypernet_hash.search(hypernet_key)\r\n+            if result is not None and result[1] == hypernet_hash:\r\n+                return hypernet_key\r\n+    else:\r\n+        # Fall back to a hypernet with the same name\r\n+        for hypernet_key in shared.hypernetworks.keys():\r\n+            if hypernet_key.lower().startswith(hypernet_name):\r\n+                return hypernet_key\r\n+\r\n+    return None\r\n+\r\n+\r\n def parse_generation_parameters(x: str):\r\n     \"\"\"parses generation parameters string, the one you see in text field under the picture in UI:\r\n ```\r\n@@ -188,6 +213,14 @@ def parse_generation_parameters(x: str):\n     if \"Clip skip\" not in res:\r\n         res[\"Clip skip\"] = \"1\"\r\n \r\n+    if \"Hypernet strength\" not in res:\r\n+        res[\"Hypernet strength\"] = \"1\"\r\n+\r\n+    if \"Hypernet\" in res:\r\n+        hypernet_name = res[\"Hypernet\"]\r\n+        hypernet_hash = res.get(\"Hypernet hash\", None)\r\n+        res[\"Hypernet\"] = find_hypernetwork_key(hypernet_name, hypernet_hash)\r\n+\r\n     return res\r\n \r\n \r\ndiff --git a/modules/processing.py b/modules/processing.py\nindex 24c537d14ec..6dd7491b846 100644\n--- a/modules/processing.py\n+++ b/modules/processing.py\n@@ -314,7 +314,7 @@ def js(self):\n \r\n         return json.dumps(obj)\r\n \r\n-    def infotext(self,  p: StableDiffusionProcessing, index):\r\n+    def infotext(self, p: StableDiffusionProcessing, index):\r\n         return create_infotext(p, self.all_prompts, self.all_seeds, self.all_subseeds, comments=[], position_in_batch=index % self.batch_size, iteration=index // self.batch_size)\r\n \r\n \r\n@@ -429,6 +429,7 @@ def create_infotext(p, all_prompts, all_seeds, all_subseeds, comments, iteration\n         \"Model hash\": getattr(p, 'sd_model_hash', None if not opts.add_model_hash_to_info or not shared.sd_model.sd_model_hash else shared.sd_model.sd_model_hash),\r\n         \"Model\": (None if not opts.add_model_name_to_info or not shared.sd_model.sd_checkpoint_info.model_name else shared.sd_model.sd_checkpoint_info.model_name.replace(',', '').replace(':', '')),\r\n         \"Hypernet\": (None if shared.loaded_hypernetwork is None else shared.loaded_hypernetwork.name),\r\n+        \"Hypernet hash\": (None if shared.loaded_hypernetwork is None else sd_models.model_hash(shared.loaded_hypernetwork.filename)),\r\n         \"Hypernet strength\": (None if shared.loaded_hypernetwork is None or shared.opts.sd_hypernetwork_strength >= 1 else shared.opts.sd_hypernetwork_strength),\r\n         \"Batch size\": (None if p.batch_size < 2 else p.batch_size),\r\n         \"Batch pos\": (None if p.batch_size < 2 else position_in_batch),\r\n@@ -446,7 +447,7 @@ def create_infotext(p, all_prompts, all_seeds, all_subseeds, comments, iteration\n \r\n     generation_params_text = \", \".join([k if k == v else f'{k}: {generation_parameters_copypaste.quote(v)}' for k, v in generation_params.items() if v is not None])\r\n \r\n-    negative_prompt_text = \"\\nNegative prompt: \" + p.all_negative_prompts[index] if  p.all_negative_prompts[index] else \"\"\r\n+    negative_prompt_text = \"\\nNegative prompt: \" + p.all_negative_prompts[index] if p.all_negative_prompts[index] else \"\"\r\n \r\n     return f\"{all_prompts[index]}{negative_prompt_text}\\n{generation_params_text}\".strip()\r\n \r\n", "test_patch": "", "problem_statement": "[Bug]: PNG Info - Send to txt2img: Not work Hypernet\n### Is there an existing issue for this?\n\n- [X] I have searched the existing issues and checked the recent builds/commits\n\n### What happened?\n\nSelect Hypernetwork is Empty\n\n### Steps to reproduce the problem\n\n1. Quicksettings list\r\nsd_hypernetwork\r\n\r\nCheckbox is OFF:\r\nWhen reading generation parameters from text into UI (from PNG info or pasted text), do not change the selected model/checkpoint.\r\n\r\n2. txt2img\r\nSelect Hypernetwork: helloasuka\r\n\r\ngirl eating hamburger\r\nSteps: 15, Sampler: DPM++ 2M Karras, CFG scale: 7, Seed: 4099472692, Size: 512x1024, Model hash: 7460a6fa, Hypernet: helloasuka\r\n\r\n3. drop Image to PNG Info\r\n4. Click Send to txt2img\r\n5. Hypernetwork to Empty\r\n\r\nSelect None -> Send txt2img -> Empty\r\nSelect helloasuka -> Send txt2img -> Empty\r\nSelect otherpt -> Send txt2img -> Empty\r\n\n\n### What should have happened?\n\nSelect helloasuka\n\n### Commit where the problem happens\n\n98947d173e3f1667eba29c904f681047dea9de90\n\n### What platforms do you use to access UI ?\n\nWindows\n\n### What browsers do you use to access the UI ?\n\nMozilla Firefox\n\n### Command Line Arguments\n\n_No response_\n\n### Additional information, context and logs\n\nInconsistent specifications\r\n\r\nPNG Info:\r\nshared.loaded_hypernetwork.name\r\n\r\nSelect Hypernetwork:\r\nfilename(sd_model_hash)\r\n\r\nHow about changing it like this.\r\nSelect Hypernetwork:\r\nshared.loaded_hypernetwork.name (filename.pt)\r\n\n", "hints_text": "https://github.com/aka7774/sd_infotext_ex", "created_at": "2022-12-13T22:33:31Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 5542, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-5542", "issue_numbers": ["5372"], "base_commit": "44c46f0ed395967cd3830dd481a2db759fda5b3b", "patch": "diff --git a/README.md b/README.md\nindex 8a4ffadeb27..5599058146e 100644\n--- a/README.md\n+++ b/README.md\n@@ -135,6 +135,7 @@ The documentation was moved from this README over to the project's [wiki](https:\n - SwinIR - https://github.com/JingyunLiang/SwinIR\r\n - Swin2SR - https://github.com/mv-lab/swin2sr\r\n - LDSR - https://github.com/Hafiidz/latent-diffusion\r\n+- MiDaS - https://github.com/isl-org/MiDaS\r\n - Ideas for optimizations - https://github.com/basujindal/stable-diffusion\r\n - Cross Attention layer optimization - Doggettx - https://github.com/Doggettx/stable-diffusion, original idea for prompt editing.\r\n - Cross Attention layer optimization - InvokeAI, lstein - https://github.com/invoke-ai/InvokeAI (originally http://github.com/lstein/stable-diffusion)\r\ndiff --git a/modules/processing.py b/modules/processing.py\nindex 3d2c4dc9f82..0417ffc5f61 100644\n--- a/modules/processing.py\n+++ b/modules/processing.py\n@@ -21,7 +21,10 @@\n import modules.images as images\r\n import modules.styles\r\n import logging\r\n+from ldm.data.util import AddMiDaS\r\n+from ldm.models.diffusion.ddpm import LatentDepth2ImageDiffusion\r\n \r\n+from einops import repeat, rearrange\r\n \r\n # some of those options should not be changed at all because they would break the model, so I removed them from options.\r\n opt_C = 4\r\n@@ -150,11 +153,26 @@ def txt2img_image_conditioning(self, x, width=None, height=None):\n \r\n         return image_conditioning\r\n \r\n-    def img2img_image_conditioning(self, source_image, latent_image, image_mask = None):\r\n-        if self.sampler.conditioning_key not in {'hybrid', 'concat'}:\r\n-            # Dummy zero conditioning if we're not using inpainting model.\r\n-            return latent_image.new_zeros(latent_image.shape[0], 5, 1, 1)\r\n+    def depth2img_image_conditioning(self, source_image):\r\n+        # Use the AddMiDaS helper to Format our source image to suit the MiDaS model\r\n+        transformer = AddMiDaS(model_type=\"dpt_hybrid\")\r\n+        transformed = transformer({\"jpg\": rearrange(source_image[0], \"c h w -> h w c\")})\r\n+        midas_in = torch.from_numpy(transformed[\"midas_in\"][None, ...]).to(device=shared.device)\r\n+        midas_in = repeat(midas_in, \"1 ... -> n ...\", n=self.batch_size)\r\n+\r\n+        conditioning_image = self.sd_model.get_first_stage_encoding(self.sd_model.encode_first_stage(source_image))\r\n+        conditioning = torch.nn.functional.interpolate(\r\n+            self.sd_model.depth_model(midas_in),\r\n+            size=conditioning_image.shape[2:],\r\n+            mode=\"bicubic\",\r\n+            align_corners=False,\r\n+        )\r\n+\r\n+        (depth_min, depth_max) = torch.aminmax(conditioning)\r\n+        conditioning = 2. * (conditioning - depth_min) / (depth_max - depth_min) - 1.\r\n+        return conditioning\r\n \r\n+    def inpainting_image_conditioning(self, source_image, latent_image, image_mask = None):\r\n         self.is_using_inpainting_conditioning = True\r\n \r\n         # Handle the different mask inputs\r\n@@ -191,6 +209,18 @@ def img2img_image_conditioning(self, source_image, latent_image, image_mask = No\n \r\n         return image_conditioning\r\n \r\n+    def img2img_image_conditioning(self, source_image, latent_image, image_mask=None):\r\n+        # HACK: Using introspection as the Depth2Image model doesn't appear to uniquely\r\n+        # identify itself with a field common to all models. The conditioning_key is also hybrid.\r\n+        if isinstance(self.sd_model, LatentDepth2ImageDiffusion):\r\n+            return self.depth2img_image_conditioning(source_image)\r\n+\r\n+        if self.sampler.conditioning_key in {'hybrid', 'concat'}:\r\n+            return self.inpainting_image_conditioning(source_image, latent_image, image_mask=image_mask)\r\n+\r\n+        # Dummy zero conditioning if we're not using inpainting or depth model.\r\n+        return latent_image.new_zeros(latent_image.shape[0], 5, 1, 1)\r\n+\r\n     def init(self, all_prompts, all_seeds, all_subseeds):\r\n         pass\r\n \r\ndiff --git a/modules/sd_models.py b/modules/sd_models.py\nindex 283cf1cda46..139952baaf4 100644\n--- a/modules/sd_models.py\n+++ b/modules/sd_models.py\n@@ -7,6 +7,9 @@\n import re\r\n import safetensors.torch\r\n from omegaconf import OmegaConf\r\n+from os import mkdir\r\n+from urllib import request\r\n+import ldm.modules.midas as midas\r\n \r\n from ldm.util import instantiate_from_config\r\n \r\n@@ -36,6 +39,7 @@ def setup_model():\n         os.makedirs(model_path)\r\n \r\n     list_models()\r\n+    enable_midas_autodownload()\r\n \r\n \r\n def checkpoint_tiles(): \r\n@@ -227,6 +231,48 @@ def load_model_weights(model, checkpoint_info, vae_file=\"auto\"):\n     sd_vae.load_vae(model, vae_file)\r\n \r\n \r\n+def enable_midas_autodownload():\r\n+    \"\"\"\r\n+    Gives the ldm.modules.midas.api.load_model function automatic downloading.\r\n+\r\n+    When the 512-depth-ema model, and other future models like it, is loaded,\r\n+    it calls midas.api.load_model to load the associated midas depth model.\r\n+    This function applies a wrapper to download the model to the correct\r\n+    location automatically.\r\n+    \"\"\"\r\n+\r\n+    midas_path = os.path.join(models_path, 'midas')\r\n+\r\n+    # stable-diffusion-stability-ai hard-codes the midas model path to\r\n+    # a location that differs from where other scripts using this model look.\r\n+    # HACK: Overriding the path here.\r\n+    for k, v in midas.api.ISL_PATHS.items():\r\n+        file_name = os.path.basename(v)\r\n+        midas.api.ISL_PATHS[k] = os.path.join(midas_path, file_name)\r\n+\r\n+    midas_urls = {\r\n+        \"dpt_large\": \"https://github.com/intel-isl/DPT/releases/download/1_0/dpt_large-midas-2f21e586.pt\",\r\n+        \"dpt_hybrid\": \"https://github.com/intel-isl/DPT/releases/download/1_0/dpt_hybrid-midas-501f0c75.pt\",\r\n+        \"midas_v21\": \"https://github.com/AlexeyAB/MiDaS/releases/download/midas_dpt/midas_v21-f6b98070.pt\",\r\n+        \"midas_v21_small\": \"https://github.com/AlexeyAB/MiDaS/releases/download/midas_dpt/midas_v21_small-70d6b9c8.pt\",\r\n+    }\r\n+\r\n+    midas.api.load_model_inner = midas.api.load_model\r\n+\r\n+    def load_model_wrapper(model_type):\r\n+        path = midas.api.ISL_PATHS[model_type]\r\n+        if not os.path.exists(path):\r\n+            if not os.path.exists(midas_path):\r\n+                mkdir(midas_path)\r\n+    \r\n+            print(f\"Downloading midas model weights for {model_type} to {path}\")\r\n+            request.urlretrieve(midas_urls[model_type], path)\r\n+            print(f\"{model_type} downloaded\")\r\n+\r\n+        return midas.api.load_model_inner(model_type)\r\n+\r\n+    midas.api.load_model = load_model_wrapper\r\n+\r\n def load_model(checkpoint_info=None):\r\n     from modules import lowvram, sd_hijack\r\n     checkpoint_info = checkpoint_info or select_checkpoint()\r\n", "test_patch": "", "problem_statement": "[Bug]: Cannot run \"512-depth-ema.ckpt\" model\n### Is there an existing issue for this?\n\n- [X] I have searched the existing issues and checked the recent builds/commits\n\n### What happened?\n\nI want to run the depth model (512-depth-ema.ckpt (d522022)) of stable diffusion 2.0, but it came to an error. \r\nThough I have run the basic model (768-v-ema.ckpt (2c02b20a)) successfully on previous, the depth one yet not. I emphasize that I put the \"512-depth-ema.yaml\", which is renamed from \"v2-inferance.yaml\", correctly in /models/stable-diffusion/, next to the model. \r\nPuzzled, I redownload it, nothing different happen. I stress once again that I have run the basic one smoothly before!\n\n### Steps to reproduce the problem\n\n1. Add \"512-depth-ema.yaml\" and \"512-depth-ema.yaml\" to /models/stable-diffusion/\r\n2. Go to webui-user.bat and run it\r\n3. select the 512-depth-ema.ckpt (d0522d12) in WEBUI page\r\n4. oops, it crash!!\n\n### What should have happened?\n\nI think no error should occur and I can produce depth image by send image in.\n\n### Commit where the problem happens\n\nin loading 512-depth-ema.ckpt, the commit hash is ce049c471b4a1d22f5a8fe8f527788edcf934eda\n\n### What platforms do you use to access UI ?\n\nWindows\n\n### What browsers do you use to access the UI ?\n\nMicrosoft Edge\n\n### Command Line Arguments\n\n```Shell\nLoading config from: D:\\AI6\\stable-diffusion-webui\\models\\Stable-diffusion\\512-depth-ema.yaml\r\nLatentDiffusion: Running in v-prediction mode\r\nDiffusionWrapper has 865.91 M params.\r\nLoading weights [d0522d12] from D:\\AI6\\stable-diffusion-webui\\models\\Stable-diffusion\\512-depth-ema.ckpt\r\nTraceback (most recent call last):\r\n  File \"D:\\AI6\\stable-diffusion-webui\\venv\\lib\\site-packages\\gradio\\routes.py\", line 284, in run_predict\r\n    output = await app.blocks.process_api(\r\n  File \"D:\\AI6\\stable-diffusion-webui\\venv\\lib\\site-packages\\gradio\\blocks.py\", line 982, in process_api\r\n    result = await self.call_function(fn_index, inputs, iterator)\r\n  File \"D:\\AI6\\stable-diffusion-webui\\venv\\lib\\site-packages\\gradio\\blocks.py\", line 824, in call_function\r\n    prediction = await anyio.to_thread.run_sync(\r\n  File \"D:\\AI6\\stable-diffusion-webui\\venv\\lib\\site-packages\\anyio\\to_thread.py\", line 31, in run_sync\r\n    return await get_asynclib().run_sync_in_worker_thread(\r\n  File \"D:\\AI6\\stable-diffusion-webui\\venv\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 937, in run_sync_in_worker_thread\r\n    return await future\r\n  File \"D:\\AI6\\stable-diffusion-webui\\venv\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 867, in run\r\n    result = context.run(func, *args)\r\n  File \"D:\\AI6\\stable-diffusion-webui\\modules\\ui.py\", line 1616, in <lambda>\r\n    fn=lambda value, k=k: run_settings_single(value, key=k),\r\n  File \"D:\\AI6\\stable-diffusion-webui\\modules\\ui.py\", line 1457, in run_settings_single\r\n    if not opts.set(key, value):\r\n  File \"D:\\AI6\\stable-diffusion-webui\\modules\\shared.py\", line 478, in set\r\n    self.data_labels[key].onchange()\r\n  File \"D:\\AI6\\stable-diffusion-webui\\modules\\call_queue.py\", line 15, in f\r\n    res = func(*args, **kwargs)\r\n  File \"D:\\AI6\\stable-diffusion-webui\\webui.py\", line 62, in <lambda>\r\n    shared.opts.onchange(\"sd_model_checkpoint\", wrap_queued_call(lambda: modules.sd_models.reload_model_weights()))\r\n  File \"D:\\AI6\\stable-diffusion-webui\\modules\\sd_models.py\", line 292, in reload_model_weights\r\n    load_model(checkpoint_info)\r\n  File \"D:\\AI6\\stable-diffusion-webui\\modules\\sd_models.py\", line 261, in load_model\r\n    load_model_weights(sd_model, checkpoint_info)\r\n  File \"D:\\AI6\\stable-diffusion-webui\\modules\\sd_models.py\", line 192, in load_model_weights\r\n    model.load_state_dict(sd, strict=False)\r\n  File \"D:\\AI6\\stable-diffusion-webui\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1604, in load_state_dict\r\n    raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\r\nRuntimeError: Error(s) in loading state_dict for LatentDiffusion:\r\n        size mismatch for model.diffusion_model.input_blocks.0.0.weight: copying a param with shape torch.Size([320, 5, 3, 3]) from checkpoint, the shape in current model is torch.Size([320, 4, 3, 3]).\n```\n\n\n### Additional information, context and logs\n\nThe model hash is d522d022, and SHA1: a81e98c0f932a55d8dd08058c1b9ce68525ac6a5\r\nThe error seems coming from bad torch size setting.\n", "hints_text": "Version 2.0 works differently from 1.x versions, and right now only the 768-v-ema.ckpt model is supported \n> Version 2.0 works differently from 1.x versions, and right now only the 768-v-ema.ckpt model is supported\r\n\r\nThanks, guy! I have replaced it by extension \"depthmap2mask\" [Extraltodeus](https://github.com/Extraltodeus).\r\nHowever, I still want to use it in future...:-(\n> Version 2.0 works differently from 1.x versions, and right now only the 768-v-ema.ckpt model is supported\r\n\r\nNow! Boy! I have found a leak of yaml, which can run the 512-basic-ema.ckpt smoothly with webui.\r\n[v2-inference-e.zip](https://github.com/AUTOMATIC1111/stable-diffusion-webui/files/10148302/v2-inference-e.zip)\r\n\n> I have found a leak of yaml, which can run the 512-basic-ema.ckpt smoothly with webui.\r\n\r\nThanks @LieDeath. This yaml config file worked for me. \r\n\r\nI was not able to load 512-basic-ema with original instructions.\r\n\r\nWhere is that file coming from if I may ask? \n@healthyfat Did you download the right one from the original instructions? They're currently confusingly-phrased, with the location of the 512 file tacked on as an afterthought.\r\n\r\n![image](https://user-images.githubusercontent.com/118340091/206285490-548829e0-1da2-4ccc-a0d1-0f5f6416c9f3.png)\r\n\r\n(Amusingly: All of these files get downloaded for us to `repositories\\stable-diffusion-stability-ai\\configs\\stable-diffusion` anyway when the SD2.0 repo is cloned into a subfolder; it's just a matter of copying them to a place that this repo looks for them)\n@AnonymousCervine \r\n\r\n> Did you download the right one from the original instructions?\r\n\r\nYes. I used the config from the link you highlighted. It doesn't work for me. \r\nOnly v2-inference-e.yaml does. \nInteresting. I wonder what the difference between our setups is.\r\n\r\n(Probably as much as should be spoken of here; it's off-topic to this issue and probably needs its own?)\n> Interesting. I wonder what the difference between our setups is.\r\n> \r\n> (Probably as much as should be spoken of here; it's off-topic to this issue and probably needs its own?)\r\n\r\nI take a review and find the only difference between **v2-inference-e.yaml** and **v2-inference.yaml** is _a line break_ at the end of the config file. Perhaps on some devices the missing line break in the **v2-inference.yaml** could cause problems:-(.", "created_at": "2022-12-09T01:49:04Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 5304, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-5304", "issue_numbers": ["4713"], "base_commit": "4b3c5bc24bffdf429c463a465763b3077fe55eb8", "patch": "diff --git a/modules/generation_parameters_copypaste.py b/modules/generation_parameters_copypaste.py\nindex 01980dcaea2..44fe1a6c2dc 100644\n--- a/modules/generation_parameters_copypaste.py\n+++ b/modules/generation_parameters_copypaste.py\n@@ -184,6 +184,10 @@ def parse_generation_parameters(x: str):\n         else:\r\n             res[k] = v\r\n \r\n+    # Missing CLIP skip means it was set to 1 (the default)\r\n+    if \"Clip skip\" not in res:\r\n+        res[\"Clip skip\"] = \"1\"\r\n+\r\n     return res\r\n \r\n \r\ndiff --git a/modules/shared.py b/modules/shared.py\nindex c36ee211ac2..b4ecc7ca445 100644\n--- a/modules/shared.py\n+++ b/modules/shared.py\n@@ -371,7 +371,7 @@ def list_samplers():\n     \"enable_batch_seeds\": OptionInfo(True, \"Make K-diffusion samplers produce same images in a batch as when making a single image\"),\r\n     \"comma_padding_backtrack\": OptionInfo(20, \"Increase coherency by padding from the last comma within n tokens when using more than 75 tokens\", gr.Slider, {\"minimum\": 0, \"maximum\": 74, \"step\": 1 }),\r\n     \"filter_nsfw\": OptionInfo(False, \"Filter NSFW content\"),\r\n-    'CLIP_stop_at_last_layers': OptionInfo(1, \"Stop At last layers of CLIP model\", gr.Slider, {\"minimum\": 1, \"maximum\": 12, \"step\": 1}),\r\n+    'CLIP_stop_at_last_layers': OptionInfo(1, \"Stop at last layers of CLIP model (CLIP skip)\", gr.Slider, {\"minimum\": 1, \"maximum\": 12, \"step\": 1}),\r\n     \"random_artist_categories\": OptionInfo([], \"Allowed categories for random artists selection when using the Roll button\", gr.CheckboxGroup, {\"choices\": artist_db.categories()}),\r\n }))\r\n \r\n", "test_patch": "", "problem_statement": "[Bug]: gens from same seed/params/model permanently changed\n### Is there an existing issue for this?\r\n\r\n- [X] I have searched the existing issues and checked the recent builds/commits\r\n\r\n### What happened?\r\n\r\nthe gens for any given seed/parameter no longer match the original gen\r\n\r\n This has been recurring for me over the last week or so without any clear cause or explanation.  Only solution is a clean reclone of the complete repo, at which point it properly gens again for a while, then does it again.\r\n\r\nThe two images below are generated from the exact same seed, parameters, model, run about two minutes apart.   Important to note that if I do a clean re-install and run the gen again, it will come out identical to the first instance, and once something happens it always comes out identical to the second instance. Further, I have instances of the first gen that go back at least six weeks, so I would call that the \"correct\" gen for that seed/model/prompt.\r\n\r\nOnce the generations are modified, the only solution I've found that works is to completely wipe the install, do a clean git clone of the repo and restart.  Afterwards the problem can come back as quickly as two gens or not for some time,\r\n\r\n\r\n\r\n\r\n\r\n### Steps to reproduce the problem\r\n\r\n1. install A1111 and set the model to F111 [or whatever other model]\r\n2. Load an image into png info, select send to text to image, and gen an image\r\n3.  continue changing models and input images for 2-X events\r\n4. Intermittently check the original gen/model combo\r\n\r\n\r\n\r\n### What should have happened?\r\n\r\nEvery time the model, seed, prompt and gradio settings are the same, the gen /output should remain the same, over time.\r\n\r\n\r\n### Commit where the problem happens\r\n\r\n98947d173e3f1667eba29c904f681047dea9de90\r\n\r\n### What platforms do you use to access UI ?\r\n\r\nWindows\r\n\r\n### What browsers do you use to access the UI ?\r\n\r\nMozilla Firefox\r\n\r\n### Command Line Arguments\r\n\r\n```Shell\r\nnone\r\n```\r\n\r\n\r\n### Additional information, context and logs\r\n\r\nThe issue doesn't seem to be associated with a specific model, although F111 is the one I've had enough history on to have a baseline image to compare it for the longest.  Once the switch happens, all models have their outputs changed.\r\n\r\nI've reviewed most or all of the files and directories in the a111 distro to see which ones have been revised pre-post the symptom, but only params and config seem to be materially revised.\r\n\r\n\r\n\r\n\r\n\n", "hints_text": "Just noticed this exact same thing happening to me as well. Also did a new clone, it worked fine for a bit but then it started happening again.\nOK I've identified the cause and can correct the issue manually, but it still feels like a bug.  This appears to result from clip skip > 1 being \"inherited\" by subsequent gens from a seed that may not have had a clip skip in its original prompt.  Forcing clipskip =1 in settings results in the image gen correctly matching the legacy input seed/params.   To replicate the issue:\r\n\r\n1. Load an image whose png info does not include clipskip. Generate should match the original. Reviewing the params.txt file, there is no \"clip skip\" parameter \r\n2. Load a different  image whose png info includes clipskip =2 . Generate.  Review the params file. it will now show clipskip 2, despite the source image png info being default/none or 1( when I  switch between clip skip 1 and 2 in settings, I get the same set of results - 1 = matches original exactly, 2= different from original, but always the same)\r\n4. reload the 1st image without the clipskip info and generate.  Review the parameters in both settings and the params.txt file - the clip skip for the original image will now be 2.\r\n\r\nThis feels like it's still a bug because the _global_ clip_ skip value in the settings menu is being reset to whatever the png info tells it to (rather than just rendering that specific instance with clip skip value specific to that image parameter.   \r\nWould it be possible for png-modified clip-skips only to be passed to that rendering argument, rather than modifying clip skip for _all_ gens?\r\n\r\nIt's additionally worth noting that a **brand new image gen** (as of this commit anyways) that has clip skip 1 does not retain ANY clip skip info, so while > 1 re-writes the global clipskip setting to whatever that is, a clipskip 1 image will not do so. So any clipskip 1's require manually updating the global setting every time.\r\n\r\n\r\n\r\n\r\n\n> OK I've identified the cause and can correct the issue manually, but it still feels like a bug. This appears to result from clip skip > 1 being \"inherited\" by subsequent gens from a seed that may not have had a clip skip in its original prompt. Forcing clipskip =1 in settings results in the image gen correctly matching the legacy input seed/params. To replicate the issue:\r\n> \r\n> 1. Load an image whose png info does not include clipskip. Generate should match the original. Reviewing the params.txt file, there is no \"clip skip\" parameter\r\n> 2. Load a different  image whose png info includes clipskip =2 . Generate.  Review the params file. it will now show clipskip 2, despite the source image png info being default/none or 1( when I  switch between clip skip 1 and 2 in settings, I get the same set of results - 1 = matches original exactly, 2= different from original, but always the same)\r\n> 3. reload the 1st image without the clipskip info and generate.  Review the parameters in both settings and the params.txt file - the clip skip for the original image will now be 2.\r\n> \r\n> This feels like it's still a bug because the _global_ clip_ skip value in the settings menu is being reset to whatever the png info tells it to (rather than just rendering that specific instance with clip skip value specific to that image parameter. Would it be possible for png-modified clip-skips only to be passed to that rendering argument, rather than modifying clip skip for _all_ gens?\r\n> \r\n> It's additionally worth noting that a **brand new image gen** (as of this commit anyways) that has clip skip 1 does not retain ANY clip skip info, so while > 1 re-writes the global clipskip setting to whatever that is, a clipskip 1 image will not do so. So any clipskip 1's require manually updating the global setting every time.\r\n\r\nThis was very useful and solved the problem for me too. Thanks!\n> clip skip 1 does not retain ANY clip skip info\r\n\r\nI saw an issue here (but I cannot find it now) which proposed these changes:\r\n1. Always output clipskip to meta-info.\r\n2. Treat non-existing clipskip in meta-info as \"1\".\r\n\r\nLooks like both are need to be implemented, otherwise the issues like this will be countless!\nRan across the same issue, I was scratching my head about how my seeds could have been changed, but it turns out I had a non-zero clip-skip that wasn't being overridden by the prompt missing a `Clip skip` entry", "created_at": "2022-12-01T19:35:29Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 5194, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-5194", "issue_numbers": ["4119"], "base_commit": "4b3c5bc24bffdf429c463a465763b3077fe55eb8", "patch": "diff --git a/modules/devices.py b/modules/devices.py\nindex f00079c6bdc..046460fa08b 100644\n--- a/modules/devices.py\n+++ b/modules/devices.py\n@@ -66,24 +66,15 @@ def enable_tf32():\n \n \n def randn(seed, shape):\n-    # Pytorch currently doesn't handle setting randomness correctly when the metal backend is used.\n-    if device.type == 'mps':\n-        generator = torch.Generator(device=cpu)\n-        generator.manual_seed(seed)\n-        noise = torch.randn(shape, generator=generator, device=cpu).to(device)\n-        return noise\n-\n     torch.manual_seed(seed)\n+    if device.type == 'mps':\n+        return torch.randn(shape, device=cpu).to(device)\n     return torch.randn(shape, device=device)\n \n \n def randn_without_seed(shape):\n-    # Pytorch currently doesn't handle setting randomness correctly when the metal backend is used.\n     if device.type == 'mps':\n-        generator = torch.Generator(device=cpu)\n-        noise = torch.randn(shape, generator=generator, device=cpu).to(device)\n-        return noise\n-\n+        return torch.randn(shape, device=cpu).to(device)\n     return torch.randn(shape, device=device)\n \n \ndiff --git a/modules/hypernetworks/hypernetwork.py b/modules/hypernetworks/hypernetwork.py\nindex 8466887f60f..eb5ae372f60 100644\n--- a/modules/hypernetworks/hypernetwork.py\n+++ b/modules/hypernetworks/hypernetwork.py\n@@ -495,7 +495,7 @@ def train_hypernetwork(hypernetwork_name, learn_rate, batch_size, gradient_step,\n                 if shared.state.interrupted:\r\n                     break\r\n \r\n-                with torch.autocast(\"cuda\"):\r\n+                with devices.autocast():\r\n                     x = batch.latent_sample.to(devices.device, non_blocking=pin_memory)\r\n                     if tag_drop_out != 0 or shuffle_tags:\r\n                         shared.sd_model.cond_stage_model.to(devices.device)\r\ndiff --git a/modules/interrogate.py b/modules/interrogate.py\nindex 9769aa347e7..40c6b08277b 100644\n--- a/modules/interrogate.py\n+++ b/modules/interrogate.py\n@@ -148,8 +148,7 @@ def interrogate(self, pil_image):\n \r\n             clip_image = self.clip_preprocess(pil_image).unsqueeze(0).type(self.dtype).to(devices.device_interrogate)\r\n \r\n-            precision_scope = torch.autocast if shared.cmd_opts.precision == \"autocast\" else contextlib.nullcontext\r\n-            with torch.no_grad(), precision_scope(\"cuda\"):\r\n+            with torch.no_grad(), devices.autocast():\r\n                 image_features = self.clip_model.encode_image(clip_image).type(self.dtype)\r\n \r\n                 image_features /= image_features.norm(dim=-1, keepdim=True)\r\ndiff --git a/modules/sd_hijack.py b/modules/sd_hijack.py\nindex b824b5bffca..ce583950469 100644\n--- a/modules/sd_hijack.py\n+++ b/modules/sd_hijack.py\n@@ -182,11 +182,7 @@ def register_buffer(self, name, attr):\n \r\n     if type(attr) == torch.Tensor:\r\n         if attr.device != devices.device:\r\n-\r\n-            if devices.has_mps():\r\n-                attr = attr.to(device=\"mps\", dtype=torch.float32)\r\n-            else:\r\n-                attr = attr.to(devices.device)\r\n+            attr = attr.to(device=devices.device, dtype=(torch.float32 if devices.device.type == 'mps' else None))\r\n \r\n     setattr(self, name, attr)\r\n \r\ndiff --git a/modules/sd_samplers.py b/modules/sd_samplers.py\nindex 5fefb2277b0..4c123d3b6eb 100644\n--- a/modules/sd_samplers.py\n+++ b/modules/sd_samplers.py\n@@ -6,6 +6,7 @@\n from PIL import Image\r\n import inspect\r\n import k_diffusion.sampling\r\n+import torchsde._brownian.brownian_interval\r\n import ldm.models.diffusion.ddim\r\n import ldm.models.diffusion.plms\r\n from modules import prompt_parser, devices, processing, images\r\n@@ -364,7 +365,23 @@ def randn_like(self, x):\n             if noise.shape == x.shape:\r\n                 return noise\r\n \r\n-        return torch.randn_like(x)\r\n+        if x.device.type == 'mps':\r\n+            return torch.randn_like(x, device=devices.cpu).to(x.device)\r\n+        else:\r\n+            return torch.randn_like(x)\r\n+\r\n+\r\n+# MPS fix for randn in torchsde\r\n+def torchsde_randn(size, dtype, device, seed):\r\n+    if device.type == 'mps':\r\n+        generator = torch.Generator(devices.cpu).manual_seed(int(seed))\r\n+        return torch.randn(size, dtype=dtype, device=devices.cpu, generator=generator).to(device)\r\n+    else:\r\n+        generator = torch.Generator(device).manual_seed(int(seed))\r\n+        return torch.randn(size, dtype=dtype, device=device, generator=generator)\r\n+\r\n+\r\n+torchsde._brownian.brownian_interval._randn = torchsde_randn\r\n \r\n \r\n class KDiffusionSampler:\r\n@@ -415,8 +432,7 @@ def initialize(self, p):\n         self.model_wrap.step = 0\r\n         self.eta = p.eta or opts.eta_ancestral\r\n \r\n-        if self.sampler_noises is not None:\r\n-            k_diffusion.sampling.torch = TorchHijack(self.sampler_noises)\r\n+        k_diffusion.sampling.torch = TorchHijack(self.sampler_noises if self.sampler_noises is not None else [])\r\n \r\n         extra_params_kwargs = {}\r\n         for param_name in self.extra_params:\r\ndiff --git a/modules/swinir_model.py b/modules/swinir_model.py\nindex facd262db57..483eabd4adc 100644\n--- a/modules/swinir_model.py\n+++ b/modules/swinir_model.py\n@@ -13,10 +13,6 @@\n from modules.swinir_model_arch_v2 import Swin2SR as net2\n from modules.upscaler import Upscaler, UpscalerData\n \n-precision_scope = (\n-    torch.autocast if cmd_opts.precision == \"autocast\" else contextlib.nullcontext\n-)\n-\n \n class UpscalerSwinIR(Upscaler):\n     def __init__(self, dirname):\n@@ -112,7 +108,7 @@ def upscale(\n     img = np.moveaxis(img, 2, 0) / 255\n     img = torch.from_numpy(img).float()\n     img = img.unsqueeze(0).to(devices.device_swinir)\n-    with torch.no_grad(), precision_scope(\"cuda\"):\n+    with torch.no_grad(), devices.autocast():\n         _, _, h_old, w_old = img.size()\n         h_pad = (h_old // window_size + 1) * window_size - h_old\n         w_pad = (w_old // window_size + 1) * window_size - w_old\ndiff --git a/modules/textual_inversion/dataset.py b/modules/textual_inversion/dataset.py\nindex e5725f33f49..2dc64c3c2c2 100644\n--- a/modules/textual_inversion/dataset.py\n+++ b/modules/textual_inversion/dataset.py\n@@ -82,7 +82,7 @@ def __init__(self, data_root, width, height, repeats, flip_p=0.5, placeholder_to\n             torchdata = torch.from_numpy(npimage).permute(2, 0, 1).to(device=device, dtype=torch.float32)\r\n             latent_sample = None\r\n \r\n-            with torch.autocast(\"cuda\"):\r\n+            with devices.autocast():\r\n                 latent_dist = model.encode_first_stage(torchdata.unsqueeze(dim=0))\r\n \r\n             if latent_sampling_method == \"once\" or (latent_sampling_method == \"deterministic\" and not isinstance(latent_dist, DiagonalGaussianDistribution)):\r\n@@ -101,7 +101,7 @@ def __init__(self, data_root, width, height, repeats, flip_p=0.5, placeholder_to\n                 entry.cond_text = self.create_text(filename_text)\r\n \r\n             if include_cond and not (self.tag_drop_out != 0 or self.shuffle_tags):\r\n-                with torch.autocast(\"cuda\"):\r\n+                with devices.autocast():\r\n                     entry.cond = cond_model([entry.cond_text]).to(devices.cpu).squeeze(0)\r\n \r\n             self.dataset.append(entry)\r\ndiff --git a/modules/textual_inversion/textual_inversion.py b/modules/textual_inversion/textual_inversion.py\nindex 4eb75cb5149..daf8d1b84c0 100644\n--- a/modules/textual_inversion/textual_inversion.py\n+++ b/modules/textual_inversion/textual_inversion.py\n@@ -316,7 +316,7 @@ def train_embedding(embedding_name, learn_rate, batch_size, gradient_step, data_\n                 if shared.state.interrupted:\r\n                     break\r\n \r\n-                with torch.autocast(\"cuda\"):\r\n+                with devices.autocast():\r\n                     # c = stack_conds(batch.cond).to(devices.device)\r\n                     # mask = torch.tensor(batch.emb_index).to(devices.device, non_blocking=pin_memory)\r\n                     # print(mask)\r\n", "test_patch": "", "problem_statement": "[Bug]: Seed doesn't work on Mac\n### Is there an existing issue for this?\r\n\r\n- [X] I have searched the existing issues and checked the recent builds/commits\r\n\r\n### What happened?\r\n\r\nUsing a fixed seed either setting manually or by the reuse seed function seems to not work on Mac, meaning each time it generates a completely random new image. Thus comparing different settings with X/Y is impossible. Is there a way to make it work on Mac (apple silicon, M1)?\r\n\r\n### Steps to reproduce the problem\r\n\r\n1. Use the same seed on Mac and you get totally random images\r\n\r\n### What should have happened?\r\n\r\nShould get the same image, so X/Y comparison is actually useful to explore the effect of different settings.\r\n\r\n### Commit where the problem happens\r\n\r\nlatest\r\n\r\n### What platforms do you use to access UI ?\r\n\r\nMacOS\r\n\r\n### What browsers do you use to access the UI ?\r\n\r\nMozilla Firefox, Google Chrome, Brave, Apple Safari\r\n\r\n### Command Line Arguments\r\n\r\n_No response_\r\n\r\n### Additional information, context and logs\r\n\r\nExample, I run this 2x, where I even set the variation seed to the same value as the seed\r\n\r\nBeeple by Greg Rutkowski\r\nSteps: 20, Sampler: Euler a, CFG scale: 13, Seed: 3774371817, Size: 512x512, Model hash: a9263745\r\n\r\n![01097-3774371817-Beeple by Greg Rutkowski](https://user-images.githubusercontent.com/2893181/199352396-91fc629e-dd74-4af1-b2b0-d07e7c910151.png)\r\n![01096-3774371817-Beeple by Greg Rutkowski](https://user-images.githubusercontent.com/2893181/199352403-c6f37414-25e6-4642-88d0-3fc7e93a3202.png)\n", "hints_text": "I've had the same issue with Euler a and some of the other sampling methods on my M1 Mac since install. **Standard Euler does work** for me, and produces consistent values off of seed.\r\n\r\nFrom what I remember from my previous googling, it seems to be related to the way that Tensorflow manages random values and seeding while running on CPU vs. GPU or different architectures, etc.\r\n\r\nI don't remember 100%, but it was way simpler to just use Euler as a default instead of Euler a than to try and get it working. It would be great if there's a way to get it working on Mac, but I'd try out the other sampling methods and see if they have the same problem.\nthis is probably related to random noise configuration in Settings > Sampler parameters\n@remixer-dec and @abrisene you guys seriously rock! I could finally make `Euler a` reproduce the same thing.\r\n1. on Settings tab set `eta (noise multiplier) for ancestral samplers` to 0\r\n2. on txt2img tab set `variation seed` (tick the `extra` checkbox to reveal the option)  to the same as the `seed`\r\n\r\nI don't have extensive knowledge why this is the way it is, but I think when a seed is specifically set, it should generate the same image.\r\n\r\n\"Viking by greg rutkowski\", 20 step, 7 cfg, seed: 2640744521, variation seed: 2640744521\r\n\r\n![01122-2640744521-Viking by greg rutkowski](https://user-images.githubusercontent.com/2893181/199719606-0729cb61-3d9d-4f30-8d1c-cfe686b667c9.png)\r\n\r\n_note:_ with variation seed -1, and seed 2640744521, it generated slightly different images sometimes, eg:\r\n![01120-2640744521-Viking by greg rutkowski](https://user-images.githubusercontent.com/2893181/199722158-9a22661c-772c-4463-80a3-59229424c912.png)\nI also had this issue. Seting the eta (noise multiplier) for ancestral samplers to 0 helps with Eular a. But I have a bigger issue with the seed. I get consistent results for the same seed but it is different than the results I get when I use automatic1111 on a colab or use Dream Studio or some other version of SD. In all these other versions of SD, the same seed and settings give me the same results, but on my Mac, it produces something very diffrent. Its really really annoying because I cannot try to work on some of the ideas that I created on other platforms! Does anyone else have this issue? Is there some way to fix this?\nI also have this issue.  However, when I follow your example, I still get a different image.\r\nI assume you are using SD 1.5?\r\n\r\nI cleaned out my install and installed the latest version from here.\r\n\r\nThis is my prompt:\r\nViking by greg rutkowski\r\nSteps: 20, Sampler: Euler a, CFG scale: 7, Seed: 2640744521, Size: 512x512, Model hash: 81761151, Eta: 0\r\neta noise sampler for ancestral samplers is set to 0.\r\n\r\nThis is the image I get:\r\n![tmpdnmnzgxx](https://user-images.githubusercontent.com/1089209/200768537-802a67d0-fdc1-48bb-80c9-bc15e3aeec8f.png)\r\n\r\nHowever the variation seed does not affect the output at all for me.\r\n\r\nIt's close.  Perhaps hardware has something to do with it?  I have a base model Mac Studio.  At least now I know I can reproduce some prompts with an expectation I'll get something reasonably close.  Prior, it was totally different.\nThat's incredible, I get the exact same result as you do!\r\n![00437-2640744521-Viking by greg rutkowski](https://user-images.githubusercontent.com/112779953/200794944-e31e572c-bf9d-4298-b07a-f7ba08e89dee.png)\r\nRunning on the full 7gb 1.5 checkpoint (a9263745) gives me the results that thesved produced above:\r\n![00438-2640744521-Viking by greg rutkowski](https://user-images.githubusercontent.com/112779953/200796971-4e065b0e-025e-4adb-99f5-62419564edfa.png)\r\n\r\nYet the same settings in Dream Studio (and Automatic1111 running on a collab) would give you this:\r\n![2640744521_Viking_by_greg_rutkowski](https://user-images.githubusercontent.com/112779953/200795410-98cd8a4c-8e43-48cb-aeb1-cbb010a73d41.png)\r\nI likewise am running on a Mac Studio. Why is that our Macs are generating these seeds in the same manner but different to every other version of SD?\nYes, with a9263745, I also match your output and the earlier one as well.\r\n\r\nGetting consistent results is a good first step. I'm hoping for better matches between M1 and other SD platforms now.\r\nIf I have the ancestral samplers set to 0, the variation seed does not affect output.  However, it will if it is set to anything other than 0, the changes get more and more pronounced as the number get higher - this is with the seed value set the same in both fields.", "created_at": "2022-11-29T04:54:13Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 5165, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-5165", "issue_numbers": ["3059"], "base_commit": "0b5dcb3d7ce397ad38312dbfc70febe7bb42dcc3", "patch": "diff --git a/modules/processing.py b/modules/processing.py\nindex edceb5323d8..fd995b8aa93 100644\n--- a/modules/processing.py\n+++ b/modules/processing.py\n@@ -530,8 +530,8 @@ def infotext(iteration=0, position_in_batch=0):\n             with devices.autocast():\r\n                 samples_ddim = p.sample(conditioning=c, unconditional_conditioning=uc, seeds=seeds, subseeds=subseeds, subseed_strength=p.subseed_strength, prompts=prompts)\r\n \r\n-            samples_ddim = samples_ddim.to(devices.dtype_vae)\r\n-            x_samples_ddim = decode_first_stage(p.sd_model, samples_ddim)\r\n+            x_samples_ddim = [decode_first_stage(p.sd_model, samples_ddim[i:i+1].to(dtype=devices.dtype_vae))[0].cpu() for i in range(samples_ddim.size(0))]\r\n+            x_samples_ddim = torch.stack(x_samples_ddim).float()\r\n             x_samples_ddim = torch.clamp((x_samples_ddim + 1.0) / 2.0, min=0.0, max=1.0)\r\n \r\n             del samples_ddim\r\n", "test_patch": "", "problem_statement": "even with -medvram,VRAM usage spikes to 6GB at peak,causing generation paused for a while\n\r\nIt consuming like 5G vram at most time which is perfect\r\n![image](https://user-images.githubusercontent.com/115866056/196428911-7ec96c15-fe7d-4004-ac15-46abf16468fb.png)\r\n\r\n\r\n\r\nbut sometime it spikes to 5.8~5.9, causing generator stops for minutes\r\n![image](https://user-images.githubusercontent.com/115866056/196429502-500e582c-7458-46a5-96c7-499ad0636828.png)\r\n\r\n\r\naleady add this line to the .bat file\r\n\r\nset COMMANDLINE_ARGS=--precision full --no-half --medvram  --always-batch-cond-uncond --xformers\r\n\r\nno ideal how it happens and how to fix it.\r\nOS:win 10\r\n1660super 6G VRAM \r\n32G RAM\r\n\r\n\n", "hints_text": "Remove `--precision full --no-half` then?\n> Remove `--precision full --no-half` then?\r\n\r\nI tried to removed them but it keeps on generating black squares.\n> I tried to removed them but it keeps on generating black squares.\r\n\r\nOh, you're on 1660, sorry.\nDid anyone had an ideal to fix it?", "created_at": "2022-11-28T12:39:22Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 5065, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-5065", "issue_numbers": ["3449"], "base_commit": "828438b4a190759807f9054932cae3a8b880ddf1", "patch": "diff --git a/modules/sd_samplers.py b/modules/sd_samplers.py\nindex 4fe67854412..44112f99697 100644\n--- a/modules/sd_samplers.py\n+++ b/modules/sd_samplers.py\n@@ -1,4 +1,4 @@\n-from collections import namedtuple\r\n+from collections import namedtuple, deque\r\n import numpy as np\r\n from math import floor\r\n import torch\r\n@@ -335,18 +335,28 @@ def forward(self, x, sigma, uncond, cond, cond_scale, image_cond):\n \r\n \r\n class TorchHijack:\r\n-    def __init__(self, kdiff_sampler):\r\n-        self.kdiff_sampler = kdiff_sampler\r\n+    def __init__(self, sampler_noises):\r\n+        # Using a deque to efficiently receive the sampler_noises in the same order as the previous index-based\r\n+        # implementation.\r\n+        self.sampler_noises = deque(sampler_noises)\r\n \r\n     def __getattr__(self, item):\r\n         if item == 'randn_like':\r\n-            return self.kdiff_sampler.randn_like\r\n+            return self.randn_like\r\n \r\n         if hasattr(torch, item):\r\n             return getattr(torch, item)\r\n \r\n         raise AttributeError(\"'{}' object has no attribute '{}'\".format(type(self).__name__, item))\r\n \r\n+    def randn_like(self, x):\r\n+        if self.sampler_noises:\r\n+            noise = self.sampler_noises.popleft()\r\n+            if noise.shape == x.shape:\r\n+                return noise\r\n+\r\n+        return torch.randn_like(x)\r\n+\r\n \r\n class KDiffusionSampler:\r\n     def __init__(self, funcname, sd_model):\r\n@@ -356,7 +366,6 @@ def __init__(self, funcname, sd_model):\n         self.extra_params = sampler_extra_params.get(funcname, [])\r\n         self.model_wrap_cfg = CFGDenoiser(self.model_wrap)\r\n         self.sampler_noises = None\r\n-        self.sampler_noise_index = 0\r\n         self.stop_at = None\r\n         self.eta = None\r\n         self.default_eta = 1.0\r\n@@ -389,26 +398,14 @@ def launch_sampling(self, steps, func):\n     def number_of_needed_noises(self, p):\r\n         return p.steps\r\n \r\n-    def randn_like(self, x):\r\n-        noise = self.sampler_noises[self.sampler_noise_index] if self.sampler_noises is not None and self.sampler_noise_index < len(self.sampler_noises) else None\r\n-\r\n-        if noise is not None and x.shape == noise.shape:\r\n-            res = noise\r\n-        else:\r\n-            res = torch.randn_like(x)\r\n-\r\n-        self.sampler_noise_index += 1\r\n-        return res\r\n-\r\n     def initialize(self, p):\r\n         self.model_wrap_cfg.mask = p.mask if hasattr(p, 'mask') else None\r\n         self.model_wrap_cfg.nmask = p.nmask if hasattr(p, 'nmask') else None\r\n         self.model_wrap.step = 0\r\n-        self.sampler_noise_index = 0\r\n         self.eta = p.eta or opts.eta_ancestral\r\n \r\n         if self.sampler_noises is not None:\r\n-            k_diffusion.sampling.torch = TorchHijack(self)\r\n+            k_diffusion.sampling.torch = TorchHijack(self.sampler_noises)\r\n \r\n         extra_params_kwargs = {}\r\n         for param_name in self.extra_params:\r\n", "test_patch": "", "problem_statement": "[Bug]: Inconsistent stability when changing from V1.5 model to Inpainting V1.5 model. \n### Is there an existing issue for this?\r\n\r\n- [X] I have searched the existing issues and checked the recent builds/commits\r\n\r\n### What happened?\r\n\r\nAbout 60% of the time I try to change from V1.5 to Inpainting my GPU throws a cuda error about not enough allocated space or my system complete freezes up and my GPU gets hotter and hotter and hotter until I force my system off. \r\n\r\nFrom my limited understanding it seems like the program is trying to load the V1.5 inpainting model without unloading the currently selected model from VRAM.\r\n\r\n\r\n\r\n### Steps to reproduce the problem\r\n\r\nLoad WEBUI and try to switch to inpainting model. I usually run the regular V1.5 model a few times before switching to inpainting, I'm not sure if that is required to produce the problem but it's what I would be doing.\r\n\r\n### What should have happened?\r\n\r\nIt should have given me 101 million dollars but instead it crashed my GPU, lol\r\n\r\n### Commit where the problem happens\r\n\r\n606519813dd998140a741096f9029c732ee52d2a\r\n\r\n### What platforms do you use to access UI ?\r\n\r\nWindows\r\n\r\n### What browsers do you use to access the UI ?\r\n\r\nMozilla Firefox\r\n\r\n### Command Line Arguments\r\n\r\n```Shell\r\n--xformers --medvram\r\n```\r\n\r\n\r\n### Additional information, context and logs\r\n\r\nI don't have fancy specs but I had no issue switching other models consistently without issues before. The issue has only appeared with the V1.5 models.\r\n\r\nAlso want to note: sometimes when it does work after switching to the inpainting model it will throw cuda allocated space errors when trying to generate anything. I believe this is because 2 models are loaded onto the GPU because If I load just the inpainting model (as the first model loaded during a fresh restart) it works fine.\r\n\r\nProcessor:AMD Ryzen 7 2700 Eight-Core Processor             3.20 GHz\r\nInstalled RAM:16.0 GB\r\nGPU: 2060RTX 6gig\r\n\r\nTY for everything, I wish I was smart enough to help more.\n", "hints_text": "After turning off xformers the problem isn't happening, will report back if it does.\ngetting the same issue without xformers\nI'm also seeing extremely high RAM usage when switching models on `--use-cpu=all`. Maybe a duplicate of #2180.\nChanging to an inpainting model is calling the `load_model()` and creating a new model, but the previous model is not being removed from memory, even calling `gc.collect()` is not removing the old model from memory.\r\n\r\nSo if you keep changing from inpainting to not inpainting or vice versa the leak keep increasing.\r\n\r\nI also think this inpainting hijack logic should be improved, it is hijacking even when it isn't an inpainting model, it always call `do_inpainting_hijack()` and that method doesn't check anything.\n`do_inpainting_hijack` just applies the code changes from the Runway repo: [https://github.com/runwayml/stable-diffusion](https://github.com/runwayml/stable-diffusion). It doesn't change any model behavior, and at some point the current stable-diffusion requirement should be updated to just point to that repo if the Compvis one doesn't get updates. \r\n\r\nThis is a more fundamental problem of how to deal with loading a model with a different config, not just the inpainting model. Any model with the same config can just update the weight in-place and memory usage is preserved. The fact that `gc.collect()` doesn't clear the old model is interesting however. This means that something is keeping a pointer to the old model alive and preventing it from being cleaned up. \r\n\r\nI guess the following could be a stop-gap. This will first move the current model's weights to the cpu to free up gpu memory.\r\n```diff\r\n--- a/modules/sd_models.py\r\n+++ b/modules/sd_models.py\r\n@@ -255,6 +255,8 @@ def reload_model_weights(sd_model, info=None):\r\n\r\n     if sd_model.sd_checkpoint_info.config != checkpoint_info.config or should_hijack_inpainting(checkpoint_info) != should_hijack_inpainting(sd_model.sd_checkpoint_info):\r\n         checkpoints_loaded.clear()\r\n+        shared.sd_model.cpu()\r\n+        del shared.sd_model\r\n         load_model(checkpoint_info)\r\n         return shared.sd_model\r\n```\n@random-thoughtss I tried with those lines to delete the sd_model but it didn't worked, it must be something else.\r\n\r\nAbout the inpainting logic, the `LatentInpaintDiffusion` just defines the properties `masked_image_key` and `concat_keys` that aren't being used anywhere, in the runwayml/stable-diffusion the `LatentInpaintDiffusion` also overwrites the `get_input` method and use those properties there.\r\n\r\nI may be wrong but it looks like the `LatentInpaintDiffusion` in this repository is doing nothing.\r\n\r\nAlso if the `do_inpainting_hijack()` is a global hijack not only to inpainting it should be inside sd_hijack and be named properly to what it does.\r\n\r\nAll this code added to enable the runwayml inpainting looks like a workaround for a more generic problem: to use models that differ from the default sd models and need a custom config, what may kind of already exists, adding a yaml file next to the ckpt file will use it as the config file, if it is working with the current code so the `LatentInpaintDiffusion` is useless and it just needs a yaml file with the correct parameters.\r\n\r\nSo, for me a better solution would be, if the global codes are an improvement it should be added to sd_hijack and a yaml file for the config should be used for the runwayml and added along with the runwayml inpainting ckpt, as it is for other ckpt files that requires custom config files.\r\n\r\nAbout the loading leak that concerns this issue, it needs more research to find the bug.\r\n\r\nEdit:\r\n\r\nJust confirmed, the runwayml inpainting works removing the inpainting hijack and adding a yaml file with the correct conditioning_key and in_channels.\n@jn-jairo The changes were made to make it compatible with the official in-painting model config: [https://github.com/runwayml/stable-diffusion/blob/main/configs/stable-diffusion/v1-inpainting-inference.yaml](https://github.com/runwayml/stable-diffusion/blob/main/configs/stable-diffusion/v1-inpainting-inference.yaml). The only problem is that this config is located in a separate repo from the checkpoint weights and in a pretty obscure spot. Most people using the model (including you it seems) have no idea this config exists, so we made the decision to hardcore the changes for now to ease installation. Perhaps we should ask the Runway if they could include config along with the model.\r\n\r\nI do agree that the name should probably be updated to `do_runway_hijack` at this point, but its probably better to just switch to the runway repo at some point instead of keeping the differences locally. The only risk is the Compvis and Runway codebases could diverge if Compvis decides to continue development. \r\n\r\n> I tried with those lines to delete the sd_model but it didn't worked\r\n\r\nWhat exactly didn't work? Are you still taking up GPU memory, or are the models taking more CPU memory? The `.cpu()` call moves the model weights to the CPU inplace, so they should not take up more vram. If that does not fix it, then something else is leaking into VRAM, not the model weights. One thing to note, If you are tracking the memory usage using `nvidia-smi`, is that torch does not immediately return GPU memory when it is freed, and it reuses the parts its already allocated but freed to create new tensors.\n@random-thoughtss it is the RAM not the VRAM, the RAM increases every time `load_model()` is called, and because the leak is the same size of the model I thought it should be the model.\r\n\r\nI know it is hard to find the yaml file, but a better option should be to include a list of yaml files in this repo and to have a configuration to enable/disable the match of the config by the name of the file, like:\r\n\r\n```\r\nconfigs/sd-v1-5-inpainting.yaml\r\nmodels/Stable-diffusion/sd-v1-5-inpainting.ckpt\r\n```\r\n\r\nThat way we can add the yaml files for the popular models to help the user that want it, without hard coding the changes. \nI've been experiencing this on model swap. My whole system becomes unresponsive as SD takes up the system ram. I believe Python is failing to trigger GC soon enough and the system slows or locks. I have a workaround that seems to be working. Using prlimit to set the process max mem usage to about 80% of my available RAM (16 GB * 0.8) and running with the nice command.\r\n\r\n> $ prlimit --as=1669578752\r\n> $ nice bash webui.sh\r\n\r\nWatching the system with \"top\" I can see python coming up to the limit, then goes back down without tanking the system.\r\n\r\nThis has helped, but isn't 100%. Loading v1-5 pruned is still big and likely to tank.\r\n\r\nMore testing. I increased my swap file from 2 MiB to 8 MiB. This allowed me to switch models three times. The fourth switch locked the system. I believe the models are not being released from memory on swap.\nJust to notify the progress I made, It is indeed a reference problem, some places are keeping a reference of the model, what prevents the garbage collector to free the memory.\r\n\r\nI am checking it with `ctypes.c_long.from_address(id(shared.sd_model)).value` and there are multiples references.\r\n\r\nI am eliminating the references but there are still some to find, It will take a while to find everything.\n@jn-jairo With your PRs getting merged, is this issue resolved now? Or are there still more issues here still?\r\n\r\n- https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/4098\r\n- https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/4142\nSame thing happens, even with fresh installed Automatic1111", "created_at": "2022-11-26T02:47:14Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 4919, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-4919", "issue_numbers": ["4901", "4945"], "base_commit": "828438b4a190759807f9054932cae3a8b880ddf1", "patch": "diff --git a/modules/deepbooru.py b/modules/deepbooru.py\nindex b9066d8177e..31ec7e17197 100644\n--- a/modules/deepbooru.py\n+++ b/modules/deepbooru.py\n@@ -58,7 +58,7 @@ def tag_multi(self, pil_image, force_disable_ranks=False):\n         a = np.expand_dims(np.array(pic, dtype=np.float32), 0) / 255\n \n         with torch.no_grad(), devices.autocast():\n-            x = torch.from_numpy(a).cuda()\n+            x = torch.from_numpy(a).to(devices.device)\n             y = self.model(x)[0].detach().cpu().numpy()\n \n         probability_dict = {}\n", "test_patch": "", "problem_statement": "[Bug]: Torch deepdanbooru refuses to run on CPU\n### Is there an existing issue for this?\n\n- [X] I have searched the existing issues and checked the recent builds/commits\n\n### What happened?\n\nWhen trying to launch deepdanbooru with `--use-cpu all` it fails with `RuntimeError: No CUDA GPUs are available`.\n\n### Steps to reproduce the problem\n\n1. Launch webui in CPU only mode.\r\n2. Press \"Interrogate Deepdanbooru\"\r\n3. Exception occurs\r\n\n\n### What should have happened?\n\nShould work in CPU like the older implementation.\n\n### Commit where the problem happens\n\nc81d440d876dfd2ab3560410f37442ef56fc6632\n\n### What platforms do you use to access UI ?\n\nLinux\n\n### What browsers do you use to access the UI ?\n\nMozilla Firefox\n\n### Command Line Arguments\n\n```Shell\nACCELERATE=\"True\" CUDA_VISIBLE_DEVICES=\"-1\" bash webui.sh --use-cpu all --precision=full --no-half --no-half-vae --skip-torch-cuda-test --deepdanbooru --opt-channelslast --always-batch-cond-uncond\n```\n\n\n### Additional information, context and logs\n\n_No response_\n[Bug]: DeepDanBooru Interrogate fails if using --device-id 1 on startup\n### Is there an existing issue for this?\n\n- [X] I have searched the existing issues and checked the recent builds/commits\n\n### What happened?\n\nI use 2 GPUs in my system. device 0 is for training and 1 is what I use for the webui.\r\n\r\nUsing the webui on default or device-id 0, works as I expect. However if I use 1, I can not interrogate anymore.\n\n### Steps to reproduce the problem\n\nAdd --device-id 1 to your startup commands.\r\n\r\nadd or send an image to img2img\r\n\r\npress Interrogate DeepBooru\n\n### What should have happened?\n\nThe prompt should have been populated, instead this error is generated:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\sd\\forks\\auto1111\\venv\\lib\\site-packages\\gradio\\routes.py\", line 284, in run_predict\r\n    output = await app.blocks.process_api(\r\n  File \"C:\\sd\\forks\\auto1111\\venv\\lib\\site-packages\\gradio\\blocks.py\", line 982, in process_api\r\n    result = await self.call_function(fn_index, inputs, iterator)\r\n  File \"C:\\sd\\forks\\auto1111\\venv\\lib\\site-packages\\gradio\\blocks.py\", line 824, in call_function\r\n    prediction = await anyio.to_thread.run_sync(\r\n  File \"C:\\sd\\forks\\auto1111\\venv\\lib\\site-packages\\anyio\\to_thread.py\", line 31, in run_sync\r\n    return await get_asynclib().run_sync_in_worker_thread(\r\n  File \"C:\\sd\\forks\\auto1111\\venv\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 937, in run_sync_in_worker_thread\r\n    return await future\r\n  File \"C:\\sd\\forks\\auto1111\\venv\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 867, in run\r\n    result = context.run(func, *args)\r\n  File \"C:\\sd\\forks\\auto1111\\modules\\ui.py\", line 352, in interrogate_deepbooru\r\n    prompt = deepbooru.model.tag(image)\r\n  File \"C:\\sd\\forks\\auto1111\\modules\\deepbooru.py\", line 45, in tag\r\n    res = self.tag_multi(pil_image)\r\n  File \"C:\\sd\\forks\\auto1111\\modules\\deepbooru.py\", line 62, in tag_multi\r\n    y = self.model(x)[0].detach().cpu().numpy()\r\n  File \"C:\\sd\\forks\\auto1111\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1130, in _call_impl\r\n    return forward_call(*input, **kwargs)\r\n  File \"C:\\sd\\forks\\auto1111\\modules\\deepbooru_model.py\", line 199, in forward\r\n    t_360 = self.n_Conv_0(t_359_padded)\r\n  File \"C:\\sd\\forks\\auto1111\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1130, in _call_impl\r\n    return forward_call(*input, **kwargs)\r\n  File \"C:\\sd\\forks\\auto1111\\venv\\lib\\site-packages\\torch\\nn\\modules\\conv.py\", line 457, in forward\r\n    return self._conv_forward(input, self.weight, self.bias)\r\n  File \"C:\\sd\\forks\\auto1111\\venv\\lib\\site-packages\\torch\\nn\\modules\\conv.py\", line 453, in _conv_forward\r\n    return F.conv2d(input, weight, bias, self.stride,\r\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cuda:1! (when checking argument for argument weight in method wrapper__cudnn_convolution)\n\n### Commit where the problem happens\n\nCommit hash: 828438b4a190759807f9054932cae3a8b880ddf1\n\n### What platforms do you use to access UI ?\n\nWindows\n\n### What browsers do you use to access the UI ?\n\nGoogle Chrome, Microsoft Edge\n\n### Command Line Arguments\n\n```Shell\n--api --force-enable-xformers --ckpt-dir \"C:\\sd\\models\" --embeddings-dir \"C:\\sd\\embeddings\" --hypernetwork-dir \"C:\\sd\\hypernetworks\" --vae-path \"C:\\sd\\models\\VAE\" --device-id 1\n```\n\n\n### Additional information, context and logs\n\n_No response_\n", "hints_text": "\n", "created_at": "2022-11-21T08:05:54Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 4913, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-4913", "issue_numbers": ["4902"], "base_commit": "828438b4a190759807f9054932cae3a8b880ddf1", "patch": "diff --git a/modules/ui.py b/modules/ui.py\nindex e6da1b2aee0..aba13926e11 100644\n--- a/modules/ui.py\n+++ b/modules/ui.py\n@@ -478,9 +478,7 @@ def create_toprow(is_img2img):\n         if is_img2img:\r\n             with gr.Column(scale=1, elem_id=\"interrogate_col\"):\r\n                 button_interrogate = gr.Button('Interrogate\\nCLIP', elem_id=\"interrogate\")\r\n-\r\n-                if cmd_opts.deepdanbooru:\r\n-                    button_deepbooru = gr.Button('Interrogate\\nDeepBooru', elem_id=\"deepbooru\")\r\n+                button_deepbooru = gr.Button('Interrogate\\nDeepBooru', elem_id=\"deepbooru\")\r\n \r\n         with gr.Column(scale=1):\r\n             with gr.Row():\r\n@@ -1004,11 +1002,10 @@ def create_ui(wrap_gradio_gpu_call):\n                 outputs=[img2img_prompt],\r\n             )\r\n \r\n-            if cmd_opts.deepdanbooru:\r\n-                img2img_deepbooru.click(\r\n-                    fn=interrogate_deepbooru,\r\n-                    inputs=[init_img],\r\n-                    outputs=[img2img_prompt],\r\n+            img2img_deepbooru.click(\r\n+                fn=interrogate_deepbooru,\r\n+                inputs=[init_img],\r\n+                outputs=[img2img_prompt],\r\n             )\r\n \r\n \r\n@@ -1240,7 +1237,7 @@ def create_ui(wrap_gradio_gpu_call):\n                         process_split = gr.Checkbox(label='Split oversized images')\r\n                         process_focal_crop = gr.Checkbox(label='Auto focal point crop')\r\n                         process_caption = gr.Checkbox(label='Use BLIP for caption')\r\n-                        process_caption_deepbooru = gr.Checkbox(label='Use deepbooru for caption', visible=True if cmd_opts.deepdanbooru else False)\r\n+                        process_caption_deepbooru = gr.Checkbox(label='Use deepbooru for caption', visible=True)\r\n \r\n                     with gr.Row(visible=False) as process_split_extra_row:\r\n                         process_split_threshold = gr.Slider(label='Split image threshold', value=0.5, minimum=0.0, maximum=1.0, step=0.05)\r\n", "test_patch": "", "problem_statement": "[Bug]: Deepdanbooru command line arg not fully deprecated\n### Is there an existing issue for this?\n\n- [X] I have searched the existing issues and checked the recent builds/commits\n\n### What happened?\n\nIt seems like the command line arg --deepdanbooru has some left effects on the webui pages. When the argument is absent, the button \"Interrogate DeepBooru\" is missing.\r\n\r\nP.S. I am using a slightly personalized setup. Specifically, currently I am still keeping the original Deepdanbooru repo in my sd folder/forked branch. But AFAIK, it shouldn't result in something like this, because I had synced the branch and everything relevant should be up to date.\n\n### Steps to reproduce the problem\n\n1. Delete --deepdanbooru argument in webui-user.bat(as the commit c81d440d876dfd2ab3560410f37442ef56fc6632 suggests)\r\n2. Run the bat\r\n3. Go to img2img tab\r\n\n\n### What should have happened?\n\nThere should be \"Interrogate DeepBooru\" button, even if argument --deepdanbooru is absent in webui-user.bat. Or there should be a setting to turn the deepdanbooru on or off.\r\n\r\nP.S. This is with the expect of that the --deepdanbooru argument will be entirely deprecated. If this is not the case, then please inform me and I will close this issue.\n\n### Commit where the problem happens\n\nc81d440d876dfd2ab3560410f37442ef56fc6632\n\n### What platforms do you use to access UI ?\n\nWindows\n\n### What browsers do you use to access the UI ?\n\nMicrosoft Edge\n\n### Command Line Arguments\n\n```Shell\n--xformers --no-half-vae\n```\n\n\n### Additional information, context and logs\n\n_No response_\n", "hints_text": "Just came across this as well, saw that I didn't need --deepdanbooru anymore so I removed it, just went to pull tags for an image I just generated only to find the interrogate deepbooru button was missing.\r\n\r\nI am not really doing anything custom and am not directly using the Deepdanbooru repo myself(Only whatever the webui itself downloads).\r\n\r\nUsing this in my webui-user.bat:\r\nset COMMANDLINE_ARGS=--enable-console-prompts --api --no-half-vae\nSame issue here", "created_at": "2022-11-21T03:55:17Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 4844, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-4844", "issue_numbers": ["4651"], "base_commit": "d9fd4525a5d684100997130cc4132736bab1e4d9", "patch": "diff --git a/modules/sd_models.py b/modules/sd_models.py\nindex 80addf030fb..c59151e036a 100644\n--- a/modules/sd_models.py\n+++ b/modules/sd_models.py\n@@ -165,16 +165,9 @@ def load_model_weights(model, checkpoint_info, vae_file=\"auto\"):\n \r\n     cache_enabled = shared.opts.sd_checkpoint_cache > 0\r\n \r\n-    if cache_enabled:\r\n-        sd_vae.restore_base_vae(model)\r\n-\r\n-    vae_file = sd_vae.resolve_vae(checkpoint_file, vae_file=vae_file)\r\n-\r\n     if cache_enabled and checkpoint_info in checkpoints_loaded:\r\n         # use checkpoint cache\r\n-        vae_name = sd_vae.get_filename(vae_file) if vae_file else None\r\n-        vae_message = f\" with {vae_name} VAE\" if vae_name else \"\"\r\n-        print(f\"Loading weights [{sd_model_hash}]{vae_message} from cache\")\r\n+        print(f\"Loading weights [{sd_model_hash}] from cache\")\r\n         model.load_state_dict(checkpoints_loaded[checkpoint_info])\r\n     else:\r\n         # load from file\r\n@@ -220,6 +213,7 @@ def load_model_weights(model, checkpoint_info, vae_file=\"auto\"):\n     model.sd_model_checkpoint = checkpoint_file\r\n     model.sd_checkpoint_info = checkpoint_info\r\n \r\n+    vae_file = sd_vae.resolve_vae(checkpoint_file, vae_file=vae_file)\r\n     sd_vae.load_vae(model, vae_file)\r\n \r\n \r\ndiff --git a/modules/sd_vae.py b/modules/sd_vae.py\nindex 0b5f0213e45..9c120975a56 100644\n--- a/modules/sd_vae.py\n+++ b/modules/sd_vae.py\n@@ -91,7 +91,7 @@ def get_vae_from_settings(vae_file=\"auto\"):\n         # if VAE selected but not found, fallback to auto\n         if vae_file not in default_vae_values and not os.path.isfile(vae_file):\n             vae_file = \"auto\"\n-            print(\"Selected VAE doesn't exist\")\n+            print(f\"Selected VAE doesn't exist: {vae_file}\")\n     return vae_file\n \n \n@@ -101,15 +101,15 @@ def resolve_vae(checkpoint_file=None, vae_file=\"auto\"):\n     # if vae_file argument is provided, it takes priority, but not saved\n     if vae_file and vae_file not in default_vae_list:\n         if not os.path.isfile(vae_file):\n+            print(f\"VAE provided as function argument doesn't exist: {vae_file}\")\n             vae_file = \"auto\"\n-            print(\"VAE provided as function argument doesn't exist\")\n     # for the first load, if vae-path is provided, it takes priority, saved, and failure is reported\n     if first_load and shared.cmd_opts.vae_path is not None:\n         if os.path.isfile(shared.cmd_opts.vae_path):\n             vae_file = shared.cmd_opts.vae_path\n             shared.opts.data['sd_vae'] = get_filename(vae_file)\n         else:\n-            print(\"VAE provided as command line argument doesn't exist\")\n+            print(f\"VAE provided as command line argument doesn't exist: {vae_file}\")\n     # fallback to selector in settings, if vae selector not set to act as default fallback\n     if not shared.opts.sd_vae_as_default:\n         vae_file = get_vae_from_settings(vae_file)\n@@ -117,20 +117,20 @@ def resolve_vae(checkpoint_file=None, vae_file=\"auto\"):\n     if vae_file == \"auto\" and shared.cmd_opts.vae_path is not None:\n         if os.path.isfile(shared.cmd_opts.vae_path):\n             vae_file = shared.cmd_opts.vae_path\n-            print(\"Using VAE provided as command line argument\")\n+            print(f\"Using VAE provided as command line argument: {vae_file}\")\n     # if still not found, try look for \".vae.pt\" beside model\n     model_path = os.path.splitext(checkpoint_file)[0]\n     if vae_file == \"auto\":\n         vae_file_try = model_path + \".vae.pt\"\n         if os.path.isfile(vae_file_try):\n             vae_file = vae_file_try\n-            print(\"Using VAE found beside selected model\")\n+            print(f\"Using VAE found similar to selected model: {vae_file}\")\n     # if still not found, try look for \".vae.ckpt\" beside model\n     if vae_file == \"auto\":\n         vae_file_try = model_path + \".vae.ckpt\"\n         if os.path.isfile(vae_file_try):\n             vae_file = vae_file_try\n-            print(\"Using VAE found beside selected model\")\n+            print(f\"Using VAE found similar to selected model: {vae_file}\")\n     # No more fallbacks for auto\n     if vae_file == \"auto\":\n         vae_file = None\n@@ -146,6 +146,7 @@ def load_vae(model, vae_file=None):\n     # save_settings = False\n \n     if vae_file:\n+        assert os.path.isfile(vae_file), f\"VAE file doesn't exist: {vae_file}\"\n         print(f\"Loading VAE weights from: {vae_file}\")\n         vae_ckpt = torch.load(vae_file, map_location=shared.weight_load_location)\n         vae_dict_1 = {k: v for k, v in vae_ckpt[\"state_dict\"].items() if k[0:4] != \"loss\" and k not in vae_ignore_keys}\ndiff --git a/modules/shared.py b/modules/shared.py\nindex 1c42641d1b8..84567c8ee4c 100644\n--- a/modules/shared.py\n+++ b/modules/shared.py\n@@ -334,7 +334,7 @@ def options_section(section_identifier, options_dict):\n options_templates.update(options_section(('sd', \"Stable Diffusion\"), {\r\n     \"sd_model_checkpoint\": OptionInfo(None, \"Stable Diffusion checkpoint\", gr.Dropdown, lambda: {\"choices\": modules.sd_models.checkpoint_tiles()}, refresh=sd_models.list_models),\r\n     \"sd_checkpoint_cache\": OptionInfo(0, \"Checkpoints to cache in RAM\", gr.Slider, {\"minimum\": 0, \"maximum\": 10, \"step\": 1}),\r\n-    \"sd_vae\": OptionInfo(\"auto\", \"SD VAE\", gr.Dropdown, lambda: {\"choices\": list(sd_vae.vae_list)}, refresh=sd_vae.refresh_vae_list),\r\n+    \"sd_vae\": OptionInfo(\"auto\", \"SD VAE\", gr.Dropdown, lambda: {\"choices\": sd_vae.vae_list}, refresh=sd_vae.refresh_vae_list),\r\n     \"sd_vae_as_default\": OptionInfo(False, \"Ignore selected VAE for stable diffusion checkpoints that have their own .vae.pt next to them\"),\r\n     \"sd_hypernetwork\": OptionInfo(\"None\", \"Hypernetwork\", gr.Dropdown, lambda: {\"choices\": [\"None\"] + [x for x in hypernetworks.keys()]}, refresh=reload_hypernetworks),\r\n     \"sd_hypernetwork_strength\": OptionInfo(1.0, \"Hypernetwork strength\", gr.Slider, {\"minimum\": 0.0, \"maximum\": 1.0, \"step\": 0.001}),\r\n", "test_patch": "", "problem_statement": "[Bug]: Cannot switch checkpoints\n### Is there an existing issue for this?\r\n\r\n- [X] I have searched the existing issues and checked the recent builds/commits\r\n\r\n### What happened?\r\n\r\nI cannot change the checkpoint in the WebUI anymore since updating today. This is the error message I get:\r\n\r\nLatentDiffusion: Running in eps-prediction mode\r\nTraceback (most recent call last):\r\n  File \"E:\\sd\\stable-diffusion-webui\\venv\\lib\\site-packages\\gradio\\routes.py\", line 284, in run_predict\r\n    output = await app.blocks.process_api(\r\n  File \"E:\\sd\\stable-diffusion-webui\\venv\\lib\\site-packages\\gradio\\blocks.py\", line 982, in process_api\r\n    result = await self.call_function(fn_index, inputs, iterator)\r\n  File \"E:\\sd\\stable-diffusion-webui\\venv\\lib\\site-packages\\gradio\\blocks.py\", line 824, in call_function\r\n    prediction = await anyio.to_thread.run_sync(\r\n  File \"E:\\sd\\stable-diffusion-webui\\venv\\lib\\site-packages\\anyio\\to_thread.py\", line 31, in run_sync\r\n    return await get_asynclib().run_sync_in_worker_thread(\r\n  File \"E:\\sd\\stable-diffusion-webui\\venv\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 937, in run_sync_in_worker_thread\r\n    return await future\r\n  File \"E:\\sd\\stable-diffusion-webui\\venv\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 867, in run\r\n    result = context.run(func, *args)\r\n  File \"E:\\sd\\stable-diffusion-webui\\modules\\ui.py\", line 443, in update_token_counter\r\n    tokens, token_count, max_length = max([model_hijack.tokenize(prompt) for prompt in prompts], key=lambda args: args[1])\r\n  File \"E:\\sd\\stable-diffusion-webui\\modules\\ui.py\", line 443, in <listcomp>\r\n    tokens, token_count, max_length = max([model_hijack.tokenize(prompt) for prompt in prompts], key=lambda args: args[1])\r\n  File \"E:\\sd\\stable-diffusion-webui\\modules\\sd_hijack.py\", line 116, in tokenize\r\n    _, remade_batch_tokens, _, _, _, token_count = self.clip.process_text([text])\r\nAttributeError: 'NoneType' object has no attribute 'process_text'\r\nDiffusionWrapper has 859.52 M params.\r\nmaking attention of type 'vanilla' with 512 in_channels\r\nWorking with z of shape (1, 4, 32, 32) = 4096 dimensions.\r\nmaking attention of type 'vanilla' with 512 in_channels\r\nTraceback (most recent call last):\r\n  File \"E:\\sd\\stable-diffusion-webui\\venv\\lib\\site-packages\\gradio\\routes.py\", line 284, in run_predict\r\n    output = await app.blocks.process_api(\r\n  File \"E:\\sd\\stable-diffusion-webui\\venv\\lib\\site-packages\\gradio\\blocks.py\", line 982, in process_api\r\n    result = await self.call_function(fn_index, inputs, iterator)\r\n  File \"E:\\sd\\stable-diffusion-webui\\venv\\lib\\site-packages\\gradio\\blocks.py\", line 824, in call_function\r\n    prediction = await anyio.to_thread.run_sync(\r\n  File \"E:\\sd\\stable-diffusion-webui\\venv\\lib\\site-packages\\anyio\\to_thread.py\", line 31, in run_sync\r\n    return await get_asynclib().run_sync_in_worker_thread(\r\n  File \"E:\\sd\\stable-diffusion-webui\\venv\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 937, in run_sync_in_worker_thread\r\n    return await future\r\n  File \"E:\\sd\\stable-diffusion-webui\\venv\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 867, in run\r\n    result = context.run(func, *args)\r\n  File \"E:\\sd\\stable-diffusion-webui\\modules\\ui.py\", line 1662, in <lambda>\r\n    fn=lambda value, k=k: run_settings_single(value, key=k),\r\n  File \"E:\\sd\\stable-diffusion-webui\\modules\\ui.py\", line 1504, in run_settings_single\r\n    opts.data_labels[key].onchange()\r\n  File \"E:\\sd\\stable-diffusion-webui\\webui.py\", line 41, in f\r\n    res = func(*args, **kwargs)\r\n  File \"E:\\sd\\stable-diffusion-webui\\webui.py\", line 83, in <lambda>\r\n    shared.opts.onchange(\"sd_model_checkpoint\", wrap_queued_call(lambda: modules.sd_models.reload_model_weights()))\r\n  File \"E:\\sd\\stable-diffusion-webui\\modules\\sd_models.py\", line 285, in reload_model_weights\r\n    load_model(checkpoint_info)\r\n  File \"E:\\sd\\stable-diffusion-webui\\modules\\sd_models.py\", line 254, in load_model\r\n    load_model_weights(sd_model, checkpoint_info)\r\n  File \"E:\\sd\\stable-diffusion-webui\\modules\\sd_models.py\", line 169, in load_model_weights\r\n    sd_vae.restore_base_vae(model)\r\n  File \"E:\\sd\\stable-diffusion-webui\\modules\\sd_vae.py\", line 54, in restore_base_vae\r\n    if base_vae is not None and checkpoint_info == model.sd_checkpoint_info:\r\n  File \"E:\\sd\\stable-diffusion-webui\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1207, in __getattr__\r\n    raise AttributeError(\"'{}' object has no attribute '{}'\".format(\r\nAttributeError: 'LatentDiffusion' object has no attribute 'sd_checkpoint_info'\r\n\r\nThen, I cannot generate any images. I get this error message:\r\n\r\nError completing request\r\nArguments: ('ppp', '', 'None', 'None', 36, 2, False, False, 1, 6, 7, -1.0, -1.0, 0, 0, 0, False, 512, 512, False, 0.7, 0, 0, 0, 0.9, 5, '0.0001', False, 'None', '', 0.1, False, '<div id=\"dynamic-prompting\">\\n    <h3><strong>Combinations</strong></h3>\\n\\n    Choose a number of terms from a list, in this case we choose two artists: \\n    <code class=\"codeblock\">{2$$artist1|artist2|artist3}</code><br/>\\n\\n    If $$ is not provided, then 1$$ is assumed.<br/><br/>\\n\\n    If the chosen number of terms is greater than the available terms, then some terms will be duplicated, otherwise chosen terms will be unique. This is useful in the case of wildcards, e.g.\\n    <code class=\"codeblock\">{2$$__artist__}</code> is equivalent to <code class=\"codeblock\">{2$$__artist__|__artist__}</code><br/><br/>\\n\\n    A range can be provided:\\n    <code class=\"codeblock\">{1-3$$artist1|artist2|artist3}</code><br/>\\n    In this case, a random number of artists between 1 and 3 is chosen.<br/><br/>\\n\\n    Wildcards can be used and the joiner can also be specified:\\n    <code class=\"codeblock\">{{1-$$and$$__adjective__}}</code><br/>\\n\\n    Here, a random number between 1 and 3 words from adjective.txt will be chosen and joined together with the word \\'and\\' instead of the default comma.\\n\\n    <br/><br/>\\n\\n    <h3><strong>Wildcards</strong></h3>\\n    \\n\\n    <br/>\\n    If the groups wont drop down click <strong onclick=\"check_collapsibles()\" style=\"cursor: pointer\">here</strong> to fix the issue.\\n\\n    <br/><br/>\\n\\n    <code class=\"codeblock\">WILDCARD_DIR: E:\\\\sd\\\\stable-diffusion-webui\\\\extensions\\\\sd-dynamic-prompts\\\\wildcards</code><br/>\\n    <small onload=\"check_collapsibles()\">You can add more wildcards by creating a text file with one term per line and name is mywildcards.txt. Place it in E:\\\\sd\\\\stable-diffusion-webui\\\\extensions\\\\sd-dynamic-prompts\\\\wildcards. <code class=\"codeblock\">__&#60;folder&#62;/mywildcards__</code> will then become available.</small>\\n</div>\\n\\n', False, 1, False, 100, 0.7, False, False, 'Not set', 'Not set', 'Not set', 'Not set', 'No focus', 'None', False, False, False, '', 1, '', 0, '', True, False, False, '', 'None', 30, 4, 0, 0, False, 'None', '<br>', 'None', 30, 4, 0, 0, 4, 0.4, True, 32, 1.0, 2.0, 'a painting in', 'style', 'picture frame, portrait photo', None) {}\r\nTraceback (most recent call last):\r\n  File \"E:\\sd\\stable-diffusion-webui\\venv\\lib\\site-packages\\gradio\\routes.py\", line 284, in run_predict\r\n    output = await app.blocks.process_api(\r\n  File \"E:\\sd\\stable-diffusion-webui\\venv\\lib\\site-packages\\gradio\\blocks.py\", line 982, in process_api\r\n    result = await self.call_function(fn_index, inputs, iterator)\r\n  File \"E:\\sd\\stable-diffusion-webui\\venv\\lib\\site-packages\\gradio\\blocks.py\", line 824, in call_function\r\n    prediction = await anyio.to_thread.run_sync(\r\n  File \"E:\\sd\\stable-diffusion-webui\\venv\\lib\\site-packages\\anyio\\to_thread.py\", line 31, in run_sync\r\n    return await get_asynclib().run_sync_in_worker_thread(\r\n  File \"E:\\sd\\stable-diffusion-webui\\venv\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 937, in run_sync_in_worker_thread\r\n    return await future\r\n  File \"E:\\sd\\stable-diffusion-webui\\venv\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 867, in run\r\n    result = context.run(func, *args)\r\n  File \"E:\\sd\\stable-diffusion-webui\\modules\\ui.py\", line 443, in update_token_counter\r\n    tokens, token_count, max_length = max([model_hijack.tokenize(prompt) for prompt in prompts], key=lambda args: args[1])\r\n  File \"E:\\sd\\stable-diffusion-webui\\modules\\ui.py\", line 443, in <listcomp>\r\n    tokens, token_count, max_length = max([model_hijack.tokenize(prompt) for prompt in prompts], key=lambda args: args[1])\r\n  File \"E:\\sd\\stable-diffusion-webui\\modules\\sd_hijack.py\", line 116, in tokenize\r\n    _, remade_batch_tokens, _, _, _, token_count = self.clip.process_text([text])\r\nAttributeError: 'NoneType' object has no attribute 'process_text'\r\nTraceback (most recent call last):\r\n  File \"E:\\sd\\stable-diffusion-webui\\modules\\ui.py\", line 185, in f\r\n    res = list(func(*args, **kwargs))\r\n  File \"E:\\sd\\stable-diffusion-webui\\webui.py\", line 54, in f\r\n    res = func(*args, **kwargs)\r\n  File \"E:\\sd\\stable-diffusion-webui\\modules\\txt2img.py\", line 48, in txt2img\r\n    processed = process_images(p)\r\n  File \"E:\\sd\\stable-diffusion-webui\\modules\\processing.py\", line 423, in process_images\r\n    res = process_images_inner(p)\r\n  File \"E:\\sd\\stable-diffusion-webui\\modules\\processing.py\", line 441, in process_images_inner\r\n    processed = Processed(p, [], p.seed, \"\")\r\n  File \"E:\\sd\\stable-diffusion-webui\\modules\\processing.py\", line 220, in __init__\r\n    self.sd_model_hash = shared.sd_model.sd_model_hash\r\nAttributeError: 'NoneType' object has no attribute 'sd_model_hash'\r\n\r\n\r\n\r\n### Steps to reproduce the problem\r\n\r\n1. Change the checkpoint in the dropdown at the top of the WebUI.\r\n\r\nI had the inpainting model loaded last, and now I cannot switch to any other. Then, I cannot generate.\r\n\r\n### What should have happened?\r\n\r\nIt should have normally switched it to a checkpoint.\r\n\r\n### Commit where the problem happens\r\n\r\n98947d173e3f1667eba29c904f681047dea9de90\r\n\r\n### What platforms do you use to access UI ?\r\n\r\nWindows\r\n\r\n### What browsers do you use to access the UI ?\r\n\r\nMicrosoft Edge\r\n\r\n### Command Line Arguments\r\n\r\n```Shell\r\n--precision full --medvram --no-half --ckpt-dir \"C:\\SD\\models\" --listen --enable-insecure-extension-access --xformers --vae-path \"C:\\SD\\moremodels\\v1-5-pruned-emaonly.vae.pt\" --api --cors-allow-origins=*\r\n```\r\n\r\n\r\n### Additional information, context and logs\r\n\r\nIt seems to be related to this PR: #4514\n", "hints_text": "trying to reproduce the problem, what is your checkpoint cache setting set to ?\r\n\r\n@R-N  do you think we can just remove the restore_base_vae() as you mentioned ?\n> trying to reproduce the problem, what is your checkpoint cache setting set to ?\r\n> \r\n> @R-N do you think we can just remove the restore_base_vae() as you mentioned ?\r\n\r\nI had the cache set to 2. I set it to 0 and that fixed the problem\ni think adding back in the _and hasattr(model, \"sd_checkpoint_info\")_ check would fix the problem.\r\nif the model has no sd_checkpoint_info yet, do not try to restore.\r\n\r\nhonestly i am not sure if the restore is still needed now after we changed the caching a bit.\r\n\r\nline 168\r\n```\r\n    if cache_enabled and hasattr(model, \"sd_checkpoint_info\"):\r\n        sd_vae.restore_base_vae(model)\r\n```\r\ncould you test that ?\n> @R-N do you think we can just remove the restore_base_vae() as you mentioned ?\r\n\r\nYeah that's what I suggested in #4514 . If you have a checkpoint with config file to trigger it, please try it.\r\n\r\n> But that restore_base_vae call was from when the caching was done at the start of load_model_weights. It was called so that the caching won't cache the separately loaded VAE. Now that the caching is done right after the checkpoint weight was loaded, that restore_base_vae call can just be removed.", "created_at": "2022-11-19T05:30:23Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 4514, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-4514", "issue_numbers": ["4448"], "base_commit": "ac085628540d0ec6a988fad93f5b8f2154209571", "patch": "diff --git a/modules/sd_models.py b/modules/sd_models.py\nindex 34c57bfa73a..80addf030fb 100644\n--- a/modules/sd_models.py\n+++ b/modules/sd_models.py\n@@ -163,13 +163,21 @@ def load_model_weights(model, checkpoint_info, vae_file=\"auto\"):\n     checkpoint_file = checkpoint_info.filename\r\n     sd_model_hash = checkpoint_info.hash\r\n \r\n-    if shared.opts.sd_checkpoint_cache > 0 and hasattr(model, \"sd_checkpoint_info\"):\r\n+    cache_enabled = shared.opts.sd_checkpoint_cache > 0\r\n+\r\n+    if cache_enabled:\r\n         sd_vae.restore_base_vae(model)\r\n-        checkpoints_loaded[model.sd_checkpoint_info] = model.state_dict().copy()\r\n \r\n     vae_file = sd_vae.resolve_vae(checkpoint_file, vae_file=vae_file)\r\n \r\n-    if checkpoint_info not in checkpoints_loaded:\r\n+    if cache_enabled and checkpoint_info in checkpoints_loaded:\r\n+        # use checkpoint cache\r\n+        vae_name = sd_vae.get_filename(vae_file) if vae_file else None\r\n+        vae_message = f\" with {vae_name} VAE\" if vae_name else \"\"\r\n+        print(f\"Loading weights [{sd_model_hash}]{vae_message} from cache\")\r\n+        model.load_state_dict(checkpoints_loaded[checkpoint_info])\r\n+    else:\r\n+        # load from file\r\n         print(f\"Loading weights [{sd_model_hash}] from {checkpoint_file}\")\r\n \r\n         pl_sd = torch.load(checkpoint_file, map_location=shared.weight_load_location)\r\n@@ -180,6 +188,10 @@ def load_model_weights(model, checkpoint_info, vae_file=\"auto\"):\n         del pl_sd\r\n         model.load_state_dict(sd, strict=False)\r\n         del sd\r\n+        \r\n+        if cache_enabled:\r\n+            # cache newly loaded model\r\n+            checkpoints_loaded[checkpoint_info] = model.state_dict().copy()\r\n \r\n         if shared.cmd_opts.opt_channelslast:\r\n             model.to(memory_format=torch.channels_last)\r\n@@ -199,14 +211,9 @@ def load_model_weights(model, checkpoint_info, vae_file=\"auto\"):\n \r\n         model.first_stage_model.to(devices.dtype_vae)\r\n \r\n-    else:\r\n-        vae_name = sd_vae.get_filename(vae_file) if vae_file else None\r\n-        vae_message = f\" with {vae_name} VAE\" if vae_name else \"\"\r\n-        print(f\"Loading weights [{sd_model_hash}]{vae_message} from cache\")\r\n-        model.load_state_dict(checkpoints_loaded[checkpoint_info])\r\n-\r\n-    if shared.opts.sd_checkpoint_cache > 0:\r\n-        while len(checkpoints_loaded) > shared.opts.sd_checkpoint_cache:\r\n+    # clean up cache if limit is reached\r\n+    if cache_enabled:\r\n+        while len(checkpoints_loaded) > shared.opts.sd_checkpoint_cache + 1: # we need to count the current model\r\n             checkpoints_loaded.popitem(last=False)  # LRU\r\n \r\n     model.sd_model_hash = sd_model_hash\r\n", "test_patch": "", "problem_statement": "[Bug]: x/y Checkpoint plot giving wrong results - wrong ckpt weight's loaded from cache if checkpoint cache is enabled in settings (>2)\n### Is there an existing issue for this?\r\n\r\n- [X] I have searched the existing issues and checked the recent builds/commits\r\n\r\n### What happened?\r\n\r\nUsing x/y plot with multiple checkpoints and with the same seed, does not generate consistent results.\r\nIt seems that the checkpoint order gets swapped around with each button press.\r\nAnd the currently selected checkpoints also plays a role, it seems the weight from the currently selected checkpoint is mixed into the plot.\r\n\r\nThe result changes depending on which checkpoint is currently selected, even though, the selected checkpoint should not matter in this case, since it should only use the weights from the x/y plot checkpoints\r\n\r\n### Steps to reproduce the problem\r\n\r\n1. in the settings, set _Checkpoints to cache in RAM_ to a value >= 2\r\n\r\n2. use two checkpoints in an x/y plot:\r\n![image](https://user-images.githubusercontent.com/1590330/200419537-ce03242b-0c67-4422-bd17-b7328cc7856e.png)\r\n\r\n3. use a different checkpoint as the currently selected in the main drop down\r\n![image](https://user-images.githubusercontent.com/1590330/200421634-f5334e06-ad12-4d9c-87b8-c98c1cca7563.png)\r\n\r\n4. generate an image multiple times, the labels stay in the same order, but the images swap with each new button press\r\n![image](https://user-images.githubusercontent.com/1590330/200419751-516a3c3e-c926-4d2b-8f94-7b10eaaf2d86.png)\r\n![image](https://user-images.githubusercontent.com/1590330/200419809-3e4c8eec-bd15-4c7c-b3a4-f688cf8f71a5.png)\r\n![image](https://user-images.githubusercontent.com/1590330/200419865-9cfa62d8-9c0f-41c5-b472-46881a912e9b.png)\r\n\r\n\r\n### What should have happened?\r\n\r\nResult should be consistent.\r\nCurrently selected checkpoint should not matter.\r\n\r\n### Commit where the problem happens\r\n\r\n804d9fb83d0c63ca3acd36378707ce47b8f12599\r\n\r\n### What platforms do you use to access UI ?\r\n\r\nWindows\r\n\r\n### What browsers do you use to access the UI ?\r\n\r\nMozilla Firefox\r\n\r\n### Command Line Arguments\r\n\r\n```Shell\r\nLaunching Web UI with arguments: --xformers --force-enable-xformers\r\n```\r\n\r\n\r\n### Additional information, context and logs\r\nCheckpoints to cache in RAM needs to be set to >= 2\r\n\r\n\r\n\n", "hints_text": "The problem only happens, if the setting **Checkpoints to cache in RAM**  is set to > 0.", "created_at": "2022-11-09T05:04:20Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 4371, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-4371", "issue_numbers": ["800"], "base_commit": "2f47724b73c40b96e158bea9ac2c6e84fbad3e73", "patch": "diff --git a/modules/ldsr_model_arch.py b/modules/ldsr_model_arch.py\nindex 14db507668c..90e0a2f064f 100644\n--- a/modules/ldsr_model_arch.py\n+++ b/modules/ldsr_model_arch.py\n@@ -101,8 +101,8 @@ def super_resolution(self, image, steps=100, target_scale=2, half_attention=Fals\n         down_sample_rate = target_scale / 4\n         wd = width_og * down_sample_rate\n         hd = height_og * down_sample_rate\n-        width_downsampled_pre = int(wd)\n-        height_downsampled_pre = int(hd)\n+        width_downsampled_pre = int(np.ceil(wd))\n+        height_downsampled_pre = int(np.ceil(hd))\n \n         if down_sample_rate != 1:\n             print(\n@@ -110,7 +110,12 @@ def super_resolution(self, image, steps=100, target_scale=2, half_attention=Fals\n             im_og = im_og.resize((width_downsampled_pre, height_downsampled_pre), Image.LANCZOS)\n         else:\n             print(f\"Down sample rate is 1 from {target_scale} / 4 (Not downsampling)\")\n-        logs = self.run(model[\"model\"], im_og, diffusion_steps, eta)\n+        \n+        # pad width and height to multiples of 64, pads with the edge values of image to avoid artifacts\n+        pad_w, pad_h = np.max(((2, 2), np.ceil(np.array(im_og.size) / 64).astype(int)), axis=0) * 64 - im_og.size\n+        im_padded = Image.fromarray(np.pad(np.array(im_og), ((0, pad_h), (0, pad_w), (0, 0)), mode='edge'))\n+        \n+        logs = self.run(model[\"model\"], im_padded, diffusion_steps, eta)\n \n         sample = logs[\"sample\"]\n         sample = sample.detach().cpu()\n@@ -120,6 +125,9 @@ def super_resolution(self, image, steps=100, target_scale=2, half_attention=Fals\n         sample = np.transpose(sample, (0, 2, 3, 1))\n         a = Image.fromarray(sample[0])\n \n+        # remove padding\n+        a = a.crop((0, 0) + tuple(np.array(im_og.size) * 4))\n+\n         del model\n         gc.collect()\n         torch.cuda.empty_cache()\n", "test_patch": "", "problem_statement": "Problems with LDSR upscaling\nWhen using the new LDSR upscaling feature, be it via SD upscaling in img2img or directly in \"Extras\", black rectangles appear in the outputs on the right and at the bottom when upscaling a 230x219 photo. \r\n\r\n![image 7](https://user-images.githubusercontent.com/111315671/191547996-47082733-9e6a-49d0-9ba5-6df01c2bb175.png)\r\noriginal\r\n\r\n![00021](https://user-images.githubusercontent.com/111315671/191548044-7799ad8f-02d1-4ac5-9699-4957d25a9442.png)\r\nupscaled\r\n\r\nno issues when running it on a 512x704 image\r\n\r\n![00151-747985091-four squirrels by Tom Whalen](https://user-images.githubusercontent.com/111315671/191548282-fd4c4572-21d9-45ee-9e76-db5060dd6469.png)\r\noriginal\r\n\r\n![00020](https://user-images.githubusercontent.com/111315671/191548167-f08ba06b-6c4f-4c5f-b3ac-2bb9662f871b.png)\r\nupscaled\n", "hints_text": "Same issue here. Only seems to happen for images with non-square aspect ratios.\nI\u2019m having the same trouble when upscaling perfectly square images. It seems that aspect ratio does not matter. And the thickness of both horizontal and vertical black bars seems to somehow correlate with the chosen Resize factor and the resulting dimensions. Here\u2019s an example (orange frames do not belong to images):\r\n![ldsr_black_rects](https://user-images.githubusercontent.com/5103155/193427848-c177853b-7fea-413e-b008-704b6dc92796.png)", "created_at": "2022-11-06T03:36:56Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 4368, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-4368", "issue_numbers": ["3451"], "base_commit": "2f47724b73c40b96e158bea9ac2c6e84fbad3e73", "patch": "diff --git a/scripts/prompt_matrix.py b/scripts/prompt_matrix.py\nindex e49c9b205f9..fd314b550d8 100644\n--- a/scripts/prompt_matrix.py\n+++ b/scripts/prompt_matrix.py\n@@ -82,6 +82,6 @@ def run(self, p, put_at_start):\n         processed.images.insert(0, grid)\r\n \r\n         if opts.grid_save:\r\n-            images.save_image(processed.images[0], p.outpath_grids, \"prompt_matrix\", prompt=original_prompt, seed=processed.seed, grid=True, p=p)\r\n+            images.save_image(processed.images[0], p.outpath_grids, \"prompt_matrix\", extension=opts.grid_format, prompt=original_prompt, seed=processed.seed, grid=True, p=p)\r\n \r\n         return processed\r\ndiff --git a/scripts/xy_grid.py b/scripts/xy_grid.py\nindex 417ed0d43b1..45a78db2e6a 100644\n--- a/scripts/xy_grid.py\n+++ b/scripts/xy_grid.py\n@@ -393,6 +393,6 @@ def cell(x, y):\n             )\r\n \r\n         if opts.grid_save:\r\n-            images.save_image(processed.images[0], p.outpath_grids, \"xy_grid\", prompt=p.prompt, seed=processed.seed, grid=True, p=p)\r\n+            images.save_image(processed.images[0], p.outpath_grids, \"xy_grid\", extension=opts.grid_format, prompt=p.prompt, seed=processed.seed, grid=True, p=p)\r\n \r\n         return processed\r\n", "test_patch": "", "problem_statement": "[Bug]: X/Y plot ignores file format settings for grids\n### Is there an existing issue for this?\n\n- [x] I have searched the existing issues and checked the recent builds/commits\n\n### What happened?\n\nFiles are saved in PNG format.\n\n### Steps to reproduce the problem\n\n1. Set File format for grids to jpg\r\n2. Generate grid using X/Y plot\r\n\n\n### What should have happened?\n\nFiles should be saved in specified format.\n\n### Commit where the problem happens\n\nca5a9e79dc28eeaa3a161427a82e34703bf15765\n\n### What platforms do you use to access UI ?\n\nOther/Cloud\n\n### What browsers do you use to access the UI ?\n\nGoogle Chrome\n\n### Command Line Arguments\n\n_No response_\n\n### Additional information, context and logs\n\n_No response_\n", "hints_text": "Can confirm, I found that the img2img is jpg correctly, only txt2img grid bugged", "created_at": "2022-11-06T02:17:38Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 4320, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-4320", "issue_numbers": ["948"], "base_commit": "30b1bcc64e67ad50c5d3af3a6fe1bd1e9553f34e", "patch": "diff --git a/modules/shared.py b/modules/shared.py\nindex 962115f618e..7a20c3afaaa 100644\n--- a/modules/shared.py\n+++ b/modules/shared.py\n@@ -86,6 +86,9 @@\n parser.add_argument(\"--ui-debug-mode\", action='store_true', help=\"Don't load model to quickly launch UI\")\r\n parser.add_argument(\"--device-id\", type=str, help=\"Select the default CUDA device to use (export CUDA_VISIBLE_DEVICES=0,1,etc might be needed before)\", default=None)\r\n parser.add_argument(\"--administrator\", action='store_true', help=\"Administrator rights\", default=False)\r\n+parser.add_argument(\"--tls-keyfile\", type=str, help=\"Partially enables TLS, requires --tls-certfile to fully function\", default=None)\r\n+parser.add_argument(\"--tls-certfile\", type=str, help=\"Partially enables TLS, requires --tls-keyfile to fully function\", default=None)\r\n+parser.add_argument(\"--server-name\", type=str, help=\"Sets hostname of server\", default=None)\r\n \r\n cmd_opts = parser.parse_args()\r\n restricted_opts = {\r\ndiff --git a/webui.py b/webui.py\nindex 81df09dd293..222dbeee7c1 100644\n--- a/webui.py\n+++ b/webui.py\n@@ -34,7 +34,7 @@\n import modules.hypernetworks.hypernetwork\r\n \r\n queue_lock = threading.Lock()\r\n-\r\n+server_name = \"0.0.0.0\" if cmd_opts.listen else cmd_opts.server_name\r\n \r\n def wrap_queued_call(func):\r\n     def f(*args, **kwargs):\r\n@@ -85,6 +85,20 @@ def initialize():\n     shared.opts.onchange(\"sd_hypernetwork\", wrap_queued_call(lambda: modules.hypernetworks.hypernetwork.load_hypernetwork(shared.opts.sd_hypernetwork)))\r\n     shared.opts.onchange(\"sd_hypernetwork_strength\", modules.hypernetworks.hypernetwork.apply_strength)\r\n \r\n+    if cmd_opts.tls_keyfile is not None and cmd_opts.tls_keyfile is not None:\r\n+\r\n+        try:\r\n+            if not os.path.exists(cmd_opts.tls_keyfile):\r\n+                print(\"Invalid path to TLS keyfile given\")\r\n+            if not os.path.exists(cmd_opts.tls_certfile):\r\n+                print(f\"Invalid path to TLS certfile: '{cmd_opts.tls_certfile}'\")\r\n+        except TypeError:\r\n+            cmd_opts.tls_keyfile = cmd_opts.tls_certfile = None\r\n+            print(\"TLS setup invalid, running webui without TLS\")\r\n+        else:\r\n+            print(\"Running with TLS\")\r\n+\r\n+\r\n     # make the program just exit at ctrl+c without waiting for anything\r\n     def sigint_handler(sig, frame):\r\n         print(f'Interrupted with signal {sig} in {frame}')\r\n@@ -131,8 +145,10 @@ def webui():\n \r\n         app, local_url, share_url = demo.launch(\r\n             share=cmd_opts.share,\r\n-            server_name=\"0.0.0.0\" if cmd_opts.listen else None,\r\n+            server_name=server_name,\r\n             server_port=cmd_opts.port,\r\n+            ssl_keyfile=cmd_opts.tls_keyfile,\r\n+            ssl_certfile=cmd_opts.tls_certfile,\r\n             debug=cmd_opts.gradio_debug,\r\n             auth=[tuple(cred.split(':')) for cred in cmd_opts.gradio_auth.strip('\"').split(',')] if cmd_opts.gradio_auth else None,\r\n             inbrowser=cmd_opts.autolaunch,\r\n", "test_patch": "", "problem_statement": "https (ssl) support\n**Is your feature request related to a problem? Please describe.**\r\nability to start it in https (ssl crypted) mode, specifying the cert+key+servername on the cli.\r\n\r\n**Describe the solution you'd like**\r\ngradio already implemented it, just pass the ssl_keyfile and ssl_certfile parameters to launch():\r\nhttps://github.com/gradio-app/gradio/issues/563\r\n\r\nand it's recommended (required?) to use real hostname instead of 0.0.0.0, so --listen should have a parameter to specify name/ip address.\r\n\r\ni've modified your webui.py to test and it works fine:\r\n    demo.launch(\r\n        share=cmd_opts.share,\r\n        server_name=\"xxx.yyy.hu\",\r\n        server_port=cmd_opts.port,\r\n        debug=cmd_opts.gradio_debug,\r\n        auth=[tuple(cred.split(':')) for cred in cmd_opts.gradio_auth.strip('\"').split(',')] if cmd_opts.gradio_auth else None,\r\n        inbrowser=cmd_opts.autolaunch,\r\n        ssl_keyfile=\"/etc/ssl/xxx.yyy.key\",\r\n        ssl_certfile=\"/etc/ssl/xxx.yyy.pem\"\r\n    )\r\n\r\n\r\n**Describe alternatives you've considered**\r\nfirst i've setup an apache reverse proxy (proxypass to http://127.0.0.1:7860) but it timeouts after a few minutes, so if the render took several minutes then you'll never receive it and the whole ui becames unresponsible:\r\n\r\n[Fri Sep 23 21:03:51.138645 2022] [proxy_http:error] [pid 3946:tid 139649922103040] (70007)The timeout specified has expired: [client 37.191.45.201:57969] AH01102: error reading status line from remote server 127.0.0.1:7860\r\n[Fri Sep 23 21:03:51.138669 2022] [proxy:error] [pid 3946:tid 139649922103040] [client 37.191.45.201:57969] AH00898: Error reading from remote server returned by /api/predict/\r\n\r\n**Additional context**\r\nthe whole https thing is REQUIRED for browser notifications to work, because (at least in firefox) it's not allowed with http urls.\r\n\n", "hints_text": "> first i've setup an apache reverse proxy (proxypass to http://127.0.0.1:7860/) but it timeouts after a few minutes, so if the render took several minutes then you'll never receive it and the whole ui becames unresponsible:\r\n\r\nIncrease the timeout.\nI use mine exclusively through nginx with ssl and it's super snappy.  \n@MrKuenning that sounds like what I want to do. Could you share any setup instructions?  \nI have a dynamic DNS to my firewall.\r\nI have nginx proxy manager running on a docker vm.\r\nI run sd with a custom port.\r\n\r\nNginx manager has let's encrypt built in.\r\nI then set a proxy of ai.domain.com to fwd to sd.\r\nI also setup a remote script to start and stop sd.\r\n\r\nNow I can go to ai.domain.com from my local machine or my phone from anywhere.\nVery nice! Thanks!\nIt'll be nice if there was also an easy self-signed certificate option. Some of us just want to use notifications but that functionality is gated behind HTTPS. \r\n\r\nhttps://github.com/AUTOMATIC1111/stable-diffusion-webui/issues/1013", "created_at": "2022-11-05T09:17:30Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 4304, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-4304", "issue_numbers": ["4280"], "base_commit": "30b1bcc64e67ad50c5d3af3a6fe1bd1e9553f34e", "patch": "diff --git a/launch.py b/launch.py\nindex 2a51f20eec0..5fa115606f9 100644\n--- a/launch.py\n+++ b/launch.py\n@@ -142,7 +142,7 @@ def prepare_enviroment():\n \r\n     stable_diffusion_commit_hash = os.environ.get('STABLE_DIFFUSION_COMMIT_HASH', \"69ae4b35e0a0f6ee1af8bb9a5d0016ccb27e36dc\")\r\n     taming_transformers_commit_hash = os.environ.get('TAMING_TRANSFORMERS_COMMIT_HASH', \"24268930bf1dce879235a7fddd0b2355b84d7ea6\")\r\n-    k_diffusion_commit_hash = os.environ.get('K_DIFFUSION_COMMIT_HASH', \"f4e99857772fc3a126ba886aadf795a332774878\")\r\n+    k_diffusion_commit_hash = os.environ.get('K_DIFFUSION_COMMIT_HASH', \"60e5042ca0da89c14d1dd59d73883280f8fce991\")\r\n     codeformer_commit_hash = os.environ.get('CODEFORMER_COMMIT_HASH', \"c5b4593074ba6214284d6acd5f1719b6c5d739af\")\r\n     blip_commit_hash = os.environ.get('BLIP_COMMIT_HASH', \"48211a1594f1321b00f14c9f7a5b4813144b2fb9\")\r\n \r\ndiff --git a/modules/sd_samplers.py b/modules/sd_samplers.py\nindex c7c414ef5a8..1e88f7eeb4f 100644\n--- a/modules/sd_samplers.py\n+++ b/modules/sd_samplers.py\n@@ -24,11 +24,15 @@\n     ('Heun', 'sample_heun', ['k_heun'], {}),\r\n     ('DPM2', 'sample_dpm_2', ['k_dpm_2'], {}),\r\n     ('DPM2 a', 'sample_dpm_2_ancestral', ['k_dpm_2_a'], {}),\r\n+    ('DPM-Solver++(2S) a', 'sample_dpmpp_2s_ancestral', ['k_dpmpp_2s_a'], {}),\r\n+    ('DPM-Solver++(2M)', 'sample_dpmpp_2m', ['k_dpmpp_2m'], {}),\r\n     ('DPM fast', 'sample_dpm_fast', ['k_dpm_fast'], {}),\r\n     ('DPM adaptive', 'sample_dpm_adaptive', ['k_dpm_ad'], {}),\r\n     ('LMS Karras', 'sample_lms', ['k_lms_ka'], {'scheduler': 'karras'}),\r\n     ('DPM2 Karras', 'sample_dpm_2', ['k_dpm_2_ka'], {'scheduler': 'karras'}),\r\n     ('DPM2 a Karras', 'sample_dpm_2_ancestral', ['k_dpm_2_a_ka'], {'scheduler': 'karras'}),\r\n+    ('DPM-Solver++(2S) a Karras', 'sample_dpmpp_2s_ancestral', ['k_dpmpp_2s_a_ka'], {'scheduler': 'karras'}),\r\n+    ('DPM-Solver++(2M) Karras', 'sample_dpmpp_2m', ['k_dpmpp_2m_ka'], {'scheduler': 'karras'}),\r\n ]\r\n \r\n samplers_data_k_diffusion = [\r\n", "test_patch": "", "problem_statement": "[Feature Request]: Add newest DPM-Solver++\n### Is there an existing issue for this?\r\n\r\n- [X] I have searched the existing issues and checked the recent builds/commits\r\n\r\n### What would your feature do ?\r\n\r\nSupport the newest DPM-Solver++, the state-of-the-art fast sampler for guided sampling by diffusion models.\r\n\r\nDPM-Solver++ can achieve great sample quality in only 15 to 20 steps.\r\n\r\nReferences:\r\nhttps://github.com/LuChengTHU/dpm-solver\r\n\r\nhttps://arxiv.org/abs/2211.01095\r\n\r\nhttps://github.com/huggingface/diffusers/pull/1132\r\n\r\nhttps://github.com/CompVis/stable-diffusion/pull/440\r\n\r\n### Proposed workflow\r\n\r\nI can help to add the algorithm code.\r\n\r\n### Additional information\r\n\r\n_No response_\n", "hints_text": "please! this looks very clean.\nasked also in https://github.com/crowsonkb/k-diffusion/issues/40\nLooks like a useful additional sampler to add to A111.", "created_at": "2022-11-04T23:04:16Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 4271, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-4271", "issue_numbers": ["4137", "4137"], "base_commit": "5cd5a672f7889dcc018c3873ec557d645ebe35d0", "patch": "diff --git a/modules/hypernetworks/hypernetwork.py b/modules/hypernetworks/hypernetwork.py\nindex eb5ae372f60..c406ffb379a 100644\n--- a/modules/hypernetworks/hypernetwork.py\n+++ b/modules/hypernetworks/hypernetwork.py\n@@ -433,7 +433,10 @@ def train_hypernetwork(hypernetwork_name, learn_rate, batch_size, gradient_step,\n \r\n     dl = modules.textual_inversion.dataset.PersonalizedDataLoader(ds, latent_sampling_method=latent_sampling_method, batch_size=ds.batch_size, pin_memory=pin_memory)\r\n \r\n+    old_parallel_processing_allowed = shared.parallel_processing_allowed\r\n+\r\n     if unload:\r\n+        shared.parallel_processing_allowed = False\r\n         shared.sd_model.cond_stage_model.to(devices.cpu)\r\n         shared.sd_model.first_stage_model.to(devices.cpu)\r\n     \r\n@@ -612,10 +615,12 @@ def train_hypernetwork(hypernetwork_name, learn_rate, batch_size, gradient_step,\n     if shared.opts.save_optimizer_state:\r\n         hypernetwork.optimizer_state_dict = optimizer.state_dict()\r\n     save_hypernetwork(hypernetwork, checkpoint, hypernetwork_name, filename)\r\n+\r\n     del optimizer\r\n     hypernetwork.optimizer_state_dict = None  # dereference it after saving, to save memory.\r\n     shared.sd_model.cond_stage_model.to(devices.device)\r\n     shared.sd_model.first_stage_model.to(devices.device)\r\n+    shared.parallel_processing_allowed = old_parallel_processing_allowed\r\n \r\n     return hypernetwork, filename\r\n \r\ndiff --git a/modules/textual_inversion/textual_inversion.py b/modules/textual_inversion/textual_inversion.py\nindex daf8d1b84c0..e28c357ab47 100644\n--- a/modules/textual_inversion/textual_inversion.py\n+++ b/modules/textual_inversion/textual_inversion.py\n@@ -269,6 +269,7 @@ def train_embedding(embedding_name, learn_rate, batch_size, gradient_step, data_\n \r\n    # dataset loading may take a while, so input validations and early returns should be done before this\r\n     shared.state.textinfo = f\"Preparing dataset from {html.escape(data_root)}...\"\r\n+    old_parallel_processing_allowed = shared.parallel_processing_allowed\r\n     \r\n     pin_memory = shared.opts.pin_memory\r\n     \r\n@@ -279,6 +280,7 @@ def train_embedding(embedding_name, learn_rate, batch_size, gradient_step, data_\n     dl = modules.textual_inversion.dataset.PersonalizedDataLoader(ds, latent_sampling_method=latent_sampling_method, batch_size=ds.batch_size, pin_memory=pin_memory)\r\n \r\n     if unload:\r\n+        shared.parallel_processing_allowed = False\r\n         shared.sd_model.first_stage_model.to(devices.cpu)\r\n \r\n     embedding.vec.requires_grad = True\r\n@@ -450,6 +452,7 @@ def train_embedding(embedding_name, learn_rate, batch_size, gradient_step, data_\n         pbar.leave = False\r\n         pbar.close()\r\n         shared.sd_model.first_stage_model.to(devices.device)\r\n+        shared.parallel_processing_allowed = old_parallel_processing_allowed\r\n \r\n     return embedding, filename\r\n \r\n", "test_patch": "", "problem_statement": "[Bug]: Torch.cuda error during Textual Inversion training\n### Is there an existing issue for this?\r\n\r\n- [X] I have searched the existing issues and checked the recent builds/commits\r\n\r\n### What happened?\r\n\r\nSomething is not 100% for me either, because when I copy a previous step back and then resume the training, I get interesting torch errors. After that neither the preview nor the process updated on the UI. I stopped the workout, then resumed it, then it went fine for a while, but sometimes it came up again, sometimes not. I don't use `--xformers --medvram --precision full --no-half` options. My card is RTX 3060 12GB.\r\n\r\n\r\nTraceback (most recent call last):       | 260/20000 [06:55<1:16:29,  4.30it/s]\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\venv\\lib\\site-packages\\gradio\\routes.py\", line 275, in run_predict\r\n    output = await app.blocks.process_api(\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\venv\\lib\\site-packages\\gradio\\blocks.py\", line 787, in process_api\r\n    result = await self.call_function(fn_index, inputs, iterator)\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\venv\\lib\\site-packages\\gradio\\blocks.py\", line 694, in call_function\r\n    prediction = await anyio.to_thread.run_sync(\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\venv\\lib\\site-packages\\anyio\\to_thread.py\", line 31, in run_sync\r\n    return await get_asynclib().run_sync_in_worker_thread(\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\venv\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 937, in run_sync_in_worker_thread\r\n    return await future\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\venv\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 867, in run\r\n    result = context.run(func, *args)\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\modules\\ui.py\", line 526, in <lambda>\r\n    fn=lambda: check_progress_call(id_part),\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\modules\\ui.py\", line 284, in check_progress_call\r\n    shared.state.current_image = modules.sd_samplers.samples_to_image_grid(shared.state.current_latent)\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\modules\\sd_samplers.py\", line 100, in samples_to_image_grid\r\n    return images.image_grid([single_sample_to_image(sample) for sample in samples])\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\modules\\sd_samplers.py\", line 100, in <listcomp>\r\n    return images.image_grid([single_sample_to_image(sample) for sample in samples])\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\modules\\sd_samplers.py\", line 88, in single_sample_to_image\r\n    x_sample = processing.decode_first_stage(shared.sd_model, sample.unsqueeze(0))[0]\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\modules\\processing.py\", line 367, in decode_first_stage\r\n    x = model.decode_first_stage(x)\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\venv\\lib\\site-packages\\torch\\autograd\\grad_mode.py\", line 27, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\repositories\\stable-diffusion\\ldm\\models\\diffusion\\ddpm.py\", line 763, in decode_first_stage\r\n    return self.first_stage_model.decode(z)\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\repositories\\stable-diffusion\\ldm\\models\\autoencoder.py\", line 332, in decode\r\n    dec = self.decoder(z)\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1130, in _call_impl\r\n    return forward_call(*input, **kwargs)\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\repositories\\stable-diffusion\\ldm\\modules\\diffusionmodules\\model.py\", line 553, in forward\r\n    h = self.up[i_level].block[i_block](h, temb)\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1130, in _call_impl\r\n    return forward_call(*input, **kwargs)\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\repositories\\stable-diffusion\\ldm\\modules\\diffusionmodules\\model.py\", line 125, in forward\r\n    h = self.conv1(h)\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1130, in _call_impl\r\n    return forward_call(*input, **kwargs)\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\venv\\lib\\site-packages\\torch\\nn\\modules\\conv.py\", line 457, in forward\r\n    return self._conv_forward(input, self.weight, self.bias)\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\venv\\lib\\site-packages\\torch\\nn\\modules\\conv.py\", line 453, in _conv_forward\r\n    return F.conv2d(input, weight, bias, self.stride,\r\nRuntimeError: Input type (torch.cuda.HalfTensor) and weight type (torch.HalfTensor) should be the same\r\n\r\n### Steps to reproduce the problem\r\n\r\n1. Create a Textual Inversion embedding with 4 vector, 18 images, Initialization text: caricature, image size is 512x512, Learning rate: 5e-04:200, 5e-05:500, 5e-06:800, 5e-07:1000 Max steps: 1000, preview and embedding save on every 50th step\r\n2. Train embedding\r\n3. Try the train again with a different name if you didn't get an error\r\n\r\n\r\n### What should have happened?\r\n\r\nIn previous versions there was no error during training\r\n\r\n### Commit where the problem happens\r\n\r\n198a1ffcfc963a3d74674fad560e87dbebf7949f\r\n\r\n### What platforms do you use to access UI ?\r\n\r\nWindows\r\n\r\n### What browsers do you use to access the UI ?\r\n\r\nGoogle Chrome\r\n\r\n### Command Line Arguments\r\n\r\n```Shell\r\n--ui-config-file 1my/ui-config-my.json --ui-settings-file 1my/config-my.json --autolaunch --gradio-img2img-tool color-sketch --vae-path \"models\\Stable-diffusion\\newVAE.vae.pt\"\r\n```\r\n\r\n\r\n### Additional information, context and logs\r\n\r\nMy train file content here:\r\n\r\n```\r\na caricature art by [name]\r\na caricature, art by [name]\r\na caricature by [name]\r\nart by [name]\r\n```\r\n\n[Bug]: Torch.cuda error during Textual Inversion training\n### Is there an existing issue for this?\r\n\r\n- [X] I have searched the existing issues and checked the recent builds/commits\r\n\r\n### What happened?\r\n\r\nSomething is not 100% for me either, because when I copy a previous step back and then resume the training, I get interesting torch errors. After that neither the preview nor the process updated on the UI. I stopped the workout, then resumed it, then it went fine for a while, but sometimes it came up again, sometimes not. I don't use `--xformers --medvram --precision full --no-half` options. My card is RTX 3060 12GB.\r\n\r\n\r\nTraceback (most recent call last):       | 260/20000 [06:55<1:16:29,  4.30it/s]\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\venv\\lib\\site-packages\\gradio\\routes.py\", line 275, in run_predict\r\n    output = await app.blocks.process_api(\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\venv\\lib\\site-packages\\gradio\\blocks.py\", line 787, in process_api\r\n    result = await self.call_function(fn_index, inputs, iterator)\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\venv\\lib\\site-packages\\gradio\\blocks.py\", line 694, in call_function\r\n    prediction = await anyio.to_thread.run_sync(\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\venv\\lib\\site-packages\\anyio\\to_thread.py\", line 31, in run_sync\r\n    return await get_asynclib().run_sync_in_worker_thread(\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\venv\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 937, in run_sync_in_worker_thread\r\n    return await future\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\venv\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 867, in run\r\n    result = context.run(func, *args)\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\modules\\ui.py\", line 526, in <lambda>\r\n    fn=lambda: check_progress_call(id_part),\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\modules\\ui.py\", line 284, in check_progress_call\r\n    shared.state.current_image = modules.sd_samplers.samples_to_image_grid(shared.state.current_latent)\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\modules\\sd_samplers.py\", line 100, in samples_to_image_grid\r\n    return images.image_grid([single_sample_to_image(sample) for sample in samples])\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\modules\\sd_samplers.py\", line 100, in <listcomp>\r\n    return images.image_grid([single_sample_to_image(sample) for sample in samples])\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\modules\\sd_samplers.py\", line 88, in single_sample_to_image\r\n    x_sample = processing.decode_first_stage(shared.sd_model, sample.unsqueeze(0))[0]\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\modules\\processing.py\", line 367, in decode_first_stage\r\n    x = model.decode_first_stage(x)\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\venv\\lib\\site-packages\\torch\\autograd\\grad_mode.py\", line 27, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\repositories\\stable-diffusion\\ldm\\models\\diffusion\\ddpm.py\", line 763, in decode_first_stage\r\n    return self.first_stage_model.decode(z)\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\repositories\\stable-diffusion\\ldm\\models\\autoencoder.py\", line 332, in decode\r\n    dec = self.decoder(z)\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1130, in _call_impl\r\n    return forward_call(*input, **kwargs)\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\repositories\\stable-diffusion\\ldm\\modules\\diffusionmodules\\model.py\", line 553, in forward\r\n    h = self.up[i_level].block[i_block](h, temb)\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1130, in _call_impl\r\n    return forward_call(*input, **kwargs)\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\repositories\\stable-diffusion\\ldm\\modules\\diffusionmodules\\model.py\", line 125, in forward\r\n    h = self.conv1(h)\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1130, in _call_impl\r\n    return forward_call(*input, **kwargs)\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\venv\\lib\\site-packages\\torch\\nn\\modules\\conv.py\", line 457, in forward\r\n    return self._conv_forward(input, self.weight, self.bias)\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\venv\\lib\\site-packages\\torch\\nn\\modules\\conv.py\", line 453, in _conv_forward\r\n    return F.conv2d(input, weight, bias, self.stride,\r\nRuntimeError: Input type (torch.cuda.HalfTensor) and weight type (torch.HalfTensor) should be the same\r\n\r\n### Steps to reproduce the problem\r\n\r\n1. Create a Textual Inversion embedding with 4 vector, 18 images, Initialization text: caricature, image size is 512x512, Learning rate: 5e-04:200, 5e-05:500, 5e-06:800, 5e-07:1000 Max steps: 1000, preview and embedding save on every 50th step\r\n2. Train embedding\r\n3. Try the train again with a different name if you didn't get an error\r\n\r\n\r\n### What should have happened?\r\n\r\nIn previous versions there was no error during training\r\n\r\n### Commit where the problem happens\r\n\r\n198a1ffcfc963a3d74674fad560e87dbebf7949f\r\n\r\n### What platforms do you use to access UI ?\r\n\r\nWindows\r\n\r\n### What browsers do you use to access the UI ?\r\n\r\nGoogle Chrome\r\n\r\n### Command Line Arguments\r\n\r\n```Shell\r\n--ui-config-file 1my/ui-config-my.json --ui-settings-file 1my/config-my.json --autolaunch --gradio-img2img-tool color-sketch --vae-path \"models\\Stable-diffusion\\newVAE.vae.pt\"\r\n```\r\n\r\n\r\n### Additional information, context and logs\r\n\r\nMy train file content here:\r\n\r\n```\r\na caricature art by [name]\r\na caricature, art by [name]\r\na caricature by [name]\r\nart by [name]\r\n```\r\n\n", "hints_text": "This first training session I got the above message at 600 steps with vector 1. I stopped at 800, deleted the file and log, then recreated it, this time training with 5e-03:200, 5e-04:400, 5e-05:800, 5e-06:1000 LR for the second time. At 150 steps I got this:\r\n\r\n\r\nTraceback (most recent call last):       | 380/20000 [10:32<1:19:31,  4.11it/s]\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\venv\\lib\\site-packages\\gradio\\routes.py\", line 283, in run_predict\r\n    output = await app.blocks.process_api(\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\venv\\lib\\site-packages\\gradio\\blocks.py\", line 936, in process_api\r\n    result = await self.call_function(fn_index, inputs, iterator)\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\venv\\lib\\site-packages\\gradio\\blocks.py\", line 777, in call_function\r\n    prediction = await anyio.to_thread.run_sync(\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\venv\\lib\\site-packages\\anyio\\to_thread.py\", line 31, in run_sync\r\n    return await get_asynclib().run_sync_in_worker_thread(\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\venv\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 937, in run_sync_in_worker_thread\r\n    return await future\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\venv\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 867, in run\r\n    result = context.run(func, *args)\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\modules\\ui.py\", line 526, in <lambda>\r\n    fn=lambda: check_progress_call(id_part),\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\modules\\ui.py\", line 284, in check_progress_call\r\n    shared.state.current_image = modules.sd_samplers.samples_to_image_grid(shared.state.current_latent)\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\modules\\sd_samplers.py\", line 101, in samples_to_image_grid\r\n    return images.image_grid([single_sample_to_image(sample) for sample in samples])\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\modules\\sd_samplers.py\", line 101, in <listcomp>\r\n    return images.image_grid([single_sample_to_image(sample) for sample in samples])\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\modules\\sd_samplers.py\", line 89, in single_sample_to_image\r\n    x_sample = processing.decode_first_stage(shared.sd_model, sample.unsqueeze(0))[0]\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\modules\\processing.py\", line 371, in decode_first_stage\r\n    x = model.decode_first_stage(x)\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\venv\\lib\\site-packages\\torch\\autograd\\grad_mode.py\", line 27, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\repositories\\stable-diffusion\\ldm\\models\\diffusion\\ddpm.py\", line 763, in decode_first_stage\r\n    return self.first_stage_model.decode(z)\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\repositories\\stable-diffusion\\ldm\\models\\autoencoder.py\", line 332, in decode\r\n    dec = self.decoder(z)\r\n[Epoch 0: 150/1800]loss: 0.0754993:  15%|\u258d  | 150/1000 [01:37<31:12,  2.20s/it]  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1130, in _call_impl\r\n    return forward_call(*input, **kwargs)\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\repositories\\stable-diffusion\\ldm\\modules\\diffusionmodules\\model.py\", line 553, in forward\r\n    h = self.up[i_level].block[i_block](h, temb)\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1130, in _call_impl\r\n    return forward_call(*input, **kwargs)\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\repositories\\stable-diffusion\\ldm\\modules\\diffusionmodules\\model.py\", line 123, in forward\r\n    h = self.norm1(h)\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1130, in _call_impl\r\n    return forward_call(*input, **kwargs)\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\venv\\lib\\site-packages\\torch\\nn\\modules\\normalization.py\", line 272, in forward\r\n    return F.group_norm(\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\venv\\lib\\site-packages\\torch\\nn\\functional.py\", line 2516, in group_norm\r\n    return torch.group_norm(input, num_groups, weight, bias, eps, torch.backends.cudnn.enabled)\r\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument weight in method wrapper__native_group_norm)\r\n\r\n\r\nI don't know where that 380/20000 came from, because I don't have 20000 anywhere, so that's weird too. After this error, the training became unusable if I continued. I had to start again.\r\n\r\nUpdate 1: I restarted SD and now at 300 steps the error in the original post came up. Must be some interesting error that a workout doesn't go through. Fortunately, despite the torch error, the model doesn't break in this case, just the interface doesn't update, while for the tensor and devices error, the further results are no longer usable.\r\nIt was included with yesterday's codes because it went bad after an update. \r\n\r\nUpdate 2: the bug is still present in 172c4bc09f0866e7dd114068ebe0f9abfe79ef33, already at the 50th step this devices problem occurred.\nI just got a new error after the first 50 steps (auto-save point) in the latest version cd5eafaf03a25d2b0e35154666947b9489078af9:\r\n\r\n[Epoch 0: 50/1000]loss: 0.1734919:   5%|\u258f    | 49/1000 [00:36<11:44,  1.35it/s]\r\nApplying cross attention optimization (Doggettx).\r\nError completing request\r\nArguments: ('rejtocartoon', '5e-03:200, 5e-04:500, 5e-05:800, 5e-06:1000', 1, 'H:\\\\Stable-Diffusion-Automatic\\\\textual inversion\\\\rejto\\\\dest', 'H:\\\\Stable-Diffusion-Automatic\\\\textual inversion\\\\rejto\\\\log', 512, 512, 1000, 50, 50, 'H:\\\\Stable-Diffusion-Automatic\\\\textual inversion\\\\rejto\\\\cartoonstyle.txt', False, False, '', '', 20, 0, 7, -1.0, 640, 640) {}\r\nTraceback (most recent call last):\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\modules\\ui.py\", line 185, in f\r\n    res = list(func(*args, **kwargs))\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\webui.py\", line 55, in f\r\n    res = func(*args, **kwargs)\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\modules\\textual_inversion\\ui.py\", line 33, in train_embedding\r\n    embedding, filename = modules.textual_inversion.textual_inversion.train_embedding(*args)\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\modules\\textual_inversion\\textual_inversion.py\", line 365, in train_embedding\r\n    shared.sd_model.first_stage_model.to(devices.cpu)\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\venv\\lib\\site-packages\\pytorch_lightning\\core\\mixins\\device_dtype_mixin.py\", line 113, in to\r\n    return super().to(*args, **kwargs)\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 927, in to\r\n    return self._apply(convert)\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 579, in _apply\r\n    module._apply(fn)\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 579, in _apply\r\n    module._apply(fn)\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 579, in _apply\r\n    module._apply(fn)\r\n  [Previous line repeated 3 more times]\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 602, in _apply\r\n    param_applied = fn(param)\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 925, in convert\r\n    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\r\nRuntimeError: CUDA error: an illegal memory access was encountered\r\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\r\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\r\n\r\nTraceback (most recent call last):\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\venv\\lib\\site-packages\\gradio\\routes.py\", line 283, in run_predict\r\n    output = await app.blocks.process_api(\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\venv\\lib\\site-packages\\gradio\\blocks.py\", line 936, in process_api\r\n    result = await self.call_function(fn_index, inputs, iterator)\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\venv\\lib\\site-packages\\gradio\\blocks.py\", line 777, in call_function\r\n    prediction = await anyio.to_thread.run_sync(\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\venv\\lib\\site-packages\\anyio\\to_thread.py\", line 31, in run_sync\r\n    return await get_asynclib().run_sync_in_worker_thread(\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\venv\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 937, in run_sync_in_worker_thread\r\n    return await future\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\venv\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 867, in run\r\n    result = context.run(func, *args)\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\modules\\ui.py\", line 518, in <lambda>\r\n    fn=lambda: check_progress_call(id_part),\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\modules\\ui.py\", line 280, in check_progress_call\r\n    shared.state.set_current_image()\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\modules\\shared.py\", line 194, in set_current_image\r\n    self.current_image = sd_samplers.samples_to_image_grid(self.current_latent)\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\modules\\sd_samplers.py\", line 101, in samples_to_image_grid\r\n    return images.image_grid([single_sample_to_image(sample) for sample in samples])\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\modules\\sd_samplers.py\", line 101, in <listcomp>\r\n    return images.image_grid([single_sample_to_image(sample) for sample in samples])\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\modules\\sd_samplers.py\", line 89, in single_sample_to_image\r\n    x_sample = processing.decode_first_stage(shared.sd_model, sample.unsqueeze(0))[0]\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\modules\\processing.py\", line 371, in decode_first_stage\r\n    x = model.decode_first_stage(x)\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\venv\\lib\\site-packages\\torch\\autograd\\grad_mode.py\", line 27, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\repositories\\stable-diffusion\\ldm\\models\\diffusion\\ddpm.py\", line 763, in decode_first_stage\r\n    return self.first_stage_model.decode(z)\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\repositories\\stable-diffusion\\ldm\\models\\autoencoder.py\", line 332, in decode\r\n    dec = self.decoder(z)\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1130, in _call_impl\r\n    return forward_call(*input, **kwargs)\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\repositories\\stable-diffusion\\ldm\\modules\\diffusionmodules\\model.py\", line 553, in forward\r\n    h = self.up[i_level].block[i_block](h, temb)\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1130, in _call_impl\r\n    return forward_call(*input, **kwargs)\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\repositories\\stable-diffusion\\ldm\\modules\\diffusionmodules\\model.py\", line 133, in forward\r\n    h = self.conv2(h)\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1130, in _call_impl\r\n    return forward_call(*input, **kwargs)\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\venv\\lib\\site-packages\\torch\\nn\\modules\\conv.py\", line 457, in forward\r\n    return self._conv_forward(input, self.weight, self.bias)\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\venv\\lib\\site-packages\\torch\\nn\\modules\\conv.py\", line 453, in _conv_forward\r\n    return F.conv2d(input, weight, bias, self.stride,\r\nRuntimeError: cuDNN error: CUDNN_STATUS_EXECUTION_FAILED\r\nYou can try to repro this exception using the following code snippet. If that doesn't trigger the error, please include your original repro script when reporting this issue.\r\n\r\nimport torch\r\ntorch.backends.cuda.matmul.allow_tf32 = True\r\ntorch.backends.cudnn.benchmark = False\r\ntorch.backends.cudnn.deterministic = False\r\ntorch.backends.cudnn.allow_tf32 = True\r\ndata = torch.randn([1, 512, 128, 128], dtype=torch.half, device='cuda', requires_grad=True)\r\nnet = torch.nn.Conv2d(512, 512, kernel_size=[3, 3], padding=[1, 1], stride=[1, 1], dilation=[1, 1], groups=1)\r\nnet = net.cuda().half()\r\nout = net(data)\r\nout.backward(torch.randn_like(out))\r\ntorch.cuda.synchronize()\r\n\r\nConvolutionParams\r\n    memory_format = Contiguous\r\n    data_type = CUDNN_DATA_HALF\r\n    padding = [1, 1, 0]\r\n    stride = [1, 1, 0]\r\n    dilation = [1, 1, 0]\r\n    groups = 1\r\n    deterministic = false\r\n    allow_tf32 = true\r\ninput: TensorDescriptor 0000022292CEB960\r\n    type = CUDNN_DATA_HALF\r\n    nbDims = 4\r\n    dimA = 1, 512, 128, 128,\r\n    strideA = 8388608, 16384, 128, 1,\r\noutput: TensorDescriptor 0000022292CEBC00\r\n    type = CUDNN_DATA_HALF\r\n    nbDims = 4\r\n    dimA = 1, 512, 128, 128,\r\n    strideA = 8388608, 16384, 128, 1,\r\nweight: FilterDescriptor 000002235A8B4220\r\n    type = CUDNN_DATA_HALF\r\n    tensor_format = CUDNN_TENSOR_NCHW\r\n    nbDims = 4\r\n    dimA = 512, 512, 3, 3,\r\nPointer addresses:\r\n    input: 0000000C4D000000\r\n    output: 0000000C36E08000\r\n    weight: 00000223EB5A4080\r\nForward algorithm: 1\r\n\r\nIt's getting more and more exciting... I am currently training with model-v1-5-pruned-ema-only.ckpt and SD VAE is none.\nOkay, you need to look at VAE loading and filling! If the SD VAE is on auto, the training works without errors, but whether it is on model or none, the training throws errors!\r\n\r\nUpdate 1: on the 900th and 1300th steps i got two devices error again... :(\r\n\r\nUpdate 2: I started a new training this morning and when generates an image, the two devices error comes out for sure. Now it came up at 200. The training went to 1000, then I continued to 10000. The UI came back to itself on the Train Embedding button, but on the first 50 saves it gave an error again and the image generation on the interface died, it just continued in the background. Would it be a Gradio error? It's like losing the tool when generating the image.\nOkay, I found the source of the bug, the optimization in build f071a1d25aa8b35bb6406a133df1d03ae5ea8d01 does not give the video card back control and confuses the training. Missing is the part that \"if xformers is not loaded...\" It has not been tested without xformers. I think the problem is lines 335 and 412, it lacks the \"if unload:\" before it, so it just passes control to the device.\r\n`shared.sd_model.first_stage_model.to(devices.device)`\r\nwhile in lines 277 and 365 the processor gets the lead in the condition:\r\n```\r\nif unload:\r\n        shared.sd_model.first_stage_model.to(devices.cpu)\r\n```\r\n@AUTOMATIC1111 or @dfaker please review this code because it causes the above errors, please withdraw or modify it because I can't update until it is included! Try it without xformers! In addition, it also screws with the new SD VAE list (I can't use it, only \"auto\" option when I'm training)\nOh yes having same problem\r\nTextual Inversion still a king here so it would be cool to fix this\nI'll wait a few days to fix the problem, but if it's not fixed, I'll revoke the code to restore it to its original state. I'm not a programmer, so I don't have experience with other types of fixes, but I'd love the new features. Maybe even @MarkovInequality could fix the problem.\nI just tested the latest build without transformers using the doggettx, and invokeAI optimizations and I can't seem to reproduce the problem.\r\nI also tested training with VAE selected as None, auto, or some specific vae and unfortunately I can't replicate the error either.\r\nI don't believe this has anything to do with whether you have xformers enabled or not, but rather something to do with moving the VAE from the GPU to CPU. You can temporarily disable this by unticking the \"Move VAE and CLIP to RAM when training if possible\" option in the settings.\r\n\r\n>lines 335 and 412, it lacks the \"if unload:\" before it, so it just passes control to the device.\r\n\r\nI don't think this is the problem either, as it unconditionally moves the VAE to the GPU just before we need it to generate the preview image. If the VAE is already on the GPU, then this should be a noop.\r\n\r\nCan you send me your launch parameters and a screenshot of the \"Training\" section in the settings page?\r\n\r\n\nI just updated to the f2b69709eaff88fc3a2bd49585556ec0883bf5ea build. I use these settings and run another workout. I always turn on the \"Move VAE and CLIP to RAM when training if possible\" option, because the training is much more efficient this way.\r\n![image](https://user-images.githubusercontent.com/6390413/199913421-039c7924-7862-4e5f-8700-f928b196b37d.png)\r\n\r\nI'll start a training now and see if it stops with these parameters: \r\n- LR: 5e-03:200, 5e-04:500, 5e-05:800, 5e-06:1000 \r\n- 512x512 \r\n- Save every 50 steps\r\n\r\n![image](https://user-images.githubusercontent.com/6390413/199914398-09c07ff0-75be-407a-8bf1-6f7675728388.png)\r\n\nOk, now I got the error at step 350:\r\n\r\n[Epoch 0: 351/600]loss: 0.0620605:  35%|\u2588\u258d  | 351/1000 [03:50<18:38,  1.72s/it]Traceback (most recent call last):\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [00:04<00:00,  4.32it/s]\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\venv\\lib\\site-packages\\gradio\\routes.py\", line 283, in run_predict\r\n    output = await app.blocks.process_api(\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\venv\\lib\\site-packages\\gradio\\blocks.py\", line 936, in process_api\r\n    result = await self.call_function(fn_index, inputs, iterator)\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\venv\\lib\\site-packages\\gradio\\blocks.py\", line 777, in call_function\r\n    prediction = await anyio.to_thread.run_sync(\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\venv\\lib\\site-packages\\anyio\\to_thread.py\", line 31, in run_sync\r\n    return await get_asynclib().run_sync_in_worker_thread(\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\venv\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 937, in run_sync_in_worker_thread\r\n    return await future\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\venv\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 867, in run\r\n    result = context.run(func, *args)\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\modules\\ui.py\", line 518, in <lambda>\r\n    fn=lambda: check_progress_call(id_part),\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\modules\\ui.py\", line 280, in check_progress_call\r\n    shared.state.set_current_image()\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\modules\\shared.py\", line 194, in set_current_image\r\n    self.current_image = sd_samplers.samples_to_image_grid(self.current_latent)\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\modules\\sd_samplers.py\", line 101, in samples_to_image_grid\r\n    return images.image_grid([single_sample_to_image(sample) for sample in samples])\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\modules\\sd_samplers.py\", line 101, in <listcomp>\r\n    return images.image_grid([single_sample_to_image(sample) for sample in samples])\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\modules\\sd_samplers.py\", line 89, in single_sample_to_image\r\n    x_sample = processing.decode_first_stage(shared.sd_model, sample.unsqueeze(0))[0]\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\modules\\processing.py\", line 363, in decode_first_stage\r\n    x = model.decode_first_stage(x)\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\venv\\lib\\site-packages\\torch\\autograd\\grad_mode.py\", line 27, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\repositories\\stable-diffusion\\ldm\\models\\diffusion\\ddpm.py\", line 763, in decode_first_stage\r\n    return self.first_stage_model.decode(z)\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\repositories\\stable-diffusion\\ldm\\models\\autoencoder.py\", line 332, in decode\r\n    dec = self.decoder(z)\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1130, in _call_impl\r\n    return forward_call(*input, **kwargs)\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\repositories\\stable-diffusion\\ldm\\modules\\diffusionmodules\\model.py\", line 553, in forward\r\n    h = self.up[i_level].block[i_block](h, temb)\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1130, in _call_impl\r\n    return forward_call(*input, **kwargs)\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\repositories\\stable-diffusion\\ldm\\modules\\diffusionmodules\\model.py\", line 125, in forward\r\n    h = self.conv1(h)\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1130, in _call_impl\r\n    return forward_call(*input, **kwargs)\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\venv\\lib\\site-packages\\torch\\nn\\modules\\conv.py\", line 457, in forward\r\n    return self._conv_forward(input, self.weight, self.bias)\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\venv\\lib\\site-packages\\torch\\nn\\modules\\conv.py\", line 453, in _conv_forward\r\n    return F.conv2d(input, weight, bias, self.stride,\r\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!\r\n\r\nI'm about to try SD VAE with \"none\" option again.\r\n\r\nUpdate 1: I got the same error now on the 700th step when VAE was in \"none\" state.\r\nI'll rewrite the code to the state before you changed it and run another test.\nI think I found the problem, moving VAE to the CPU doesn't play nicely with \"Show image creation progress every N sampling steps\". Set it 0 to disable.\r\n\r\nCan you try setting that setting to 0 and see if you still get the problem. I think the conflict is causing a race condition that apparently just works out on my computer.\r\n\r\nIn the meantime, can you try it out on hypernetworks to see if you're also getting the same problem, because I think the same problem might also apply to hypernetowrks as well.\nAfter I took out your changes, the training ran flawlessly:\r\n\r\nTraining at rate of 0.005 until step 200\r\nPreparing dataset...\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6/6 [00:02<00:00,  2.12it/s]\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [00:04<00:00,  4.13it/s]\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [00:04<00:00,  4.14it/s]\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [00:04<00:00,  4.11it/s]\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [00:04<00:00,  4.10it/s]\r\nTraining at rate of 0.0005 until step 500\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [00:04<00:00,  4.10it/s]\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [00:04<00:00,  4.10it/s]\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [00:04<00:00,  4.12it/s]\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [00:04<00:00,  4.07it/s]\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [00:04<00:00,  4.10it/s]\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [00:04<00:00,  4.10it/s]\r\nTraining at rate of 5e-05 until step 800\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [00:04<00:00,  4.11it/s]\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [00:04<00:00,  4.10it/s]\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [00:04<00:00,  4.10it/s]\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [00:04<00:00,  4.11it/s]\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [00:04<00:00,  4.12it/s]\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [00:05<00:00,  4.00it/s]\r\nTraining at rate of 5e-06 until step 1000\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [00:04<00:00,  4.10it/s]\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [00:04<00:00,  4.10it/s]\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [00:04<00:00,  4.11it/s]\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [00:04<00:00,  4.10it/s]\r\n[Epoch 1: 400/600]loss: 0.0674431: 100%|\u2588\u2588\u2588| 1000/1000 [10:40<00:00,  1.56it/s]\r\nApplying cross attention optimization (Doggettx).20/20 [00:04<00:00,  4.22it/s]\r\n\r\nI'll undo my changes now and set the preview generation from 5 to 0, then I'll come back. I do not generate HN.\nCan you please try to train a hypernetwork with \"Show image creation progress every N sampling steps\" set to 0, with my optimization with the \"Move VAE and CLIP to RAM when training if possible\" setting both on and off?\r\n\r\nI can't seem to trigger the race condition on my computer so I'll have to rely on you to test before I make a PR to fix this issue\nThis first training session I got the above message at 600 steps with vector 1. I stopped at 800, deleted the file and log, then recreated it, this time training with 5e-03:200, 5e-04:400, 5e-05:800, 5e-06:1000 LR for the second time. At 150 steps I got this:\r\n\r\n\r\nTraceback (most recent call last):       | 380/20000 [10:32<1:19:31,  4.11it/s]\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\venv\\lib\\site-packages\\gradio\\routes.py\", line 283, in run_predict\r\n    output = await app.blocks.process_api(\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\venv\\lib\\site-packages\\gradio\\blocks.py\", line 936, in process_api\r\n    result = await self.call_function(fn_index, inputs, iterator)\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\venv\\lib\\site-packages\\gradio\\blocks.py\", line 777, in call_function\r\n    prediction = await anyio.to_thread.run_sync(\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\venv\\lib\\site-packages\\anyio\\to_thread.py\", line 31, in run_sync\r\n    return await get_asynclib().run_sync_in_worker_thread(\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\venv\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 937, in run_sync_in_worker_thread\r\n    return await future\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\venv\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 867, in run\r\n    result = context.run(func, *args)\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\modules\\ui.py\", line 526, in <lambda>\r\n    fn=lambda: check_progress_call(id_part),\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\modules\\ui.py\", line 284, in check_progress_call\r\n    shared.state.current_image = modules.sd_samplers.samples_to_image_grid(shared.state.current_latent)\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\modules\\sd_samplers.py\", line 101, in samples_to_image_grid\r\n    return images.image_grid([single_sample_to_image(sample) for sample in samples])\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\modules\\sd_samplers.py\", line 101, in <listcomp>\r\n    return images.image_grid([single_sample_to_image(sample) for sample in samples])\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\modules\\sd_samplers.py\", line 89, in single_sample_to_image\r\n    x_sample = processing.decode_first_stage(shared.sd_model, sample.unsqueeze(0))[0]\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\modules\\processing.py\", line 371, in decode_first_stage\r\n    x = model.decode_first_stage(x)\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\venv\\lib\\site-packages\\torch\\autograd\\grad_mode.py\", line 27, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\repositories\\stable-diffusion\\ldm\\models\\diffusion\\ddpm.py\", line 763, in decode_first_stage\r\n    return self.first_stage_model.decode(z)\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\repositories\\stable-diffusion\\ldm\\models\\autoencoder.py\", line 332, in decode\r\n    dec = self.decoder(z)\r\n[Epoch 0: 150/1800]loss: 0.0754993:  15%|\u258d  | 150/1000 [01:37<31:12,  2.20s/it]  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1130, in _call_impl\r\n    return forward_call(*input, **kwargs)\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\repositories\\stable-diffusion\\ldm\\modules\\diffusionmodules\\model.py\", line 553, in forward\r\n    h = self.up[i_level].block[i_block](h, temb)\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1130, in _call_impl\r\n    return forward_call(*input, **kwargs)\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\repositories\\stable-diffusion\\ldm\\modules\\diffusionmodules\\model.py\", line 123, in forward\r\n    h = self.norm1(h)\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1130, in _call_impl\r\n    return forward_call(*input, **kwargs)\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\venv\\lib\\site-packages\\torch\\nn\\modules\\normalization.py\", line 272, in forward\r\n    return F.group_norm(\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\venv\\lib\\site-packages\\torch\\nn\\functional.py\", line 2516, in group_norm\r\n    return torch.group_norm(input, num_groups, weight, bias, eps, torch.backends.cudnn.enabled)\r\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument weight in method wrapper__native_group_norm)\r\n\r\n\r\nI don't know where that 380/20000 came from, because I don't have 20000 anywhere, so that's weird too. After this error, the training became unusable if I continued. I had to start again.\r\n\r\nUpdate 1: I restarted SD and now at 300 steps the error in the original post came up. Must be some interesting error that a workout doesn't go through. Fortunately, despite the torch error, the model doesn't break in this case, just the interface doesn't update, while for the tensor and devices error, the further results are no longer usable.\r\nIt was included with yesterday's codes because it went bad after an update. \r\n\r\nUpdate 2: the bug is still present in 172c4bc09f0866e7dd114068ebe0f9abfe79ef33, already at the 50th step this devices problem occurred.\nI just got a new error after the first 50 steps (auto-save point) in the latest version cd5eafaf03a25d2b0e35154666947b9489078af9:\r\n\r\n[Epoch 0: 50/1000]loss: 0.1734919:   5%|\u258f    | 49/1000 [00:36<11:44,  1.35it/s]\r\nApplying cross attention optimization (Doggettx).\r\nError completing request\r\nArguments: ('rejtocartoon', '5e-03:200, 5e-04:500, 5e-05:800, 5e-06:1000', 1, 'H:\\\\Stable-Diffusion-Automatic\\\\textual inversion\\\\rejto\\\\dest', 'H:\\\\Stable-Diffusion-Automatic\\\\textual inversion\\\\rejto\\\\log', 512, 512, 1000, 50, 50, 'H:\\\\Stable-Diffusion-Automatic\\\\textual inversion\\\\rejto\\\\cartoonstyle.txt', False, False, '', '', 20, 0, 7, -1.0, 640, 640) {}\r\nTraceback (most recent call last):\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\modules\\ui.py\", line 185, in f\r\n    res = list(func(*args, **kwargs))\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\webui.py\", line 55, in f\r\n    res = func(*args, **kwargs)\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\modules\\textual_inversion\\ui.py\", line 33, in train_embedding\r\n    embedding, filename = modules.textual_inversion.textual_inversion.train_embedding(*args)\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\modules\\textual_inversion\\textual_inversion.py\", line 365, in train_embedding\r\n    shared.sd_model.first_stage_model.to(devices.cpu)\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\venv\\lib\\site-packages\\pytorch_lightning\\core\\mixins\\device_dtype_mixin.py\", line 113, in to\r\n    return super().to(*args, **kwargs)\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 927, in to\r\n    return self._apply(convert)\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 579, in _apply\r\n    module._apply(fn)\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 579, in _apply\r\n    module._apply(fn)\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 579, in _apply\r\n    module._apply(fn)\r\n  [Previous line repeated 3 more times]\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 602, in _apply\r\n    param_applied = fn(param)\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 925, in convert\r\n    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\r\nRuntimeError: CUDA error: an illegal memory access was encountered\r\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\r\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\r\n\r\nTraceback (most recent call last):\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\venv\\lib\\site-packages\\gradio\\routes.py\", line 283, in run_predict\r\n    output = await app.blocks.process_api(\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\venv\\lib\\site-packages\\gradio\\blocks.py\", line 936, in process_api\r\n    result = await self.call_function(fn_index, inputs, iterator)\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\venv\\lib\\site-packages\\gradio\\blocks.py\", line 777, in call_function\r\n    prediction = await anyio.to_thread.run_sync(\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\venv\\lib\\site-packages\\anyio\\to_thread.py\", line 31, in run_sync\r\n    return await get_asynclib().run_sync_in_worker_thread(\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\venv\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 937, in run_sync_in_worker_thread\r\n    return await future\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\venv\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 867, in run\r\n    result = context.run(func, *args)\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\modules\\ui.py\", line 518, in <lambda>\r\n    fn=lambda: check_progress_call(id_part),\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\modules\\ui.py\", line 280, in check_progress_call\r\n    shared.state.set_current_image()\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\modules\\shared.py\", line 194, in set_current_image\r\n    self.current_image = sd_samplers.samples_to_image_grid(self.current_latent)\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\modules\\sd_samplers.py\", line 101, in samples_to_image_grid\r\n    return images.image_grid([single_sample_to_image(sample) for sample in samples])\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\modules\\sd_samplers.py\", line 101, in <listcomp>\r\n    return images.image_grid([single_sample_to_image(sample) for sample in samples])\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\modules\\sd_samplers.py\", line 89, in single_sample_to_image\r\n    x_sample = processing.decode_first_stage(shared.sd_model, sample.unsqueeze(0))[0]\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\modules\\processing.py\", line 371, in decode_first_stage\r\n    x = model.decode_first_stage(x)\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\venv\\lib\\site-packages\\torch\\autograd\\grad_mode.py\", line 27, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\repositories\\stable-diffusion\\ldm\\models\\diffusion\\ddpm.py\", line 763, in decode_first_stage\r\n    return self.first_stage_model.decode(z)\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\repositories\\stable-diffusion\\ldm\\models\\autoencoder.py\", line 332, in decode\r\n    dec = self.decoder(z)\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1130, in _call_impl\r\n    return forward_call(*input, **kwargs)\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\repositories\\stable-diffusion\\ldm\\modules\\diffusionmodules\\model.py\", line 553, in forward\r\n    h = self.up[i_level].block[i_block](h, temb)\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1130, in _call_impl\r\n    return forward_call(*input, **kwargs)\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\repositories\\stable-diffusion\\ldm\\modules\\diffusionmodules\\model.py\", line 133, in forward\r\n    h = self.conv2(h)\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1130, in _call_impl\r\n    return forward_call(*input, **kwargs)\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\venv\\lib\\site-packages\\torch\\nn\\modules\\conv.py\", line 457, in forward\r\n    return self._conv_forward(input, self.weight, self.bias)\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\venv\\lib\\site-packages\\torch\\nn\\modules\\conv.py\", line 453, in _conv_forward\r\n    return F.conv2d(input, weight, bias, self.stride,\r\nRuntimeError: cuDNN error: CUDNN_STATUS_EXECUTION_FAILED\r\nYou can try to repro this exception using the following code snippet. If that doesn't trigger the error, please include your original repro script when reporting this issue.\r\n\r\nimport torch\r\ntorch.backends.cuda.matmul.allow_tf32 = True\r\ntorch.backends.cudnn.benchmark = False\r\ntorch.backends.cudnn.deterministic = False\r\ntorch.backends.cudnn.allow_tf32 = True\r\ndata = torch.randn([1, 512, 128, 128], dtype=torch.half, device='cuda', requires_grad=True)\r\nnet = torch.nn.Conv2d(512, 512, kernel_size=[3, 3], padding=[1, 1], stride=[1, 1], dilation=[1, 1], groups=1)\r\nnet = net.cuda().half()\r\nout = net(data)\r\nout.backward(torch.randn_like(out))\r\ntorch.cuda.synchronize()\r\n\r\nConvolutionParams\r\n    memory_format = Contiguous\r\n    data_type = CUDNN_DATA_HALF\r\n    padding = [1, 1, 0]\r\n    stride = [1, 1, 0]\r\n    dilation = [1, 1, 0]\r\n    groups = 1\r\n    deterministic = false\r\n    allow_tf32 = true\r\ninput: TensorDescriptor 0000022292CEB960\r\n    type = CUDNN_DATA_HALF\r\n    nbDims = 4\r\n    dimA = 1, 512, 128, 128,\r\n    strideA = 8388608, 16384, 128, 1,\r\noutput: TensorDescriptor 0000022292CEBC00\r\n    type = CUDNN_DATA_HALF\r\n    nbDims = 4\r\n    dimA = 1, 512, 128, 128,\r\n    strideA = 8388608, 16384, 128, 1,\r\nweight: FilterDescriptor 000002235A8B4220\r\n    type = CUDNN_DATA_HALF\r\n    tensor_format = CUDNN_TENSOR_NCHW\r\n    nbDims = 4\r\n    dimA = 512, 512, 3, 3,\r\nPointer addresses:\r\n    input: 0000000C4D000000\r\n    output: 0000000C36E08000\r\n    weight: 00000223EB5A4080\r\nForward algorithm: 1\r\n\r\nIt's getting more and more exciting... I am currently training with model-v1-5-pruned-ema-only.ckpt and SD VAE is none.\nOkay, you need to look at VAE loading and filling! If the SD VAE is on auto, the training works without errors, but whether it is on model or none, the training throws errors!\r\n\r\nUpdate 1: on the 900th and 1300th steps i got two devices error again... :(\r\n\r\nUpdate 2: I started a new training this morning and when generates an image, the two devices error comes out for sure. Now it came up at 200. The training went to 1000, then I continued to 10000. The UI came back to itself on the Train Embedding button, but on the first 50 saves it gave an error again and the image generation on the interface died, it just continued in the background. Would it be a Gradio error? It's like losing the tool when generating the image.\nOkay, I found the source of the bug, the optimization in build f071a1d25aa8b35bb6406a133df1d03ae5ea8d01 does not give the video card back control and confuses the training. Missing is the part that \"if xformers is not loaded...\" It has not been tested without xformers. I think the problem is lines 335 and 412, it lacks the \"if unload:\" before it, so it just passes control to the device.\r\n`shared.sd_model.first_stage_model.to(devices.device)`\r\nwhile in lines 277 and 365 the processor gets the lead in the condition:\r\n```\r\nif unload:\r\n        shared.sd_model.first_stage_model.to(devices.cpu)\r\n```\r\n@AUTOMATIC1111 or @dfaker please review this code because it causes the above errors, please withdraw or modify it because I can't update until it is included! Try it without xformers! In addition, it also screws with the new SD VAE list (I can't use it, only \"auto\" option when I'm training)\nOh yes having same problem\r\nTextual Inversion still a king here so it would be cool to fix this\nI'll wait a few days to fix the problem, but if it's not fixed, I'll revoke the code to restore it to its original state. I'm not a programmer, so I don't have experience with other types of fixes, but I'd love the new features. Maybe even @MarkovInequality could fix the problem.\nI just tested the latest build without transformers using the doggettx, and invokeAI optimizations and I can't seem to reproduce the problem.\r\nI also tested training with VAE selected as None, auto, or some specific vae and unfortunately I can't replicate the error either.\r\nI don't believe this has anything to do with whether you have xformers enabled or not, but rather something to do with moving the VAE from the GPU to CPU. You can temporarily disable this by unticking the \"Move VAE and CLIP to RAM when training if possible\" option in the settings.\r\n\r\n>lines 335 and 412, it lacks the \"if unload:\" before it, so it just passes control to the device.\r\n\r\nI don't think this is the problem either, as it unconditionally moves the VAE to the GPU just before we need it to generate the preview image. If the VAE is already on the GPU, then this should be a noop.\r\n\r\nCan you send me your launch parameters and a screenshot of the \"Training\" section in the settings page?\r\n\r\n\nI just updated to the f2b69709eaff88fc3a2bd49585556ec0883bf5ea build. I use these settings and run another workout. I always turn on the \"Move VAE and CLIP to RAM when training if possible\" option, because the training is much more efficient this way.\r\n![image](https://user-images.githubusercontent.com/6390413/199913421-039c7924-7862-4e5f-8700-f928b196b37d.png)\r\n\r\nI'll start a training now and see if it stops with these parameters: \r\n- LR: 5e-03:200, 5e-04:500, 5e-05:800, 5e-06:1000 \r\n- 512x512 \r\n- Save every 50 steps\r\n\r\n![image](https://user-images.githubusercontent.com/6390413/199914398-09c07ff0-75be-407a-8bf1-6f7675728388.png)\r\n\nOk, now I got the error at step 350:\r\n\r\n[Epoch 0: 351/600]loss: 0.0620605:  35%|\u2588\u258d  | 351/1000 [03:50<18:38,  1.72s/it]Traceback (most recent call last):\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [00:04<00:00,  4.32it/s]\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\venv\\lib\\site-packages\\gradio\\routes.py\", line 283, in run_predict\r\n    output = await app.blocks.process_api(\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\venv\\lib\\site-packages\\gradio\\blocks.py\", line 936, in process_api\r\n    result = await self.call_function(fn_index, inputs, iterator)\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\venv\\lib\\site-packages\\gradio\\blocks.py\", line 777, in call_function\r\n    prediction = await anyio.to_thread.run_sync(\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\venv\\lib\\site-packages\\anyio\\to_thread.py\", line 31, in run_sync\r\n    return await get_asynclib().run_sync_in_worker_thread(\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\venv\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 937, in run_sync_in_worker_thread\r\n    return await future\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\venv\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 867, in run\r\n    result = context.run(func, *args)\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\modules\\ui.py\", line 518, in <lambda>\r\n    fn=lambda: check_progress_call(id_part),\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\modules\\ui.py\", line 280, in check_progress_call\r\n    shared.state.set_current_image()\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\modules\\shared.py\", line 194, in set_current_image\r\n    self.current_image = sd_samplers.samples_to_image_grid(self.current_latent)\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\modules\\sd_samplers.py\", line 101, in samples_to_image_grid\r\n    return images.image_grid([single_sample_to_image(sample) for sample in samples])\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\modules\\sd_samplers.py\", line 101, in <listcomp>\r\n    return images.image_grid([single_sample_to_image(sample) for sample in samples])\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\modules\\sd_samplers.py\", line 89, in single_sample_to_image\r\n    x_sample = processing.decode_first_stage(shared.sd_model, sample.unsqueeze(0))[0]\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\modules\\processing.py\", line 363, in decode_first_stage\r\n    x = model.decode_first_stage(x)\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\venv\\lib\\site-packages\\torch\\autograd\\grad_mode.py\", line 27, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\repositories\\stable-diffusion\\ldm\\models\\diffusion\\ddpm.py\", line 763, in decode_first_stage\r\n    return self.first_stage_model.decode(z)\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\repositories\\stable-diffusion\\ldm\\models\\autoencoder.py\", line 332, in decode\r\n    dec = self.decoder(z)\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1130, in _call_impl\r\n    return forward_call(*input, **kwargs)\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\repositories\\stable-diffusion\\ldm\\modules\\diffusionmodules\\model.py\", line 553, in forward\r\n    h = self.up[i_level].block[i_block](h, temb)\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1130, in _call_impl\r\n    return forward_call(*input, **kwargs)\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\repositories\\stable-diffusion\\ldm\\modules\\diffusionmodules\\model.py\", line 125, in forward\r\n    h = self.conv1(h)\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1130, in _call_impl\r\n    return forward_call(*input, **kwargs)\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\venv\\lib\\site-packages\\torch\\nn\\modules\\conv.py\", line 457, in forward\r\n    return self._conv_forward(input, self.weight, self.bias)\r\n  File \"H:\\Stable-Diffusion-Automatic\\stable-diffusion-webui\\venv\\lib\\site-packages\\torch\\nn\\modules\\conv.py\", line 453, in _conv_forward\r\n    return F.conv2d(input, weight, bias, self.stride,\r\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!\r\n\r\nI'm about to try SD VAE with \"none\" option again.\r\n\r\nUpdate 1: I got the same error now on the 700th step when VAE was in \"none\" state.\r\nI'll rewrite the code to the state before you changed it and run another test.\nI think I found the problem, moving VAE to the CPU doesn't play nicely with \"Show image creation progress every N sampling steps\". Set it 0 to disable.\r\n\r\nCan you try setting that setting to 0 and see if you still get the problem. I think the conflict is causing a race condition that apparently just works out on my computer.\r\n\r\nIn the meantime, can you try it out on hypernetworks to see if you're also getting the same problem, because I think the same problem might also apply to hypernetowrks as well.\nAfter I took out your changes, the training ran flawlessly:\r\n\r\nTraining at rate of 0.005 until step 200\r\nPreparing dataset...\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6/6 [00:02<00:00,  2.12it/s]\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [00:04<00:00,  4.13it/s]\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [00:04<00:00,  4.14it/s]\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [00:04<00:00,  4.11it/s]\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [00:04<00:00,  4.10it/s]\r\nTraining at rate of 0.0005 until step 500\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [00:04<00:00,  4.10it/s]\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [00:04<00:00,  4.10it/s]\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [00:04<00:00,  4.12it/s]\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [00:04<00:00,  4.07it/s]\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [00:04<00:00,  4.10it/s]\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [00:04<00:00,  4.10it/s]\r\nTraining at rate of 5e-05 until step 800\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [00:04<00:00,  4.11it/s]\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [00:04<00:00,  4.10it/s]\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [00:04<00:00,  4.10it/s]\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [00:04<00:00,  4.11it/s]\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [00:04<00:00,  4.12it/s]\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [00:05<00:00,  4.00it/s]\r\nTraining at rate of 5e-06 until step 1000\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [00:04<00:00,  4.10it/s]\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [00:04<00:00,  4.10it/s]\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [00:04<00:00,  4.11it/s]\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [00:04<00:00,  4.10it/s]\r\n[Epoch 1: 400/600]loss: 0.0674431: 100%|\u2588\u2588\u2588| 1000/1000 [10:40<00:00,  1.56it/s]\r\nApplying cross attention optimization (Doggettx).20/20 [00:04<00:00,  4.22it/s]\r\n\r\nI'll undo my changes now and set the preview generation from 5 to 0, then I'll come back. I do not generate HN.\nCan you please try to train a hypernetwork with \"Show image creation progress every N sampling steps\" set to 0, with my optimization with the \"Move VAE and CLIP to RAM when training if possible\" setting both on and off?\r\n\r\nI can't seem to trigger the race condition on my computer so I'll have to rely on you to test before I make a PR to fix this issue", "created_at": "2022-11-04T09:54:33Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 4210, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-4210", "issue_numbers": ["3904"], "base_commit": "d98eacea40c7a40227f36dbea9cf92f90489310b", "patch": "diff --git a/javascript/edit-attention.js b/javascript/edit-attention.js\nindex c0d29a7498b..b947cbecdbb 100644\n--- a/javascript/edit-attention.js\n+++ b/javascript/edit-attention.js\n@@ -1,7 +1,6 @@\n addEventListener('keydown', (event) => {\r\n \tlet target = event.originalTarget || event.composedPath()[0];\r\n-\tif (!target.hasAttribute(\"placeholder\")) return;\r\n-\tif (!target.placeholder.toLowerCase().includes(\"prompt\")) return;\r\n+\tif (!target.matches(\"#toprow textarea.gr-text-input[placeholder]\")) return;\r\n \tif (! (event.metaKey || event.ctrlKey)) return;\r\n \r\n \r\n", "test_patch": "", "problem_statement": "[Bug]: hotkey to edit weight of tag is not working in (most) localizations\n### Is there an existing issue for this?\n\n- [X] I have searched the existing issues and checked the recent builds/commits\n\n### What happened?\n\nSelect a tag and `ctrl+\u2191/\u2193` to change its weight is not working when using localization\n\n### Steps to reproduce the problem\n\n1. Use non-English localization, for example `zh-CN`\r\n2. Type `dog` in prompt textarea and select it\r\n3. Press `ctrl + \u2191`\r\n\n\n### What should have happened?\n\nPrompt changes to `(dog:1.1)`\n\n### Commit where the problem happens\n\n35c45df28b303a05d56a13cb56d4046f08cf8c25\n\n### What platforms do you use to access UI ?\n\nWindows\n\n### What browsers do you use to access the UI ?\n\nMozilla Firefox\n\n### Command Line Arguments\n\n_No response_\n\n### Additional information, context and logs\n\nCause: hard-coding.jpg\r\n\r\nhttps://github.com/AUTOMATIC1111/stable-diffusion-webui/blob/35c45df28b303a05d56a13cb56d4046f08cf8c25/javascript/edit-attention.js#L4\n", "hints_text": "https://github.com/AUTOMATIC1111/stable-diffusion-webui/issues/3334\r\n", "created_at": "2022-11-03T10:56:47Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 4086, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-4086", "issue_numbers": ["3875"], "base_commit": "5c9b3625fa03f18649e1843b5e9f2df2d4de94f9", "patch": "diff --git a/modules/images.py b/modules/images.py\nindex a0728553d39..ae705cbd5bc 100644\n--- a/modules/images.py\n+++ b/modules/images.py\n@@ -510,8 +510,9 @@ def exif_bytes():\n \r\n     if extension.lower() == '.png':\r\n         pnginfo_data = PngImagePlugin.PngInfo()\r\n-        for k, v in params.pnginfo.items():\r\n-            pnginfo_data.add_text(k, str(v))\r\n+        if opts.enable_pnginfo:\r\n+            for k, v in params.pnginfo.items():\r\n+                pnginfo_data.add_text(k, str(v))\r\n \r\n         image.save(fullfn, quality=opts.jpeg_quality, pnginfo=pnginfo_data)\r\n \r\n", "test_patch": "", "problem_statement": "[Bug]: Cannot disable writing of PNG Info\n### Is there an existing issue for this?\n\n- [X] I have searched the existing issues and checked the recent builds/commits\n\n### What happened?\n\nUnchecking this setting (and applying) \r\nSave text information about generation parameters as chunks to png files\r\n\r\nNo longer prevents this information being written to the PNG.\r\n\n\n### Steps to reproduce the problem\n\n1. Go to Settings\r\n2. Uncheck \"Save text information about generation parameters as chunks to png files\"\r\n3. Apply settings (also tested after a restart)\r\n4. Generate image and check with PNG Info and NotePad++ Info is still there.\r\n\n\n### What should have happened?\n\nWhen this option is unchecked generation parameters should not be written to png\n\n### Commit where the problem happens\n\nCommit hash: 737eb28faca8be2bb996ee0930ec77d1f7ebd939\n\n### What platforms do you use to access UI ?\n\nWindows\n\n### What browsers do you use to access the UI ?\n\nGoogle Chrome\n\n### Command Line Arguments\n\n_No response_\n\n### Additional information, context and logs\n\nTested with and without aesthetic extension and any embeddings \n", "hints_text": "Forgot to mention this was operating as expected on a version downloaded about the 03/10/22\nThis seems to have started happening on 0cd74602.", "created_at": "2022-11-01T00:31:43Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 4036, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-4036", "issue_numbers": ["4035"], "base_commit": "352b33106a64a8c34d1f5d79fcb377a73b02e39c", "patch": "diff --git a/modules/sd_models.py b/modules/sd_models.py\nindex ae427a5c986..63e07a12ff9 100644\n--- a/modules/sd_models.py\n+++ b/modules/sd_models.py\n@@ -163,11 +163,11 @@ def load_model_weights(model, checkpoint_info, vae_file=\"auto\"):\n     checkpoint_file = checkpoint_info.filename\r\n     sd_model_hash = checkpoint_info.hash\r\n \r\n-    vae_file = sd_vae.resolve_vae(checkpoint_file, vae_file=vae_file)\r\n+    if shared.opts.sd_checkpoint_cache > 0 and hasattr(model, \"sd_checkpoint_info\"):\r\n+        sd_vae.restore_base_vae(model)\r\n+        checkpoints_loaded[model.sd_checkpoint_info] = model.state_dict().copy()\r\n \r\n-    checkpoint_key = checkpoint_info\r\n-\r\n-    if checkpoint_key not in checkpoints_loaded:\r\n+    if checkpoint_info not in checkpoints_loaded:\r\n         print(f\"Loading weights [{sd_model_hash}] from {checkpoint_file}\")\r\n \r\n         pl_sd = torch.load(checkpoint_file, map_location=shared.weight_load_location)\r\n@@ -197,18 +197,15 @@ def load_model_weights(model, checkpoint_info, vae_file=\"auto\"):\n \r\n         model.first_stage_model.to(devices.dtype_vae)\r\n \r\n-        if shared.opts.sd_checkpoint_cache > 0:\r\n-            # if PR #4035 were to get merged, restore base VAE first before caching\r\n-            checkpoints_loaded[checkpoint_key] = model.state_dict().copy()\r\n-            while len(checkpoints_loaded) > shared.opts.sd_checkpoint_cache:\r\n-                checkpoints_loaded.popitem(last=False)  # LRU\r\n-\r\n     else:\r\n         vae_name = sd_vae.get_filename(vae_file) if vae_file else None\r\n         vae_message = f\" with {vae_name} VAE\" if vae_name else \"\"\r\n         print(f\"Loading weights [{sd_model_hash}]{vae_message} from cache\")\r\n-        checkpoints_loaded.move_to_end(checkpoint_key)\r\n-        model.load_state_dict(checkpoints_loaded[checkpoint_key])\r\n+        model.load_state_dict(checkpoints_loaded[checkpoint_info])\r\n+\r\n+    if shared.opts.sd_checkpoint_cache > 0:\r\n+        while len(checkpoints_loaded) > shared.opts.sd_checkpoint_cache:\r\n+            checkpoints_loaded.popitem(last=False)  # LRU\r\n \r\n     model.sd_model_hash = sd_model_hash\r\n     model.sd_model_checkpoint = checkpoint_file\r\n", "test_patch": "", "problem_statement": "[Bug]: \"1\" checkpoint caching is useless\n### Is there an existing issue for this?\r\n\r\n- [X] I have searched the existing issues and checked the recent builds/commits\r\n\r\n### What happened?\r\n\r\n\"1\" checkpoint caching is useless. When you set the webui to keep n caches, only n-1 is actually useful. This happens because model is cached after loading it. When you set caching to 1, what's in the cache is really just your currently loaded checkpoint. When you load another one, the other checkpoint can't possibly be in the cache, so it's loaded and inserted into cache dict. Then, the cache dict now has 2 entries, and so the oldest one will be discarded. Leaving you with 1 cache of the current model, again. So when caching is set to 1, there's practically no caching. There's always the current model taking up 1 cache \"space\" (it's not duplicated in memory though). \r\n\r\n### Steps to reproduce the problem\r\n\r\n1. Set caching to 1\r\n2. Load a model\r\n3. Load another model\r\n4. Load the first model\r\n\r\n### What should have happened?\r\n\r\nStep 4 should be loading the first model from cache, but it doesn't. Because the current model in cache is from step 3.\r\n\r\n### Commit where the problem happens\r\n\r\n17a2076f72562b428052ee3fc8c43d19c03ecd1e\r\n\r\n### What platforms do you use to access UI ?\r\n\r\nOther/Cloud\r\n\r\n### What browsers do you use to access the UI ?\r\n\r\nGoogle Chrome\r\n\r\n### Command Line Arguments\r\n\r\n```Shell\r\n--autolaunch --ckpt {workspace3}/novelai/final_pruned.ckpt --vae-path {workspace3}/novelai/animevae.pt --deepdanbooru --disable-safe-unpickle  --no-half-vae --xformers\r\n```\r\n\r\n\r\n### Additional information, context and logs\r\n\r\nCaching should keep cache of previous models, not current model. \r\n\r\n<details>\r\n<summary>my log</summary>\r\n\r\n```\r\nTo create a public link, set `share=True` in `launch()`.\r\n(CheckpointInfo(filename='/content/nai/stable-diffusion-webui/models/Stable-diffusion/novelai/final_pruned.ckpt', title='novelai/final_pruned.ckpt [925997e9]', hash='925997e9', model_name='novelai_final_pruned', config='/content/nai/stable-diffusion-webui/repositories/stable-diffusion/configs/stable-diffusion/v1-inference.yaml'), '/content/nai/stable-diffusion-webui/models/Stable-diffusion/novelai/animevae.pt')\r\n{(CheckpointInfo(filename='/content/nai/stable-diffusion-webui/models/Stable-diffusion/novelai/final_pruned.ckpt', title='novelai/final_pruned.ckpt [925997e9]', hash='925997e9', model_name='novelai_final_pruned', config='/content/nai/stable-diffusion-webui/repositories/stable-diffusion/configs/stable-diffusion/v1-inference.yaml'), '/content/nai/stable-diffusion-webui/models/Stable-diffusion/novelai/animevae.pt')}\r\nLoading weights [925997e9] with animevae VAE from cache\r\nApplying xformers cross attention optimization.\r\nWeights loaded.\r\n(CheckpointInfo(filename='/content/nai/stable-diffusion-webui/models/Stable-diffusion/novelai/final_pruned.ckpt', title='novelai/final_pruned.ckpt [925997e9]', hash='925997e9', model_name='novelai_final_pruned', config='/content/nai/stable-diffusion-webui/repositories/stable-diffusion/configs/stable-diffusion/v1-inference.yaml'), None)\r\n{(CheckpointInfo(filename='/content/nai/stable-diffusion-webui/models/Stable-diffusion/novelai/final_pruned.ckpt', title='novelai/final_pruned.ckpt [925997e9]', hash='925997e9', model_name='novelai_final_pruned', config='/content/nai/stable-diffusion-webui/repositories/stable-diffusion/configs/stable-diffusion/v1-inference.yaml'), '/content/nai/stable-diffusion-webui/models/Stable-diffusion/novelai/animevae.pt')}\r\nLoading weights [925997e9] from /content/nai/stable-diffusion-webui/models/Stable-diffusion/novelai/final_pruned.ckpt\r\n{(CheckpointInfo(filename='/content/nai/stable-diffusion-webui/models/Stable-diffusion/novelai/final_pruned.ckpt', title='novelai/final_pruned.ckpt [925997e9]', hash='925997e9', model_name='novelai_final_pruned', config='/content/nai/stable-diffusion-webui/repositories/stable-diffusion/configs/stable-diffusion/v1-inference.yaml'), None)}\r\nApplying xformers cross attention optimization.\r\nWeights loaded.\r\n(CheckpointInfo(filename='/content/nai/stable-diffusion-webui/models/Stable-diffusion/novelai/final_pruned.ckpt', title='novelai/final_pruned.ckpt [925997e9]', hash='925997e9', model_name='novelai_final_pruned', config='/content/nai/stable-diffusion-webui/repositories/stable-diffusion/configs/stable-diffusion/v1-inference.yaml'), '/content/nai/stable-diffusion-webui/models/Stable-diffusion/novelai/animevae.pt')\r\n{(CheckpointInfo(filename='/content/nai/stable-diffusion-webui/models/Stable-diffusion/novelai/final_pruned.ckpt', title='novelai/final_pruned.ckpt [925997e9]', hash='925997e9', model_name='novelai_final_pruned', config='/content/nai/stable-diffusion-webui/repositories/stable-diffusion/configs/stable-diffusion/v1-inference.yaml'), None)}\r\nLoading weights [925997e9] from /content/nai/stable-diffusion-webui/models/Stable-diffusion/novelai/final_pruned.ckpt\r\nLoading VAE weights from: /content/nai/stable-diffusion-webui/models/Stable-diffusion/novelai/animevae.pt\r\n{(CheckpointInfo(filename='/content/nai/stable-diffusion-webui/models/Stable-diffusion/novelai/final_pruned.ckpt', title='novelai/final_pruned.ckpt [925997e9]', hash='925997e9', model_name='novelai_final_pruned', config='/content/nai/stable-diffusion-webui/repositories/stable-diffusion/configs/stable-diffusion/v1-inference.yaml'), '/content/nai/stable-diffusion-webui/models/Stable-diffusion/novelai/animevae.pt')}\r\nApplying xformers cross attention optimization.\r\nWeights loaded.\r\n(CheckpointInfo(filename='/content/nai/stable-diffusion-webui/models/Stable-diffusion/novelai/final_pruned.ckpt', title='novelai/final_pruned.ckpt [925997e9]', hash='925997e9', model_name='novelai_final_pruned', config='/content/nai/stable-diffusion-webui/repositories/stable-diffusion/configs/stable-diffusion/v1-inference.yaml'), None)\r\n{(CheckpointInfo(filename='/content/nai/stable-diffusion-webui/models/Stable-diffusion/novelai/final_pruned.ckpt', title='novelai/final_pruned.ckpt [925997e9]', hash='925997e9', model_name='novelai_final_pruned', config='/content/nai/stable-diffusion-webui/repositories/stable-diffusion/configs/stable-diffusion/v1-inference.yaml'), '/content/nai/stable-diffusion-webui/models/Stable-diffusion/novelai/animevae.pt')}\r\nLoading weights [925997e9] from /content/nai/stable-diffusion-webui/models/Stable-diffusion/novelai/final_pruned.ckpt\r\n2022-10-31T08:19:07Z INF Initiating graceful shutdown due to signal interrupt ...\r\n```\r\n</details>\n", "hints_text": "", "created_at": "2022-10-31T08:43:11Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 3923, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-3923", "issue_numbers": ["3888", "3888"], "base_commit": "20a860b525cb7a319a42994f75a94bbca9a54d89", "patch": "diff --git a/modules/masking.py b/modules/masking.py\nindex fd8d92418ee..a5c4d2da521 100644\n--- a/modules/masking.py\n+++ b/modules/masking.py\n@@ -49,7 +49,7 @@ def expand_crop_region(crop_region, processing_width, processing_height, image_w\n     ratio_processing = processing_width / processing_height\r\n \r\n     if ratio_crop_region > ratio_processing:\r\n-        desired_height = (x2 - x1) * ratio_processing\r\n+        desired_height = (x2 - x1) / ratio_processing\r\n         desired_height_diff = int(desired_height - (y2-y1))\r\n         y1 -= desired_height_diff//2\r\n         y2 += desired_height_diff - desired_height_diff//2\r\ndiff --git a/modules/processing.py b/modules/processing.py\nindex 3a364b5fc28..2168208c498 100644\n--- a/modules/processing.py\n+++ b/modules/processing.py\n@@ -134,11 +134,7 @@ def txt2img_image_conditioning(self, x, width=None, height=None):\n             # Dummy zero conditioning if we're not using inpainting model.\r\n             # Still takes up a bit of memory, but no encoder call.\r\n             # Pretty sure we can just make this a 1x1 image since its not going to be used besides its batch size.\r\n-            return torch.zeros(\r\n-                x.shape[0], 5, 1, 1, \r\n-                dtype=x.dtype, \r\n-                device=x.device\r\n-            )\r\n+            return x.new_zeros(x.shape[0], 5, 1, 1)\r\n \r\n         height = height or self.height\r\n         width = width or self.width\r\n@@ -156,11 +152,7 @@ def txt2img_image_conditioning(self, x, width=None, height=None):\n     def img2img_image_conditioning(self, source_image, latent_image, image_mask = None):\r\n         if self.sampler.conditioning_key not in {'hybrid', 'concat'}:\r\n             # Dummy zero conditioning if we're not using inpainting model.\r\n-            return torch.zeros(\r\n-                latent_image.shape[0], 5, 1, 1,\r\n-                dtype=latent_image.dtype,\r\n-                device=latent_image.device\r\n-            )\r\n+            return latent_image.new_zeros(latent_image.shape[0], 5, 1, 1)\r\n \r\n         # Handle the different mask inputs\r\n         if image_mask is not None:\r\n@@ -174,11 +166,11 @@ def img2img_image_conditioning(self, source_image, latent_image, image_mask = No\n                 # Inpainting model uses a discretized mask as input, so we round to either 1.0 or 0.0\r\n                 conditioning_mask = torch.round(conditioning_mask)\r\n         else:\r\n-            conditioning_mask = torch.ones(1, 1, *source_image.shape[-2:])\r\n+            conditioning_mask = source_image.new_ones(1, 1, *source_image.shape[-2:])\r\n \r\n         # Create another latent image, this time with a masked version of the original input.\r\n         # Smoothly interpolate between the masked and unmasked latent conditioning image using a parameter.\r\n-        conditioning_mask = conditioning_mask.to(source_image.device)\r\n+        conditioning_mask = conditioning_mask.to(source_image.device).to(source_image.dtype)\r\n         conditioning_image = torch.lerp(\r\n             source_image,\r\n             source_image * (1.0 - conditioning_mask),\r\n@@ -674,6 +666,13 @@ def save_intermediate(image, index):\n \r\n         if opts.use_scale_latent_for_hires_fix:\r\n             samples = torch.nn.functional.interpolate(samples, size=(self.height // opt_f, self.width // opt_f), mode=\"bilinear\")\r\n+            \r\n+            # Avoid making the inpainting conditioning unless necessary as \r\n+            # this does need some extra compute to decode / encode the image again.\r\n+            if getattr(self, \"inpainting_mask_weight\", shared.opts.inpainting_mask_weight) < 1.0:\r\n+                image_conditioning = self.img2img_image_conditioning(decode_first_stage(self.sd_model, samples), samples)\r\n+            else:\r\n+                image_conditioning = self.txt2img_image_conditioning(samples)\r\n \r\n             for i in range(samples.shape[0]):\r\n                 save_intermediate(samples, i)\r\n@@ -700,14 +699,14 @@ def save_intermediate(image, index):\n \r\n             samples = self.sd_model.get_first_stage_encoding(self.sd_model.encode_first_stage(decoded_samples))\r\n \r\n+            image_conditioning = self.img2img_image_conditioning(decoded_samples, samples)\r\n+\r\n         shared.state.nextjob()\r\n \r\n         self.sampler = sd_samplers.create_sampler_with_index(sd_samplers.samplers, self.sampler_index, self.sd_model)\r\n \r\n         noise = create_random_tensors(samples.shape[1:], seeds=seeds, subseeds=subseeds, subseed_strength=subseed_strength, seed_resize_from_h=self.seed_resize_from_h, seed_resize_from_w=self.seed_resize_from_w, p=self)\r\n \r\n-        image_conditioning = self.txt2img_image_conditioning(x)\r\n-\r\n         # GC now before running the next img2img to prevent running out of memory\r\n         x = None\r\n         devices.torch_gc()\r\n", "test_patch": "", "problem_statement": "[Bug]: Error when generating with 'Highres. fix' and 'Upscale latent space image when doing hires. fix'\n### Is there an existing issue for this?\r\n\r\n- [X] I have searched the existing issues and checked the recent builds/commits\r\n\r\n### What happened?\r\n\r\nAttempting to use 'Highres. fix' with 'Upscale latent space image when doing hires. fix' results in the error message \"UnboundLocalError: local variable 'decoded_samples' referenced before assignment\" when it attempts to do the second batch of steps in the console.\r\n\r\nWhen trying to generate \"a cat\" using stable diffusion 1.5, console prints:\r\n```\r\nError completing request4:21,  7.00it/s]\r\nArguments: ('a cat', '', 'None', 'None', 28, 0, False, False, 1, 1, 7, -1.0, -1.0, 0, 0, 0, False, 1024, 1024, True, 0.7, 0, 0, 0, False, False, None, '', 1, '', 0, '', True, False, False) {}\r\nTraceback (most recent call last):\r\n  File \"...\\modules\\ui.py\", line 221, in f\r\n    res = list(func(*args, **kwargs))\r\n  File \"...\\webui.py\", line 63, in f\r\n    res = func(*args, **kwargs)\r\n  File \"...\\modules\\txt2img.py\", line 48, in txt2img\r\n    processed = process_images(p)\r\n  File \"...\\modules\\processing.py\", line 426, in process_images\r\n    res = process_images_inner(p)\r\n  File \"...\\modules\\processing.py\", line 519, in process_images_inner\r\n    samples_ddim = p.sample(conditioning=c, unconditional_conditioning=uc, seeds=seeds, subseeds=subseeds, subseed_strength=p.subseed_strength)\r\n  File \"...\\modules\\processing.py\", line 688, in sample\r\n    decoded_samples,\r\nUnboundLocalError: local variable 'decoded_samples' referenced before assignment\r\n```\r\n\r\n### Steps to reproduce the problem\r\n\r\n1. Enable 'Upscale latent space image when doing hires. fix' in Settings\r\n2. Enable 'Highres. fix'\r\n3. Generate\r\n\r\n### What should have happened?\r\n\r\nThe image should have generated\r\n\r\n### Commit where the problem happens\r\n\r\nd885a4a57b72152745ca76192ef1bdda29e6461d\r\n\r\n### What platforms do you use to access UI ?\r\n\r\nWindows\r\n\r\n### What browsers do you use to access the UI ?\r\n\r\nGoogle Chrome\n[Bug]: Error when generating with 'Highres. fix' and 'Upscale latent space image when doing hires. fix'\n### Is there an existing issue for this?\r\n\r\n- [X] I have searched the existing issues and checked the recent builds/commits\r\n\r\n### What happened?\r\n\r\nAttempting to use 'Highres. fix' with 'Upscale latent space image when doing hires. fix' results in the error message \"UnboundLocalError: local variable 'decoded_samples' referenced before assignment\" when it attempts to do the second batch of steps in the console.\r\n\r\nWhen trying to generate \"a cat\" using stable diffusion 1.5, console prints:\r\n```\r\nError completing request4:21,  7.00it/s]\r\nArguments: ('a cat', '', 'None', 'None', 28, 0, False, False, 1, 1, 7, -1.0, -1.0, 0, 0, 0, False, 1024, 1024, True, 0.7, 0, 0, 0, False, False, None, '', 1, '', 0, '', True, False, False) {}\r\nTraceback (most recent call last):\r\n  File \"...\\modules\\ui.py\", line 221, in f\r\n    res = list(func(*args, **kwargs))\r\n  File \"...\\webui.py\", line 63, in f\r\n    res = func(*args, **kwargs)\r\n  File \"...\\modules\\txt2img.py\", line 48, in txt2img\r\n    processed = process_images(p)\r\n  File \"...\\modules\\processing.py\", line 426, in process_images\r\n    res = process_images_inner(p)\r\n  File \"...\\modules\\processing.py\", line 519, in process_images_inner\r\n    samples_ddim = p.sample(conditioning=c, unconditional_conditioning=uc, seeds=seeds, subseeds=subseeds, subseed_strength=p.subseed_strength)\r\n  File \"...\\modules\\processing.py\", line 688, in sample\r\n    decoded_samples,\r\nUnboundLocalError: local variable 'decoded_samples' referenced before assignment\r\n```\r\n\r\n### Steps to reproduce the problem\r\n\r\n1. Enable 'Upscale latent space image when doing hires. fix' in Settings\r\n2. Enable 'Highres. fix'\r\n3. Generate\r\n\r\n### What should have happened?\r\n\r\nThe image should have generated\r\n\r\n### Commit where the problem happens\r\n\r\nd885a4a57b72152745ca76192ef1bdda29e6461d\r\n\r\n### What platforms do you use to access UI ?\r\n\r\nWindows\r\n\r\n### What browsers do you use to access the UI ?\r\n\r\nGoogle Chrome\n", "hints_text": "I just encountered this bug as well. The problem is specifically when using the highres fix with the \"Upscale latent space image when doing hires. fix\" option ticked on the settings page. It seems that when that is checked, in the conditional beginning at https://github.com/AUTOMATIC1111/stable-diffusion-webui/blob/78b879b4426d3730acdf3f4737d37a2ee29cb023/modules/processing.py#L654\r\nthe later-referenced variable `decoded_samples` is indeed never set in the true branch.\n+1 seeing this as well, looks like the bug was introduced in this commit https://github.com/AUTOMATIC1111/stable-diffusion-webui/commit/26a3fd2fe9314330336fb0e28d1e9ca7d2abe10e\r\n\r\nSpecifically caused by enabling Scale Latent\nYeah just tried it without scaling latent space and it worked, I'll update the issue to include that\n+1, I also just ran into this problem.\nSame issue here. Log:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\user\\Desktop\\automatic1111\\modules\\ui.py\", line 185, in f\r\n    res = list(func(*args, **kwargs))\r\n  File \"C:\\Users\\user\\Desktop\\automatic1111\\webui.py\", line 63, in f\r\n    res = func(*args, **kwargs)\r\n  File \"C:\\Users\\user\\Desktop\\automatic1111\\modules\\txt2img.py\", line 48, in txt2img\r\n    processed = process_images(p)\r\n  File \"C:\\Users\\user\\Desktop\\automatic1111\\modules\\processing.py\", line 426, in process_images\r\n    res = process_images_inner(p)\r\n  File \"C:\\Users\\user\\Desktop\\automatic1111\\modules\\processing.py\", line 519, in process_images_inner\r\n    samples_ddim = p.sample(conditioning=c, unconditional_conditioning=uc, seeds=seeds, subseeds=subseeds, subseed_strength=p.subseed_strength)\r\n  File \"C:\\Users\\user\\Desktop\\automatic1111\\modules\\processing.py\", line 688, in sample\r\n    decoded_samples,\r\nUnboundLocalError: local variable 'decoded_samples' referenced before assignment\r\n```\nAlso have this issue, same error above. \r\n\nI also have this issue.\n> Yeah just tried it without scaling latent space and it worked, I'll update the issue to include that\r\n\r\nScale latent has been essential for higher quality generations. Is there a way to fix this without removing it?\nTemporary fix is to just revert to commit a0a7024c679056dd66beb1832e52041b10143130 \nI just encountered this bug as well. The problem is specifically when using the highres fix with the \"Upscale latent space image when doing hires. fix\" option ticked on the settings page. It seems that when that is checked, in the conditional beginning at https://github.com/AUTOMATIC1111/stable-diffusion-webui/blob/78b879b4426d3730acdf3f4737d37a2ee29cb023/modules/processing.py#L654\r\nthe later-referenced variable `decoded_samples` is indeed never set in the true branch.\n+1 seeing this as well, looks like the bug was introduced in this commit https://github.com/AUTOMATIC1111/stable-diffusion-webui/commit/26a3fd2fe9314330336fb0e28d1e9ca7d2abe10e\r\n\r\nSpecifically caused by enabling Scale Latent\nYeah just tried it without scaling latent space and it worked, I'll update the issue to include that\n+1, I also just ran into this problem.\nSame issue here. Log:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\user\\Desktop\\automatic1111\\modules\\ui.py\", line 185, in f\r\n    res = list(func(*args, **kwargs))\r\n  File \"C:\\Users\\user\\Desktop\\automatic1111\\webui.py\", line 63, in f\r\n    res = func(*args, **kwargs)\r\n  File \"C:\\Users\\user\\Desktop\\automatic1111\\modules\\txt2img.py\", line 48, in txt2img\r\n    processed = process_images(p)\r\n  File \"C:\\Users\\user\\Desktop\\automatic1111\\modules\\processing.py\", line 426, in process_images\r\n    res = process_images_inner(p)\r\n  File \"C:\\Users\\user\\Desktop\\automatic1111\\modules\\processing.py\", line 519, in process_images_inner\r\n    samples_ddim = p.sample(conditioning=c, unconditional_conditioning=uc, seeds=seeds, subseeds=subseeds, subseed_strength=p.subseed_strength)\r\n  File \"C:\\Users\\user\\Desktop\\automatic1111\\modules\\processing.py\", line 688, in sample\r\n    decoded_samples,\r\nUnboundLocalError: local variable 'decoded_samples' referenced before assignment\r\n```\nAlso have this issue, same error above. \r\n\nI also have this issue.\n> Yeah just tried it without scaling latent space and it worked, I'll update the issue to include that\r\n\r\nScale latent has been essential for higher quality generations. Is there a way to fix this without removing it?\nTemporary fix is to just revert to commit a0a7024c679056dd66beb1832e52041b10143130 ", "created_at": "2022-10-29T17:09:53Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 3858, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-3858", "issue_numbers": ["3847"], "base_commit": "737eb28faca8be2bb996ee0930ec77d1f7ebd939", "patch": "diff --git a/modules/hypernetworks/hypernetwork.py b/modules/hypernetworks/hypernetwork.py\nindex 8113b35bd72..a02979978e4 100644\n--- a/modules/hypernetworks/hypernetwork.py\n+++ b/modules/hypernetworks/hypernetwork.py\n@@ -428,7 +428,9 @@ def train_hypernetwork(hypernetwork_name, learn_rate, batch_size, data_root, log\n \r\n             optimizer.step()\r\n \r\n-        if torch.isnan(losses[hypernetwork.step % losses.shape[0]]):\r\n+        steps_done = hypernetwork.step + 1\r\n+\r\n+        if torch.isnan(losses[hypernetwork.step % losses.shape[0]]): \r\n             raise RuntimeError(\"Loss diverged.\")\r\n         \r\n         if len(previous_mean_losses) > 1:\r\n@@ -438,9 +440,9 @@ def train_hypernetwork(hypernetwork_name, learn_rate, batch_size, data_root, log\n         dataset_loss_info = f\"dataset loss:{mean(previous_mean_losses):.3f}\" + u\"\\u00B1\" + f\"({std / (len(previous_mean_losses) ** 0.5):.3f})\"\r\n         pbar.set_description(dataset_loss_info)\r\n \r\n-        if hypernetwork.step > 0 and hypernetwork_dir is not None and hypernetwork.step % save_hypernetwork_every == 0:\r\n+        if hypernetwork_dir is not None and steps_done % save_hypernetwork_every == 0:\r\n             # Before saving, change name to match current checkpoint.\r\n-            hypernetwork.name = f'{hypernetwork_name}-{hypernetwork.step}'\r\n+            hypernetwork.name = f'{hypernetwork_name}-{steps_done}'\r\n             last_saved_file = os.path.join(hypernetwork_dir, f'{hypernetwork.name}.pt')\r\n             hypernetwork.save(last_saved_file)\r\n \r\n@@ -449,8 +451,8 @@ def train_hypernetwork(hypernetwork_name, learn_rate, batch_size, data_root, log\n             \"learn_rate\": scheduler.learn_rate\r\n         })\r\n \r\n-        if hypernetwork.step > 0 and images_dir is not None and hypernetwork.step % create_image_every == 0:\r\n-            forced_filename = f'{hypernetwork_name}-{hypernetwork.step}'\r\n+        if images_dir is not None and steps_done % create_image_every == 0:\r\n+            forced_filename = f'{hypernetwork_name}-{steps_done}'\r\n             last_saved_image = os.path.join(images_dir, forced_filename)\r\n \r\n             optimizer.zero_grad()\r\ndiff --git a/modules/textual_inversion/learn_schedule.py b/modules/textual_inversion/learn_schedule.py\nindex 2062726ad22..3a73606599b 100644\n--- a/modules/textual_inversion/learn_schedule.py\n+++ b/modules/textual_inversion/learn_schedule.py\n@@ -52,7 +52,7 @@ def __init__(self, learn_rate, max_steps, cur_step=0, verbose=True):\n         self.finished = False\r\n \r\n     def apply(self, optimizer, step_number):\r\n-        if step_number <= self.end_step:\r\n+        if step_number < self.end_step:\r\n             return\r\n \r\n         try:\r\ndiff --git a/modules/textual_inversion/textual_inversion.py b/modules/textual_inversion/textual_inversion.py\nindex ff002d3e99f..17dfb22399d 100644\n--- a/modules/textual_inversion/textual_inversion.py\n+++ b/modules/textual_inversion/textual_inversion.py\n@@ -184,9 +184,8 @@ def write_loss(log_directory, filename, step, epoch_len, values):\n     if shared.opts.training_write_csv_every == 0:\r\n         return\r\n \r\n-    if step % shared.opts.training_write_csv_every != 0:\r\n+    if (step + 1) % shared.opts.training_write_csv_every != 0:\r\n         return\r\n-\r\n     write_csv_header = False if os.path.exists(os.path.join(log_directory, filename)) else True\r\n \r\n     with open(os.path.join(log_directory, filename), \"a+\", newline='') as fout:\r\n@@ -196,11 +195,11 @@ def write_loss(log_directory, filename, step, epoch_len, values):\n             csv_writer.writeheader()\r\n \r\n         epoch = step // epoch_len\r\n-        epoch_step = step - epoch * epoch_len\r\n+        epoch_step = step % epoch_len \r\n \r\n         csv_writer.writerow({\r\n             \"step\": step + 1,\r\n-            \"epoch\": epoch + 1,\r\n+            \"epoch\": epoch,\r\n             \"epoch_step\": epoch_step + 1,\r\n             **values,\r\n         })\r\n@@ -282,15 +281,16 @@ def train_embedding(embedding_name, learn_rate, batch_size, data_root, log_direc\n             loss.backward()\r\n             optimizer.step()\r\n \r\n+        steps_done = embedding.step + 1\r\n \r\n         epoch_num = embedding.step // len(ds)\r\n-        epoch_step = embedding.step - (epoch_num * len(ds)) + 1\r\n+        epoch_step = embedding.step % len(ds)\r\n \r\n-        pbar.set_description(f\"[Epoch {epoch_num}: {epoch_step}/{len(ds)}]loss: {losses.mean():.7f}\")\r\n+        pbar.set_description(f\"[Epoch {epoch_num}: {epoch_step+1}/{len(ds)}]loss: {losses.mean():.7f}\")\r\n \r\n-        if embedding.step > 0 and embedding_dir is not None and embedding.step % save_embedding_every == 0:\r\n+        if embedding_dir is not None and steps_done % save_embedding_every == 0:\r\n             # Before saving, change name to match current checkpoint.\r\n-            embedding.name = f'{embedding_name}-{embedding.step}'\r\n+            embedding.name = f'{embedding_name}-{steps_done}'\r\n             last_saved_file = os.path.join(embedding_dir, f'{embedding.name}.pt')\r\n             embedding.save(last_saved_file)\r\n             embedding_yet_to_be_embedded = True\r\n@@ -300,8 +300,8 @@ def train_embedding(embedding_name, learn_rate, batch_size, data_root, log_direc\n             \"learn_rate\": scheduler.learn_rate\r\n         })\r\n \r\n-        if embedding.step > 0 and images_dir is not None and embedding.step % create_image_every == 0:\r\n-            forced_filename = f'{embedding_name}-{embedding.step}'\r\n+        if images_dir is not None and steps_done % create_image_every == 0:\r\n+            forced_filename = f'{embedding_name}-{steps_done}'\r\n             last_saved_image = os.path.join(images_dir, forced_filename)\r\n             p = processing.StableDiffusionProcessingTxt2Img(\r\n                 sd_model=shared.sd_model,\r\n@@ -334,7 +334,7 @@ def train_embedding(embedding_name, learn_rate, batch_size, data_root, log_direc\n \r\n             if save_image_with_stored_embedding and os.path.exists(last_saved_file) and embedding_yet_to_be_embedded:\r\n \r\n-                last_saved_image_chunks = os.path.join(images_embeds_dir, f'{embedding_name}-{embedding.step}.png')\r\n+                last_saved_image_chunks = os.path.join(images_embeds_dir, f'{embedding_name}-{steps_done}.png')\r\n \r\n                 info = PngImagePlugin.PngInfo()\r\n                 data = torch.load(last_saved_file)\r\n@@ -350,7 +350,7 @@ def train_embedding(embedding_name, learn_rate, batch_size, data_root, log_direc\n                 checkpoint = sd_models.select_checkpoint()\r\n                 footer_left = checkpoint.model_name\r\n                 footer_mid = '[{}]'.format(checkpoint.hash)\r\n-                footer_right = '{}v {}s'.format(vectorSize, embedding.step)\r\n+                footer_right = '{}v {}s'.format(vectorSize, steps_done)\r\n \r\n                 captioned_image = caption_image_overlay(image, title, footer_left, footer_mid, footer_right)\r\n                 captioned_image = insert_image_data_embed(captioned_image, data)\r\n", "test_patch": "", "problem_statement": "[Bug]: loss is logged after 1 step, and logging (all) is off by 1 step\n### Is there an existing issue for this?\r\n\r\n- [X] I have searched the existing issues and checked the recent builds/commits\r\n\r\n### What happened?\r\n\r\n1. Logging is off by 1 step, for the loss CSV, model, and previews\r\n\r\nI put a counter which increments after optimizer.step(). I set the training to log every 10 steps, but it does so after 11.\r\n![image](https://user-images.githubusercontent.com/1442761/198574035-21ce5f3b-c58f-4624-84da-acb3f69e521a.png)\r\n\r\nThis happened because step is defined by i (+ ititial_steps). i indicates the current step and is 0-indexed. It doesn't indicate the count of steps done. But the logging is done with step % save_hypernetwork_every == 0. So when it logs at step=10, it has actually done 11 steps (0-10).\r\n\r\n2. Loss logging is done after 1 step.\r\n\r\nLoss log in CSV always starts with steps being 1. This happened because it uses step % training_write_csv_every, while step is 0-indexed as previously mentioned. Then it logs with step+1. So every next log is off by 1 step. It didn't check if step > 0 either.\r\n\r\n### Steps to reproduce the problem\r\n\r\n1. Set a value (10 here) for csv log in settings\r\n2. Set a value (10 here) for model and preview logs\r\n3. Train a hypernetwork or textual embedding\r\n\r\n\r\n### What should have happened?\r\n\r\nLogging should be done every n steps as set by user.\r\n\r\n### Commit where the problem happens\r\n\r\n737eb28\r\n\r\n### What platforms do you use to access UI ?\r\n\r\nOther/Cloud\r\n\r\n### What browsers do you use to access the UI ?\r\n\r\nGoogle Chrome\r\n\r\n### Command Line Arguments\r\n\r\n```Shell\r\n--autolaunch --ckpt {workspace3}/novelai/final_pruned.ckpt --deepdanbooru --disable-safe-unpickle  --no-half-vae\r\n```\r\n\r\n\r\n### Additional information, context and logs\r\n\r\nI'll try \n", "hints_text": "", "created_at": "2022-10-28T13:56:57Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 3791, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-3791", "issue_numbers": ["705"], "base_commit": "737eb28faca8be2bb996ee0930ec77d1f7ebd939", "patch": "diff --git a/modules/images.py b/modules/images.py\nindex 7870b5b7aba..42363ed311e 100644\n--- a/modules/images.py\n+++ b/modules/images.py\n@@ -416,6 +416,14 @@ def get_next_sequence_number(path, basename):\n     return result + 1\r\n \r\n \r\n+def truncate_fullpath(full_path, encoding='utf-8'):\n+    dir_name, full_name = os.path.split(full_path)\n+    file_name, file_ext = os.path.splitext(full_name)\n+    max_length = os.statvfs(dir_name).f_namemax\n+    file_name_truncated = file_name.encode(encoding)[:max_length - len(file_ext)].decode(encoding, 'ignore')\n+    return os.path.join(dir_name , file_name_truncated + file_ext)\n+\n+\n def save_image(image, path, basename, seed=None, prompt=None, extension='png', info=None, short_filename=False, no_prompt=False, grid=False, pnginfo_section_name='parameters', p=None, existing_info=None, forced_filename=None, suffix=\"\", save_to_dirs=None):\r\n     \"\"\"Save an image.\r\n \r\n@@ -456,7 +464,7 @@ def save_image(image, path, basename, seed=None, prompt=None, extension='png', i\n \r\n     if save_to_dirs:\r\n         dirname = namegen.apply(opts.directories_filename_pattern or \"[prompt_words]\").lstrip(' ').rstrip('\\\\ /')\r\n-        path = os.path.join(path, dirname)\r\n+        path = truncate_fullpath(os.path.join(path, dirname))\r\n \r\n     os.makedirs(path, exist_ok=True)\r\n \r\n@@ -480,13 +488,13 @@ def save_image(image, path, basename, seed=None, prompt=None, extension='png', i\n             fullfn = None\r\n             for i in range(500):\r\n                 fn = f\"{basecount + i:05}\" if basename == '' else f\"{basename}-{basecount + i:04}\"\r\n-                fullfn = os.path.join(path, f\"{fn}{file_decoration}.{extension}\")\r\n+                fullfn = truncate_fullpath(os.path.join(path, f\"{fn}{file_decoration}.{extension}\"))\r\n                 if not os.path.exists(fullfn):\r\n                     break\r\n         else:\r\n-            fullfn = os.path.join(path, f\"{file_decoration}.{extension}\")\r\n+            fullfn = truncate_fullpath(os.path.join(path, f\"{file_decoration}.{extension}\"))\r\n     else:\r\n-        fullfn = os.path.join(path, f\"{forced_filename}.{extension}\")\r\n+        fullfn = truncate_fullpath(os.path.join(path, f\"{forced_filename}.{extension}\"))\r\n \r\n     pnginfo = existing_info or {}\r\n     if info is not None:\r\n", "test_patch": "", "problem_statement": "Long txt2img prompts cause a crash due to file name length\n**Describe the bug**\r\n\r\n```\r\nArguments: ('ornate intricate filigree framed, elf wearing ornate intricate detailed carved stained glass (((armor))), determined face, ((perfect face)), heavy makeup, led runes, inky swirling mist, gemstones, ((magic mist background)), ((eyeshadow)), (angry), detailed, intricate,(Alphonse Mucha), (Charlie Bowater), (Daniel Ridgway Knight), (Albert Lynch), (Richard S. Johnson)\\nNegative prompt: ugly, fat, obese, chubby, (((deformed))), [blurry], bad anatomy, disfigured, poorly drawn face, mutation, mutated, (extra_limb), (ugly), (poorly drawn hands), messy drawing, large_breasts, penis, nose, eyes, lips, eyelashes, text, red_eyes', '', 'None', 'None', 20, 0, False, False, 1, 1, 7, -1.0, -1.0, 0, 0, 0, 512, 384, 0, False, 1, '', 4, '', True, False, None, '') {}\r\nTraceback (most recent call last):\r\n  File \"/home/gaz/src/ai/stable-diffusion-webui/modules/ui.py\", line 134, in f\r\n    res = list(func(*args, **kwargs))\r\n  File \"/home/gaz/src/ai/stable-diffusion-webui/webui.py\", line 55, in f\r\n    res = func(*args, **kwargs)\r\n  File \"/home/gaz/src/ai/stable-diffusion-webui/modules/txt2img.py\", line 39, in txt2img\r\n    processed = process_images(p)\r\n  File \"/home/gaz/src/ai/stable-diffusion-webui/modules/processing.py\", line 383, in process_images\r\n    images.save_image(image, p.outpath_samples, \"\", seeds[i], prompts[i], opts.samples_format, info=infotext(n, i), p=p)\r\n  File \"/home/gaz/src/ai/stable-diffusion-webui/modules/images.py\", line 360, in save_image\r\n    image.save(fullfn, quality=opts.jpeg_quality, pnginfo=pnginfo)\r\n  File \"/home/gaz/src/ai/stable-diffusion-webui/venv/lib/python3.10/site-packages/PIL/Image.py\", line 2317, in save\r\n    fp = builtins.open(filename, \"w+b\")\r\nOSError: [Errno 36] File name too long: 'outputs/txt2img-images/00002-1278808392-ornate intricate filigree framed, elf wearing ornate intricate detailed carved stained glass (((armor))), determined face, ((per.png'\r\n```\r\n\r\n**To Reproduce**\r\n\r\nI accidentally pasted this whole thing in, including the text \"negative prompt\" and all the stuff after it:\r\n\r\n> ornate intricate filigree framed, elf wearing ornate intricate detailed carved stained glass (((armor))), determined face, ((perfect face)), heavy makeup, led runes, inky swirling mist, gemstones, ((magic mist background)), ((eyeshadow)), (angry), detailed, intricate,(Alphonse Mucha), (Charlie Bowater), (Daniel Ridgway Knight), (Albert Lynch), (Richard S. Johnson)\r\nNegative prompt: ugly, fat, obese, chubby, (((deformed))), [blurry], bad anatomy, disfigured, poorly drawn face, mutation, mutated, (extra_limb), (ugly), (poorly drawn hands), messy drawing, large_breasts, penis, nose, eyes, lips, eyelashes, text, red_eyes\r\n\r\nIt crashed with the stack trace above.\r\n\r\n**Expected behavior**\r\nhttps://discord.com/channels/1002292111942635562/1010578380300763146/1020288439880519762\r\n\r\n**Desktop (please complete the following information):**\r\n - OS: Linux\r\n - Browser [e.g. chrome, safari]\r\n - Commit revision: 8a32a71ca3223cf7b0911fe55db2c6dece2bacca\r\n\n", "hints_text": "I think this was addressed in ae32b8a53fd52cb4be195c4b4c329dd81a63ada7 - could you check please?\nYeah I guess that would fix it in most cases.\r\n\r\nAdding in something like this would be safer and stop this same bug from popping up in other areas:\r\n\r\n    def truncate_filename(full_path: str, hash_chars: int=5):\r\n        \"\"\"\r\n        Truncates a filename so it fits on the target filesystem, ensuring that\r\n        two files can't collide if they have different endings but the same start.\r\n\r\n        full_path: full path to the file, must have both a path and filename component.\r\n        hash_chars: number of characters to use for the hash part. each one gives \r\n            6 bits of collision resistance. defaults to 5, 30-bit; 0.05% chance for 1,000 files \r\n        \"\"\"\r\n        \r\n        containing_dir, full_name = os.path.split(full_path)\r\n        file_name, file_ext = os.path.splitext(full_name)\r\n        \r\n        # the max length depends on the containing dir's filesystem\r\n        max_length = os.statvfs(containing_dir).f_namemax\r\n        \r\n        if len(full_name) <= max_length:\r\n            return full_path\r\n        \r\n        name_hash = base64.urlsafe_b64encode(hashlib.md5(file_name.encode()).digest()).decode()\r\n        new_name = file_name[:max_length - len(file_ext) - hash_chars] + name_hash[:hash_chars]\r\n        \r\n        return new_name + file_ext\r\n\r\n\nIt might be worth doing the same thing elsewhere; have `truncate_string(full_string, max_length, hash_chars)` that returns things like `word list too longHzql1` instead of having a max number of words. I like the trick and use it a lot when making caches of downloaded URLs", "created_at": "2022-10-27T13:02:52Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 3628, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-3628", "issue_numbers": ["3524"], "base_commit": "3e15f8e0f5cc87507f77546d92435670644dbd18", "patch": "diff --git a/modules/img2img.py b/modules/img2img.py\nindex 8d9f7cf98da..9c0cf23e902 100644\n--- a/modules/img2img.py\n+++ b/modules/img2img.py\n@@ -39,6 +39,8 @@ def process_batch(p, input_dir, output_dir, args):\n             break\r\n \r\n         img = Image.open(image)\r\n+        # Use the EXIF orientation of photos taken by smartphones.\r\n+        img = ImageOps.exif_transpose(img) \r\n         p.init_images = [img] * p.batch_size\r\n \r\n         proc = modules.scripts.scripts_img2img.run(p, *args)\r\n@@ -61,19 +63,25 @@ def img2img(mode: int, prompt: str, negative_prompt: str, prompt_style: str, pro\n     is_batch = mode == 2\r\n \r\n     if is_inpaint:\r\n+        # Drawn mask\r\n         if mask_mode == 0:\r\n             image = init_img_with_mask['image']\r\n             mask = init_img_with_mask['mask']\r\n             alpha_mask = ImageOps.invert(image.split()[-1]).convert('L').point(lambda x: 255 if x > 0 else 0, mode='1')\r\n             mask = ImageChops.lighter(alpha_mask, mask.convert('L')).convert('L')\r\n             image = image.convert('RGB')\r\n+        # Uploaded mask\r\n         else:\r\n             image = init_img_inpaint\r\n             mask = init_mask_inpaint\r\n+    # No mask\r\n     else:\r\n         image = init_img\r\n         mask = None\r\n \r\n+    # Use the EXIF orientation of photos taken by smartphones.\r\n+    image = ImageOps.exif_transpose(image) \r\n+\r\n     assert 0. <= denoising_strength <= 1., 'can only work with strength in [0.0, 1.0]'\r\n \r\n     p = StableDiffusionProcessingImg2Img(\r\n", "test_patch": "", "problem_statement": "Portrait mode images generates in landscape mode in img2img [Bug]: \n### Is there an existing issue for this?\n\n- [X] I have searched the existing issues and checked the recent builds/commits\n\n### What happened?\n\nImage in portrait mode shows up fine in the preview, but when the alternative image is generated it is rotated to landscape mode.\n\n### Steps to reproduce the problem\n\n1. Load a image that was taken using a phone in portrait mode.\r\n2. Set a prompt and press generate.\r\n\n\n### What should have happened?\n\nIt should have generated the output image in portrait mode as well.\n\n### Commit where the problem happens\n\n6bd6154a92eb05c80d66df661a38f8b70cc13729\n\n### What platforms do you use to access UI ?\n\nWindows\n\n### What browsers do you use to access the UI ?\n\nMicrosoft Edge\n\n### Command Line Arguments\n\n```Shell\n--xformers\n```\n\n\n### Additional information, context and logs\n\nWhen images are taken in portrait mode, they are often stored as landscape, but have information that it is portrait so that they can be displayed correctly in image viewers, this should be used to determine how the output image should be generated.\n", "hints_text": "[This](https://stackoverflow.com/a/58116860/12572922) looks like a promising solution. Can you provide an image that exhibits the problem? \nAs seen in responses to #2374, people seem to be against features that are \"necessary for very few niche scenarios.\" Even before reading that, #3524 to me seemed borderline niche. But unlike with #2374, the solution to #3524 wouldn't add any clutter to the UI, so it seems innocuous enough to add.\n![example](https://user-images.githubusercontent.com/13852004/197708283-c46e2618-9a76-447c-8391-e533a8e372d2.png)\r\n\r\nAs you can see, the image has rotated when it was generated (I used only .2 for denoising strength so that it's clearly seen). This also makes the AI unable to properly identify features, as, for example, a person standing up won't be recognized as the person is now \"on his/her side\" and it will try to generate some kind of weird creature instead as it try to make sense of it all.\nAlso, I don't know if this should be seen as a niche scenario, as AI generation on phones are very popular and it's pretty common to take portrait photos on the phone as opposed to turning the phone sideways. As AI image to image becomes more popular on phones (through services, as obviously it doesn't have a graphics card) this will probably be a important feature.\n> [EXAMPLE]\r\n> \r\n> As you can see, the image has rotated when it was generated (I used only .2 for denoising strength so that it's clearly seen). This also makes the AI unable to properly identify features, as, for example, a person standing up won't be recognized as the person is now \"on his/her side\" and it will try to generate some kind of weird creature instead as it try to make sense of it all.\r\n\r\nIs there any way you can send me the problematic photo itself, so I can test with it? Any way I can think of sending an iPhone photo to my desktop might strip it of the problematic EXIF data.\nThe photo is from the desktop, it was taken using an Android phone:\r\n\r\n![IMG_0068](https://user-images.githubusercontent.com/13852004/197710216-c82d2760-03f7-480a-8768-9529b3294280.JPG)\r\n\r\n\nI think, in general, wysiwyg should be implemented anywhere possible, because otherwise it becomes pretty unintuitive.", "created_at": "2022-10-25T08:46:01Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 3494, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-3494", "issue_numbers": ["2404"], "base_commit": "6bd6154a92eb05c80d66df661a38f8b70cc13729", "patch": "diff --git a/launch.py b/launch.py\nindex 333f308a619..8affd4109ce 100644\n--- a/launch.py\n+++ b/launch.py\n@@ -111,7 +111,7 @@ def prepare_enviroment():\n \r\n     gfpgan_package = os.environ.get('GFPGAN_PACKAGE', \"git+https://github.com/TencentARC/GFPGAN.git@8d2447a2d918f8eba5a4a01463fd48e45126a379\")\r\n     clip_package = os.environ.get('CLIP_PACKAGE', \"git+https://github.com/openai/CLIP.git@d50d76daa670286dd6cacf3bcd80b5e4823fc8e1\")\r\n-    deepdanbooru_package = os.environ.get('DEEPDANBOORU_PACKAGE', \"git+https://github.com/KichangKim/DeepDanbooru.git@edf73df4cdaeea2cf00e9ac08bd8a9026b7a7b26\")\r\n+    deepdanbooru_package = os.environ.get('DEEPDANBOORU_PACKAGE', \"git+https://github.com/KichangKim/DeepDanbooru.git@d91a2963bf87c6a770d74894667e9ffa9f6de7ff\")\r\n \r\n     xformers_windows_package = os.environ.get('XFORMERS_WINDOWS_PACKAGE', 'https://github.com/C43H66N12O12S2/stable-diffusion-webui/releases/download/f/xformers-0.0.14.dev0-cp310-cp310-win_amd64.whl')\r\n \r\n", "test_patch": "", "problem_statement": "Deepdanbooru breaks cuda on image generation after running.\n**Describe the bug**\r\nAfter running deepdanbooru on an image, new images can't be generated anymore due to a cuda error.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Enable deepdanbooru\r\n2. Generate or upload an image to img2img\r\n3. Click \"Interrogate DeepBooru\"\r\n4. After it finishes it, try to generate any image on either generator\r\n5. Generation will halt, a cuda error will be shown\r\n\r\nError: \r\n```\r\nError completing request\r\nArguments: (0, 'Test', 'Test', 'None', 'None', <PIL.Image.Image image mode=RGB size=512x512 at 0x7F483FF50400>, None, None, None, 0, 60, 0, 4, 1, False, False, 1, 1, 7, 0.75, -1.0, -1.0, 0, 0, 0, False, 512, 512, 0, False, 32, 0, '', '', 0, 4.0, 1, 1, 0, 0, 0.0, 4.0, 0.1, 0.1, 1, True, False, False, 0, False, '', 1, False, 0, 1, False, False, False, '', '', '', 1, 50, 0, False, 4, 1, 4, 0.09, True, 1, 0, 7, False, False, '<p style=\"margin-bottom:0.75em\">Recommended settings: Sampling Steps: 80-100, Sampler: Euler a, Denoising strength: 0.8</p>', 128, 8, ['left', 'right', 'up', 'down'], 1, 0.05, 128, 4, 0, ['left', 'right', 'up', 'down'], False, False, None, '', '<p style=\"margin-bottom:0.75em\">Will upscale the image to twice the dimensions; use width and height sliders to set tile size</p>', 64, 0, 1, '', 0, '', True, False) {}\r\nTraceback (most recent call last):\r\n  File \"/home/me/src/stable-diffusion-webui/modules/ui.py\", line 184, in f\r\n    res = list(func(*args, **kwargs))\r\n  File \"/home/me/src/stable-diffusion-webui/webui.py\", line 64, in f\r\n    res = func(*args, **kwargs)\r\n  File \"/home/me/src/stable-diffusion-webui/modules/img2img.py\", line 126, in img2img\r\n    processed = process_images(p)\r\n  File \"/home/me/src/stable-diffusion-webui/modules/processing.py\", line 371, in process_images\r\n    p.init(all_prompts, all_seeds, all_subseeds)\r\n  File \"/home/me/src/stable-diffusion-webui/modules/processing.py\", line 607, in init\r\n    self.sampler = sd_samplers.create_sampler_with_index(sd_samplers.samplers_for_img2img, self.sampler_index, self.sd_model)\r\n  File \"/home/me/src/stable-diffusion-webui/modules/sd_samplers.py\", line 50, in create_sampler_with_index\r\n    sampler = config.constructor(model)\r\n  File \"/home/me/src/stable-diffusion-webui/modules/sd_samplers.py\", line 33, in <lambda>\r\n    SamplerData(label, lambda model, funcname=funcname: KDiffusionSampler(funcname, model), aliases, options)\r\n  File \"/home/me/src/stable-diffusion-webui/modules/sd_samplers.py\", line 306, in __init__\r\n    self.model_wrap = k_diffusion.external.CompVisDenoiser(sd_model, quantize=shared.opts.enable_quantization)\r\n  File \"/home/me/src/stable-diffusion-webui/repositories/k-diffusion/k_diffusion/external.py\", line 135, in __init__\r\n    super().__init__(model, model.alphas_cumprod, quantize=quantize)\r\n  File \"/home/me/src/stable-diffusion-webui/repositories/k-diffusion/k_diffusion/external.py\", line 92, in __init__\r\n    super().__init__(((1 - alphas_cumprod) / alphas_cumprod) ** 0.5, quantize)\r\n  File \"/home/me/src/stable-diffusion-webui/venv/lib64/python3.10/site-packages/torch/_tensor.py\", line 32, in wrapped\r\n    return f(*args, **kwargs)\r\n  File \"/home/me/src/stable-diffusion-webui/venv/lib64/python3.10/site-packages/torch/_tensor.py\", line 639, in __rsub__\r\n    return _C._VariableFunctions.rsub(self, other)\r\nRuntimeError: CUDA error: unspecified launch failure\r\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\r\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\r\n```\r\n\r\n**Desktop (please complete the following information):**\r\n - OS: Fedora 36, drivers and cuda from the Nvidia repo (also happens on Fedora 35, drivers from rpmfusion)\r\n - Browser Chrome\r\n - Commit revision c3c8eef9fd5a0c8b26319e32ca4a19b56204e6df\r\n\r\n**Additional context**\r\nLaunch flags: `--listen --opt-split-attention --allow-code --deepdanbooru`\r\nEverything else seems to be working fine, no issues. Just the deepdanbooru generation seems to break it.\r\n\n", "hints_text": "Same problem here. Checked Use deepbooru for caption in the pre-process image of Train.\r\nThe first one runs normally, and the second one causes an error.\r\nIf I delete the deepbooru folder from the models folder, it works again.\r\n\r\n`022-10-13 17:39:09.397944: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2022-10-13 17:39:09.704583: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21652 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090 Ti, pci bus id: 0000:0e:00.0, compute capability: 8.6\r\nWARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\r\n2022-10-13 17:39:15.516201: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8500\r\nCould not locate zlibwapi.dll. Please make sure it is in your library path!`\r\n\r\nOS : win 10, 3090ti\r\nEdge browser\r\nCommit revision 04c0e643f2eec68d93a76db171b4d70595808702\r\nLauch flags : --opt-split-attention --autolaunch --allow-code --deepdanbooru \r\n\r\n--------------------------------------------------------------------------------------\r\n\r\nI solved it by downloading zlibwapi.dll and putting it in C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.7\\bin.\r\n\r\nhttps://docs.nvidia.com/deeplearning/cudnn/install-guide/index.html#install-zlib-windows\nI face the problem like this in my colab\r\nI just change the CODA version in the colab to fix this. \n```\r\nError completing request\r\nArguments: ('', '', 'None', 'None', 1, 0, False, False, 1, 1, 7, -1.0, -1.0, 0, 0, 0, False, 512, 512, False, False, 0.7, 0, False, '', 25, True, 5.0, False, None, '', 1, '', 0, '', True, False, False) {}\r\nTraceback (most recent call last):\r\n  File \"/notebooks/SDW/modules/ui.py\", line 181, in f\r\n    res = list(func(*args, **kwargs))\r\n  File \"/notebooks/SDW/webui.py\", line 64, in f\r\n    res = func(*args, **kwargs)\r\n  File \"/notebooks/SDW/modules/txt2img.py\", line 43, in txt2img\r\n    processed = process_images(p)\r\n  File \"/notebooks/SDW/modules/processing.py\", line 397, in process_images\r\n    uc = prompt_parser.get_learned_conditioning(shared.sd_model, len(prompts) * [p.negative_prompt], p.steps)\r\n  File \"/notebooks/SDW/modules/prompt_parser.py\", line 138, in get_learned_conditioning\r\n    conds = model.get_learned_conditioning(texts)\r\n  File \"/notebooks/SDW/repositories/stable-diffusion/ldm/models/diffusion/ddpm.py\", line 558, in get_learned_conditioning\r\n    c = self.cond_stage_model(c)\r\n  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\r\n    return forward_call(*input, **kwargs)\r\n  File \"/notebooks/SDW/modules/sd_hijack.py\", line 334, in forward\r\n    z1 = self.process_tokens(tokens, multipliers)\r\n  File \"/notebooks/SDW/modules/sd_hijack.py\", line 349, in process_tokens\r\n    tokens = torch.asarray(remade_batch_tokens).to(device)\r\nRuntimeError: CUDA error: unspecified launch failure\r\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\r\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1. \r\n```\r\n\r\nNot a huge issue as I just restart after pre-processing images with deepdanbooru, probably some weird mismatch of Cuda in my Paperspace notebook.\nFWIW, I am on cuda version 11.8, Nvidia driver version 520.61.05, RTX 3090. Judging by the comments so far it sounds like that's probably too new?\nI am also having a similar issue, I'm using CUDA 11.7, Nvidia driver 515.76 and using a 3090TI\nSame issue, I'm using CUDA 11.3, torch 1.12.1, single 2080Ti, ubuntu system\r\n\r\n> ```\r\n> Error completing request\r\n> Arguments: ('', '', 'None', 'None', 1, 0, False, False, 1, 1, 7, -1.0, -1.0, 0, 0, 0, False, 512, 512, False, False, 0.7, 0, False, '', 25, True, 5.0, False, None, '', 1, '', 0, '', True, False, False) {}\r\n> Traceback (most recent call last):\r\n>   File \"/notebooks/SDW/modules/ui.py\", line 181, in f\r\n>     res = list(func(*args, **kwargs))\r\n>   File \"/notebooks/SDW/webui.py\", line 64, in f\r\n>     res = func(*args, **kwargs)\r\n>   File \"/notebooks/SDW/modules/txt2img.py\", line 43, in txt2img\r\n>     processed = process_images(p)\r\n>   File \"/notebooks/SDW/modules/processing.py\", line 397, in process_images\r\n>     uc = prompt_parser.get_learned_conditioning(shared.sd_model, len(prompts) * [p.negative_prompt], p.steps)\r\n>   File \"/notebooks/SDW/modules/prompt_parser.py\", line 138, in get_learned_conditioning\r\n>     conds = model.get_learned_conditioning(texts)\r\n>   File \"/notebooks/SDW/repositories/stable-diffusion/ldm/models/diffusion/ddpm.py\", line 558, in get_learned_conditioning\r\n>     c = self.cond_stage_model(c)\r\n>   File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\r\n>     return forward_call(*input, **kwargs)\r\n>   File \"/notebooks/SDW/modules/sd_hijack.py\", line 334, in forward\r\n>     z1 = self.process_tokens(tokens, multipliers)\r\n>   File \"/notebooks/SDW/modules/sd_hijack.py\", line 349, in process_tokens\r\n>     tokens = torch.asarray(remade_batch_tokens).to(device)\r\n> RuntimeError: CUDA error: unspecified launch failure\r\n> CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\r\n> For debugging consider passing CUDA_LAUNCH_BLOCKING=1. \r\n> ```\r\n> \r\n> Not a huge issue as I just restart after pre-processing images with deepdanbooru, probably some weird mismatch of Cuda in my Paperspace notebook.\r\n\r\n\nsame here\nUpdate: I've tried several CUDA versions and (Linux) environments and can't seem to get it working at all.\r\nI did notice there are some errors when Deepdanbooru gets executed, however. The Deepdanbooru code still gets executed, but I'm guessing it probably messes with the registered CUDA settings because it can't re-init cuBLAS, causing the next img2img/txt2img to fail.\r\n\"Error completing request\" and everything below is where image generation is triggered, above that is just deepdanbooru (the input image was just a blob of abstract purple, so it really only detected 2 tags, this part is not a bug).\r\n```\r\n2022-10-15 22:58:57.799100: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2022-10-15 22:58:57.931302: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n2022-10-15 22:58:58.869585: E tensorflow/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: CUDA_ERROR_NOT_INITIALIZED: initialization error\r\n2022-10-15 22:58:58.869614: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: 7bc68f14784c\r\n2022-10-15 22:58:58.869619: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: 7bc68f14784c\r\n2022-10-15 22:58:58.869724: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: NOT_FOUND: was unable to find libcuda.so DSO loaded into this program\r\n2022-10-15 22:58:58.869746: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5\r\n2022-10-15 22:58:58.869909: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n1/1 [==============================] - 1s 1s/step\r\n0.7328718900680542 moon\r\n0.6844184994697571 multiple_girls\r\nError completing request\r\nArguments: ('', '', 'Simple', 'None', 20, 0, False, False, 1, 1, 7, -1.0, -1.0, 0, 0, 0, False, 512, 512, False, 0.7, 0, 0, 0, False, '', False, False, None, '', 1, '', 0, '', True, True, False) {}\r\nTraceback (most recent call last):\r\n  File \"/home/root/src/stable-diffusion-webui/modules/ui.py\", line 212, in f\r\n    res = list(func(*args, **kwargs))\r\n  File \"/home/root/src/stable-diffusion-webui/webui.py\", line 64, in f\r\n    res = func(*args, **kwargs)\r\n  File \"/home/root/src/stable-diffusion-webui/modules/txt2img.py\", line 44, in txt2img\r\n    processed = process_images(p)\r\n  File \"/home/root/src/stable-diffusion-webui/modules/processing.py\", line 397, in process_images\r\n    uc = prompt_parser.get_learned_conditioning(shared.sd_model, len(prompts) * [p.negative_prompt], p.steps)\r\n  File \"/home/root/src/stable-diffusion-webui/modules/prompt_parser.py\", line 138, in get_learned_conditioning\r\n    conds = model.get_learned_conditioning(texts)\r\n  File \"/home/root/src/stable-diffusion-webui/repositories/stable-diffusion/ldm/models/diffusion/ddpm.py\", line 558, in get_learned_conditioning\r\n    c = self.cond_stage_model(c)\r\n  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\r\n    return forward_call(*input, **kwargs)\r\n  File \"/home/root/src/stable-diffusion-webui/modules/sd_hijack.py\", line 334, in forward\r\n    z1 = self.process_tokens(tokens, multipliers)\r\n  File \"/home/root/src/stable-diffusion-webui/modules/sd_hijack.py\", line 349, in process_tokens\r\n    tokens = torch.asarray(remade_batch_tokens).to(device)\r\nRuntimeError: CUDA error: unspecified launch failure\r\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\r\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\r\n```\nsame problem\r\n\r\nUbuntu 20.04.4 LTS (GNU/Linux 5.4.0-126-generic x86_64),  RTX 3090\r\n\r\nnvcc --version\r\n```\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2021 NVIDIA Corporation\r\nBuilt on Mon_May__3_19:15:13_PDT_2021\r\nCuda compilation tools, release 11.3, V11.3.109\r\nBuild cuda_11.3.r11.3/compiler.29920130_0\r\n```\r\n\r\nerror log:\r\n```\r\n2022-10-18 22:39:56.822689: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2022-10-18 22:39:57.007121: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\r\n2022-10-18 22:39:57.046040: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n2022-10-18 22:39:57.823299: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /root/miniconda3/envs/automatic/lib/python3.10/site-packages/cv2/../../lib64:\r\n2022-10-18 22:39:57.823384: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /root/miniconda3/envs/automatic/lib/python3.10/site-packages/cv2/../../lib64:\r\n2022-10-18 22:39:57.823394: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\r\n2022-10-18 22:39:58.957262: E tensorflow/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: CUDA_ERROR_NOT_INITIALIZED: initialization error\r\n2022-10-18 22:39:58.957298: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:163] no NVIDIA GPU device is present: /dev/nvidia0 does not exist\r\n2022-10-18 22:39:58.957521: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n1/1 [==============================] - 3s 3s/step\r\n0.9999967217445374 flower\r\n0.9976820945739746 maid_headdress\r\n0.9965994358062744 maid\r\n0.9940873384475708 tray\r\n0.992828369140625 hydrangea\r\n0.9785534739494324 apron\r\n0.9732893109321594 maid_apron\r\n0.9611615538597107 long_hair\r\n0.9472941160202026 pink_flower\r\n0.9466087818145752 waist_apron\r\n0.9219543933868408 multiple_girls\r\n0.9178032875061035 vase\r\n0.9112962484359741 purple_flower\r\n0.8946253061294556 white_flower\r\n0.8906084299087524 holding_tray\r\n0.8897321820259094 thighhighs\r\n0.8700262308120728 navel\r\n0.8618443012237549 lily_(flower)\r\n0.8400490880012512 waitress\r\n0.8256860375404358 enmaided\r\n0.8048192262649536 rose\r\n0.7983884811401367 wrist_cuffs\r\n0.7971858978271484 breasts\r\n0.7840455770492554 hair_flower\r\n0.7349395751953125 white_apron\r\n0.7275054454803467 looking_at_viewer\r\n0.7117940783500671 white_legwear\r\n0.7071746587753296 silver_hair\r\n0.6788415908813477 blue_eyes\r\n0.6713621020317078 teapot\r\n0.6598066091537476 table\r\n0.6584057211875916 blue_flower\r\n0.6500906944274902 garter_straps\r\n0.6371893882751465 day\r\n0.6288817524909973 daisy\r\n0.6258440017700195 cup\r\n0.6212639212608337 hair_ornament\r\n0.5960835218429565 bow\r\n0.5882710218429565 white_hair\r\n0.5618525743484497 2girls\r\n0.5511027574539185 crop_top\r\n0.5352045893669128 frills\r\n0.5238999128341675 ribbon\r\n0.5232399106025696 midriff\r\n0.5033739805221558 outdoors\r\nError completing request\r\nArguments: (0, '2girls, apron, blue_eyes, blue_flower, bow, breasts, crop_top, cup, daisy, day, enmaided, flower, frills, garter_straps, hair_flower, hair_ornament, holding_tray, hydrangea, lily_\\\\(flower\\\\), long_hair, looking_at_viewer, maid, maid_apron, maid_headdress, midriff, multiple_girls, navel, outdoors, pink_flower, purple_flower, ribbon, rose, silver_hair, table, teapot, thighhighs, tray, vase, waist_apron, waitress, white_apron, white_flower, white_hair, white_legwear, wrist_cuffs', '', 'None', 'None', <PIL.Image.Image image mode=RGB size=1280x2002 at 0x7F6F9BC90D00>, None, None, None, 0, 20, 0, 4, 1, False, False, 1, 1, 7, 0.75, -1.0, -1.0, 0, 0, 0, False, 512, 512, 0, False, 32, 0, '', '', 0, '<ul>\\n<li><code>CFG Scale</code> should be 2 or lower.</li>\\n</ul>\\n', True, True, '', '', True, 50, True, 1, 0, False, 4, 1, '<p style=\"margin-bottom:0.75em\">Recommended settings: Sampling Steps: 80-100, Sampler: Euler a, Denoising strength: 0.8</p>', 128, 8, ['left', 'right', 'up', 'down'], 1, 0.05, 128, 4, 0, ['left', 'right', 'up', 'down'], False, False, None, '', '<p style=\"margin-bottom:0.75em\">Will upscale the image to twice the dimensions; use width and height sliders to set tile size</p>', 64, 0, 1, '', 0, '', True, False, False) {}\r\nTraceback (most recent call last):\r\n  File \"/root/autodl-tmp/stable-diffusion-webui/modules/ui.py\", line 212, in f\r\n    res = list(func(*args, **kwargs))\r\n  File \"/root/autodl-tmp/stable-diffusion-webui/webui.py\", line 64, in f\r\n    res = func(*args, **kwargs)\r\n  File \"/root/autodl-tmp/stable-diffusion-webui/modules/img2img.py\", line 126, in img2img\r\n    processed = process_images(p)\r\n  File \"/root/autodl-tmp/stable-diffusion-webui/modules/processing.py\", line 370, in process_images\r\n    p.init(all_prompts, all_seeds, all_subseeds)\r\n  File \"/root/autodl-tmp/stable-diffusion-webui/modules/processing.py\", line 694, in init\r\n    image = image.to(shared.device)\r\nRuntimeError: CUDA error: unspecified launch failure\r\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\r\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\r\n```\nI have identified [the commit](https://github.com/AUTOMATIC1111/stable-diffusion-webui/commit/2e2d45b281cf3bafbf5034417dab34582a56787b) which began the errors, at least for me\r\n\r\nIf you want to fix the error temporarily, checkout fec2221eeaafb50afd26ba3e109bf6f928011e69, the last commit before the errors began\n> I have identified [the commit](https://github.com/AUTOMATIC1111/stable-diffusion-webui/commit/2e2d45b281cf3bafbf5034417dab34582a56787b) which began the errors, at least for me\r\n> \r\n> If you want to fix the error temporarily, checkout [fec2221](https://github.com/AUTOMATIC1111/stable-diffusion-webui/commit/fec2221eeaafb50afd26ba3e109bf6f928011e69), the last commit before the errors began\r\n\r\n\r\n\r\nYes, It fix `RuntimeError: CUDA error: unspecified launch failure`\uff0cBut I'd rather know how to fix libnvinfer.so.7 missing error\n> I face the problem like this in my colab I just change the CODA version in the colab to fix this.\r\n\r\nMay I ask what cuda version did you use in colab?\nI slove by install tensorflow-cpu. \r\nRemember install in the venv.\r\n\n> I slove by install tensorflow-cpu. Remember install in the venv.\r\n\r\nYou are my life saver!\n> I slove by install tensorflow-cpu. Remember install in the venv.\r\n\r\nWorth noting, while this does work, it seems to work by disabling GPU support in Tensorflow entirely, thus working around the issue of the unclean CUDA state by disabling CUDA for deepbooru (and anything else using Tensorflow) entirely.\r\n\r\nThe bug where Deepbooru fails on CUDA and leaves the GPU in an unclean state still exists, but is just avoided by not using the GPU in the first place. But any other Tensorflow-based scripts will also be deferred to CPU-only.\r\ntxt2img/img2img itself does not seem to use Tensorflow so it does not seem to affect this part.\r\n\r\nAlso, because tensorflow-cpu is essentially a CPU-only replacement package for tensorflow proper, it seems the installation order matters, if you install tensorflow-cpu before installing tensorflow proper, or update tensorflow proper after tensorflow-cpu, this workaround will not work as it will continue using tensorflow proper.\r\n\r\nIn short: this workaround works, but does not solve the problem, and may cause other problems elsewhere.\n> > I slove by install tensorflow-cpu. Remember install in the venv.\r\n> \r\n> Worth noting, while this does work, it seems to work by disabling GPU support in Tensorflow entirely, thus working around the issue of the unclean CUDA state by disabling CUDA for deepbooru (and anything else using Tensorflow) entirely.\r\n> \r\n> The bug where Deepbooru fails on CUDA and leaves the GPU in an unclean state still exists, but is just avoided by not using the GPU in the first place. But any other Tensorflow-based scripts will also be deferred to CPU-only. txt2img/img2img itself does not seem to use Tensorflow so it does not seem to affect this part.\r\n> \r\n> Also, because tensorflow-cpu is essentially a CPU-only replacement package for tensorflow proper, it seems the installation order matters, if you install tensorflow-cpu before installing tensorflow proper, or update tensorflow proper after tensorflow-cpu, this workaround will not work as it will continue using tensorflow proper.\r\n> \r\n> In short: this workaround works, but does not solve the problem, and may cause other problems elsewhere.\r\n\r\nYou are right. \r\nI tried another method that download \"zlibwapi.dll\" manually and put it in \"NVIDIA GPU Computing Toolkit\\CUDA\\v11.7\\bin\". Just as [toyxyz](https://github.com/toyxyz) mentioned and it works.\r\nIf someone tried his method and still not resolved, installing tensorflow-cpu is a simple way and can recovery anytime until the bug is fixed.  \u0669(\u00b4\u2200\uff40*)\n> You are right. I tried another method that download \"zlibwapi.dll\" manually and put it in \"NVIDIA GPU Computing Toolkit\\CUDA\\v11.7\\bin\". Just as [toyxyz](https://github.com/toyxyz) mentioned and it works. If someone tried his method and still not resolved, installing tensorflow-cpu is a simple way and can recovery anytime until the bug is fixed. \u0669(\u00b4\u2200\uff40*)\r\n\r\nI'm unsure why this would solve the issue on Windows, especially when on Linux I already have all zlib dependencies listed on the cuDNN page. It might be a different bug that the Linux users like myself experience.\n> I face the problem like this in my colab I just change the CODA version in the colab to fix this.\r\n\r\nWhich version did you change it to?\nThis was already addressed in the original PR but that change was reverted.\r\nhttps://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/1752/commits/5f12e7efd92ad802742f96788b4be3249ad02829\r\n\r\nOn *nix, multiprocessing prefers fork over spawn.\nWhen running the Deepdanbooru model, TensorFlow tries to initiate the same primary GPU that the WebUI is using, which causes the crash. I was able to resolve this bug by telling the WebUI to use a secondary GPU. This is done by passing the \"--device-id=1\" argument when launching the WebUI.\r\n\r\nIf you do not have a secondary GPU, then try making Deepdanbooru uses the CPU instead of the primary GPU. One method mentioned above is to install \"tenserflow-cpu\"; however, there are unknown implications (as also mentioned above).\n> When running the Deepdanbooru model, TensorFlow tries to initiate the same primary GPU that the WebUI is using, which causes the crash. I was able to resolve this bug by telling the WebUI to use a secondary GPU. This is done by passing the \"--device-id=1\" argument when launching the WebUI.\r\n> \r\n> If you do not have a secondary GPU, then try making Deepdanbooru uses the CPU instead of the primary GPU. One method mentioned above is to install \"tenserflow-cpu\"; however, there are unknown implications (as also mentioned above).\r\n\r\nThis should be resolved with #3421 \n> > When running the Deepdanbooru model, TensorFlow tries to initiate the same primary GPU that the WebUI is using, which causes the crash. I was able to resolve this bug by telling the WebUI to use a secondary GPU. This is done by passing the \"--device-id=1\" argument when launching the WebUI.\r\n> > If you do not have a secondary GPU, then try making Deepdanbooru uses the CPU instead of the primary GPU. One method mentioned above is to install \"tenserflow-cpu\"; however, there are unknown implications (as also mentioned above).\r\n> \r\n> This should be resolved with #3421\r\n\r\nI have been getting this error since this commit.\r\nWebui is running on Colab T4.\r\n[notebook](https://github.com/ddPn08/automatic1111-colab)\r\n\r\n```\r\n2022-10-23 06:29:38.974494: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2022-10-23 06:29:40.327359: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n2022-10-23 06:29:42.545022: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/envs/automatic/lib/python3.10/site-packages/cv2/../../lib64:/usr/lib64-nvidia\r\n2022-10-23 06:29:42.545270: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/envs/automatic/lib/python3.10/site-packages/cv2/../../lib64:/usr/lib64-nvidia\r\n2022-10-23 06:29:42.545293: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\r\nProcess SpawnProcess-2:\r\nTraceback (most recent call last):\r\n  File \"/usr/local/envs/automatic/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\r\n    self.run()\r\n  File \"/usr/local/envs/automatic/lib/python3.10/multiprocessing/process.py\", line 108, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"/content/stable-diffusion-webui/modules/deepbooru.py\", line 35, in deepbooru_process\r\n    model, tags = get_deepbooru_tags_model()\r\n  File \"/content/stable-diffusion-webui/modules/deepbooru.py\", line 96, in get_deepbooru_tags_model\r\n    from basicsr.utils.download_util import load_file_from_url\r\n  File \"/usr/local/envs/automatic/lib/python3.10/site-packages/basicsr/__init__.py\", line 3, in <module>\r\n    from .archs import *\r\n  File \"/usr/local/envs/automatic/lib/python3.10/site-packages/basicsr/archs/__init__.py\", line 5, in <module>\r\n    from basicsr.utils import get_root_logger, scandir\r\n  File \"/usr/local/envs/automatic/lib/python3.10/site-packages/basicsr/utils/__init__.py\", line 1, in <module>\r\n    from .color_util import bgr2ycbcr, rgb2ycbcr, rgb2ycbcr_pt, ycbcr2bgr, ycbcr2rgb\r\n  File \"/usr/local/envs/automatic/lib/python3.10/site-packages/basicsr/utils/color_util.py\", line 2, in <module>\r\n    import torch\r\n  File \"/usr/local/envs/automatic/lib/python3.10/site-packages/torch/__init__.py\", line 202, in <module>\r\n    from torch._C import *  # noqa: F403\r\nImportError: /usr/local/envs/automatic/lib/python3.10/site-packages/torch/lib/libtorch_cuda_cpp.so: symbol cudaGraphRetainUserObject version libcudart.so.11.0 not defined in file libcudart.so.11.0 with link time reference\r\nInterrupted with signal 2 in <frame at 0x7f4448d2adc0, file '/content/stable-diffusion-webui/webui.py', line 105, code wait_on_server>\r\n```\r\n\r\n", "created_at": "2022-10-23T12:43:37Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 3364, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-3364", "issue_numbers": ["3305"], "base_commit": "f49c08ea566385db339c6628f65c3a121033f67c", "patch": "diff --git a/scripts/img2imgalt.py b/scripts/img2imgalt.py\nindex d438175ced3..88abc0939e1 100644\n--- a/scripts/img2imgalt.py\n+++ b/scripts/img2imgalt.py\n@@ -34,6 +34,9 @@ def find_noise_for_image(p, cond, uncond, cfg_scale, steps):\n         sigma_in = torch.cat([sigmas[i] * s_in] * 2)\r\n         cond_in = torch.cat([uncond, cond])\r\n \r\n+        image_conditioning = torch.cat([p.image_conditioning] * 2)\r\n+        cond_in = {\"c_concat\": [image_conditioning], \"c_crossattn\": [cond_in]}\r\n+\r\n         c_out, c_in = [K.utils.append_dims(k, x_in.ndim) for k in dnw.get_scalings(sigma_in)]\r\n         t = dnw.sigma_to_t(sigma_in)\r\n \r\n@@ -78,6 +81,9 @@ def find_noise_for_image_sigma_adjustment(p, cond, uncond, cfg_scale, steps):\n         sigma_in = torch.cat([sigmas[i - 1] * s_in] * 2)\r\n         cond_in = torch.cat([uncond, cond])\r\n \r\n+        image_conditioning = torch.cat([p.image_conditioning] * 2)\r\n+        cond_in = {\"c_concat\": [image_conditioning], \"c_crossattn\": [cond_in]}\r\n+\r\n         c_out, c_in = [K.utils.append_dims(k, x_in.ndim) for k in dnw.get_scalings(sigma_in)]\r\n \r\n         if i == 1:\r\n@@ -194,7 +200,7 @@ def sample_extra(conditioning, unconditional_conditioning, seeds, subseeds, subs\n             \r\n             p.seed = p.seed + 1\r\n             \r\n-            return sampler.sample_img2img(p, p.init_latent, noise_dt, conditioning, unconditional_conditioning)\r\n+            return sampler.sample_img2img(p, p.init_latent, noise_dt, conditioning, unconditional_conditioning, image_conditioning=p.image_conditioning)\r\n \r\n         p.sample = sample_extra\r\n \r\n", "test_patch": "", "problem_statement": "[Bug]: TypeError: 'NoneType' object is not subscriptable when using img2img alternative test\n### Is there an existing issue for this?\n\n- [X] I have searched the existing issues and checked the recent builds/commits\n\n### What happened?\n\nWebui fails to generate an image if using the img2img alternative test since the runway inpainting support update.\r\nUnrelated to this, it does not support the unlimited tokens either.\n\n### Steps to reproduce the problem\n\n1. Go to imgimg\r\n2. Choose img2img alternative test\r\n3. Upload image and write prompt, parameters, settings, etc.\r\n4. Generate.\r\n\n\n### What should have happened?\n\nAn image should generate.\n\n### Commit where the problem happens\n\nbf30673f5132c8f28357b31224c54331e788d3e7\n\n### What platforms do you use to access UI ?\n\nWindows\n\n### What browsers do you use to access the UI ?\n\nGoogle Chrome\n\n### Command Line Arguments\n\n```Shell\n--deepdanbooru --xformers --gradio-img2img-tool color-sketch\n```\n\n\n### Additional information, context and logs\n\nTraceback (most recent call last):\r\n  File \"G:\\stable-webui\\modules\\ui.py\", line 212, in f\r\n    res = list(func(*args, **kwargs))\r\n  File \"G:\\stable-webui\\webui.py\", line 63, in f\r\n    res = func(*args, **kwargs)\r\n  File \"G:\\stable-webui\\modules\\img2img.py\", line 124, in img2img\r\n    processed = modules.scripts.scripts_img2img.run(p, *args)\r\n  File \"G:\\stable-webui\\modules\\scripts.py\", line 173, in run\r\n    processed = script.run(p, *script_args)\r\n  File \"G:\\stable-webui\\scripts\\img2imgalt.py\", line 208, in run\r\n    processed = processing.process_images(p)\r\n  File \"G:\\stable-webui\\modules\\processing.py\", line 411, in process_images\r\n    samples_ddim = p.sample(conditioning=c, unconditional_conditioning=uc, seeds=seeds, subseeds=subseeds, subseed_strength=p.subseed_strength)\r\n  File \"G:\\stable-webui\\scripts\\img2imgalt.py\", line 197, in sample_extra\r\n    return sampler.sample_img2img(p, p.init_latent, noise_dt, conditioning, unconditional_conditioning)\r\n  File \"G:\\stable-webui\\modules\\sd_samplers.py\", line 423, in sample_img2img\r\n    samples = self.launch_sampling(steps, lambda: self.func(self.model_wrap_cfg, xi, extra_args={\r\n  File \"G:\\stable-webui\\modules\\sd_samplers.py\", line 356, in launch_sampling\r\n    return func()\r\n  File \"G:\\stable-webui\\modules\\sd_samplers.py\", line 423, in <lambda>\r\n    samples = self.launch_sampling(steps, lambda: self.func(self.model_wrap_cfg, xi, extra_args={\r\n  File \"G:\\stable-webui\\venv\\lib\\site-packages\\torch\\autograd\\grad_mode.py\", line 27, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"G:\\stable-webui\\repositories\\k-diffusion\\k_diffusion\\sampling.py\", line 64, in sample_euler\r\n    denoised = model(x, sigma_hat * s_in, **extra_args)\r\n  File \"G:\\stable-webui\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1130, in _call_impl\r\n    return forward_call(*input, **kwargs)\r\n  File \"G:\\stable-webui\\modules\\sd_samplers.py\", line 269, in forward\r\n    image_cond_in = torch.cat([torch.stack([image_cond[i] for _ in range(n)]) for i, n in enumerate(repeats)] + [image_cond])\r\n  File \"G:\\stable-webui\\modules\\sd_samplers.py\", line 269, in <listcomp>\r\n    image_cond_in = torch.cat([torch.stack([image_cond[i] for _ in range(n)]) for i, n in enumerate(repeats)] + [image_cond])\r\n  File \"G:\\stable-webui\\modules\\sd_samplers.py\", line 269, in <listcomp>\r\n    image_cond_in = torch.cat([torch.stack([image_cond[i] for _ in range(n)]) for i, n in enumerate(repeats)] + [image_cond])\r\nTypeError: 'NoneType' object is not subscriptable\n", "hints_text": "Error is caused the line below not handling image_cond being uninitialised.\r\nhttps://github.com/AUTOMATIC1111/stable-diffusion-webui/blob/f49c08ea566385db339c6628f65c3a121033f67c/modules/sd_samplers.py#L269\r\n\r\nPutting an IF check fixes the issue for the img2imgalt script but I'm not sure what this line is meant to do which is a question for @random-thoughtss \r\n```\r\n        image_cond_in = None\r\n        if image_cond is not None:\r\n            image_cond_in = torch.cat([torch.stack([image_cond[i] for _ in range(n)]) for i, n in enumerate(repeats)] + [image_cond])\r\n```\nSetting it to `None` would crash the in-painting model and would break if there is any batch size or large prompt.\r\n\r\nThis is the diff for the fix. Just need to add image conditioning to the img2imgalt script since it bypasses the regular img2img sampling call.\r\n\r\n[https://gist.github.com/random-thoughtss/f3ba564f4b7840136107763bdad26541](https://gist.github.com/random-thoughtss/f3ba564f4b7840136107763bdad26541)", "created_at": "2022-10-21T18:43:36Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 3338, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-3338", "issue_numbers": ["3145"], "base_commit": "24ce67a13bd74202d298cd8e2a306d90214980d8", "patch": "diff --git a/modules/sd_samplers.py b/modules/sd_samplers.py\nindex f58a29b9fa9..8884993f916 100644\n--- a/modules/sd_samplers.py\n+++ b/modules/sd_samplers.py\n@@ -219,7 +219,7 @@ def sample_img2img(self, p, x, noise, conditioning, unconditional_conditioning,\n             unconditional_conditioning = {\"c_concat\": [image_conditioning], \"c_crossattn\": [unconditional_conditioning]}\r\n             \r\n             \r\n-        samples = self.launch_sampling(steps, lambda: self.sampler.decode(x1, conditioning, t_enc, unconditional_guidance_scale=p.cfg_scale, unconditional_conditioning=unconditional_conditioning))\r\n+        samples = self.launch_sampling(t_enc + 1, lambda: self.sampler.decode(x1, conditioning, t_enc, unconditional_guidance_scale=p.cfg_scale, unconditional_conditioning=unconditional_conditioning))\r\n \r\n         return samples\r\n \r\n@@ -420,7 +420,7 @@ def sample_img2img(self, p, x, noise, conditioning, unconditional_conditioning,\n         self.model_wrap_cfg.init_latent = x\r\n         self.last_latent = x\r\n \r\n-        samples = self.launch_sampling(steps, lambda: self.func(self.model_wrap_cfg, xi, extra_args={\r\n+        samples = self.launch_sampling(t_enc + 1, lambda: self.func(self.model_wrap_cfg, xi, extra_args={\r\n             'cond': conditioning, \r\n             'image_cond': image_conditioning, \r\n             'uncond': unconditional_conditioning, \r\n", "test_patch": "", "problem_statement": "Progress bar percentage is broken\n**Describe the bug**\r\nProgress bar  finishes on 65-80%, image processing bar and total progress bar are have different steps amount.\r\nI've already seen this issue https://github.com/AUTOMATIC1111/stable-diffusion-webui/issues/3093, I think it's related to this, but it covers a slightly different problem.\r\n\r\n![image](https://user-images.githubusercontent.com/38957619/196678705-5599f336-9e38-41e0-b7ce-dabc7188e8c0.png)\r\n\r\nEven with \"do exactly the amount of steps the slider specifies\" setting turned on it still shows strange percentage, for some reason it doubles the amount of steps:\r\n\r\n![image](https://user-images.githubusercontent.com/38957619/196679701-7b2a251a-23c0-4204-ad03-8ff2a106a8d7.png)\r\n\r\nAlso, It doesn't work SD upscaling too:\r\n\r\n![image](https://user-images.githubusercontent.com/38957619/196681622-e5f0f3ee-3853-45b1-a6a7-26504346f400.png)\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Go to img2img tab\r\n2. Select an image\r\n3. Press Generate button\r\n4. See incorrect percentage display on progress bar\r\n\r\nP.S. Try to use SD upscale or turn on \"do exactly the amount of steps the slider specifies\" setting to see that problem is still appears.\r\n\r\n**Expected behavior**\r\nTotal progress bar should have the same amount of steps as sum of all steps for each image processing\r\n\r\n**Desktop (please complete the following information):**\r\n - OS: Windows\r\n - Browser: Google Chrome\r\n - Commit revision: f894dd552f68bea27476f1f360ab8e79f3a65b4f\n", "hints_text": "", "created_at": "2022-10-21T15:01:38Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 2924, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-2924", "issue_numbers": ["2750"], "base_commit": "c8045c5ad4f99deb3a19add06e0457de1df62b05", "patch": "diff --git a/javascript/imageMaskFix.js b/javascript/imageMaskFix.js\nindex 3d77bfe9b45..9fe7a60309c 100644\n--- a/javascript/imageMaskFix.js\n+++ b/javascript/imageMaskFix.js\n@@ -31,8 +31,8 @@ function imageMaskResize() {\n \n     wrapper.style.width = `${wW}px`;\n     wrapper.style.height = `${wH}px`;\n-    wrapper.style.left = `${(w-wW)/2}px`;\n-    wrapper.style.top = `${(h-wH)/2}px`;\n+    wrapper.style.left = `0px`;\n+    wrapper.style.top = `0px`;\n \n     canvases.forEach( c => {\n         c.style.width = c.style.height = '';\n@@ -42,4 +42,4 @@ function imageMaskResize() {\n     });\n  }\n   \n- onUiUpdate(() => imageMaskResize());\n\\ No newline at end of file\n+ onUiUpdate(() => imageMaskResize());\n", "test_patch": "", "problem_statement": "Inpaint function creates two pictures on next to the other\nFirst bug report, sorry if it's bad.\r\n\r\nWhen importing a picture in the inpaint function, both using drag and drop or from browsing files, the picture gets put twice in the preview where you're supposed to paint, one of the copies is put in the down right corner and it's the real copy. The function seems to be working as intended anyway.\r\n\r\nSteps to reproduce:\r\n\r\nImport a picture in the inpainting function\n", "hints_text": "You ignored the bug report template.\n![image](https://user-images.githubusercontent.com/112255193/196009113-99d069ee-16b3-4cb2-bda5-5f054df0c254.png)\r\nI'm having the same issue. \n> You ignored the bug report template.\r\n\r\ndoes there need to be a template if it happens 100% of the time? I have the same issue, screenshot above. it doubles up and one on left is all broken, one on right is where you inpaint, and if u try to x out it tries to immediately summon a drop image into it window again, etc, so it's registering clicks like twice or something too.\nI'll do it, this is kinda app breaking\r\n\r\n**Describe the bug**\r\nSome images seem to break inpainting\r\n\r\n**To Reproduce**\r\nHere's one of simplest way\r\n1. Copy this image:\r\n![image](https://user-images.githubusercontent.com/20817233/196012975-029479a7-4650-48cb-81b7-c3b7f555964f.png)\r\n2. Paste it in the inpaint tab and see this\r\n![image](https://user-images.githubusercontent.com/20817233/196013008-2ec45661-8128-4c0e-9a6d-72458fe10fcd.png)\r\n\r\n\r\n**Expected behavior**\r\nTo just display it once in the center and properly align the masking tools\r\n\r\n**Desktop (please complete the following information):**\r\n - OS: W11\r\n - Browser Brave\r\n - Commit revision be1596ce30b1ead6998da0c62003003dcce5eb2c **_Clean Install_**\r\n \r\n**Additional context**\r\nMay I add that clicking the X button to remove the image brings up the browse file dialog, idk if this is a feature but as I only use copy paste this could be an issue\ni get it on chrome browser and like above post says, i also instantly get the browse file dialog on hitting X... though for copy/paste that's a non-issue since you could paste over it i think.\nAlso encountered the same problem, i am using Firefox\nFirefox - Occurs after first drag-drop of PNG into window.\r\nChrome - Occurs on hitting 'x' then dragging dropping a different image into window' *Also get browse file dialogue on hitting x, where this was not happening before.\r\n\r\nIf I go straight to img2img window it works, but if I click \"Send to inpaint\" it does not work and simply says \"Error\"\n> i get it on chrome browser and like above post says, i also instantly get the browse file dialog on hitting X... though for copy/paste that's a non-issue since you could paste over it i think.\r\n\r\nyou know what you're right and that's what I've been doing for so long, It's probably why I haven't noticed until today, where the glitch pushed me to try to remove the image\nConfirmed.  Illustrated here: https://github.com/AUTOMATIC1111/stable-diffusion-webui/discussions/2875#discussion-4477775\nI narrowed this down to the move from gradio 3.4.1 to 3.5. Reverting 4ed99d599640bb86bc793aa3cbed31c6d0bd6957 and e8729dd0511f8410db967d9ef192645cfef1be8a fixes the issue.\n> I narrowed this down to the move from gradio 3.4.1 to 3.5. Reverting [4ed99d5](https://github.com/AUTOMATIC1111/stable-diffusion-webui/commit/4ed99d599640bb86bc793aa3cbed31c6d0bd6957) and [e8729dd](https://github.com/AUTOMATIC1111/stable-diffusion-webui/commit/e8729dd0511f8410db967d9ef192645cfef1be8a) fixes the issue.\r\n\r\nis there a way to do just this while keeping current updates?\nGradio 3.5 is needed for iPads. But I've come to the same conclusion that the problems with the inpaint display is tied to the Gradio update by rolling back the distro to just before the Gradio update.\n> Gradio 3.5 is needed for iPads. But I've come to the same conclusion that the problems with the inpaint display is tied to the Gradio update by rolling back the distro to just before the Gradio update.\r\n\r\nCould you give us the hash ? Wondering if it's 7d6042b908c064774ee10961309d396eabdc6c4a\r\n\r\n\r\n\n> > I narrowed this down to the move from gradio 3.4.1 to 3.5. Reverting [4ed99d5](https://github.com/AUTOMATIC1111/stable-diffusion-webui/commit/4ed99d599640bb86bc793aa3cbed31c6d0bd6957) and [e8729dd](https://github.com/AUTOMATIC1111/stable-diffusion-webui/commit/e8729dd0511f8410db967d9ef192645cfef1be8a) fixes the issue.\r\n> \r\n> is there a way to do just this while keeping current updates?\r\n\r\nI just manually put the version back to 3.4.1 in requirements.txt and requirements_versions.txt. You can use git stash to keep local changes while pulling new commits. I.e.\r\n```\r\ngit stash\r\ngit pull\r\ngit stash pop\r\n```\n> > Gradio 3.5 is needed for iPads. But I've come to the same conclusion that the problems with the inpaint display is tied to the Gradio update by rolling back the distro to just before the Gradio update.\r\n> \r\n> Could you give us the hash ? Wondering if it's [7d6042b](https://github.com/AUTOMATIC1111/stable-diffusion-webui/commit/7d6042b908c064774ee10961309d396eabdc6c4a)\r\n\r\nYes, that is the last commit before the gradio version bump that caused the inpaint display issue.\nSome information in dupe bug #2795, but it's mostly just a rehash of what was discussed above. No need to read it unless you are looking for overly-specific information about my exact observations etc.", "created_at": "2022-10-17T03:33:01Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 2645, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-2645", "issue_numbers": ["2326"], "base_commit": "9b75ab144f5fa3669166374dacd5ffc340984078", "patch": "diff --git a/javascript/progressbar.js b/javascript/progressbar.js\nindex 4395a2159a4..076f0a9736f 100644\n--- a/javascript/progressbar.js\n+++ b/javascript/progressbar.js\n@@ -1,5 +1,7 @@\n // code related to showing and updating progressbar shown as the image is being made\n global_progressbars = {}\n+galleries = {}\n+galleryObservers = {}\n \n function check_progressbar(id_part, id_progressbar, id_progressbar_span, id_skip, id_interrupt, id_preview, id_gallery){\n     var progressbar = gradioApp().getElementById(id_progressbar)\n@@ -31,13 +33,24 @@ function check_progressbar(id_part, id_progressbar, id_progressbar_span, id_skip\n                 preview.style.width = gallery.clientWidth + \"px\"\n                 preview.style.height = gallery.clientHeight + \"px\"\n \n+\t\t\t\t//only watch gallery if there is a generation process going on\n+                check_gallery(id_gallery);    \n+\n                 var progressDiv = gradioApp().querySelectorAll('#' + id_progressbar_span).length > 0;\n                 if(!progressDiv){\n                     if (skip) {\n                         skip.style.display = \"none\"\n                     }\n                     interrupt.style.display = \"none\"\n+\t\t\t\n+                    //disconnect observer once generation finished, so user can close selected image if they want\n+                    if (galleryObservers[id_gallery]) {\n+                        galleryObservers[id_gallery].disconnect();\n+                        galleries[id_gallery] = null;\n+                    }    \n                 }\n+\n+\n             }\n \n             window.setTimeout(function() { requestMoreProgress(id_part, id_progressbar_span, id_skip, id_interrupt) }, 500)\n@@ -46,6 +59,28 @@ function check_progressbar(id_part, id_progressbar, id_progressbar_span, id_skip\n \t}\n }\n \n+function check_gallery(id_gallery){\n+    let gallery = gradioApp().getElementById(id_gallery)\n+    // if gallery has no change, no need to setting up observer again.\n+    if (gallery && galleries[id_gallery] !== gallery){\n+        galleries[id_gallery] = gallery;\n+        if(galleryObservers[id_gallery]){\n+            galleryObservers[id_gallery].disconnect();\n+        }\n+        let prevSelectedIndex = selected_gallery_index();\n+        galleryObservers[id_gallery] = new MutationObserver(function (){\n+            let galleryButtons = gradioApp().querySelectorAll('#'+id_gallery+' .gallery-item')\n+            let galleryBtnSelected = gradioApp().querySelector('#'+id_gallery+' .gallery-item.\\\\!ring-2')\n+            if (prevSelectedIndex !== -1 && galleryButtons.length>prevSelectedIndex && !galleryBtnSelected) {\n+                //automatically re-open previously selected index (if exists)\n+                galleryButtons[prevSelectedIndex].click();\n+\t\tshowGalleryImage();\n+            }\n+        })\n+        galleryObservers[id_gallery].observe( gallery, { childList:true, subtree:false })\n+    }\n+}\n+\n onUiUpdate(function(){\n     check_progressbar('txt2img', 'txt2img_progressbar', 'txt2img_progress_span', 'txt2img_skip', 'txt2img_interrupt', 'txt2img_preview', 'txt2img_gallery')\n     check_progressbar('img2img', 'img2img_progressbar', 'img2img_progress_span', 'img2img_skip', 'img2img_interrupt', 'img2img_preview', 'img2img_gallery')\n", "test_patch": "", "problem_statement": "Media Viewer keeps minimizing the image after finishing job\nNot sure if this is a bug, but this didn't use to happen in the web ui, but whenever an image is finished generating, it minimizes the image when it used to leave the viewer open.\r\n\r\nThis wouldn't be a problem, but every time an image is generated, you need to click on the image now to zoom in on it. This can get annoying when generating tons of images.\r\n\r\nBasically this. First pic is how it used to look. Second pic is how it looks now.\r\n![image](https://user-images.githubusercontent.com/4240036/195232609-e3fdb870-222a-4073-a787-7e6b9e6c08a3.png)\r\n![brave_cOHBgMYjZJ](https://user-images.githubusercontent.com/4240036/195232688-bdf1b9ad-f09b-4dfe-b0ee-7d4589a23636.png)\r\n\n", "hints_text": "Since yesterday, generated image in preview window is always getting small, going into top left corner of the preview window, no matter what I do. Before yesterday, when I turn on web user, generate first image, it's small, then I click on it and it stays enlarged in the preview window. when I change the dimensions, it changes in the preview window with it but still filling the preview window, not getting small and into the top left corner. \r\nDoes anyone knows a workaround or which files to edit to make it like before? \r\nI think it's since the upgraded gradio yesterday. Not sure. Been at it all morning but not being a \"code\" person I'm just hovering around style.css and user.css... +1 on this one. \nThis commit appears to be the culprit: https://github.com/AUTOMATIC1111/stable-diffusion-webui/commit/9e5f6b558072f6cdfa0f7010fa819662952fcaf1\nNoticed this as well.\nReverting 9e5f6b558072f6cdfa0f7010fa819662952fcaf1 didn't have any effect on the issue.\r\n\r\n<details>\r\n\r\n1. Closed UI process.\r\n2. Cleared cache and closed browser.\r\n3. `git revert 9e5f6b558072f6cdfa0f7010fa819662952fcaf1`, save and exit editor.\r\n4. Start UI and browser and test.\r\n5. Issue stays.\r\n\r\nI did `git reset --hard origin/master` afterwards to throw away my changes to avoid git history conflicts.\r\n</details>\r\n\r\nIt must be some other commit.\r\n\nGradio version bump issue, a bit annoying. :shrug: \nYes, and it sucks.  I mean it sucks so badly I am tempted to go back to grisk (losing all of these improvements) just so I can escape gradio (which I do not like at all).  Not sure how streamit is but about to find out.\nWill this be resolved at all given it seems to be coming from Gradio? Any speculation? This  really sux :(", "created_at": "2022-10-14T18:34:05Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 2447, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-2447", "issue_numbers": ["2391"], "base_commit": "698d303b04e293635bfb49c525409f3bcf671dce", "patch": "diff --git a/javascript/contextMenus.js b/javascript/contextMenus.js\nindex 7636c4b331f..fe67c42e185 100644\n--- a/javascript/contextMenus.js\n+++ b/javascript/contextMenus.js\n@@ -94,7 +94,7 @@ contextMenuInit = function(){\n     }\r\n     gradioApp().addEventListener(\"click\", function(e) {\r\n       let source = e.composedPath()[0]\r\n-      if(source.id && source.indexOf('check_progress')>-1){\r\n+      if(source.id && source.id.indexOf('check_progress')>-1){\r\n         return\r\n       }\r\n       \r\n", "test_patch": "", "problem_statement": "There is an inconspicuous bug in  \\javascript\\contextMenu.js\nfile:  \\javascript\\contextMenu.js\r\n\r\nfunction : addContextMenuEventListener\r\n\r\nline:97\r\n`if(source.id && source.indexOf('check_progress')>-1){  \r\n        return  \r\n      }  `\r\n\r\nWhen clicking the control with ID, the browser console will report an error\uff1a\r\n`(index):360 Uncaught TypeError: source.indexOf is not a function`\r\n\r\n. Although the normal function will not be affected, it is always uncomfortable for developers to see the error\r\n\n", "hints_text": "Got it.", "created_at": "2022-10-13T03:11:48Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 2344, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-2344", "issue_numbers": ["2225"], "base_commit": "6ac2ec2b78bc5fabd09cb866dd9a71061d669269", "patch": "diff --git a/modules/ui.py b/modules/ui.py\nindex 1204eef7b34..bd09f28545c 100644\n--- a/modules/ui.py\n+++ b/modules/ui.py\n@@ -131,6 +131,8 @@ def __init__(self, d=None):\n         images = [images[index]]\r\n         start_index = index\r\n \r\n+    os.makedirs(opts.outdir_save, exist_ok=True)\r\n+\r\n     with open(os.path.join(opts.outdir_save, \"log.csv\"), \"a\", encoding=\"utf8\", newline='') as file:\r\n         at_start = file.tell() == 0\r\n         writer = csv.writer(file)\r\n", "test_patch": "", "problem_statement": "Save feature fails until output directories created manually\n**Describe the bug**\r\nSave feature of txt2img panel does not work until output directories are created manually\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Check out a clean copy of the project add the model file and launch it via `webui.bat`, wait for downloads to complete\r\n2. Load the app in the browser\r\n3. Focus the `txt2img` tab focused (should already be by default)\r\n4. Enter any prompt and click the `Generate` button\r\n5. Wait for the images to complete\r\n6. Click the `Save` button below the generated images\r\n\r\n**Expected behavior**\r\n1. Images should be saved to `log/images`\r\n2. CSV should be created/updated with info about the images\r\n\r\n**Actual behavior**\r\n1. Save info panel displays a red `Error` indicator\r\n2. In the server output, save fails due to the output directory not existing\r\n\r\n    Log Output:\r\n```bash\r\nTraceback (most recent call last):\r\n  File \"<PATH>\\modules\\ui.py\", line 176, in f\r\n    res = list(func(*args, **kwargs))\r\n  File \"<PATH>\\modules\\ui.py\", line 128, in save_files\r\n    with open(os.path.join(opts.outdir_save, \"log.csv\"), \"a\", encoding=\"utf8\", newline='') as file:\r\nFileNotFoundError: [Errno 2] No such file or directory: 'log/images\\\\log.csv'\r\n\r\nTraceback (most recent call last):\r\n  File \"<PATH>\\venv\\lib\\site-packages\\gradio\\routes.py\", line 273, in run_predict\r\n    output = await app.blocks.process_api(\r\n  File \"<PATH>\\venv\\lib\\site-packages\\gradio\\blocks.py\", line 746, in process_api\r\n    predictions = self.postprocess_data(fn_index, result[\"prediction\"], state)\r\n  File \"<PATH>\\venv\\lib\\site-packages\\gradio\\blocks.py\", line 700, in postprocess_data\r\n    if predictions[i] is components._Keywords.FINISHED_ITERATING:\r\nIndexError: tuple index out of range\r\n```\r\n\r\n**Desktop (please complete the following information):**\r\n - OS: Windows\r\n - Browser: Chrome\r\n - Commit revision: 42bf5fa3256bff5e4640e5a626e750d4e49e01e1\r\n\r\n**Additional context**\r\nAfter manually creating the `log` and `images` folders, save works as expected\r\n\n", "hints_text": "", "created_at": "2022-10-12T05:13:27Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 2295, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-2295", "issue_numbers": ["2275"], "base_commit": "9e5f6b558072f6cdfa0f7010fa819662952fcaf1", "patch": "diff --git a/modules/sd_hijack.py b/modules/sd_hijack.py\nindex ac70f8767ac..2753d4fa833 100644\n--- a/modules/sd_hijack.py\n+++ b/modules/sd_hijack.py\n@@ -321,7 +321,17 @@ def forward(self, text):\n                         fixes.append(fix[1])\r\n                 self.hijack.fixes.append(fixes)\r\n             \r\n-            z1 = self.process_tokens([x[:75] for x in remade_batch_tokens], [x[:75] for x in batch_multipliers])\r\n+            tokens = []\r\n+            multipliers = []\r\n+            for i in range(len(remade_batch_tokens)):\r\n+                if len(remade_batch_tokens[i]) > 0:\r\n+                    tokens.append(remade_batch_tokens[i][:75])\r\n+                    multipliers.append(batch_multipliers[i][:75])\r\n+                else:\r\n+                    tokens.append([self.wrapped.tokenizer.eos_token_id] * 75)\r\n+                    multipliers.append([1.0] * 75)\r\n+\r\n+            z1 = self.process_tokens(tokens, multipliers)\r\n             z = z1 if z is None else torch.cat((z, z1), axis=-2)\r\n             \r\n             remade_batch_tokens = rem_tokens\r\n", "test_patch": "", "problem_statement": "Error if more than 75 tokens used\n**Describe the bug**\r\nWhen typing the prompt (txt2img) the number of tokens increases from 75 to 150, but then it errors when I try to generate images. It will run if the number of tokens are below 75.\r\n\r\n> Traceback (most recent call last):\r\n>   File \"F:\\stable-diffusion-webui\\modules\\ui.py\", line 182, in f\r\n>     res = list(func(*args, **kwargs))\r\n>   File \"F:\\stable-diffusion-webui\\webui.py\", line 69, in f\r\n>     res = func(*args, **kwargs)\r\n>   File \"F:\\stable-diffusion-webui\\modules\\txt2img.py\", line 43, in txt2img\r\n>     processed = process_images(p)\r\n>   File \"F:\\stable-diffusion-webui\\modules\\processing.py\", line 394, in process_images\r\n>     c = prompt_parser.get_multicond_learned_conditioning(shared.sd_model, prompts, p.steps)\r\n>   File \"F:\\stable-diffusion-webui\\modules\\prompt_parser.py\", line 203, in get_multicond_learned_conditioning\r\n>     learned_conditioning = get_learned_conditioning(model, prompt_flat_list, steps)\r\n>   File \"F:\\stable-diffusion-webui\\modules\\prompt_parser.py\", line 138, in get_learned_conditioning\r\n>     conds = model.get_learned_conditioning(texts)\r\n>   File \"F:\\stable-diffusion-webui\\repositories\\stable-diffusion\\ldm\\models\\diffusion\\ddpm.py\", line 558, in get_learned_conditioning\r\n>     c = self.cond_stage_model(c)\r\n>   File \"F:\\stable-diffusion-webui\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1130, in _call_impl\r\n>     return forward_call(*input, **kwargs)\r\n>   File \"F:\\stable-diffusion-webui\\modules\\sd_hijack.py\", line 315, in forward\r\n>     z1 = self.process_tokens([x[:75] for x in remade_batch_tokens], [x[:75] for x in batch_multipliers])\r\n>   File \"F:\\stable-diffusion-webui\\modules\\sd_hijack.py\", line 330, in process_tokens\r\n>     tokens = torch.asarray(remade_batch_tokens).to(device)\r\n> ValueError: expected sequence of length 2 at dim 1 (got 77)\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\nSee above\r\n\r\n**Expected behavior**\r\nNo errors and image is created.\r\n\r\n**Desktop (please complete the following information):**\r\n - OS: Windows 11\r\n - Browser: Chrome\r\n - Commit revision: a05c824384f21dfd729e521e8d6cef8da3250bfc \r\n\r\n\n", "hints_text": "Can you send the prompt + all the parameters if possible?\nThis works, with 75/75 tokens:\r\n\r\n> solo, a close-up portrait of a petite blonde anamorphic [woman:black vines:9], ((naked)), eerie glow, biomutant, dystopian, ((gnarly roots growing out of her skin)), (fingers and toes growing into the ground), stern face, green_eyes, sorceress, glowing, in a laboratory covered in black oil, cyberpunk, by greg rutkowski and wlop\r\n> Steps: 25, Sampler: Euler a, CFG scale: 11, Seed: 4099293485, Size: 576x576, Model hash: 7460a6fa\r\n\r\nBut if I change one word in the prompt (green_eyes to bloodshot_eyes), nothing else, the token count goes to 82/150 and the error appears.\r\n\r\n> solo, a close-up portrait of a petite blonde anamorphic [woman:black vines:9],  ((naked)), eerie glow, biomutant, dystopian, ((gnarly roots growing out of her skin)), (fingers and toes growing into the ground), stern face, bloodshot_eyes, sorceress, glowing, in a laboratory covered in black oil, cyberpunk, by greg rutkowski and wlop\nI just tried some random prompts that were larger than 75/75 (115/150 and 272/300 in these example) and it works, even if I include the prompt above as part of it?!\nAh, I found it.\nIf I use the same, failing prompt and increase it a little, it works. It seems to be something about the specific 82/150 size. Add the word \"art\" to the end, and it's 83/150...works. ", "created_at": "2022-10-11T18:50:52Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 2206, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-2206", "issue_numbers": ["2088"], "base_commit": "f98338faa84ecce503e68d8ba13d5f7bbae52730", "patch": "diff --git a/javascript/contextMenus.js b/javascript/contextMenus.js\nindex 2d82269fc52..7852793c137 100644\n--- a/javascript/contextMenus.js\n+++ b/javascript/contextMenus.js\n@@ -147,10 +147,6 @@ generateOnRepeatId = appendContextMenuOption('#txt2img_generate','Generate forev\n \r\n cancelGenerateForever = function(){ \r\n   clearInterval(window.generateOnRepeatInterval) \r\n-  let interruptbutton = gradioApp().querySelector('#txt2img_interrupt');\r\n-  if(interruptbutton.offsetParent){\r\n-      interruptbutton.click();\r\n-  }\r\n }\r\n \r\n appendContextMenuOption('#txt2img_interrupt','Cancel generate forever',cancelGenerateForever)\r\n", "test_patch": "", "problem_statement": "[Enhancement] Generate Forever\u2014Provide option to \"Cancel generate forever after finishing current generation\"\n**Is your feature request related to a problem? Please describe.**\r\nCancel Generate forever (#1932)\u2014while functioning as intended so far as cancelling automatic generations\u2014also interrupts the current generation.\r\n\r\n**Describe the solution you'd like**\r\nIt would be nice if we could cancel generate forever while also allowing the current generation to conclude.\r\n\r\n**Describe alternatives you've considered**\r\nIf anything, maybe we could just add another button to the context menu:\r\n\"Cancel generate forever _after current generation_ is finished\"\r\n\r\nNot sure how annoying this might be to implement, as it currently seems to map directly to the interrupt action...\r\n\r\ncc @dfaker \n", "hints_text": "Yeah, I don't see why not, both modes seem pretty much the same to me. Should be the matter of deleting a line of code.\nI agree, it would be nice to have an option to interrupt and finish current iterations, or interrupt without finishing, but in that case, not saving the images in the current batch. Now it is saving unfinished previews when interrupting", "created_at": "2022-10-10T20:53:46Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 2138, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-2138", "issue_numbers": ["2106"], "base_commit": "45fbd1c5fec887988ab555aac75a999d4f3aff40", "patch": "diff --git a/modules/sd_hijack.py b/modules/sd_hijack.py\nindex 437acce4c7e..3edc0e9d85a 100644\n--- a/modules/sd_hijack.py\n+++ b/modules/sd_hijack.py\n@@ -43,10 +43,7 @@ def undo_optimizations():\n \r\n \r\n def get_target_prompt_token_count(token_count):\r\n-    if token_count < 75:\r\n-        return 75\r\n-\r\n-    return math.ceil(token_count / 10) * 10\r\n+    return math.ceil(max(token_count, 1) / 75) * 75\r\n \r\n \r\n class StableDiffusionModelHijack:\r\n@@ -127,7 +124,6 @@ def __init__(self, wrapped, hijack):\n                 self.token_mults[ident] = mult\r\n \r\n     def tokenize_line(self, line, used_custom_terms, hijack_comments):\r\n-        id_start = self.wrapped.tokenizer.bos_token_id\r\n         id_end = self.wrapped.tokenizer.eos_token_id\r\n \r\n         if opts.enable_emphasis:\r\n@@ -154,7 +150,13 @@ def tokenize_line(self, line, used_custom_terms, hijack_comments):\n                     i += 1\r\n                 else:\r\n                     emb_len = int(embedding.vec.shape[0])\r\n-                    fixes.append((len(remade_tokens), embedding))\r\n+                    iteration = len(remade_tokens) // 75\r\n+                    if (len(remade_tokens) + emb_len) // 75 != iteration:\r\n+                        rem = (75 * (iteration + 1) - len(remade_tokens))\r\n+                        remade_tokens += [id_end] * rem\r\n+                        multipliers += [1.0] * rem\r\n+                        iteration += 1\r\n+                    fixes.append((iteration, (len(remade_tokens) % 75, embedding)))\r\n                     remade_tokens += [0] * emb_len\r\n                     multipliers += [weight] * emb_len\r\n                     used_custom_terms.append((embedding.name, embedding.checksum()))\r\n@@ -162,10 +164,10 @@ def tokenize_line(self, line, used_custom_terms, hijack_comments):\n \r\n         token_count = len(remade_tokens)\r\n         prompt_target_length = get_target_prompt_token_count(token_count)\r\n-        tokens_to_add = prompt_target_length - len(remade_tokens) + 1\r\n+        tokens_to_add = prompt_target_length - len(remade_tokens)\r\n \r\n-        remade_tokens = [id_start] + remade_tokens + [id_end] * tokens_to_add\r\n-        multipliers = [1.0] + multipliers + [1.0] * tokens_to_add\r\n+        remade_tokens = remade_tokens + [id_end] * tokens_to_add\r\n+        multipliers = multipliers + [1.0] * tokens_to_add\r\n \r\n         return remade_tokens, fixes, multipliers, token_count\r\n \r\n@@ -260,29 +262,55 @@ def process_text_old(self, text):\n             hijack_fixes.append(fixes)\r\n             batch_multipliers.append(multipliers)\r\n         return batch_multipliers, remade_batch_tokens, used_custom_terms, hijack_comments, hijack_fixes, token_count\r\n-\r\n+    \r\n     def forward(self, text):\r\n-\r\n-        if opts.use_old_emphasis_implementation:\r\n+        use_old = opts.use_old_emphasis_implementation\r\n+        if use_old:\r\n             batch_multipliers, remade_batch_tokens, used_custom_terms, hijack_comments, hijack_fixes, token_count = self.process_text_old(text)\r\n         else:\r\n             batch_multipliers, remade_batch_tokens, used_custom_terms, hijack_comments, hijack_fixes, token_count = self.process_text(text)\r\n \r\n-        self.hijack.fixes = hijack_fixes\r\n         self.hijack.comments += hijack_comments\r\n \r\n         if len(used_custom_terms) > 0:\r\n             self.hijack.comments.append(\"Used embeddings: \" + \", \".join([f'{word} [{checksum}]' for word, checksum in used_custom_terms]))\r\n+        \r\n+        if use_old:\r\n+            self.hijack.fixes = hijack_fixes\r\n+            return self.process_tokens(remade_batch_tokens, batch_multipliers)\r\n+        \r\n+        z = None\r\n+        i = 0\r\n+        while max(map(len, remade_batch_tokens)) != 0:\r\n+            rem_tokens = [x[75:] for x in remade_batch_tokens]\r\n+            rem_multipliers = [x[75:] for x in batch_multipliers]\r\n+            \r\n+            self.hijack.fixes = []\r\n+            for unfiltered in hijack_fixes:\r\n+                fixes = []\r\n+                for fix in unfiltered:\r\n+                    if fix[0] == i:\r\n+                        fixes.append(fix[1])\r\n+                self.hijack.fixes.append(fixes)\r\n+            \r\n+            z1 = self.process_tokens([x[:75] for x in remade_batch_tokens], [x[:75] for x in batch_multipliers])\r\n+            z = z1 if z is None else torch.cat((z, z1), axis=-2)\r\n+            \r\n+            remade_batch_tokens = rem_tokens\r\n+            batch_multipliers = rem_multipliers\r\n+            i += 1\r\n+        \r\n+        return z\r\n+        \r\n+    \r\n+    def process_tokens(self, remade_batch_tokens, batch_multipliers):\r\n+        if not opts.use_old_emphasis_implementation:\r\n+            remade_batch_tokens = [[self.wrapped.tokenizer.bos_token_id] + x[:75] + [self.wrapped.tokenizer.eos_token_id] for x in remade_batch_tokens]\r\n+            batch_multipliers = [[1.0] + x[:75] + [1.0] for x in batch_multipliers]\r\n+        \r\n+        tokens = torch.asarray(remade_batch_tokens).to(device)\r\n+        outputs = self.wrapped.transformer(input_ids=tokens, output_hidden_states=-opts.CLIP_stop_at_last_layers)\r\n \r\n-        target_token_count = get_target_prompt_token_count(token_count) + 2\r\n-\r\n-        position_ids_array = [min(x, 75) for x in range(target_token_count-1)] + [76]\r\n-        position_ids = torch.asarray(position_ids_array, device=devices.device).expand((1, -1))\r\n-\r\n-        remade_batch_tokens_of_same_length = [x + [self.wrapped.tokenizer.eos_token_id] * (target_token_count - len(x)) for x in remade_batch_tokens]\r\n-        tokens = torch.asarray(remade_batch_tokens_of_same_length).to(device)\r\n-\r\n-        outputs = self.wrapped.transformer(input_ids=tokens, position_ids=position_ids, output_hidden_states=-opts.CLIP_stop_at_last_layers)\r\n         if opts.CLIP_stop_at_last_layers > 1:\r\n             z = outputs.hidden_states[-opts.CLIP_stop_at_last_layers]\r\n             z = self.wrapped.transformer.text_model.final_layer_norm(z)\r\n@@ -290,7 +318,7 @@ def forward(self, text):\n             z = outputs.last_hidden_state\r\n \r\n         # restoring original mean is likely not correct, but it seems to work well to prevent artifacts that happen otherwise\r\n-        batch_multipliers_of_same_length = [x + [1.0] * (target_token_count - len(x)) for x in batch_multipliers]\r\n+        batch_multipliers_of_same_length = [x + [1.0] * (75 - len(x)) for x in batch_multipliers]\r\n         batch_multipliers = torch.asarray(batch_multipliers_of_same_length).to(device)\r\n         original_mean = z.mean()\r\n         z *= batch_multipliers.reshape(batch_multipliers.shape + (1,)).expand(z.shape)\r\n", "test_patch": "", "problem_statement": "Uncapped token limit does not work past the ~85th token\n**Describe the bug**\r\nTokens past ~85 do not really affect the prompt in a expected or consistant way. Tokens past ~90 do not do anything (except provide noise). It would be nice if someone could try on NovelAI (as they have a cap of ~200) if the behavior is the same.\r\n\r\n**To Reproduce**\r\nI have generated an image with the same seed multiple times, with the text `green eyes` at different token positions and varying attention. Commas are used as an unobtrusive padding to help get up to the nth token, without changing the image in any significant way. The end of the prompt is also padded up to the 99th token.\r\n\r\nAll images are generated with: Steps: 20, Sampler: Euler, CFG scale: 11, Seed: 2347389309, Size: 512x640, Model hash: 925997e9, Clip skip: 2\r\nNegative prompt: `lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry, artist name`\r\nUsing a certain trendy model (final-pruned), with the VAE and no hypernetworks. This model has been demonstrated to support up to ~200 tokens on the premium UI.\r\n\r\nWith `green eyes` at the:\r\n\r\n- **76**th: `masterpiece, 1girl, cute, , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , green eyes, , , , , , , , , , , , , , , , , , , , , ,` (pass)\r\n![86](https://user-images.githubusercontent.com/112723046/194773620-b109e954-60ee-4b9f-9bf3-34d4ee1ee699.png)\r\n- **81**st: `masterpiece, 1girl, cute, , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , green eyes , , , , , , , , , , , , , , , , ,` (fail)\r\n![81](https://user-images.githubusercontent.com/112723046/194773920-e710777e-03f3-4bad-a41c-75a6a6eba55d.png)\r\n- Still **81**st (x1.5 attn): `masterpiece, 1girl, cute, , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , (green eyes:1.5), , , , , , , , , , , , , , , , ,` (pass, attention helped)\r\n![86_1 5](https://user-images.githubusercontent.com/112723046/194774011-955c4559-aa85-48f1-9c59-34cfb84fb4cd.png)\r\n- **86**th (x1.5 attn): `masterpiece, 1girl, cute, , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , (green eyes:1.5), , , , , , , , , , , ,` (fail, eyes are no longer green)\r\n![86_1 5](https://user-images.githubusercontent.com/112723046/194774117-7d5e6f4b-d38c-406a-852c-daff7c8fe8d0.png)\r\n- Still **86**th (x2.0 attn): `masterpiece, 1girl, cute, , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , (green eyes:2.0), , , , , , , , , , , ,` (fail, increasing multiplier did not make the eyes green)\r\n![86_2 0](https://user-images.githubusercontent.com/112723046/194774157-a00e10cf-28c1-471f-8f11-2fc9f384ae5c.png)\r\n- Still **86**th (x2.5 attn): `masterpiece, 1girl, cute, , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , (green eyes:2.5), , , , , , , , , , , ,` (fail, increasing any more would just destroy the image)\r\n![86_2 5](https://user-images.githubusercontent.com/112723046/194774304-73fc0333-98a9-4515-9ac9-df195e4afbe6.png)\r\n- **91**st (x2.5 attn): `masterpiece, 1girl, cute, , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , (green eyes:2.5), , , , , , ,` (fail, no green at all now) \r\n![91_2 5](https://user-images.githubusercontent.com/112723046/194774411-4e509ae8-5325-40eb-932c-e606cb5d4eac.png)\r\n\r\n\r\n**Additional context**\r\nI've only demonstrated with 1 seed here, but the behavior is consistent with any seed.\r\nCommas are used, but behavior is the same with normal words/tags.\n", "hints_text": "", "created_at": "2022-10-10T04:42:19Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 2124, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-2124", "issue_numbers": ["2114"], "base_commit": "6a9ea5b41cf92cd9e980349bb5034439f4e7a58b", "patch": "diff --git a/javascript/edit-attention.js b/javascript/edit-attention.js\nindex 0280c603f46..79566a2e2e6 100644\n--- a/javascript/edit-attention.js\n+++ b/javascript/edit-attention.js\n@@ -38,4 +38,7 @@ addEventListener('keydown', (event) => {\n \t\ttarget.selectionStart = selectionStart;\r\n \t\ttarget.selectionEnd = selectionEnd;\r\n \t}\r\n+\t// Since we've modified a Gradio Textbox component manually, we need to simulate an `input` DOM event to ensure its\r\n+\t// internal Svelte data binding remains in sync.\r\n+\ttarget.dispatchEvent(new Event(\"input\", { bubbles: true }));\r\n });\r\n", "test_patch": "", "problem_statement": "Prompt weights set using up/down arrow keys are ignored  \n**Describe the bug**\r\nThere is a bug in the UI input handling. When changing weights in the prompt using up/down keyboard arrow keys the generation does not actually use the new weights. Weights are only used for generation if typed in using keyboard number keys.  \r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Select a word in your prompt, press up or down arrow keys to set a weight, generate image\r\n2. Observe the prompt displayed below your image, the weight is not there \r\n3. Select the same word, change the weight using keyboard number keys , generate image\r\n4. Observe the prompt displayed below your image now contains the desired weights\r\n\r\n**Expected behavior**\r\nPrompt weights applied using arrow keys should not be ignored\r\n\r\n**Desktop (please complete the following information):**\r\n - OS: Linux\r\n - Browser: Firefox\r\n - Commit revision a65476718f08a35f527b973ef731e6f488bace5e\r\n \r\n\n", "hints_text": "Yep just tested and it indeed doesn't work with the shortcut, but does writing it yourself, very odd", "created_at": "2022-10-10T00:48:17Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 2056, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-2056", "issue_numbers": ["1959"], "base_commit": "0241d811d23427b99f6b1eda1540bdf8d87963d5", "patch": "diff --git a/modules/processing.py b/modules/processing.py\nindex 7fa1144e6ff..1051597d444 100644\n--- a/modules/processing.py\n+++ b/modules/processing.py\n@@ -284,6 +284,9 @@ def create_infotext(p, all_prompts, all_seeds, all_subseeds, comments, iteration\n         \"Face restoration\": (opts.face_restoration_model if p.restore_faces else None),\r\n         \"Size\": f\"{p.width}x{p.height}\",\r\n         \"Model hash\": getattr(p, 'sd_model_hash', None if not opts.add_model_hash_to_info or not shared.sd_model.sd_model_hash else shared.sd_model.sd_model_hash),\r\n+        \"Model\": (None if not opts.add_extended_model_details_to_info or not shared.sd_model.sd_checkpoint_info.model_name else shared.sd_model.sd_checkpoint_info.model_name.replace(',', '').replace(':', '')),\r\n+        \"Model VAE\": (None if not opts.add_extended_model_details_to_info or not shared.sd_model.sd_model_vae_name else shared.sd_model.sd_model_vae_name.replace(',', '').replace(':', '')),\r\n+        \"Model hypernetwork\": (None if not opts.add_extended_model_details_to_info or not opts.sd_hypernetwork else opts.sd_hypernetwork.replace(',', '').replace(':', '')),\r\n         \"Batch size\": (None if p.batch_size < 2 else p.batch_size),\r\n         \"Batch pos\": (None if p.batch_size < 2 else position_in_batch),\r\n         \"Variation seed\": (None if p.subseed_strength == 0 else all_subseeds[index]),\r\ndiff --git a/modules/sd_models.py b/modules/sd_models.py\nindex 2101b18da76..492042ba685 100644\n--- a/modules/sd_models.py\n+++ b/modules/sd_models.py\n@@ -4,7 +4,7 @@\n from collections import namedtuple\r\n import torch\r\n from omegaconf import OmegaConf\r\n-\r\n+from pathlib import Path\r\n \r\n from ldm.util import instantiate_from_config\r\n \r\n@@ -158,6 +158,7 @@ def load_model_weights(model, checkpoint_info):\n         vae_dict = {k: v for k, v in vae_ckpt[\"state_dict\"].items() if k[0:4] != \"loss\"}\r\n \r\n         model.first_stage_model.load_state_dict(vae_dict)\r\n+        model.sd_model_vae_name = Path(vae_file).stem\r\n \r\n     model.sd_model_hash = sd_model_hash\r\n     model.sd_model_checkpoint = checkpoint_file\r\ndiff --git a/modules/shared.py b/modules/shared.py\nindex dffa0094b71..ca63f7d8e48 100644\n--- a/modules/shared.py\n+++ b/modules/shared.py\n@@ -242,6 +242,7 @@ def options_section(section_identifier, options_dict):\n     \"return_grid\": OptionInfo(True, \"Show grid in results for web\"),\r\n     \"do_not_show_images\": OptionInfo(False, \"Do not show any images in results for web\"),\r\n     \"add_model_hash_to_info\": OptionInfo(True, \"Add model hash to generation information\"),\r\n+    \"add_extended_model_details_to_info\": OptionInfo(False, \"Add extended model details to generation information (model name, VAE, hypernetwork)\"),\r\n     \"font\": OptionInfo(\"\", \"Font for image grids that have text\"),\r\n     \"js_modal_lightbox\": OptionInfo(True, \"Enable full page image viewer\"),\r\n     \"js_modal_lightbox_initially_zoomed\": OptionInfo(True, \"Show images zoomed in by default in full page image viewer\"),\r\n", "test_patch": "", "problem_statement": "(Feature Request) Add model/vae/hypernetwork file name to be saved in image info\nAdd selected model / vae / hypernetwork file name to be saved in image info too.\r\n\r\nMaybe make it optional.\r\n\r\nI do not know what i change in settings tab, but now i can't recreate same image again. \r\nI have few  models and hypernetworks for tests.\n", "hints_text": "", "created_at": "2022-10-09T09:10:43Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 2030, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-2030", "issue_numbers": ["1984"], "base_commit": "050a6a798cec90ae2f881c2ddd3f0221e69907dc", "patch": "diff --git a/modules/hypernetwork.py b/modules/hypernetwork.py\nindex 7f06224285f..19f1c227066 100644\n--- a/modules/hypernetwork.py\n+++ b/modules/hypernetwork.py\n@@ -40,18 +40,25 @@ def __init__(self, filename):\n             self.layers[size] = (HypernetworkModule(size, sd[0]), HypernetworkModule(size, sd[1]))\r\n \r\n \r\n-def load_hypernetworks(path):\r\n+def list_hypernetworks(path):\r\n     res = {}\r\n-\r\n     for filename in glob.iglob(os.path.join(path, '**/*.pt'), recursive=True):\r\n+        name = os.path.splitext(os.path.basename(filename))[0]\r\n+        res[name] = filename\r\n+    return res\r\n+\r\n+\r\n+def load_hypernetwork(filename):\r\n+    print(f\"Loading hypernetwork {filename}\")\r\n+    path = shared.hypernetworks.get(filename, None)\r\n+    if (path is not None):\r\n         try:\r\n-            hn = Hypernetwork(filename)\r\n-            res[hn.name] = hn\r\n+            shared.loaded_hypernetwork = Hypernetwork(path)\r\n         except Exception:\r\n-            print(f\"Error loading hypernetwork {filename}\", file=sys.stderr)\r\n+            print(f\"Error loading hypernetwork {path}\", file=sys.stderr)\r\n             print(traceback.format_exc(), file=sys.stderr)\r\n-\r\n-    return res\r\n+    else:\r\n+        shared.loaded_hypernetwork = None\r\n \r\n \r\n def attention_CrossAttention_forward(self, x, context=None, mask=None):\r\n@@ -60,7 +67,7 @@ def attention_CrossAttention_forward(self, x, context=None, mask=None):\n     q = self.to_q(x)\r\n     context = default(context, x)\r\n \r\n-    hypernetwork = shared.selected_hypernetwork()\r\n+    hypernetwork = shared.loaded_hypernetwork\r\n     hypernetwork_layers = (hypernetwork.layers if hypernetwork is not None else {}).get(context.shape[2], None)\r\n \r\n     if hypernetwork_layers is not None:\r\ndiff --git a/modules/sd_hijack_optimizations.py b/modules/sd_hijack_optimizations.py\nindex c4396bb9b7a..634fb4b24e9 100644\n--- a/modules/sd_hijack_optimizations.py\n+++ b/modules/sd_hijack_optimizations.py\n@@ -28,7 +28,7 @@ def split_cross_attention_forward_v1(self, x, context=None, mask=None):\n     q_in = self.to_q(x)\r\n     context = default(context, x)\r\n \r\n-    hypernetwork = shared.selected_hypernetwork()\r\n+    hypernetwork = shared.loaded_hypernetwork\r\n     hypernetwork_layers = (hypernetwork.layers if hypernetwork is not None else {}).get(context.shape[2], None)\r\n \r\n     if hypernetwork_layers is not None:\r\n@@ -68,7 +68,7 @@ def split_cross_attention_forward(self, x, context=None, mask=None):\n     q_in = self.to_q(x)\r\n     context = default(context, x)\r\n \r\n-    hypernetwork = shared.selected_hypernetwork()\r\n+    hypernetwork = shared.loaded_hypernetwork\r\n     hypernetwork_layers = (hypernetwork.layers if hypernetwork is not None else {}).get(context.shape[2], None)\r\n \r\n     if hypernetwork_layers is not None:\r\n@@ -132,7 +132,7 @@ def xformers_attention_forward(self, x, context=None, mask=None):\n     h = self.heads\r\n     q_in = self.to_q(x)\r\n     context = default(context, x)\r\n-    hypernetwork = shared.selected_hypernetwork()\r\n+    hypernetwork = shared.loaded_hypernetwork\r\n     hypernetwork_layers = (hypernetwork.layers if hypernetwork is not None else {}).get(context.shape[2], None)\r\n     if hypernetwork_layers is not None:\r\n         k_in = self.to_k(hypernetwork_layers[0](context))\r\ndiff --git a/modules/shared.py b/modules/shared.py\nindex 2dc092d6878..00e647d5887 100644\n--- a/modules/shared.py\n+++ b/modules/shared.py\n@@ -78,11 +78,8 @@\n xformers_available = False\r\n config_filename = cmd_opts.ui_settings_file\r\n \r\n-hypernetworks = hypernetwork.load_hypernetworks(os.path.join(models_path, 'hypernetworks'))\r\n-\r\n-\r\n-def selected_hypernetwork():\r\n-    return hypernetworks.get(opts.sd_hypernetwork, None)\r\n+hypernetworks = hypernetwork.list_hypernetworks(os.path.join(models_path, 'hypernetworks'))\r\n+loaded_hypernetwork = None\r\n \r\n \r\n class State:\r\ndiff --git a/webui.py b/webui.py\nindex 18de8e16537..270584f7771 100644\n--- a/webui.py\n+++ b/webui.py\n@@ -82,6 +82,9 @@ def f(*args, **kwargs):\n shared.sd_model = modules.sd_models.load_model()\r\n shared.opts.onchange(\"sd_model_checkpoint\", wrap_queued_call(lambda: modules.sd_models.reload_model_weights(shared.sd_model)))\r\n \r\n+loaded_hypernetwork = modules.hypernetwork.load_hypernetwork(shared.opts.sd_hypernetwork)\r\n+shared.opts.onchange(\"sd_hypernetwork\", wrap_queued_call(lambda: modules.hypernetwork.load_hypernetwork(shared.opts.sd_hypernetwork)))\r\n+\r\n \r\n def webui():\r\n     # make the program just exit at ctrl+c without waiting for anything\r\n", "test_patch": "", "problem_statement": "VRAM usage increased by presence of hypernets when \"None\" is selected\nEdit: See bottom of report for what the actual issue appears to be.\r\n\r\n**Describe the bug**\r\nI typically run with --medvram --always-batch-cond-uncond --opt-split-attention (before it was default) on my RX 570 4GB(my max resolution I can generate with these settings are 576x576), however in some instances I have run with --medvram --always-batch-cond-uncond --no-half for testing purposes. I would not be able to generate 512x512, but would be able to generate 448x448. As of commit 3061cdb7b610d4ba7f1ea695d9d6364b591e5bc7 and probably any commits within the last day, trying to generate images of any size, including as low as 64x64 will result in going OOM and this error. (64x64 width/height set)\r\n```\r\nError completing request\r\nArguments: ('', '', 'None', 'None', 1, 0, False, False, 1, 1, 7, -1.0, -1.0, 0, 0, 0, False, 64, 64, False, False, 0.7, 0, False, False, None, '', 1, '', 4, '', True, False) {}\r\nTraceback (most recent call last):\r\n  File \"/home/user/Documents/stable-diffusion-webui/modules/ui.py\", line 158, in f\r\n    res = list(func(*args, **kwargs))\r\n  File \"webui.py\", line 68, in f\r\n    res = func(*args, **kwargs)\r\n  File \"/home/user/Documents/stable-diffusion-webui/modules/txt2img.py\", line 43, in txt2img\r\n    processed = process_images(p)\r\n  File \"/home/user/Documents/stable-diffusion-webui/modules/processing.py\", line 381, in process_images\r\n    samples_ddim = p.sample(conditioning=c, unconditional_conditioning=uc, seeds=seeds, subseeds=subseeds, subseed_strength=p.subseed_strength)\r\n  File \"/home/user/Documents/stable-diffusion-webui/modules/processing.py\", line 508, in sample\r\n    samples = self.sampler.sample(self, x, conditioning, unconditional_conditioning)\r\n  File \"/home/user/Documents/stable-diffusion-webui/modules/sd_samplers.py\", line 399, in sample\r\n    samples = self.func(self.model_wrap_cfg, x, extra_args={'cond': conditioning, 'uncond': unconditional_conditioning, 'cond_scale': p.cfg_scale}, disable=False, callback=self.callback_state, **extra_params_kwargs)\r\n  File \"/home/user/miniconda3/envs/ldm/lib/python3.8/site-packages/torch/autograd/grad_mode.py\", line 27, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/home/user/Documents/stable-diffusion-webui/repositories/k-diffusion/k_diffusion/sampling.py\", line 80, in sample_euler_ancestral\r\n    denoised = model(x, sigmas[i] * s_in, **extra_args)\r\n  File \"/home/user/miniconda3/envs/ldm/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\r\n    return forward_call(*input, **kwargs)\r\n  File \"/home/user/Documents/stable-diffusion-webui/modules/sd_samplers.py\", line 239, in forward\r\n    x_out = self.inner_model(x_in, sigma_in, cond=cond_in)\r\n  File \"/home/user/miniconda3/envs/ldm/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\r\n    return forward_call(*input, **kwargs)\r\n  File \"/home/user/Documents/stable-diffusion-webui/repositories/k-diffusion/k_diffusion/external.py\", line 112, in forward\r\n    eps = self.get_eps(input * c_in, self.sigma_to_t(sigma), **kwargs)\r\n  File \"/home/user/Documents/stable-diffusion-webui/repositories/k-diffusion/k_diffusion/external.py\", line 138, in get_eps\r\n    return self.inner_model.apply_model(*args, **kwargs)\r\n  File \"/home/user/Documents/stable-diffusion-webui/repositories/stable-diffusion/ldm/models/diffusion/ddpm.py\", line 987, in apply_model\r\n    x_recon = self.model(x_noisy, t, **cond)\r\n  File \"/home/user/miniconda3/envs/ldm/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1117, in _call_impl\r\n    result = hook(self, input)\r\n  File \"/home/user/Documents/stable-diffusion-webui/modules/lowvram.py\", line 36, in send_me_to_gpu\r\n    module.to(gpu)\r\n  File \"/home/user/miniconda3/envs/ldm/lib/python3.8/site-packages/pytorch_lightning/core/mixins/device_dtype_mixin.py\", line 121, in to\r\n    return super().to(*args, **kwargs)\r\n  File \"/home/user/miniconda3/envs/ldm/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 907, in to\r\n    return self._apply(convert)\r\n  File \"/home/user/miniconda3/envs/ldm/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 578, in _apply\r\n    module._apply(fn)\r\n  File \"/home/user/miniconda3/envs/ldm/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 578, in _apply\r\n    module._apply(fn)\r\n  File \"/home/user/miniconda3/envs/ldm/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 578, in _apply\r\n    module._apply(fn)\r\n  [Previous line repeated 5 more times]\r\n  File \"/home/user/miniconda3/envs/ldm/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 601, in _apply\r\n    param_applied = fn(param)\r\n  File \"/home/user/miniconda3/envs/ldm/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 905, in convert\r\n    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\r\nRuntimeError: HIP out of memory. Tried to allocate 20.00 MiB (GPU 0; 4.00 GiB total capacity; 3.69 GiB already allocated; 122.00 MiB free; 3.85 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_HIP_ALLOC_CONF\r\n```\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Launch webui.py or launch.py with --medvram --no-half --always-batch-cond-uncond\r\n2. Try to generate an image of any size in the txt2img tab.\r\n3. Get an error and an OOM message no matter how small the image size is.\r\n\r\n**Expected behavior**\r\nTo be able to use --no-half and generate images up to 448x448 as I was able to before as recently as a few days ago.\r\n\r\n**Desktop (please complete the following information):**\r\n - OS: Linux\r\n - Browser: Firefox\r\n - Commit revision: 3061cdb7b610d4ba7f1ea695d9d6364b591e5bc7\r\n\r\n**Additional context**\r\nWebui does not let go of the VRAM allocated until terminating it.\r\n``` --lowvram ``` seems to work as it did before, allowing me to generate 448x448, albeit at a much lower speed.\r\n\r\nEDIT: I think --medvram is either not working, or not working as effectively as before in reducing VRAM usage. I'm no longer able to generate 576x576 images with --medvram --always-batch-cond-uncond --opt-split-attention now either. Going back to before vae/hypernetwork support was added allows me to generate 576x576 images with the --medvram flag again. Screenshots of commit a5a08b0bee531f4402525cae81cdca0013141dfa.\r\n\r\n![during](https://user-images.githubusercontent.com/8052832/194719843-4cb28460-5f6c-4e9e-b43d-fca603ddd5ed.jpg)\r\n![after](https://user-images.githubusercontent.com/8052832/194719848-c40518fd-a01c-403c-b779-bb45d2aea1ad.jpg)\r\n\r\nFurther context, I was not using anything with VAE weights loaded or a hypernetwork selected.\r\n\r\nEdit: To be thorough, bad7cb29cecac51c5c0f39afec332b007ed73133 is the exact commit it stops working. With 2995107fa24cfd72b0a991e18271dcde148c2807 being the last commit prior to it that works.\r\n\r\nEdit: Being more thorough I tried both these commits without --medvram. On 2995107fa24cfd72b0a991e18271dcde148c2807 I was able to generate a 512x512 image without going OOM. While on bad7cb29cecac51c5c0f39afec332b007ed73133 I ran OOM trying to generate a 512x512. It's not that the flag is no longer working, but rather the overall VRAM usage has gone up since the addition of hypernetwork support. No hypernets were selected at this time.\r\n\r\nEdit: Removing the hypernets folder an its contents entirely reduces VRAM usage. I am yet again able to generate 576x576 images on the latest commit. I think what is happening is that the hypernets are being loaded into VRAM by being present and stay in VRAM regardless of if you select one to use. \r\n\r\nEdit: Can also confirm --no-half works again for 448x448. I guess if there was anything to \"fix\" is an option to unload hypernets from VRAM when not explicitly selected. Otherwise, a workaround is to remove them when not intended for use.\n", "hints_text": "", "created_at": "2022-10-09T02:52:38Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 1851, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-1851", "issue_numbers": ["576"], "base_commit": "2995107fa24cfd72b0a991e18271dcde148c2807", "patch": "diff --git a/launch.py b/launch.py\nindex 75edb66a970..a592e1ba7a4 100644\n--- a/launch.py\n+++ b/launch.py\n@@ -4,6 +4,7 @@\n import sys\r\n import importlib.util\r\n import shlex\r\n+import platform\r\n \r\n dir_repos = \"repositories\"\r\n dir_tmp = \"tmp\"\r\n@@ -31,6 +32,7 @@ def extract_arg(args, name):\n \r\n \r\n args, skip_torch_cuda_test = extract_arg(args, '--skip-torch-cuda-test')\r\n+args, xformers = extract_arg(args, '--xformers')\r\n \r\n \r\n def repo_dir(name):\r\n@@ -124,6 +126,12 @@ def git_clone(url, dir, name, commithash=None):\n if not is_installed(\"clip\"):\r\n     run_pip(f\"install {clip_package}\", \"clip\")\r\n \r\n+if not is_installed(\"xformers\") and xformers:\r\n+    if platform.system() == \"Windows\":\r\n+        run_pip(\"install https://github.com/C43H66N12O12S2/stable-diffusion-webui/releases/download/a/xformers-0.0.14.dev0-cp310-cp310-win_amd64.whl\", \"xformers\")\r\n+    elif platform.system() == \"Linux\":\r\n+        run_pip(\"install xformers\", \"xformers\")\r\n+\r\n os.makedirs(dir_repos, exist_ok=True)\r\n \r\n git_clone(\"https://github.com/CompVis/stable-diffusion.git\", repo_dir('stable-diffusion'), \"Stable Diffusion\", stable_diffusion_commit_hash)\r\ndiff --git a/modules/sd_hijack.py b/modules/sd_hijack.py\nindex a6fa890c424..5b30539fefc 100644\n--- a/modules/sd_hijack.py\n+++ b/modules/sd_hijack.py\n@@ -21,11 +21,13 @@\n \r\n def apply_optimizations():\r\n     ldm.modules.diffusionmodules.model.nonlinearity = silu\r\n-\r\n-    if cmd_opts.opt_split_attention_v1:\r\n+    if not cmd_opts.disable_opt_xformers_attention and not (cmd_opts.opt_split_attention or torch.version.hip) and shared.xformers_available:\r\n+        ldm.modules.attention.CrossAttention.forward = sd_hijack_optimizations.xformers_attention_forward\r\n+        ldm.modules.diffusionmodules.model.AttnBlock.forward = sd_hijack_optimizations.xformers_attnblock_forward\r\n+    elif cmd_opts.opt_split_attention_v1:\r\n         ldm.modules.attention.CrossAttention.forward = sd_hijack_optimizations.split_cross_attention_forward_v1\r\n-    elif not cmd_opts.disable_opt_split_attention and (cmd_opts.opt_split_attention or torch.cuda.is_available()):\r\n-        ldm.modules.attention.CrossAttention.forward = sd_hijack_optimizations.split_cross_attention_forward\r\n+    elif cmd_opts.opt_split_attention or torch.cuda.is_available():\r\n+        ldm.modules.attention_CrossAttention_forward = sd_hijack_optimizations.split_cross_attention_forward\r\n         ldm.modules.diffusionmodules.model.AttnBlock.forward = sd_hijack_optimizations.cross_attention_attnblock_forward\r\n \r\n \r\ndiff --git a/modules/sd_hijack_optimizations.py b/modules/sd_hijack_optimizations.py\nindex ea4cfdfcd66..be09ec8f4f5 100644\n--- a/modules/sd_hijack_optimizations.py\n+++ b/modules/sd_hijack_optimizations.py\n@@ -1,7 +1,14 @@\n import math\r\n import torch\r\n from torch import einsum\r\n-\r\n+try:\r\n+    import xformers.ops\r\n+    import functorch\r\n+    xformers._is_functorch_available = True\r\n+    shared.xformers_available = True\r\n+except:\r\n+    print('Cannot find xformers, defaulting to split attention. Try setting --xformers in your webui-user file if you wish to install it.')\r\n+    continue\r\n from ldm.util import default\r\n from einops import rearrange\r\n \r\n@@ -92,6 +99,25 @@ def split_cross_attention_forward(self, x, context=None, mask=None):\n \r\n     return self.to_out(r2)\r\n \r\n+def xformers_attention_forward(self, x, context=None, mask=None):\r\n+    h = self.heads\r\n+    q_in = self.to_q(x)\r\n+    context = default(context, x)\r\n+    hypernetwork = shared.selected_hypernetwork()\r\n+    hypernetwork_layers = (hypernetwork.layers if hypernetwork is not None else {}).get(context.shape[2], None)\r\n+    if hypernetwork_layers is not None:\r\n+        k_in = self.to_k(hypernetwork_layers[0](context))\r\n+        v_in = self.to_v(hypernetwork_layers[1](context))\r\n+    else:\r\n+        k_in = self.to_k(context)\r\n+        v_in = self.to_v(context)\r\n+    q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b n h d', h=h), (q_in, k_in, v_in))\r\n+    del q_in, k_in, v_in\r\n+    out = xformers.ops.memory_efficient_attention(q, k, v, attn_bias=None)\r\n+\r\n+    out = rearrange(out, 'b n h d -> b n (h d)', h=h)\r\n+    return self.to_out(out)\r\n+\r\n def cross_attention_attnblock_forward(self, x):\r\n         h_ = x\r\n         h_ = self.norm(h_)\r\n@@ -154,3 +180,13 @@ def cross_attention_attnblock_forward(self, x):\n         h3 += x\r\n \r\n         return h3\r\n+    \r\n+def xformers_attnblock_forward(self, x):\r\n+        h_ = x\r\n+        h_ = self.norm(h_)\r\n+        q1 = self.q(h_).contiguous()\r\n+        k1 = self.k(h_).contiguous()\r\n+        v = self.v(h_).contiguous()\r\n+        out = xformers.ops.memory_efficient_attention(q1, k1, v)\r\n+        out = self.proj_out(out)\r\n+        return x+out\r\ndiff --git a/modules/shared.py b/modules/shared.py\nindex 25bb6e6c944..6ed4b802120 100644\n--- a/modules/shared.py\n+++ b/modules/shared.py\n@@ -43,6 +43,7 @@\n parser.add_argument(\"--scunet-models-path\", type=str, help=\"Path to directory with ScuNET model file(s).\", default=os.path.join(models_path, 'ScuNET'))\r\n parser.add_argument(\"--swinir-models-path\", type=str, help=\"Path to directory with SwinIR model file(s).\", default=os.path.join(models_path, 'SwinIR'))\r\n parser.add_argument(\"--ldsr-models-path\", type=str, help=\"Path to directory with LDSR model file(s).\", default=os.path.join(models_path, 'LDSR'))\r\n+parser.add_argument(\"--disable-opt-xformers-attention\", action='store_true', help=\"force-disables xformers attention optimization\")\r\n parser.add_argument(\"--opt-split-attention\", action='store_true', help=\"force-enables cross-attention layer optimization. By default, it's on for torch.cuda and off for other torch devices.\")\r\n parser.add_argument(\"--disable-opt-split-attention\", action='store_true', help=\"force-disables cross-attention layer optimization\")\r\n parser.add_argument(\"--opt-split-attention-v1\", action='store_true', help=\"enable older version of split attention optimization that does not consume all the VRAM it can find\")\r\n@@ -73,7 +74,7 @@\n \r\n batch_cond_uncond = cmd_opts.always_batch_cond_uncond or not (cmd_opts.lowvram or cmd_opts.medvram)\r\n parallel_processing_allowed = not cmd_opts.lowvram and not cmd_opts.medvram\r\n-\r\n+xformers_available = False\r\n config_filename = cmd_opts.ui_settings_file\r\n \r\n \r\ndiff --git a/requirements.txt b/requirements.txt\nindex 631fe616adb..81641d68fa1 100644\n--- a/requirements.txt\n+++ b/requirements.txt\n@@ -23,3 +23,4 @@ resize-right\n torchdiffeq\r\n kornia\r\n lark\r\n+functorch\r\ndiff --git a/requirements_versions.txt b/requirements_versions.txt\nindex fdff2687873..fec3e9d5b72 100644\n--- a/requirements_versions.txt\n+++ b/requirements_versions.txt\n@@ -22,3 +22,4 @@ resize-right==0.0.2\n torchdiffeq==0.2.3\r\n kornia==0.6.7\r\n lark==1.1.2\r\n+functorch==0.2.1\r\n", "test_patch": "", "problem_statement": "New memory efficient cross attention\nYou've maybe already come across this but just in case I thought I'd add it here as a feature request.\r\n\r\nUp to 2x speedup on GPUs using memory efficient attention\r\nhttps://github.com/huggingface/diffusers/pull/532\r\n\r\n\r\nThanks for all your contributions!\n", "hints_text": "Hello, I am the author of the PR on the HF repository. If your code is based on CompVis, just copy pasting my modifications to this [file](https://github.com/MatthieuTPHR/diffusers/blob/memory_efficient_attention/src/diffusers/models/attention.py) should work. \r\n\r\nI think the HF `attention.py ` is copy-pasted from CompVis so hopefully this should be easy for you to integrate :) \r\n\r\n\r\nHere is my setup to make it work: \r\n\r\n```\r\nsudo docker run -it --gpus=all --ipc=host -v /home:/home nvcr.io/nvidia/pytorch:22.08-py3 bash\r\n\r\n# Then \r\npip install git+https://github.com/facebookresearch/xformers@51dd119#egg=xformers\r\npip install transformers ftfy scipy\r\n\r\n# Followed by\r\ncd PATH_TO_DIFFUSER_FOLDER\r\ngit checkout memory_efficient_attention\r\npip install -e . \r\n```\r\n\r\nThe acceleration is GPU dependent, my experiments were made on an A6000, I will add more comparisons on other GPUs soon\n@MatthieuTPHR \r\n\r\nThis repo uses Doggetx's attention and opts to hijack the CompVis repo instead of shipping a fork. I plan to work on this but the major pain point is the xformers dependency which is a pain to install. Ideally, HF or somebody would distribute a prebuilt version so that we could make it a necessary dependancy.\r\n\r\n@AUTOMATIC1111 How do you think the xformers dependency should be handled?\n@C43H66N12O12S2 I agree, this is the main reason I converted my PR to a draft. Will update you when we find a clean way of handling xformers. \nI can't even install xformers. Pip dies when trying to make docs. \r\n\r\n```\r\n  Running command git submodule update --init --recursive -q\r\n  error: unable to create file docs/classcutlass_1_1epilogue_1_1thread_1_1LinearCombinationRelu_3_01ElementOutput___00_01Count_00_014d4e40c4295be6a8d8778d86e94fe14a.html: Filename too long\r\n  error: unable to create file docs/classcutlass_1_1epilogue_1_1thread_1_1LinearCombinationRelu_3_01ElementOutput___00_01Count_00_01int_00_01float_00_01Round_01_4.html: Filename too long\r\n  ...\r\n  error: unable to create file examples/29_ampere_3xtf32_fast_accurate_tensorop_complex_gemm/29_ampere_3xtf32_fast_accurate_tensorop_complex_gemm.cu: Filename too long\r\n  fatal: Unable to checkout '319a389f42b776fae5701afcb943fc03be5b5c25' in submodule path 'third_party/flash-attention/csrc/flash_attn/cutlass'\r\n  fatal: Failed to recurse into submodule path 'third_party/flash-attention'\r\n  error: subprocess-exited-with-error\r\n```\nThis speedup is only available for GPUs, so make sure to build the env on a GPU equipped machine.\r\nIt would also most likely for only work for Nvidia GPUs\r\nI find that using the Nvidia NGC containers make the process a bit easier. \r\n\nIs this even applicable to Windows?\n@AUTOMATIC1111 Yes, I tried very hard as well and I believe it is impossible to build on Windows - a common theme in Meta projects. Your error is actually easily solvable but there are many other blockers.\r\n\r\nThis is impossible to implement until - if - prebuilt packages are released.\nWe could just try copying code if it's not too much. Or are they using compiled stuff?\nThey're using memory_efficient_attention which I believe depends on CUDA code - and those error out as well. Somebody skilled in cross compilation could build it, I suppose.\nInstalls easily on colab, good place to start to try adapting attention.py\nYup colab helped me many times to lookup dependencies and solve them by copying every damn thing\nI believe the actual attention.py patch would be easy to implement (hopefully), the major roadblock here is getting xformers running in the first place.\r\n\r\nSure, I could run it on Colab - but that wouldn't help this repo as its users wouldn't be able to run it locally - me neither - which kind of renders the whole effort pointless.\nI am not very familiar with the users of this repo, may I ask what kind of environment do they use (OS, GPU, .. ) .\r\n\r\nI got comparable speed up using TensorRT for the unet but that's not the easiest set up either :/\nWe don't have concrete stats but this repo has been oriented towards Windows and Nvidia from the start so I would imagine the overwhelming majority are on Nvidia and Windows.\n@C43H66N12O12S2 thank you, in that case maybe TensorRT could be a better solution for some of them, it comes pre-installed in the [NVidia docker images](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/pytorch). \r\nFor a given release / GPU pair, the community only needs to compile the UNet once and you can benefit from the speed up directly\r\n\r\nNVFuser can also be a good option\nAFAIK TRT engines need to be localized to each machine running inference and it would preclude dynamic resolution pairs - is this wrong?\r\n\r\nTS would require a refactor of the codebase due to use of **kwargs and lambdas among other things but it is certainly the easiest to implement. Certainly easier than TensorRT :)\n> AFAIK TRT engines need to be localized to each machine running inference and it would preclude dynamic resolution pairs - is this wrong?\r\n> \r\n> TS would require a refactor of the codebase due to use of **kwargs and lambdas among other things but it is certainly the easiest to implement. Certainly easier than TensorRT :)\r\n\r\nFor TRT the easiest way to use it from Torch, is to save the TRT engine and then [embed it into a torchscript](https://pytorch.org/TensorRT/tutorials/torchtrtc.html) checkpoint using [Torch-TensorRT](https://pytorch.org/TensorRT/). \r\n\r\nFor the conversion from pytorch to TRT, if you can do it using Torch-TensorRT then that's perfect. \r\nOtherwise it is possible to convert your model to ONNX and then use the TensorRT cli `trtexec` to get the model in TensorRT before embedding it in a torchscript. \r\n\r\nThe nice thing is that if one member of the community compiles it with a specific GPU, every other member of the community using the same GPU will be able to use it through `torch.jit.load`. \r\n\r\nHere is a tutorial on how to do that: [Link](https://www.photoroom.com/tech/stable-diffusion-25-percent-faster-and-save-seconds/) using the HF unet implementation. \r\n\r\n\r\nSome speedup info for the A10 gpu: \r\n   -  Pytorch Implementation FP16 : 8.7 iterations / second\r\n   - TensorRT implementation FP16: 12.5 iterations / second\r\n   - Memory Efficient Attention implementation FP16: 15.6 iterations / second\r\nThe Pytorch result above is without NVFuser but it should be a bit below the TRT version.\n> The nice thing is that if one member of the community compiles it with a specific GPU, every other member of the community using the same GPU will be able to use it through torch.jit.load.\r\n\r\nI wasn't aware of this, that's a neat trick. I doubt TensorRT will be added though, it's too much work both from the user and the developer - especially since nvFuser through TS exists. Maybe @AUTOMATIC1111 could do that as - being the creator - they're far more familiar with the codebase than I am.\r\n\r\nAnyhow, memory efficient attention should work with nvFuser or plain PyTorch, so I'll keep an eye on this and implement it if anybody decides to release a xformers build - or the impossible happens and Meta cares about Windows.\nI've worked on this for a bit with WSL2 and NGC container.\r\n\r\n1 - As I guessed, the actual implementation would be the easiest bit. I think simply replacing r1 with xformers.ops.memory_efficient_attention would be enough. (and deleting the for loop, of course)\r\n\r\n2 - The stable version of xformers lacks AttentionOpDispatch and the trunk failed to compile, probably due to oom-killer killing the compiler.\r\n\r\n3 - I believe nvcc cross-compilation between Windows-Linux is not possible - or at least not documented anywhere.\r\n\r\n4 - We may need to use the maybe_init function as the default selected kernel for me was FP32-only, though that might be due to using the stable release (0.0.12)\r\n\r\n5 - AFAIK the current implementation lacks masking support, so we would need to check that as well.\r\n\r\n6 - If we implement this, it will most likely be Linux/WSL2 only, as there is no Windows release and I've failed to find a way to cross-compile.\n@C43H66N12O12S2 I used wsl2 linux, and xformers compiled successfully, and I got this error when I try SD: \r\n\r\n`Could not run 'xformers::efficient_attention_forward_generic' with arguments from the 'CUDA' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build)`\n> Anyhow, memory efficient attention should work with nvFuser or plain PyTorch, so I'll keep an eye on this and implement it if anybody decides to release a xformers build - or the impossible happens and Meta cares about Windows.\r\n\r\nso the trick is that mem efficient attention has to be implemented at the kernel level, there are a couple of existing takes around (~3 in xformers, one in the triton repo) and they all do that. It amounts to computing `softmax(QKt).V` in a very different way from what the naive matmuls would do, which is what the pytorch primitives will give you (they will materialize QKt which will both kill the memory and slow things down). \r\n\r\nYou can dive into @tridao's paper to know more (Flash attention), but rest assured that this is not done this way to be a nuisance for windows users, it's just non trivial stuff and everyone has a limited scope somehow.\n> @C43H66N12O12S2 I used wsl2 linux, and xformers compiled successfully, and I got this error when I try SD:\r\n> \r\n> `Could not run 'xformers::efficient_attention_forward_generic' with arguments from the 'CUDA' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build)`\r\n\r\nthis means that the setup did not compile the cuda extensions, xformers should probably be more verbose about that (you can pip install with `--verbose` to know more). It's often because of mismatching cuda backend in the torch pre-built that you have, and the available nvcc compiler, the setup just cannot compile anything cuda related at that point since it will not be compatible with your torch. \r\n\r\nIn short the cuda toolkit and the torch that you have should be aligned cuda-version-wise, and that's unfortunately hard to get right. Conda or docker images are your friend\n> > Anyhow, memory efficient attention should work with nvFuser or plain PyTorch, so I'll keep an eye on this and implement it if anybody decides to release a xformers build - or the impossible happens and Meta cares about Windows.\r\n> \r\n> so the trick is that mem efficient attention has to be implemented at the kernel level, there are a couple of existing takes around (~3 in xformers, one in the triton repo) and they all do that. It amounts to computing `softmax(QKt).V` in a very different way from what the naive matmuls would do, which is what the pytorch primitives will give you (they will materialize QKt which will both kill the memory and slow things down).\r\n> \r\n> You can dive into @tridao's paper to know more (Flash attention), but rest assured that this is not done this way to be a nuisance for windows users, it's just non trivial stuff and everyone has a limited scope somehow.\r\n\r\n@blefaudeux \r\n\r\nOh, I don't believe you're malicious or anything like that - I was just venting my frustration after trying and failing to produce a build for nearly 2 hours :) I apologize if I have caused offence.\r\n\r\nIt would be really helpful if Meta would distribute a Windows package - then we could implement this. I've learned from my initial attempts that there are a few easy to fix blockers (mostly explicitly setting the Cpp STD version for MSVC) but there are code-related issues, such as use of GCC pragmas and Windows NVCC incompatible code which are beyond my skill to fix.\n> @C43H66N12O12S2 I used wsl2 linux, and xformers compiled successfully, and I got this error when I try SD:\r\n> \r\n> `Could not run 'xformers::efficient_attention_forward_generic' with arguments from the 'CUDA' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build)`\r\n\r\nIs this the first time you've setup WSL2? If so, you're probably missing CUDA. It's crucial you don't install any Linux Nvidia driver as the Windows driver ships a WSL compatible libcuda.so https://docs.nvidia.com/cuda/wsl-user-guide/index.html\n> > > Anyhow, memory efficient attention should work with nvFuser or plain PyTorch, so I'll keep an eye on this and implement it if anybody decides to release a xformers build - or the impossible happens and Meta cares about Windows.\r\n> > \r\n> > \r\n> > so the trick is that mem efficient attention has to be implemented at the kernel level, there are a couple of existing takes around (~3 in xformers, one in the triton repo) and they all do that. It amounts to computing `softmax(QKt).V` in a very different way from what the naive matmuls would do, which is what the pytorch primitives will give you (they will materialize QKt which will both kill the memory and slow things down).\r\n> > You can dive into @tridao's paper to know more (Flash attention), but rest assured that this is not done this way to be a nuisance for windows users, it's just non trivial stuff and everyone has a limited scope somehow.\r\n> \r\n> @blefaudeux\r\n> \r\n> Oh, I don't believe you're malicious or anything like that - I was just venting my frustration after trying and failing to produce a build for nearly 2 hours :) I apologize if I have caused offence.\r\n> \r\n> It would be really helpful if Meta would distribute a Windows package - then we could implement this. I've learned from my initial attempts that there are a few easy to fix blockers (mostly explicitly setting the Cpp STD version for MSVC) but there are code-related issues, such as use of GCC pragmas and Windows NVCC incompatible code which are beyond my skill to fix.\r\n\r\nI think that you may overestimate the company investment in this repo, this is under facebookresearch and was initially purely research code. I'm personally grateful that this got to be opensource so that many people could benefit from it for free, tinker and run with it as it made sense to them, but a commercially supported product this is not. Windows is not really used for ML research, so if there's a missing feature here, feel free to add it. Not being happy that it's not there out of the box is missing the forest for fhe tree, but that's only my take.\r\n\r\nDisclaimer in that I'm not at Meta anymore, but I think that this is true of most /facebookresearch repos, and I'm still glad that they're here.\nTested on GTX 1070ti :\r\nWithout Memory efficient cross attention at 512x512 : **1.78 it/s**\r\n\r\nWith Memory efficient cross attention at 512x512 : **2.34 it/s**\r\n\r\n**+31%** increase in speed \r\n\r\nwith optimized SD + Dogettx : at 512x512 : **2.0 it/s** (+12%  increase in speed)\r\n\r\nWith Only Doggettx modification, the speed isn't affected : **2.34 it/s**\r\n\n> Tested on GTX 1070ti : Without Memory efficient cross attention at 512x512 : **1.78 it/s**\r\n> \r\n> With Memory efficient cross attention at 512x512 : **2.34 it/s**\r\n> \r\n> **+31%** increase in speed\r\n> \r\n> with optimized SD + Dogettx : at 512x512 : **2.0 it/s** (+12% increase in speed)\r\n> \r\n> With Only Doggettx modification, the speed isn't affected : **2.34 it/s**\r\n\r\nthanks for the numbers, it's better with the newer architectures for sure.. Have you checked the ram use ? This should be the same regardless of the GPU arch, massive improvement\nIs there an updated way to run this?  Is pasting Matthieu's code into attention.py the recommended way to going about it? \n@blefaudeux after rechecking, I found out that there is actually a huge improvement of VRAM consumption, resolution from 512x768 max to 1024x832 max.\r\n\r\n\nCan you please explain exactly how you can implement this optimization? It's probably obvious for someone familiar with the code but it's less so for the not so knowledgeable. \n> Can you please explain exactly how you can implement this optimization? It's probably obvious for someone familiar with the code but it's less so for the not so knowledgeable.\r\n\r\nUse this Docker image nvcr.io/nvidia/pytorch:22.08-py3 and clone AUTOMATIC1111's stable diffusion in it, pip install the requirements manually (don't install torch and torchvision or CUDA, they are already installed), then install xformers with : \r\n`pip install git+https://github.com/facebookresearch/xformers@51dd119#egg=xformers`\r\nand `pip install triton==2.0.0.dev20220701`\r\n\r\nMake sure you're not using OptimizedSD, \r\n\r\nBackup your **attention.py**, then :\r\n\r\nReplace the **class BasicTransformerBlock(nn.Module)** in attention.py with :\r\n\r\n```\r\nclass BasicTransformerBlock(nn.Module):\r\n    def __init__(self, dim, n_heads, d_head, dropout=0., context_dim=None, gated_ff=True, checkpoint=True):\r\n        super().__init__()\r\n        AttentionBuilder = MemoryEfficientCrossAttention        \r\n        self.attn1 = AttentionBuilder(query_dim=dim, heads=n_heads, dim_head=d_head, dropout=dropout)  # is a self-attention\r\n        self.ff = FeedForward(dim, dropout=dropout, glu=gated_ff)\r\n        self.attn2 = AttentionBuilder(query_dim=dim, context_dim=context_dim,\r\n                                    heads=n_heads, dim_head=d_head, dropout=dropout)  # is self-attn if context is none\r\n        self.norm1 = nn.LayerNorm(dim)\r\n        self.norm2 = nn.LayerNorm(dim)\r\n        self.norm3 = nn.LayerNorm(dim)\r\n        self.checkpoint = checkpoint\r\n        \r\n    def _set_attention_slice(self, slice_size):\r\n        self.attn1._slice_size = slice_size\r\n        self.attn2._slice_size = slice_size\r\n\r\n    def forward(self, hidden_states, context=None):\r\n        hidden_states = hidden_states.contiguous() if hidden_states.device.type == \"mps\" else hidden_states\r\n        hidden_states = self.attn1(self.norm1(hidden_states)) + hidden_states\r\n        hidden_states = self.attn2(self.norm2(hidden_states), context=context) + hidden_states\r\n        hidden_states = self.ff(self.norm3(hidden_states)) + hidden_states\r\n        return hidden_states \r\n```       \r\n\r\n\r\nand add this to attention.py :\r\n\r\n```\r\nclass MemoryEfficientCrossAttention(nn.Module):\r\n    def __init__(self, query_dim, context_dim=None, heads=8, dim_head=64, dropout=0.0):\r\n        super().__init__()\r\n        inner_dim = dim_head * heads\r\n        context_dim = default(context_dim, query_dim)\r\n\r\n        self.scale = dim_head**-0.5\r\n        self.heads = heads\r\n        self.dim_head = dim_head\r\n\r\n        self.to_q = nn.Linear(query_dim, inner_dim, bias=False)\r\n        self.to_k = nn.Linear(context_dim, inner_dim, bias=False)\r\n        self.to_v = nn.Linear(context_dim, inner_dim, bias=False)\r\n\r\n        self.to_out = nn.Sequential(nn.Linear(inner_dim, query_dim), nn.Dropout(dropout))\r\n        self.attention_op: Optional[Any] = None\r\n\r\n    def _maybe_init(self, x):\r\n        \"\"\"\r\n        Initialize the attention operator, if required We expect the head dimension to be exposed here, meaning that x\r\n        : B, Head, Length\r\n        \"\"\"\r\n        if self.attention_op is not None:\r\n            return\r\n\r\n        _, K, M = x.shape\r\n        try:\r\n            self.attention_op = xformers.ops.AttentionOpDispatch(\r\n                dtype=x.dtype,\r\n                device=x.device,\r\n                k=K,\r\n                attn_bias_type=type(None),\r\n                has_dropout=False,\r\n                kv_len=M,\r\n                q_len=M,\r\n            ).op\r\n\r\n        except NotImplementedError as err:\r\n            raise NotImplementedError(f\"Please install xformers with the flash attention / cutlass components.\\n{err}\")\r\n\r\n    def forward(self, x, context=None, mask=None):\r\n        q = self.to_q(x)\r\n        context = default(context, x)\r\n        k = self.to_k(context)\r\n        v = self.to_v(context)\r\n\r\n        b, _, _ = q.shape\r\n        q, k, v = map(\r\n            lambda t: t.unsqueeze(3)\r\n            .reshape(b, t.shape[1], self.heads, self.dim_head)\r\n            .permute(0, 2, 1, 3)\r\n            .reshape(b * self.heads, t.shape[1], self.dim_head)\r\n            .contiguous(),\r\n            (q, k, v),\r\n        )\r\n\r\n        # init the attention op, if required, using the proper dimensions\r\n        self._maybe_init(q)\r\n\r\n        # actually compute the attention, what we cannot get enough of\r\n        out = xformers.ops.memory_efficient_attention(q, k, v, attn_bias=None, op=self.attention_op)\r\n\r\n        # TODO: Use this directly in the attention operation, as a bias\r\n        if exists(mask):\r\n            raise NotImplementedError\r\n        out = (\r\n            out.unsqueeze(0)\r\n            .reshape(b, self.heads, out.shape[1], self.dim_head)\r\n            .permute(0, 2, 1, 3)\r\n            .reshape(b, out.shape[1], self.heads * self.dim_head)\r\n        )\r\n        return self.to_out(out)\r\n```\r\n\n> @blefaudeux after rechecking, I found out that there is actually a huge improvement of VRAM consumption, resolution from 512x768 max to 1024x832 max.\r\n\r\nahah, I was surprised.. ok, this was expected :D\nDoes this require an RTX card? I tried it out but it gave me an error about a lack of Tensor cores.\n> Does this require an RTX card? I tried it out but it gave me an error about a lack of Tensor cores.\r\n\r\nif flash attention doesn't support your hardware, you still get performance increase by at least 25%\r\n\nSeems like using this new cross attention is limited to certain platforms, but possible.\r\nIt would be great if support for it was integrated as an cli-option or auto-detect feature with possibility to switch it on/off in UI.\n> if flash attention doesn't support your hardware, you still get performance increase by at least 25%\n>\n\nI didn't really see much of an improvement. I edited `attention.py`, as suggested by TheLastBen. The file was located at `lib/python3.8/site-packages/diffusers/models`.\n> > if flash attention doesn't support your hardware, you still get performance increase by at least 25%\r\n> \r\n> I didn't really see much of an improvement. I edited `attention.py`, as suggested by TheLastBen. The file was located at `lib/python3.8/site-packages/diffusers/models`.\r\n\r\nYou need to have xformers installed and compiled (takes 50min)\n> > > if flash attention doesn't support your hardware, you still get performance increase by at least 25%\n> > \n> > I didn't really see much of an improvement. I edited `attention.py`, as suggested by TheLastBen. The file was located at `lib/python3.8/site-packages/diffusers/models`.\n> \n> You need to have xformers installed and compiled (takes 50min)\n\nAlready done that. Even ran the test script from the xformers repo to check.\nYou're using diffusers ?\nUse this one for diffusers : https://github.com/TheLastBen/diffusers/blob/main/src/diffusers/models/attention.py\n> Use this one for diffusers : https://github.com/TheLastBen/diffusers/blob/main/src/diffusers/models/attention.py\n\nI tried out this file but didn't notice any improvements at all compared to the times I got with the original `attention.py`. I tested with --lowvram, --medvram and no flag.\nJust to be sure, I need to replace the `attention.py` file located in `<xformers-venv>/lib/python3.8/site-packages/diffusers/models/`, right?\nI installed xformers in a conda venv as instructed in the xformers readme. \nEdit: Clarification: I setup stable-diffusion-webui in the xformers conda env.\nwould it be possible for someone to set up a docker image ready to be used with these changes?\n@sbstratos79 I think you got it wrong, what os are you using ?\r\nyou should install xformers with pip : `pip install git+https://github.com/facebookresearch/xformers@51dd119#egg=xformers`\n> @sbstratos79 I think you got it wrong, what os are you using ?\n> you should install xformers with pip : `pip install git+https://github.com/facebookresearch/xformers@51dd119#egg=xformers`\n\nTried this version as well. Still no improvement. I am using Linux. EndeavourOS. Kernel is 5.19.12\nwhich fork of stable diffusion are you using ?\n> which fork of stable diffusion are you using ?\n\nAutomatic1111\nYou mentioned \"--lowvram and --medvram\", it looks like you're using the fork AUTOMATIC1111, not the diffusers version, if it is the case, the attention.py should be placed in \"ldm/modules\" in the fork's folder. you can find the attention.py in the before last cell in this colab \r\nhttps://colab.research.google.com/github/TheLastBen/fast-stable-diffusion/blob/main/fast_stable_diffusion_AUTOMATIC1111.ipynb\r\n(remove the first two lines)\n> You mentioned \"--lowvram and --medvram\", it looks like you're using the fork AUTOMATIC1111, not the diffusers version, if it is the case, the attention.py should be placed in \"ldm/modules\" in the fork's folder. you can find the attention.py in the before last cell in this colab \n> https://colab.research.google.com/github/TheLastBen/fast-stable-diffusion/blob/main/fast_stable_diffusion_AUTOMATIC1111.ipynb\n> (remove the first two lines)\n\nThanks! It's finally working. I had just replaced the wrong `attention.py` file.\nGreat !", "created_at": "2022-10-07T02:28:30Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 1844, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-1844", "issue_numbers": ["1049", "1049"], "base_commit": "2995107fa24cfd72b0a991e18271dcde148c2807", "patch": "diff --git a/scripts/xy_grid.py b/scripts/xy_grid.py\nindex 6344e612f15..6e618744c52 100644\n--- a/scripts/xy_grid.py\n+++ b/scripts/xy_grid.py\n@@ -190,7 +190,9 @@ def ui(self, is_img2img):\n         return [x_type, x_values, y_type, y_values, draw_legend, no_fixed_seeds]\r\n \r\n     def run(self, p, x_type, x_values, y_type, y_values, draw_legend, no_fixed_seeds):\r\n-        modules.processing.fix_seed(p)\r\n+        if not no_fixed_seeds:\r\n+            modules.processing.fix_seed(p)\r\n+\r\n         p.batch_size = 1\r\n \r\n         def process_axis(opt, vals):\r\n", "test_patch": "", "problem_statement": "Keep -1 for seeds on X/Y plot checkbox isn't respected.\n**Describe the bug**\r\nKeep -1 for seeds on X/Y plot checkbox isn't respected\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. txt2img\r\n2. use Script, X/Y Plot, enter values, check \"Keep -1 for seeds on X/Y plot\"\r\n3. Generate\r\n4. See that all the seeds are all the same on both X and Y\r\n\r\n**Expected behavior**\r\nNew seed for each image generated\r\n\r\n\r\n**Desktop (please complete the following information):**\r\n - OS: Windows\r\n - Browser: Edge\r\n - ca3e5519e8b6dc020c5e7ae508738afb5dc6f3ec\r\n\r\n\nKeep -1 for seeds on X/Y plot checkbox isn't respected.\n**Describe the bug**\r\nKeep -1 for seeds on X/Y plot checkbox isn't respected\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. txt2img\r\n2. use Script, X/Y Plot, enter values, check \"Keep -1 for seeds on X/Y plot\"\r\n3. Generate\r\n4. See that all the seeds are all the same on both X and Y\r\n\r\n**Expected behavior**\r\nNew seed for each image generated\r\n\r\n\r\n**Desktop (please complete the following information):**\r\n - OS: Windows\r\n - Browser: Edge\r\n - ca3e5519e8b6dc020c5e7ae508738afb5dc6f3ec\r\n\r\n\n", "hints_text": "It actually matters only for \"Seed\" axis, and completely ignored for anything else!\r\n\r\nIf you want ALL of cells have different seed, you can make a copy of `scripts/xy_grid.py`, rename it to `xy_grid_seed.py` and edit the file.\r\n\r\nYou'll have to change `return \"X/Y plot\"` (next to `def title(self):`) to `return \"X/Y plot (random seed)\"` (it's just the name of the script), and comment out `modules.processing.fix_seed(p)` (next to `def run`), changing it to `#modules.processing.fix_seed(p)`\r\n\r\nIt worked for me today!\r\nTo retrieve the actual seed of generated images, you can refer to filenames in `outputs/txt2img-images/` (or `outputs/img2img-images/`), where individual images will be saved.\r\n\r\nP.S.\r\nDon't forget to restart the server (close console) after you change the scripts.\r\nThen choose \"X/Y plot (random seed)\" instead of \"X/Y plot\" in UI.\nGot hit by this, too.  A better fix is:\r\n\r\n    if not no_fixed_seeds:\r\n        modules.processing.fix_seed(p)\r\n\r\nThat keeps the seed as -1 only if the checkbox is selected. No need to recreate the title for a new script or anything like that. The `process_images` sub will call `fix_seed` later on, but only on the `pc` cell-copied object.\nIt actually matters only for \"Seed\" axis, and completely ignored for anything else!\r\n\r\nIf you want ALL of cells have different seed, you can make a copy of `scripts/xy_grid.py`, rename it to `xy_grid_seed.py` and edit the file.\r\n\r\nYou'll have to change `return \"X/Y plot\"` (next to `def title(self):`) to `return \"X/Y plot (random seed)\"` (it's just the name of the script), and comment out `modules.processing.fix_seed(p)` (next to `def run`), changing it to `#modules.processing.fix_seed(p)`\r\n\r\nIt worked for me today!\r\nTo retrieve the actual seed of generated images, you can refer to filenames in `outputs/txt2img-images/` (or `outputs/img2img-images/`), where individual images will be saved.\r\n\r\nP.S.\r\nDon't forget to restart the server (close console) after you change the scripts.\r\nThen choose \"X/Y plot (random seed)\" instead of \"X/Y plot\" in UI.\nGot hit by this, too.  A better fix is:\r\n\r\n    if not no_fixed_seeds:\r\n        modules.processing.fix_seed(p)\r\n\r\nThat keeps the seed as -1 only if the checkbox is selected. No need to recreate the title for a new script or anything like that. The `process_images` sub will call `fix_seed` later on, but only on the `pc` cell-copied object.", "created_at": "2022-10-06T23:37:02Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 1659, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-1659", "issue_numbers": ["1656"], "base_commit": "b32852ef037251eb3d846af76e2965594e1ac7a5", "patch": "diff --git a/modules/ui.py b/modules/ui.py\nindex 6cd6761b80e..de6342a48f9 100644\n--- a/modules/ui.py\n+++ b/modules/ui.py\n@@ -196,6 +196,11 @@ def f(*args, extra_outputs_array=extra_outputs, **kwargs):\n             res = extra_outputs_array + [f\"<div class='error'>{plaintext_to_html(type(e).__name__+': '+str(e))}</div>\"]\r\n \r\n         elapsed = time.perf_counter() - t\r\n+        elapsed_m = int(elapsed // 60)\r\n+        elapsed_s = elapsed % 60\r\n+        elapsed_text = f\"{elapsed_s:.2f}s\"\r\n+        if (elapsed_m > 0):\r\n+            elapsed_text = f\"{elapsed_m}m \"+elapsed_text\r\n \r\n         if run_memmon:\r\n             mem_stats = {k: -(v//-(1024*1024)) for k, v in shared.mem_mon.stop().items()}\r\n@@ -210,7 +215,7 @@ def f(*args, extra_outputs_array=extra_outputs, **kwargs):\n             vram_html = ''\r\n \r\n         # last item is always HTML\r\n-        res[-1] += f\"<div class='performance'><p class='time'>Time taken: <wbr>{elapsed:.2f}s</p>{vram_html}</div>\"\r\n+        res[-1] += f\"<div class='performance'><p class='time'>Time taken: <wbr>{elapsed_text}</p>{vram_html}</div>\"\r\n \r\n         shared.state.interrupted = False\r\n         shared.state.job_count = 0\r\n", "test_patch": "", "problem_statement": "Display time taken in minutes when over 60 seconds\n**Is your feature request related to a problem? Please describe.**\r\nI am always investigating how my parameters relate to the performance for a run so I can time things better. If I'm stepping off the computer for 15 minutes, it's useful for me to be better understanding how many steps and batch count I can run in that time-frame.\r\n\r\n**Describe the solution you'd like**\r\nDisplay minutes when over 60s, e.g. `Time taken: 3m 35s` instead of `Time taken: 215.08s`\r\n\r\n**Describe alternatives you've considered**\r\n\r\n**Additional context**\r\n\n", "hints_text": "", "created_at": "2022-10-04T19:13:11Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 1560, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-1560", "issue_numbers": ["1435"], "base_commit": "2a532804957e47bc36c67c8f5b104dcfa8e8f3f0", "patch": "diff --git a/launch.py b/launch.py\nindex 57405feab59..50e70a31058 100644\n--- a/launch.py\n+++ b/launch.py\n@@ -19,7 +19,7 @@\n \r\n stable_diffusion_commit_hash = os.environ.get('STABLE_DIFFUSION_COMMIT_HASH', \"69ae4b35e0a0f6ee1af8bb9a5d0016ccb27e36dc\")\r\n taming_transformers_commit_hash = os.environ.get('TAMING_TRANSFORMERS_COMMIT_HASH', \"24268930bf1dce879235a7fddd0b2355b84d7ea6\")\r\n-k_diffusion_commit_hash = os.environ.get('K_DIFFUSION_COMMIT_HASH', \"a7ec1974d4ccb394c2dca275f42cd97490618924\")\r\n+k_diffusion_commit_hash = os.environ.get('K_DIFFUSION_COMMIT_HASH', \"f4e99857772fc3a126ba886aadf795a332774878\")\r\n codeformer_commit_hash = os.environ.get('CODEFORMER_COMMIT_HASH', \"c5b4593074ba6214284d6acd5f1719b6c5d739af\")\r\n blip_commit_hash = os.environ.get('BLIP_COMMIT_HASH', \"48211a1594f1321b00f14c9f7a5b4813144b2fb9\")\r\n \r\ndiff --git a/modules/sd_samplers.py b/modules/sd_samplers.py\nindex dbf570d2ce8..4aa30478fdc 100644\n--- a/modules/sd_samplers.py\n+++ b/modules/sd_samplers.py\n@@ -26,6 +26,17 @@\n     ('DPM adaptive', 'sample_dpm_adaptive', ['k_dpm_ad']),\r\n ]\r\n \r\n+if opts.show_karras_scheduler_variants:\r\n+    k_diffusion.sampling.sample_dpm_2_ka = k_diffusion.sampling.sample_dpm_2\r\n+    k_diffusion.sampling.sample_dpm_2_ancestral_ka = k_diffusion.sampling.sample_dpm_2_ancestral\r\n+    k_diffusion.sampling.sample_lms_ka = k_diffusion.sampling.sample_lms\r\n+    samplers_k_diffusion_ka = [\r\n+        ('LMS K Scheduling', 'sample_lms_ka', ['k_lms_ka']),\r\n+        ('DPM2 K Scheduling', 'sample_dpm_2_ka', ['k_dpm_2_ka']),\r\n+        ('DPM2 a K Scheduling', 'sample_dpm_2_ancestral_ka', ['k_dpm_2_a_ka']),\r\n+    ]\r\n+    samplers_k_diffusion.extend(samplers_k_diffusion_ka)\r\n+\r\n samplers_data_k_diffusion = [\r\n     SamplerData(label, lambda model, funcname=funcname: KDiffusionSampler(funcname, model), aliases)\r\n     for label, funcname, aliases in samplers_k_diffusion\r\n@@ -315,6 +326,8 @@ def sample(self, p, x, conditioning, unconditional_conditioning, steps=None):\n \r\n         if p.sampler_noise_scheduler_override:\r\n           sigmas = p.sampler_noise_scheduler_override(steps)\r\n+        elif self.funcname.endswith('ka'):\r\n+          sigmas = k_diffusion.sampling.get_sigmas_karras(n=steps, sigma_min=0.1, sigma_max=10, device=shared.device)\r\n         else:\r\n           sigmas = self.model_wrap.get_sigmas(steps)\r\n         x = x * sigmas[0]\r\ndiff --git a/modules/shared.py b/modules/shared.py\nindex 2a599e9cf98..cf744763d3d 100644\n--- a/modules/shared.py\n+++ b/modules/shared.py\n@@ -228,6 +228,7 @@ def options_section(section_identifer, options_dict):\n     \"font\": OptionInfo(\"\", \"Font for image grids that have text\"),\r\n     \"js_modal_lightbox\": OptionInfo(True, \"Enable full page image viewer\"),\r\n     \"js_modal_lightbox_initialy_zoomed\": OptionInfo(True, \"Show images zoomed in by default in full page image viewer\"),\r\n+    \"show_karras_scheduler_variants\": OptionInfo(True, \"Show Karras scheduling variants for select samplers. Try these variants if your K sampled images suffer from excessive noise.\"),\r\n }))\r\n \r\n options_templates.update(options_section(('sampler-params', \"Sampler parameters\"), {\r\ndiff --git a/scripts/alternate_sampler_noise_schedules.py b/scripts/alternate_sampler_noise_schedules.py\nnew file mode 100644\nindex 00000000000..4f3ed8fb1f3\n--- /dev/null\n+++ b/scripts/alternate_sampler_noise_schedules.py\n@@ -0,0 +1,53 @@\n+import inspect\n+from modules.processing import Processed, process_images\n+import gradio as gr\n+import modules.scripts as scripts\n+import k_diffusion.sampling\n+import torch\n+\n+\n+class Script(scripts.Script):\n+\n+    def title(self):\n+        return \"Alternate Sampler Noise Schedules\"\n+\n+    def ui(self, is_img2img):\n+      noise_scheduler = gr.Dropdown(label=\"Noise Scheduler\", choices=['Default','Karras','Exponential', 'Variance Preserving'], value='Default', type=\"index\")\n+      sched_smin     = gr.Slider(value=0.1,   label=\"Sigma min\",                  minimum=0.0,   maximum=100.0, step=0.5,)\n+      sched_smax     = gr.Slider(value=10.0,  label=\"Sigma max\",                  minimum=0.0,   maximum=100.0, step=0.5)\n+      sched_rho      = gr.Slider(value=7.0,   label=\"Sigma rho (Karras only)\",    minimum=7.0,   maximum=100.0, step=0.5)\n+      sched_beta_d   = gr.Slider(value=19.9,  label=\"Beta distribution (VP only)\",minimum=0.0,   maximum=40.0, step=0.5)\n+      sched_beta_min = gr.Slider(value=0.1,   label=\"Beta min (VP only)\",         minimum=0.0,   maximum=40.0, step=0.1)\n+      sched_eps_s    = gr.Slider(value=0.001, label=\"Epsilon (VP only)\",          minimum=0.001, maximum=1.0,   step=0.001)\n+\n+      return [noise_scheduler, sched_smin, sched_smax, sched_rho, sched_beta_d, sched_beta_min, sched_eps_s]\n+\n+    def run(self, p, noise_scheduler, sched_smin, sched_smax, sched_rho, sched_beta_d, sched_beta_min, sched_eps_s):\n+\n+      noise_scheduler_func_name = ['-','get_sigmas_karras','get_sigmas_exponential','get_sigmas_vp'][noise_scheduler]\n+\n+      base_params = {\n+        \"sigma_min\":sched_smin, \n+        \"sigma_max\":sched_smax, \n+        \"rho\":sched_rho, \n+        \"beta_d\":sched_beta_d, \n+        \"beta_min\":sched_beta_min, \n+        \"eps_s\":sched_eps_s,\n+        \"device\":\"cuda\" if torch.cuda.is_available() else \"cpu\"\n+      }\n+\n+      if hasattr(k_diffusion.sampling,noise_scheduler_func_name):\n+\n+        sigma_func  = getattr(k_diffusion.sampling,noise_scheduler_func_name)\n+        sigma_func_kwargs = {}\n+\n+        for k,v in base_params.items():\n+          if k in inspect.signature(sigma_func).parameters:\n+            sigma_func_kwargs[k] = v\n+\n+        def substitute_noise_scheduler(n):\n+          return sigma_func(n,**sigma_func_kwargs)\n+\n+        p.sampler_noise_scheduler_override = substitute_noise_scheduler\n+\n+      return process_images(p)\n", "test_patch": "", "problem_statement": "DPM2 ancestral produces odd nosiy/sharpened output during final step\n**Describe the bug**\r\nAfter I enabled the \"show image creation progress\" option (set to 1 step), I noticed the final images generated with DPM2 a looked much worse than the images as previewed, and that interrupting the generation at a step or two before it finished actually produced much better outputs. This might a bug(?) in k-diffusion's implementation of it, but I can't really be sure. I managed to \"fix\" this by changing the `if sigma_down == 0` check in the `sample_dpm_2_ancestral` function to `if i >= len(sigmas) - 3`, this makes it run 2 euler passes instead of just one, which seems to fix the issue, though I don't really understand the underlying cause.\r\nhttps://github.com/crowsonkb/k-diffusion/blob/master/k_diffusion/sampling.py#L161\r\n\r\nEuler a might also exhibit this behavior, but it's much more subtle than DPM2.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Generate fixed seed image with DPM2 a\r\n2. Let the generation complete\r\n3. Save image\r\n4. Generate same seed again, but interrupt it 1 step before completion (timing on this is difficult, you can confirm it on the console progress output).\r\n5. Compare images, image 2 should be much clearer than image 1\r\n\r\n**Expected behavior**\r\nOutput shouldn't get noticeably worse on final step\r\n\r\n**Screenshots**\r\nDefault behavior\r\n![broken](https://user-images.githubusercontent.com/2528634/193407332-27a3c59d-5216-40bd-b17b-c7a25ccbb1fd.png)\r\n\r\n\"Fixed\" by editing k-diffusion's sampling.py as noted above\r\n![fixed](https://user-images.githubusercontent.com/2528634/193407336-1b050eee-40bd-4545-be87-5b3f99c87a73.png)\r\n\r\n\r\n\r\n**Desktop (please complete the following information):**\r\n - OS: Linux/Colab\r\n - Commit revision: 3f417566b0bda8eab05d247567aebf001c1d1725\n", "hints_text": "I had similar effects that looked like jpeg artifacts and were consistent through multiple samplers and seeds and step nums. \r\nThey also happened on the last step, you could see the live preview is fine and the last second it turned ugly.\r\n\r\nI restarted and GIT refreshed the repository, the problem did not appear again. \r\n\r\nMaybe unrelated, maybe already fixed, just thought it might be related to a last-step error\r\n\nI was able to replicate it over multiple restarts and different versions of the webui, so I don't think it's any kind of temporary issue. It could be some inherent issue with the Colab, but I'm pretty much just installing it using the standard launch.py script.\r\n\r\nIt should be pretty easy to reproduce for anyone else trying, to see if it's some specific setup issue.\nI can confirm this, in LMS this also happens, just tried the \"cancel the final render\" thing, and the \"unfinished\" render was way better.\nThat's the issue I had, consistent through samplers, steps and cfg:\r\n![03044-2987284273-a pixar woman in suit, pixar studio, high quality render](https://user-images.githubusercontent.com/78893154/193456285-886de369-16b4-40b5-86d4-d537625544e5.png)\r\n\r\nIt was an img2img from a clean photo.\r\nThe result looked clean until the last refresh, then it was noisy like a low quality JPEG compression, it looks so much like JPEG that it's quite strange \n**_\"a photography of a modern house in the forest, road path, low perspective, white balanced, cinematic light, light rays,raw photography, unreal 5 engine, octane render, rhads, Bruce Pennington, award-winning photograph, masterpiece, hyper photorealistic\"_**\r\nSteps: 40, Sampler: LMS, CFG scale: 20, Seed: 4291835239, Size: 512x768\r\n\r\nMore proof that interrupting the render reveals more quality:\r\n\r\n![00029-4291835239-40-20-LMS](https://user-images.githubusercontent.com/61936167/193460682-4483174c-83de-487d-9746-3107c7cea8c9.jpg)\r\n\r\n--\r\n**IS OFF:**\r\nEnable quantization in K samplers for sharper and cleaner results. This may change existing seeds. Requires restart to apply.\r\n\nThe problem with this thread is that we are far off from the original title, the problem is not DPM2 (or the larger problem is not related to it).\r\nbtw, in my case I have the quantization enabled, maybe that caused it.\nWhen you interrupt an image that's being generated with k-diffusion samplers, the image returned is the \"denoised\" version returned by the k-diffusion callback. These are also the images returned to the user during the previewing of the generation process.\r\n\r\nhttps://github.com/AUTOMATIC1111/stable-diffusion-webui/blob/6e063124acc06c13e8bfae37b1f1cd372f3e04ab/modules/sd_samplers.py#L250-L251\r\nhttps://github.com/AUTOMATIC1111/stable-diffusion-webui/blob/5f561ee95dcb09d92ea67bab5561dced93fe3d00/modules/processing.py#L367-L369\r\nhttps://github.com/AUTOMATIC1111/stable-diffusion-webui/blob/1955ca62a8f053c82ccef125b65b0e6571904347/modules/ui.py#L244-L246\r\n\r\nwhen the generation actually completes though, it takes the direct returned value from the k-diffusion sampler, which is slightly more processed. It seems like whatever the issue is with these lies in whatever is done between that \"denoised\" version and the version returned from the function on completion. I don't fully understand the algorithms though so it's hard to tell what exactly is going wrong here.\r\n\r\nDPM2 a: https://github.com/crowsonkb/k-diffusion/blob/ec78888a6f126f54410d3d2fc9f2c160606053f0/k_diffusion/sampling.py#L156\r\nLMS: https://github.com/crowsonkb/k-diffusion/blob/ec78888a6f126f54410d3d2fc9f2c160606053f0/k_diffusion/sampling.py#L198\nI'd recommend opening this issue in k-diffusion as well as I'm pretty sure it's not due to anything we're doing.\r\n\r\nFrom my testing, euler samplers avoid this issue altogether, and dpm_2_a suffers the worst, which may be due to its ancestral nature (though that wouldn't explain why Euler avoids the issue). regular dpm_2 exhibited similar symptoms but I think I prefer the \"noisy\" version as it's actually more detailed.\n> I'd recommend opening this issue in k-diffusion as well as I'm pretty sure it's not due to anything we're doing.\r\n\r\nDone\r\n\r\n> I think I prefer the \"noisy\" version as it's actually more detailed.\r\n\r\nI really don't think it is more detailed, the noise just adds the illusion of detail. It's a similar effect to something like film grain. Also, the example above is actually fairly forgiving, the noisy output looks particularly bad if you're generating things like faces or images with solid colors. Though maybe it would be nice to have some kind of tunable value to control the strength of it if it's ever figured out what's causing it.\nI recently (in the last couple of days) changed the way dpm_2_ancestral works so that it did an Euler step on the last step instead of a custom thing, do you know if these results are from before or after this change?\n> I managed to \"fix\" this by changing the if sigma_down == 0 check in the sample_dpm_2_ancestral function to if i >= len(sigmas) - 3, this makes it run 2 euler passes instead of just one, which seems to fix the issue\r\n\r\noh... it's from after the change, because it used to do *no* Euler passes.\r\n\r\nHow many steps are you doing? With dpm_2_ancestral it should be at least 50 for good quality I think (99 NFE). In any case I used dpm_2_ancestral to make a *lot* of Twitter art posts and never saw this sort of issue with it that I was aware of, but I always did at least 50 steps. The quality really degrades with less.\nBTW the effect of an Euler step when it is the last step (stepping to sigma=0) should be *the same*, up to floating point rounding error, as just returning `denoised`.\n@crowsonkb\r\nDPM2-A 60 steps\r\n![00016-2418091115](https://user-images.githubusercontent.com/36072735/193484303-9770cea6-4364-4401-b3cf-6b8e0a596475.png)\r\nDPM2-A 60 steps but interrupted at 53\r\n![00017-2418091115](https://user-images.githubusercontent.com/36072735/193484325-198c23d9-26fc-4934-b136-e91e182983ab.png)\r\n\r\nEven with higher steps, the effect remains, though less pronounced. I don't understand why it happens either. With euler_a for example, this doesn't happen at all no matter the step count.\r\n\r\nRegular DPM2 exhibits the same behaviour, though I sort of prefer it with DPM2.\r\n\r\n![00014-1814783737](https://user-images.githubusercontent.com/36072735/193484449-89a2aa61-6d2c-4c85-b646-73686061b9ae.png)\r\n\r\nInterrupted:\r\n![00015-1814783737](https://user-images.githubusercontent.com/36072735/193484454-f6a9fbfd-edcf-4b5d-aed7-a065fcb784da.png)\r\n\r\n\n> @crowsonkb DPM2-A 60 steps\r\n\r\nThese look fine to me actually. >_> Maybe you just don't like the details the model adds at really low noise levels and should try raising sigma_min a little?\ndpm2-a lacks a s_tmin argument, though? Unless I misunderstood what you meant by sigma_min. FWIW, DPM adaptive doesn't have this quirk.\r\n\r\nTo be clear, if this is intended behaviour, that's fine. Some people also reported similar issues #412 for example, so I wanted to make sure.\nI think without context, the DPM2a results looks acceptable, but comparing it to the interrupted one, I can see the issues.\r\nI think that's why I never noticed the issues with DPM2a until recently, because I didn't have the generation preview turned on, seeing the context from the generating image to the final one made me notice how different they look.\r\nThe final images have that odd filtered look that is sometimes common in AI-generated stuff, but I feel like the interrupted ones look much more realistic. The grass especially looks strange in the final generated image, it has that odd AI hallucination-type look to it. You can also see the noise prominently in the blurred background of the turtle image, a real photo would not have noise like that since the physics of it don't really make sense. It looks more like an oil painting than a photo. The interrupted one's blurred background feels much more natural to me.\r\n\r\nIn the end, whichever one looks better is mostly just subjective, but the fact the that result changes drastically in the final iteration kind of feels like there's something off there. I'm mostly just curious about finding the reason why it's happening, but I don't fully understand the algorithm so it's hard for me to say what the issue is, maybe it has something to do with the sigma values approaching near 0 during the final steps? Just throwing something out there since I threw some debug prints in the sampler to see what the values are.\n> dpm2-a lacks a s_tmin argument, though? Unless I misunderstood what you meant by sigma_min. FWIW, DPM adaptive doesn't have this quirk.\r\n\r\nYou specify the sigma_min when you create the schedule for dpm_2_ancestral. DPM adaptive has an `eta` parameter (a push a few minutes ago changed it to use ancestral sampling because it works more reliably with different noise levels than doing the noise additions before the ODE steps) but its default is 0, have you tried raising it? `eta=1` (after `git pull`) should make it try to add the same amount of noise as dpm_2_ancestral.\r\n\n@crowsonkb Currently, I'm using the version before you added stochastic sampling.\r\n\r\nI wonder if the noise scheduler is the issue here. We use get_sigmas of DiscreteSchedule which - unless I'm totally lost here - doesn't allow setting sigma_min.\nThe newest dpm_adaptive (eta 1) also seems to avoid the specific type of \"noisy\" output dpm_2_a returns. (same seed and prompt as my previous tortoise pictures)\r\n![00081-2418091115](https://user-images.githubusercontent.com/36072735/193502517-ff9f99d4-9298-433d-b8d4-bba871fab856.png)\n> I wonder if the noise scheduler is the issue here. We use get_sigmas of DiscreteSchedule which - unless I'm totally lost here - doesn't allow setting sigma_min.\r\n\r\nI believe you should be able to test this using the alternate sampler noise schedules script - https://gist.github.com/dfaker/f88aa62e3a14b559fe4e5f6b345db664\nDPM2_A but with get_sigmas_karras. This is more what I'd expect. I was probably correct that the noise scheduler is the problem.\r\n![00083-2418091115](https://user-images.githubusercontent.com/36072735/193503573-8284767b-0c40-4d94-918d-313a08642cca.png)\nThat looks much better, definitely seems like that was the culprit.\nHuh. I think the original Stable Diffusion noise schedule goes really fast at low noise levels compared to get_sigmas_karras which goes faster at high noise levels and slower at low noise levels.", "created_at": "2022-10-03T09:53:59Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 1454, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-1454", "issue_numbers": ["1404"], "base_commit": "852fd90c0dcda9cb5fbbfdf0c7308ce58034935c", "patch": "diff --git a/javascript/ui.js b/javascript/ui.js\nindex f94ed081d72..b1053201cf8 100644\n--- a/javascript/ui.js\n+++ b/javascript/ui.js\n@@ -218,10 +218,16 @@ function update_token_counter(button_id) {\n \t\tclearTimeout(token_timeout);\n \ttoken_timeout = setTimeout(() => gradioApp().getElementById(button_id)?.click(), wait_time);\n }\n+\n function submit_prompt(event, generate_button_id) {\n     if (event.altKey && event.keyCode === 13) {\n         event.preventDefault();\n         gradioApp().getElementById(generate_button_id).click();\n         return;\n     }\n-}\n\\ No newline at end of file\n+}\n+\n+function restart_reload(){\n+    document.body.innerHTML='<h1 style=\"font-family:monospace;margin-top:20%;color:lightgray;text-align:center;\">Reloading...</h1>';\n+    setTimeout(function(){location.reload()},2000)\n+}\ndiff --git a/modules/scripts.py b/modules/scripts.py\nindex 7c3bd5e74d2..45230f9a14c 100644\n--- a/modules/scripts.py\n+++ b/modules/scripts.py\n@@ -162,6 +162,40 @@ def run(self, p: StableDiffusionProcessing, *args):\n \r\n         return processed\r\n \r\n+    def reload_sources(self):\r\n+        for si, script in list(enumerate(self.scripts)):\r\n+            with open(script.filename, \"r\", encoding=\"utf8\") as file:\r\n+                args_from = script.args_from\r\n+                args_to = script.args_to\r\n+                filename = script.filename\r\n+                text = file.read()\r\n+\r\n+                from types import ModuleType\r\n+\r\n+                compiled = compile(text, filename, 'exec')\r\n+                module = ModuleType(script.filename)\r\n+                exec(compiled, module.__dict__)\r\n+\r\n+                for key, script_class in module.__dict__.items():\r\n+                    if type(script_class) == type and issubclass(script_class, Script):\r\n+                        self.scripts[si] = script_class()\r\n+                        self.scripts[si].filename = filename\r\n+                        self.scripts[si].args_from = args_from\r\n+                        self.scripts[si].args_to = args_to\r\n \r\n scripts_txt2img = ScriptRunner()\r\n scripts_img2img = ScriptRunner()\r\n+\r\n+def reload_script_body_only():\r\n+    scripts_txt2img.reload_sources()\r\n+    scripts_img2img.reload_sources()\r\n+\r\n+\r\n+def reload_scripts(basedir):\r\n+    global scripts_txt2img, scripts_img2img\r\n+\r\n+    scripts_data.clear()\r\n+    load_scripts(basedir)\r\n+\r\n+    scripts_txt2img = ScriptRunner()\r\n+    scripts_img2img = ScriptRunner()\r\ndiff --git a/modules/ui.py b/modules/ui.py\nindex c8f5bb84398..78a15d83acf 100644\n--- a/modules/ui.py\n+++ b/modules/ui.py\n@@ -1145,6 +1145,31 @@ def run_settings(*args):\n             _js='function(){}'\r\n         )\r\n \r\n+        with gr.Row():\r\n+            reload_script_bodies = gr.Button(value='Reload custom script bodies (No ui updates, No restart)', variant='secondary')\r\n+            restart_gradio = gr.Button(value='Restart Gradio and Refresh components (Custom Scripts, ui.py, js and css only)', variant='primary')\r\n+\r\n+\r\n+        def reload_scripts():\r\n+            modules.scripts.reload_script_body_only()\r\n+\r\n+        reload_script_bodies.click(\r\n+            fn=reload_scripts,\r\n+            inputs=[],\r\n+            outputs=[],\r\n+            _js='function(){}'\r\n+        )\r\n+\r\n+        def request_restart():\r\n+            settings_interface.gradio_ref.do_restart = True\r\n+\r\n+        restart_gradio.click(\r\n+            fn=request_restart,\r\n+            inputs=[],\r\n+            outputs=[],\r\n+            _js='function(){restart_reload()}'\r\n+        )\r\n+        \r\n         if column is not None:\r\n             column.__exit__()\r\n \r\n@@ -1170,7 +1195,9 @@ def run_settings(*args):\n         css += css_hide_progressbar\r\n \r\n     with gr.Blocks(css=css, analytics_enabled=False, title=\"Stable Diffusion\") as demo:\r\n-\r\n+        \r\n+        settings_interface.gradio_ref = demo\r\n+        \r\n         with gr.Tabs() as tabs:\r\n             for interface, label, ifid in interfaces:\r\n                 with gr.TabItem(label, id=ifid):\r\n@@ -1350,12 +1377,12 @@ def apply_field(obj, field, condition=None):\n         javascript += f\"\\n<script>{jsfile.read()}</script>\"\r\n \r\n \r\n-def template_response(*args, **kwargs):\r\n-    res = gradio_routes_templates_response(*args, **kwargs)\r\n-    res.body = res.body.replace(b'</head>', f'{javascript}</head>'.encode(\"utf8\"))\r\n-    res.init_headers()\r\n-    return res\r\n-\r\n+if 'gradio_routes_templates_response' not in globals():\r\n+    def template_response(*args, **kwargs):\r\n+        res = gradio_routes_templates_response(*args, **kwargs)\r\n+        res.body = res.body.replace(b'</head>', f'{javascript}</head>'.encode(\"utf8\"))\r\n+        res.init_headers()\r\n+        return res\r\n \r\n-gradio_routes_templates_response = gradio.routes.templates.TemplateResponse\r\n-gradio.routes.templates.TemplateResponse = template_response\r\n+    gradio_routes_templates_response = gradio.routes.templates.TemplateResponse\r\n+    gradio.routes.templates.TemplateResponse = template_response\r\ndiff --git a/webui.py b/webui.py\nindex dc72ceb8ac1..634956978e3 100644\n--- a/webui.py\n+++ b/webui.py\n@@ -1,4 +1,9 @@\n import os\r\n+import threading\r\n+import time\r\n+import importlib\r\n+from modules import devices\r\n+from modules.paths import script_path\r\n import signal\r\n import threading\r\n \r\n@@ -82,16 +87,34 @@ def sigint_handler(sig, frame):\n \r\n     signal.signal(signal.SIGINT, sigint_handler)\r\n \r\n-    demo = modules.ui.create_ui(wrap_gradio_gpu_call=wrap_gradio_gpu_call)\r\n+    while 1:\r\n+\r\n+        demo = modules.ui.create_ui(wrap_gradio_gpu_call=wrap_gradio_gpu_call)\r\n+        \r\n+        demo.launch(\r\n+            share=cmd_opts.share,\r\n+            server_name=\"0.0.0.0\" if cmd_opts.listen else None,\r\n+            server_port=cmd_opts.port,\r\n+            debug=cmd_opts.gradio_debug,\r\n+            auth=[tuple(cred.split(':')) for cred in cmd_opts.gradio_auth.strip('\"').split(',')] if cmd_opts.gradio_auth else None,\r\n+            inbrowser=cmd_opts.autolaunch,\r\n+            prevent_thread_lock=True\r\n+        )\r\n+\r\n+        while 1:\r\n+            time.sleep(0.5)\r\n+                if getattr(demo,'do_restart',False):\r\n+                    time.sleep(0.5)\r\n+                    demo.close()\r\n+                    time.sleep(0.5)\r\n+                    break\r\n+\r\n+        print('Reloading Custom Scripts')\r\n+        modules.scripts.reload_scripts(os.path.join(script_path, \"scripts\"))\r\n+        print('Reloading modules: modules.ui')\r\n+        importlib.reload(modules.ui)\r\n+        print('Restarting Gradio')\r\n \r\n-    demo.launch(\r\n-        share=cmd_opts.share,\r\n-        server_name=\"0.0.0.0\" if cmd_opts.listen else None,\r\n-        server_port=cmd_opts.port,\r\n-        debug=cmd_opts.gradio_debug,\r\n-        auth=[tuple(cred.split(':')) for cred in cmd_opts.gradio_auth.strip('\"').split(',')] if cmd_opts.gradio_auth else None,\r\n-        inbrowser=cmd_opts.autolaunch,\r\n-    )\r\n \r\n \r\n if __name__ == \"__main__\":\r\n", "test_patch": "", "problem_statement": "Script reload without restart\nI write my own scripts and its getting quite complex. I have to change settings and values quite often, and when I do, I need to restart with the webui-user to be able to see the changes.\r\n\r\nI am not an experienced python coder, and using gradio for this is too difficult for me.\r\n\r\nIs there any way to enable or implement reloading scripts without having to restart? Maybe a kind soul wants to give me a hint how I could do it?\r\n\r\nI think not many people write custom scripts so I think the chances of this being implemented are quite small, but maybe there is a right way to do it...\n", "hints_text": "Problem is, because it's python, everything has to be re-compiled every time it's run. Meaning, I think it's kinda tricky to dynamically re-load classes at runtime. \r\n\r\nImpossible? No, but probably a little hacky. Maybe something like this:\r\n\r\nhttps://stackoverflow.com/questions/9645388/dynamically-reload-a-class-definition-in-python\nmentioned also in https://github.com/AUTOMATIC1111/stable-diffusion-webui/issues/676\nThe reloading the modules is fine, the Gradio interface having all the ui set up on init and then frozen, isn't.\r\nblock.close() should close the ui allowing us to optionally restart but it looks like it's intentionally blocked by a thread in block and never exits.\r\n\r\n```python\r\n   ...\r\n        def queue_restart_gradio_action(settings_interface):\r\n          settings_interface.gradio_app_ref.restart_after_close = True\r\n          settings_interface.gradio_app_ref.close(verbose=True)\r\n\r\n        def restart_gradio_action():\r\n          from threading import Timer\r\n          settings_interface.reaper = Timer(0.0, queue_restart_gradio_action, args=(settings_interface,))\r\n          settings_interface.reaper.start()\r\n\r\n        restart_gradio = gr.Button(value='Restart Gradio')\r\n        restart_gradio.click(\r\n            fn=restart_gradio_action,\r\n            inputs=[],\r\n            outputs=[],\r\n            _js='function(){location.reload();}'\r\n        )\r\n   ...\r\n```\r\n\r\n```python\r\n   ...\r\n    with gr.Blocks(css=css, analytics_enabled=False, title=\"Stable Diffusion\") as demo:\r\n        settings_interface.gradio_app_ref = demo\r\n        with gr.Tabs() as tabs:\r\n   ...\r\n```", "created_at": "2022-10-01T17:38:00Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 1371, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-1371", "issue_numbers": ["1364"], "base_commit": "2b03f0bbda1229dff6e7ab6f656b28587eba8308", "patch": "diff --git a/modules/bsrgan_model.py b/modules/bsrgan_model.py\nindex 47346f3184c..e62c66577d7 100644\n--- a/modules/bsrgan_model.py\n+++ b/modules/bsrgan_model.py\n@@ -69,7 +69,7 @@ def load_model(self, path: str):\n         if not os.path.exists(filename) or filename is None:\n             print(f\"BSRGAN: Unable to load model from {filename}\", file=sys.stderr)\n             return None\n-        model = RRDBNet(in_nc=3, out_nc=3, nf=64, nb=23, gc=32, sf=2)  # define network\n+        model = RRDBNet(in_nc=3, out_nc=3, nf=64, nb=23, gc=32, sf=4)  # define network\n         model.load_state_dict(torch.load(filename), strict=True)\n         model.eval()\n         for k, v in model.named_parameters():\ndiff --git a/modules/ldsr_model.py b/modules/ldsr_model.py\nindex 877e7e73ea0..1c1070fc6bc 100644\n--- a/modules/ldsr_model.py\n+++ b/modules/ldsr_model.py\n@@ -22,8 +22,20 @@ def __init__(self, user_path):\n         self.scalers = [scaler_data]\n \n     def load_model(self, path: str):\n+        # Remove incorrect project.yaml file if too big\n+        yaml_path = os.path.join(self.model_path, \"project.yaml\")\n+        old_model_path = os.path.join(self.model_path, \"model.pth\")\n+        new_model_path = os.path.join(self.model_path, \"model.ckpt\")\n+        if os.path.exists(yaml_path):\n+            statinfo = os.stat(yaml_path)\n+            if statinfo.st_size >= 10485760:\n+                print(\"Removing invalid LDSR YAML file.\")\n+                os.remove(yaml_path)\n+        if os.path.exists(old_model_path):\n+            print(\"Renaming model from model.pth to model.ckpt\")\n+            os.rename(old_model_path, new_model_path)\n         model = load_file_from_url(url=self.model_url, model_dir=self.model_path,\n-                                   file_name=\"model.pth\", progress=True)\n+                                   file_name=\"model.ckpt\", progress=True)\n         yaml = load_file_from_url(url=self.yaml_url, model_dir=self.model_path,\n                                   file_name=\"project.yaml\", progress=True)\n \n@@ -41,5 +53,4 @@ def do_upscale(self, img, path):\n             print(\"NO LDSR!\")\n             return img\n         ddim_steps = shared.opts.ldsr_steps\n-        pre_scale = shared.opts.ldsr_pre_down\n         return ldsr.super_resolution(img, ddim_steps, self.scale)\ndiff --git a/modules/ldsr_model_arch.py b/modules/ldsr_model_arch.py\nindex 7faac6e1838..14db507668c 100644\n--- a/modules/ldsr_model_arch.py\n+++ b/modules/ldsr_model_arch.py\n@@ -98,9 +98,7 @@ def super_resolution(self, image, steps=100, target_scale=2, half_attention=Fals\n         im_og = image\n         width_og, height_og = im_og.size\n         # If we can adjust the max upscale size, then the 4 below should be our variable\n-        print(\"Foo\")\n         down_sample_rate = target_scale / 4\n-        print(f\"Downsample rate is {down_sample_rate}\")\n         wd = width_og * down_sample_rate\n         hd = height_og * down_sample_rate\n         width_downsampled_pre = int(wd)\n@@ -111,7 +109,7 @@ def super_resolution(self, image, steps=100, target_scale=2, half_attention=Fals\n                 f'Downsampling from [{width_og}, {height_og}] to [{width_downsampled_pre}, {height_downsampled_pre}]')\n             im_og = im_og.resize((width_downsampled_pre, height_downsampled_pre), Image.LANCZOS)\n         else:\n-            print(f\"Down sample rate is 1 from {target_scale} / 4\")\n+            print(f\"Down sample rate is 1 from {target_scale} / 4 (Not downsampling)\")\n         logs = self.run(model[\"model\"], im_og, diffusion_steps, eta)\n \n         sample = logs[\"sample\"]\ndiff --git a/modules/modelloader.py b/modules/modelloader.py\nindex 1106aeb7fd0..8c862b42f22 100644\n--- a/modules/modelloader.py\n+++ b/modules/modelloader.py\n@@ -1,3 +1,4 @@\n+import glob\n import os\n import shutil\n import importlib\n@@ -41,7 +42,7 @@ def load_models(model_path: str, model_url: str = None, command_path: str = None\n \n         for place in places:\n             if os.path.exists(place):\n-                for file in os.listdir(place):\n+                for file in glob.iglob(place + '**/**', recursive=True):\n                     full_path = os.path.join(place, file)\n                     if os.path.isdir(full_path):\n                         continue\n", "test_patch": "", "problem_statement": "LDSR: UnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position 64: invalid start byte\n**Describe the bug**\r\n\r\nAfter today's refactoring commits, using LDSR upscaling produces an error:\r\n\r\n`UnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position 64: invalid start byte`\r\n\r\nThis is on Linux, even after a fresh download (I moved the old LDSR related models aside). It looks like an issue with encodings, as utf-8 is involved. I guess it could even possibly work on Windows but not on Linux?\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Go to extras\r\n2. Click on LDSR\r\n3. Add an image\r\n4. Click Generate\r\n\r\n**Expected behavior**\r\nLDSR should work\r\n\r\n\r\n\r\n**Desktop (please complete the following information):**\r\n - OS: Fedora Linux 37 beta\r\n - Browser: Firefox\r\n - Commit revision: 5c0c778a65c8f89a85395fb10e32d3b35ea57196\r\n\r\n**Additional context**\r\n\r\nIt works in git commit 498515e7a19bb3e8ab36aab2e628eb6be7464401 (a commit from last night, before all the refactoring). Well, \"works\". Sometimes there's a black edge with missing pixels on the right and bottom. Other times, it's fine. (I think it's related to resolution and/or aspect ratio?)\r\n\r\nComplete traceback:\r\n\r\n```python\r\nTraceback (most recent call last):\r\n  File \"/var/home/garrett/Source/stable-diffusion/stable-diffusion-webui-auto/modules/ui.py\", line 153, in f\r\n    res = list(func(*args, **kwargs))\r\n  File \"/var/home/garrett/Source/stable-diffusion/stable-diffusion-webui-auto/webui.py\", line 63, in f\r\n    res = func(*args, **kwargs)\r\n  File \"/var/home/garrett/Source/stable-diffusion/stable-diffusion-webui-auto/modules/extras.py\", line 85, in run_extras\r\n    res = upscale(image, extras_upscaler_1, upscaling_resize)\r\n  File \"/var/home/garrett/Source/stable-diffusion/stable-diffusion-webui-auto/modules/extras.py\", line 79, in upscale\r\n    c = upscaler.scaler.upscale(image, resize, upscaler.data_path)\r\n  File \"/var/home/garrett/Source/stable-diffusion/stable-diffusion-webui-auto/modules/upscaler.py\", line 61, in upscale\r\n    img = self.do_upscale(img, selected_model)\r\n  File \"/var/home/garrett/Source/stable-diffusion/stable-diffusion-webui-auto/modules/ldsr_model.py\", line 45, in do_upscale\r\n    return ldsr.super_resolution(img, ddim_steps, self.scale)\r\n  File \"/var/home/garrett/Source/stable-diffusion/stable-diffusion-webui-auto/modules/ldsr_model_arch.py\", line 87, in super_resolution\r\n    model = self.load_model_from_config(half_attention)\r\n  File \"/var/home/garrett/Source/stable-diffusion/stable-diffusion-webui-auto/modules/ldsr_model_arch.py\", line 24, in load_model_from_config\r\n    config = OmegaConf.load(self.yamlPath)\r\n  File \"/var/home/garrett/.local/lib/python3.10/site-packages/omegaconf/omegaconf.py\", line 188, in load\r\n    obj = yaml.load(f, Loader=get_yaml_loader())\r\n  File \"/var/home/garrett/.local/lib/python3.10/site-packages/yaml/__init__.py\", line 79, in load\r\n    loader = Loader(stream)\r\n  File \"/var/home/garrett/.local/lib/python3.10/site-packages/yaml/loader.py\", line 34, in __init__\r\n    Reader.__init__(self, stream)\r\n  File \"/var/home/garrett/.local/lib/python3.10/site-packages/yaml/reader.py\", line 85, in __init__\r\n    self.determine_encoding()\r\n  File \"/var/home/garrett/.local/lib/python3.10/site-packages/yaml/reader.py\", line 124, in determine_encoding\r\n    self.update_raw()\r\n  File \"/var/home/garrett/.local/lib/python3.10/site-packages/yaml/reader.py\", line 178, in update_raw\r\n    data = self.stream.read(size)\r\n  File \"/usr/lib64/python3.10/codecs.py\", line 322, in decode\r\n    (result, consumed) = self._buffer_decode(data, self.errors, final)\r\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position 64: invalid start byte\r\n```\n", "hints_text": "In the fresh checkout:\r\n\r\nproject.yaml seems to be a zip file:\r\n\r\n```sh\r\n$ file *\r\nmodel.chkpt:  Zip archive data, at least v0.0 to extract, compression method=store\r\nmodel.pth:    Zip archive data, at least v0.0 to extract, compression method=store\r\nproject.yaml: Zip archive data, at least v0.0 to extract, compression method=store\r\n```\r\n\r\nAll three files are 1.9G as well.\r\n\r\nI guess the functionality to download the models is not quite correct.\nRelated to #1344 . Download [project.yaml](https://heibox.uni-heidelberg.de/f/31a76b13ea27482981b4/?dl=1), rename to project.yaml and place in models/LDSR/.\nCommit hash: 5c0c778a65c8f89a85395fb10e32d3b35ea57196\r\nDownloading: \"https://heibox.uni-heidelberg.de/f/578df07c8fc04ffbadf3/?dl=1\" to C:\\SD\\stable-diffusion-webui\\models\\LDSR\\model.pth\r\n\r\nDownloading: \"https://heibox.uni-heidelberg.de/f/578df07c8fc04ffbadf3/?dl=1\" to C:\\SD\\stable-diffusion-webui\\models\\LDSR\\project.yaml\r\n\r\nError completing request\r\nArguments: (0, <PIL.Image.Image image mode=RGB size=512x512 at 0x663948E0>, None, 0, 0, 0, 2, 4, 0, 1) {}\r\nTraceback (most recent call last):\r\n  File \"C:\\Program Files\\Python310\\lib\\urllib\\request.py\", line 1348, in do_open\r\n    h.request(req.get_method(), req.selector, req.data, headers,\r\n  File \"C:\\Program Files\\Python310\\lib\\http\\client.py\", line 1282, in request\r\n    self._send_request(method, url, body, headers, encode_chunked)\r\n  File \"C:\\Program Files\\Python310\\lib\\http\\client.py\", line 1328, in _send_request\r\n    self.endheaders(body, encode_chunked=encode_chunked)\r\n  File \"C:\\Program Files\\Python310\\lib\\http\\client.py\", line 1277, in endheaders\r\n    self._send_output(message_body, encode_chunked=encode_chunked)\r\n  File \"C:\\Program Files\\Python310\\lib\\http\\client.py\", line 1037, in _send_output\r\n    self.send(msg)\r\n  File \"C:\\Program Files\\Python310\\lib\\http\\client.py\", line 975, in send\r\n    self.connect()\r\n  File \"C:\\Program Files\\Python310\\lib\\http\\client.py\", line 1454, in connect\r\n    self.sock = self._context.wrap_socket(self.sock,\r\n  File \"C:\\Program Files\\Python310\\lib\\ssl.py\", line 513, in wrap_socket\r\n    return self.sslsocket_class._create(\r\n  File \"C:\\Program Files\\Python310\\lib\\ssl.py\", line 1071, in _create\r\n    self.do_handshake()\r\n  File \"C:\\Program Files\\Python310\\lib\\ssl.py\", line 1342, in do_handshake\r\n    self._sslobj.do_handshake()\r\nssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self signed certificate in certificate chain (_ssl.c:997)\r\n\r\n\r\n", "created_at": "2022-09-30T13:57:29Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 1326, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-1326", "issue_numbers": ["1323"], "base_commit": "498515e7a19bb3e8ab36aab2e628eb6be7464401", "patch": "diff --git a/modules/sd_samplers.py b/modules/sd_samplers.py\nindex 5642b870cdc..5e60e494b90 100644\n--- a/modules/sd_samplers.py\n+++ b/modules/sd_samplers.py\n@@ -4,7 +4,8 @@\n import tqdm\r\n from PIL import Image\r\n import inspect\r\n-\r\n+from modules.paths import paths\r\n+sys.path.insert(0, paths[\"k_diffusion\"])\r\n import k_diffusion.sampling\r\n import ldm.models.diffusion.ddim\r\n import ldm.models.diffusion.plms\r\n", "test_patch": "", "problem_statement": "New samplers are not showing up\nI just updated my version to try out the new samplers but they are not showing up. I deleted repositories/k-diffusion as a test but they still dont show up.\r\n\r\nSomeone on reddit mentioned to do \"source venv/bin/activate/\" and then to do a pip uninstall k-diffusion, but I have no idea what it means.\r\n\r\nHow can I get the new samplers to show up in the UI?\r\n\r\nEdit: They dont show up in the img2img Tab\n", "hints_text": "That isn't actually a bug, they were not able to get them working with img2img so, much like PLMS, they removed them from the img2img menu.\nI have the same Issue, i don't see them on ANY of the tabs.\r\nEdit: Uninstalling the old K-diffusion from the venv manually fixed it for me.", "created_at": "2022-09-29T23:32:48Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 1284, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-1284", "issue_numbers": ["1028", "1028"], "base_commit": "041d2aefc082c2883aa7e28ee3e4a990b3be9758", "patch": "diff --git a/modules/ui.py b/modules/ui.py\nindex 87024238209..012395b0461 100644\n--- a/modules/ui.py\n+++ b/modules/ui.py\n@@ -13,6 +13,7 @@\n import numpy as np\r\n import torch\r\n from PIL import Image, PngImagePlugin\r\n+import piexif\r\n \r\n import gradio as gr\r\n import gradio.utils\r\n@@ -111,18 +112,26 @@ def save_files(js_data, images, index):\n             writer.writerow([\"prompt\", \"seed\", \"width\", \"height\", \"sampler\", \"cfgs\", \"steps\", \"filename\", \"negative_prompt\"])\r\n \r\n         filename_base = str(int(time.time() * 1000))\r\n+        extension = opts.samples_format.lower()\r\n         for i, filedata in enumerate(images):\r\n-            filename = filename_base + (\"\" if len(images) == 1 else \"-\" + str(i + 1)) + \".png\"\r\n+            filename = filename_base + (\"\" if len(images) == 1 else \"-\" + str(i + 1)) + f\".{extension}\"\r\n             filepath = os.path.join(opts.outdir_save, filename)\r\n \r\n             if filedata.startswith(\"data:image/png;base64,\"):\r\n                 filedata = filedata[len(\"data:image/png;base64,\"):]\r\n \r\n-            pnginfo = PngImagePlugin.PngInfo()\r\n-            pnginfo.add_text('parameters', infotexts[i])\r\n-\r\n             image = Image.open(io.BytesIO(base64.decodebytes(filedata.encode('utf-8'))))\r\n-            image.save(filepath, quality=opts.jpeg_quality, pnginfo=pnginfo)\r\n+            if opts.enable_pnginfo and extension == 'png':\r\n+                pnginfo = PngImagePlugin.PngInfo()\r\n+                pnginfo.add_text('parameters', infotexts[i])\r\n+                image.save(filepath, pnginfo=pnginfo)\r\n+            else:\r\n+                image.save(filepath, quality=opts.jpeg_quality)\r\n+\r\n+            if opts.enable_pnginfo and extension in (\"jpg\", \"jpeg\", \"webp\"):\r\n+                piexif.insert(piexif.dump({\"Exif\": {\r\n+                    piexif.ExifIFD.UserComment: piexif.helper.UserComment.dump(infotexts[i], encoding=\"unicode\")\r\n+                }}), filepath)\r\n \r\n             filenames.append(filename)\r\n \r\n", "test_patch": "", "problem_statement": "\"Save\" button always saves to .png, ignoring settings\n**Describe the bug**\r\nWhen you press \"Save\", the image outputted to /log/ is always in png format, no matter what output format is selected in settings. The outputted images are properly saved to jpg, though. But in order for the \"save\" button to be useful, it should use the same format.\n\"Save\" button always saves to .png, ignoring settings\n**Describe the bug**\r\nWhen you press \"Save\", the image outputted to /log/ is always in png format, no matter what output format is selected in settings. The outputted images are properly saved to jpg, though. But in order for the \"save\" button to be useful, it should use the same format.\n", "hints_text": "\n", "created_at": "2022-09-29T02:54:05Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 1283, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-1283", "issue_numbers": ["1147"], "base_commit": "82eb8ea452b1e63535c58d15ec6db2ad2342faa8", "patch": "diff --git a/modules/extras.py b/modules/extras.py\nindex 6a0d5cb0f9b..1d9e64e5517 100644\n--- a/modules/extras.py\n+++ b/modules/extras.py\n@@ -100,6 +100,8 @@ def upscale(image, scaler_index, resize):\n \r\n         outputs.append(image)\r\n \r\n+    devices.torch_gc()\r\n+\r\n     return outputs, plaintext_to_html(info), ''\r\n \r\n \r\ndiff --git a/modules/processing.py b/modules/processing.py\nindex e567956cee7..de818d5b911 100644\n--- a/modules/processing.py\n+++ b/modules/processing.py\n@@ -11,7 +11,7 @@\n from skimage import exposure\r\n \r\n import modules.sd_hijack\r\n-from modules import devices, prompt_parser, masking, sd_samplers\r\n+from modules import devices, prompt_parser, masking, sd_samplers, lowvram\r\n from modules.sd_hijack import model_hijack\r\n from modules.shared import opts, cmd_opts, state\r\n import modules.shared as shared\r\n@@ -382,6 +382,13 @@ def infotext(iteration=0, position_in_batch=0):\n             x_samples_ddim = p.sd_model.decode_first_stage(samples_ddim)\r\n             x_samples_ddim = torch.clamp((x_samples_ddim + 1.0) / 2.0, min=0.0, max=1.0)\r\n \r\n+            del samples_ddim\r\n+\r\n+            if shared.cmd_opts.lowvram or shared.cmd_opts.medvram:\r\n+                lowvram.send_everything_to_cpu()\r\n+\r\n+            devices.torch_gc()\r\n+\r\n             if opts.filter_nsfw:\r\n                 import modules.safety as safety\r\n                 x_samples_ddim = modules.safety.censor_batch(x_samples_ddim)\r\n@@ -426,6 +433,10 @@ def infotext(iteration=0, position_in_batch=0):\n                 infotexts.append(infotext(n, i))\r\n                 output_images.append(image)\r\n \r\n+            del x_samples_ddim \r\n+\r\n+            devices.torch_gc()\r\n+\r\n             state.nextjob()\r\n \r\n         p.color_corrections = None\r\n@@ -663,4 +674,7 @@ def sample(self, conditioning, unconditional_conditioning, seeds, subseeds, subs\n         if self.mask is not None:\r\n             samples = samples * self.nmask + self.init_latent * self.mask\r\n \r\n+        del x\r\n+        devices.torch_gc()\r\n+\r\n         return samples\r\ndiff --git a/modules/sd_hijack.py b/modules/sd_hijack.py\nindex 3fa0624228e..a6fa890c424 100644\n--- a/modules/sd_hijack.py\n+++ b/modules/sd_hijack.py\n@@ -5,6 +5,7 @@\n import torch\r\n import numpy as np\r\n from torch import einsum\r\n+from torch.nn.functional import silu\r\n \r\n import modules.textual_inversion.textual_inversion\r\n from modules import prompt_parser, devices, sd_hijack_optimizations, shared\r\n@@ -19,11 +20,12 @@\n \r\n \r\n def apply_optimizations():\r\n+    ldm.modules.diffusionmodules.model.nonlinearity = silu\r\n+\r\n     if cmd_opts.opt_split_attention_v1:\r\n         ldm.modules.attention.CrossAttention.forward = sd_hijack_optimizations.split_cross_attention_forward_v1\r\n     elif not cmd_opts.disable_opt_split_attention and (cmd_opts.opt_split_attention or torch.cuda.is_available()):\r\n         ldm.modules.attention.CrossAttention.forward = sd_hijack_optimizations.split_cross_attention_forward\r\n-        ldm.modules.diffusionmodules.model.nonlinearity = sd_hijack_optimizations.nonlinearity_hijack\r\n         ldm.modules.diffusionmodules.model.AttnBlock.forward = sd_hijack_optimizations.cross_attention_attnblock_forward\r\n \r\n \r\ndiff --git a/modules/sd_hijack_optimizations.py b/modules/sd_hijack_optimizations.py\nindex 9c079e57808..ea4cfdfcd66 100644\n--- a/modules/sd_hijack_optimizations.py\n+++ b/modules/sd_hijack_optimizations.py\n@@ -92,14 +92,6 @@ def split_cross_attention_forward(self, x, context=None, mask=None):\n \r\n     return self.to_out(r2)\r\n \r\n-def nonlinearity_hijack(x):\r\n-    # swish\r\n-    t = torch.sigmoid(x)\r\n-    x *= t\r\n-    del t\r\n-\r\n-    return x\r\n-\r\n def cross_attention_attnblock_forward(self, x):\r\n         h_ = x\r\n         h_ = self.norm(h_)\r\n", "test_patch": "", "problem_statement": "A high batch count number eventually chokes the VRAM. Behavior similar to a memory leak.\n**Describe the bug**\r\nA high batch count number eventually chokes the VRAM. Behavior similar to a memory leak.\r\n\r\n**To Reproduce**\r\n1. Set batch size of 4, 512x512 random txt to image\r\n2. Set batch count to 16\r\n3. batch 3 runs, then all fails due to lack of vram\r\n\r\n**Expected behavior**\r\nRuns batch 1, cleans up, batch 2, until batch 16.\r\n\r\n\r\n\r\n**Desktop (please complete the following information):**\r\n - OS: Windows\r\n - Browser Chrome\r\n - Commit revision --precision full --no-half --medvram\r\n - GTX1660 Ti 6GB\r\n\r\n**Additional context**\r\nAdd any other context about the problem here.\r\n\n", "hints_text": "I've run similar tests with --medvram, batch size 4, can't get beyond a batch count of 2. \r\n\r\nIs this bug only due to the addition of setting a batch size > 1 ?\r\nI experience a similar problem only when adjusting batch size in conjunction with batch count. \r\nI then conclude only raising batch count is reliable to leave running overnight.\r\nI've run a custom video script overnight, most likely solely raising batch count. That worked fine.\nBatch size will attempt to send multiple image generation together in a single batch in parallel.\r\nBatch count will send multiple batches of Batch size in series.\r\n\r\nIf you increase Batch size, you're also increasing the GPU memory required - and in your case running out.\r\n\r\nI take it it runs as many batches as you like at size 3?\n> I take it it runs as many batches as you like at size 3?\r\n\r\nI think you may be misunderstanding what they're saying:\r\n\r\nIf you set Batch Size to 4 (4 images per batch), but generate 16 batches, often generation fails with an out of memory error only after generating a few batches, instead of the total. This seems like a memory leak as each batch should be separate from one another, and therefore not continuously allocate more memory purely based upon the number of batches set for a particular run.\r\n\r\nNot directly on-topic, but I believe I've run into memory leaks when changing resolutions, batch size, batch count - even if a previous resolution/batch size/count generated with no problems. Eventually stock settings will fail to allocate memory unless I restart the script.\nYeah I did see the premature stop at 4/16 at 4.\r\n\r\nTrouble is just because you can submit a batch doesn't mean you're under the memory limits, I can submit, start processing and see feedback on 1024x1024 count 16 size 8 on a 1080ti but there's no way that's going to complete without failing.\n@telans you're exactly right. @dfaker kindly reopen the issue. I can run the batch size fine, it will run the batch count to 4/16 then fail. it's akin to memory leak problem. or it's not un-allocating things it's supposed to be in VRAM when using batch count\n@dfaker it shouldn't be failing. I run a similar thing on NKMD and it doesn't have problems. \nCame here to report the same thing. I have 8GB of VRAM and nothing currently running, but my VRAM usage is still pegged at 7.3GB which isn't allowing room for any more generations.", "created_at": "2022-09-29T02:33:27Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 1270, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-1270", "issue_numbers": ["1208"], "base_commit": "0dc904aa3d24184d0efa2469cde373e25a9a0e87", "patch": "diff --git a/modules/sd_models.py b/modules/sd_models.py\nindex 9decc9115e9..e0b5cf2139e 100644\n--- a/modules/sd_models.py\n+++ b/modules/sd_models.py\n@@ -57,6 +57,11 @@ def modeltitle(path, h):\n             model_name = title.rsplit(\".\",1)[0] # remove extension if present\r\n             checkpoints_list[title] = CheckpointInfo(filename, title, h, model_name)\r\n \r\n+def get_closet_checkpoint_match(searchString):\r\n+    applicable = sorted([info for info in checkpoints_list.values() if searchString in info.title], key = lambda x:len(x.title))\r\n+    if len(applicable)>0:\r\n+        return applicable[0]\r\n+    return None\r\n \r\n def model_hash(filename):\r\n     try:\r\ndiff --git a/scripts/xy_grid.py b/scripts/xy_grid.py\nindex 24fa5a0a429..10c12d3bec0 100644\n--- a/scripts/xy_grid.py\n+++ b/scripts/xy_grid.py\n@@ -45,11 +45,8 @@ def apply_sampler(p, x, xs):\n \r\n \r\n def apply_checkpoint(p, x, xs):\r\n-    applicable = [info for info in modules.sd_models.checkpoints_list.values() if x in info.title]\r\n-    assert len(applicable) > 0, f'Checkpoint {x} for found'\r\n-\r\n-    info = applicable[0]\r\n-\r\n+    info = modules.sd_models.get_closet_checkpoint_match(x)\r\n+    assert info is not None, f'Checkpoint for {x} not found'\r\n     modules.sd_models.reload_model_weights(shared.sd_model, info)\r\n \r\n \r\n", "test_patch": "", "problem_statement": "X/Y plot does not switch between models whose names starts with the same phrase\n**Describe the bug**\r\nX/Y plot does not switch between models whose names starts with the same phrase.\r\nI have 3 models named:\r\n    model-wd.ckpt\r\n    model-wd-pruned.ckpt\r\n    model-wd-pruned-model-cn-poster-merged.ckpt\r\nI can manually switch between them in settings, but if I'm using X/Y plot script it uses model with the longest name (model-wd-pruned-model-cn-poster-merged.ckpt)\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. You should have several models whose names starts with the same phrase in models folder\r\n2. Start WebUI\r\n3. Select \"X/Y Plot\" as a script\r\n4. Select \"Checkpoint name\" as X or Y type and enter the checkpoints names \r\n5. Press \"Generate\"\r\n6. See the same picture for all models whose names starts with the same phrase\r\n\r\n**Expected behavior**\r\nX/Y Plot script should switch between all models provided in \"Checkpoint name\" field\r\n\r\n**Screenshots**\r\nSettings I use for X/Y Plot:\r\n\r\n![image](https://user-images.githubusercontent.com/38957619/192694005-ca457054-0b71-4811-b69d-af3e4f3d1067.png)\r\n\r\nFor some reason it loads other checkpoints after generating all images:\r\n\r\n![image](https://user-images.githubusercontent.com/38957619/192695136-69962a1c-95a2-4102-96c7-bccb1207cf26.png)\r\n\r\nResult:\r\n\r\n![image](https://user-images.githubusercontent.com/38957619/192694152-4a3f4f10-2790-46a0-95ab-9194cfbb6ad1.png)\r\n\r\nSame settings, but switching models manually\r\n\r\nmodel-wd:\r\n![image](https://user-images.githubusercontent.com/38957619/192694470-10692c1c-32c6-4432-8c71-1e1a74f5a893.png)\r\n\r\nmodel-wd-pruned:\r\n![image](https://user-images.githubusercontent.com/38957619/192694502-a9c4e3b1-f6db-44d5-8237-0d550ce2044c.png)\r\n\r\nmodel-wd-pruned-model-cn-poster-merged:\r\n![image](https://user-images.githubusercontent.com/38957619/192694529-9a1ef5ff-81d3-49af-a8a5-12169fb328f9.png)\r\n\r\n\r\n**Desktop:**\r\n - OS: Windows\r\n - Browser: Chrome\r\n - Commit revision: f2a4a2c3a672e22f088a7455d6039557370dd3f2\n", "hints_text": "I can't yet reproduce this, even with a couple of naming variations:\r\n\r\n![image](https://user-images.githubusercontent.com/35278260/192751051-9a51f553-7176-4516-a96a-dcda4a8fb7d1.png)\r\n![image](https://user-images.githubusercontent.com/35278260/192751798-6fb46bd8-186b-4ef8-945d-71da3d2276e2.png)\r\n![image](https://user-images.githubusercontent.com/35278260/192750298-84dfc4aa-7c2e-4671-b6d1-04a5ad9a452b.png)\r\n\r\nBatching seems to make no difference either.\r\n\r\nAny suggestions?\nI renamed the checkpoints and that seems to work\r\n\r\n![image](https://user-images.githubusercontent.com/38957619/192780433-a79ff5c8-55d5-4321-92ad-cf3dcbd5d940.png)\r\n\r\nI tried to debug, but can't see the reason why it uses the same model for all pictures\r\n\nOkay, we'll see if we can re-trigger it in that case.\r\n\r\nBy any chance were any of the models added to the models folder without the app being restarted?\n> By any chance were any of the models added to the models folder without the app being restarted?\r\n\r\nWebui was restarted many times after adding a models\nI have experienced this bug myself multiple times because I have lots of models named similarly \r\nThe issue comes from this line https://github.com/AUTOMATIC1111/stable-diffusion-webui/blob/master/scripts/xy_grid.py#L48\r\n\r\nI'm not sure if this is a bug, per se, it may also be an user error because of lack of documentation on how we should write the checkpoint name in the field (with or without extension).\r\n\r\nFor example, in my folders I have two models loaded in this order (according to what `sd_models.checkpoints_list.values()` tells me):\r\n`own-model-v13-mixed.ckpt`\r\n`own-model-v13.ckpt`\r\n\r\nWhen I'm trying to create a chart and I fill out the checkpoint name field like `own-model-v13, own-model-v12` it will use the first match from the checkpoints_list, which is not the expected result.\r\n\r\nAnother case where this happens is if you have the checkpoint name you're trying to load inside of another checkpoint's name, like\r\n`own-model-wd-mix.ckpt`\r\n`wd.ckpt`\r\n\r\nIf `own-model-wd-mix` is loaded before `wd`, when trying to load the `wd` checkpoint in the grid, it will instead load `own-model-wd-mix`.\r\n\r\n\r\n\r\n\nWonderful thank you!", "created_at": "2022-09-28T21:34:09Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 1261, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-1261", "issue_numbers": ["1224", "1224"], "base_commit": "0dc904aa3d24184d0efa2469cde373e25a9a0e87", "patch": "diff --git a/.gitignore b/.gitignore\nindex 69ea78c566e..b71e18759a8 100644\n--- a/.gitignore\n+++ b/.gitignore\n@@ -4,7 +4,7 @@ __pycache__\n /venv\n /tmp\n /model.ckpt\n-/models/*.ckpt\n+/models/**/*.ckpt\n /GFPGANv1.3.pth\n /gfpgan/weights/*.pth\n /ui-config.json\ndiff --git a/modules/ui.py b/modules/ui.py\nindex d51f7a08995..db960605321 100644\n--- a/modules/ui.py\n+++ b/modules/ui.py\n@@ -869,7 +869,7 @@ def create_ui(txt2img, img2img, run_extras, run_pnginfo, run_modelmerger):\n     with gr.Blocks() as modelmerger_interface:\r\n         with gr.Row().style(equal_height=False):\r\n             with gr.Column(variant='panel'):\r\n-                gr.HTML(value=\"<p>A merger of the two checkpoints will be generated in your <b>/models</b> directory.</p>\")\r\n+                gr.HTML(value=\"<p>A merger of the two checkpoints will be generated in your <b>checkpoint</b> directory.</p>\")\r\n                 \r\n                 with gr.Row():\r\n                     ckpt_name_list = sorted([x.title for x in modules.sd_models.checkpoints_list.values()])\r\n", "test_patch": "", "problem_statement": "Checkpoint Merger - FileNotFoundError: [Errno 2] No such file or directory:\nWhen using custom model folder: COMMANDLINE_ARGS=--ckpt-dir \"d:\\external\\models\\\" the checkpoint merger can't find the models\r\n\r\nFileNotFoundError: [Errno 2] No such file or directory: 'models/wd-v1-2-full-ema.ckpt'\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. set  COMMANDLINE_ARGS=--ckpt-dir  \"d:\\external\\models\"\r\n2. checkpint merger tab\r\n3. select models it found in the custom folder\r\n4. click run\r\n\r\n**Expected behavior**\r\nWould expect that since it knows where the models are, that it would continue to use that folder\r\n\nCheckpoint Merger - FileNotFoundError: [Errno 2] No such file or directory:\nWhen using custom model folder: COMMANDLINE_ARGS=--ckpt-dir \"d:\\external\\models\\\" the checkpoint merger can't find the models\r\n\r\nFileNotFoundError: [Errno 2] No such file or directory: 'models/wd-v1-2-full-ema.ckpt'\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. set  COMMANDLINE_ARGS=--ckpt-dir  \"d:\\external\\models\"\r\n2. checkpint merger tab\r\n3. select models it found in the custom folder\r\n4. click run\r\n\r\n**Expected behavior**\r\nWould expect that since it knows where the models are, that it would continue to use that folder\r\n\n", "hints_text": "It also doesn't support merging models in subdirectories, it can read them but then fails on saving after the merge.\nSo it fails to read the models from the checkpoint directory; I imagine it also fails to save to that directory?\nIt also doesn't support merging models in subdirectories, it can read them but then fails on saving after the merge.\nSo it fails to read the models from the checkpoint directory; I imagine it also fails to save to that directory?", "created_at": "2022-09-28T20:30:25Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 1260, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-1260", "issue_numbers": ["996"], "base_commit": "d62954c2bc149053f9f51dfe95751b9e0ea29f03", "patch": "diff --git a/requirements.txt b/requirements.txt\nindex ec30a59892f..ab2fc501f34 100644\n--- a/requirements.txt\n+++ b/requirements.txt\n@@ -16,7 +16,7 @@ realesrgan\n scikit-image>=0.19\r\n git+https://github.com/TencentARC/GFPGAN.git@8d2447a2d918f8eba5a4a01463fd48e45126a379\r\n timm==0.4.12\r\n-transformers\r\n+transformers==4.19.2\r\n torch\r\n einops\r\n jsonmerge\r\n", "test_patch": "", "problem_statement": "img2img - Interrogate error (colab - Voldemort)\nWhen trying to use Interrogate under img2img, I get an error. It will not Interrogate (know that this is when using the colab - StableDiffusionUI-Voldemort V1.1.ipynb)\r\nNote: sorry if this is in the wrong spot. Have no clue where to post this issue\r\n\r\n\r\nSteps to reproduce the behavior:\r\n1. Using Voldemort V1.1 colab, Go to img2img tab\r\n2. Upload image\r\n3. Tap Interrogate\r\n4. See error\r\n\r\n**Expected behavior**\r\nInterrogate button will interrogate the image properly\r\n\r\n\r\n**Desktop (please complete the following information):**\r\n - OS: Mac\r\n - Browser: Brave\r\n\r\n\n", "hints_text": "```\r\nload checkpoint from https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_caption_capfilt_large.pth\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 890M/890M [00:14<00:00, 64.0MiB/s]\r\nError interrogating\r\nTraceback (most recent call last):\r\n  File \"/content/stable-diffusion-webui/modules/interrogate.py\", line 136, in interrogate\r\n    caption = self.generate_caption(pil_image)\r\n  File \"/content/stable-diffusion-webui/modules/interrogate.py\", line 121, in generate_caption\r\n    caption = self.blip_model.generate(gpu_image, sample=False, num_beams=shared.opts.interrogate_clip_num_beams, min_length=shared.opts.interrogate_clip_min_length, max_length=shared.opts.interrogate_clip_max_length)\r\n  File \"/content/stable-diffusion-webui/repositories/BLIP/models/blip.py\", line 163, in generate\r\n    **model_kwargs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\", line 27, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/transformers/generation_utils.py\", line 1146, in generate\r\n    self._validate_model_kwargs(model_kwargs.copy())\r\n  File \"/usr/local/lib/python3.7/dist-packages/transformers/generation_utils.py\", line 862, in _validate_model_kwargs\r\n    f\"The following `model_kwargs` are not used by the model: {unused_model_args} (note: typos in the\"\r\nValueError: The following `model_kwargs` are not used by the model: ['encoder_hidden_states', 'encoder_attention_mask'] (note: typos in the generate arguments will also show up in this list)\r\n\r\nTraceback (most recent call last):\r\n  File \"/content/stable-diffusion-webui/modules/interrogate.py\", line 136, in interrogate\r\n    caption = self.generate_caption(pil_image)\r\n  File \"/content/stable-diffusion-webui/modules/interrogate.py\", line 121, in generate_caption\r\n    caption = self.blip_model.generate(gpu_image, sample=False, num_beams=shared.opts.interrogate_clip_num_beams, min_length=shared.opts.interrogate_clip_min_length, max_length=shared.opts.interrogate_clip_max_length)\r\n  File \"/content/stable-diffusion-webui/repositories/BLIP/models/blip.py\", line 163, in generate\r\n    **model_kwargs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\", line 27, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/transformers/generation_utils.py\", line 1146, in generate\r\n    self._validate_model_kwargs(model_kwargs.copy())\r\n  File \"/usr/local/lib/python3.7/dist-packages/transformers/generation_utils.py\", line 862, in _validate_model_kwargs\r\n    f\"The following `model_kwargs` are not used by the model: {unused_model_args} (note: typos in the\"\r\nValueError: The following `model_kwargs` are not used by the model: ['encoder_hidden_states', 'encoder_attention_mask'] (note: typos in the generate arguments will also show up in this list)\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.7/dist-packages/gradio/routes.py\", line 274, in run_predict\r\n    fn_index, raw_input, username, session_state, iterators\r\n  File \"/usr/local/lib/python3.7/dist-packages/gradio/blocks.py\", line 753, in process_api\r\n    result = await self.call_function(fn_index, inputs, iterator)\r\n  File \"/usr/local/lib/python3.7/dist-packages/gradio/blocks.py\", line 631, in call_function\r\n    block_fn.fn, *processed_input, limiter=self.limiter\r\n  File \"/usr/local/lib/python3.7/dist-packages/anyio/to_thread.py\", line 32, in run_sync\r\n    func, *args, cancellable=cancellable, limiter=limiter\r\n  File \"/usr/local/lib/python3.7/dist-packages/anyio/_backends/_asyncio.py\", line 937, in run_sync_in_worker_thread\r\n    return await future\r\n  File \"/usr/local/lib/python3.7/dist-packages/anyio/_backends/_asyncio.py\", line 867, in run\r\n    result = context.run(func, *args)\r\n  File \"/content/stable-diffusion-webui/modules/ui.py\", line 252, in interrogate\r\n    prompt = shared.interrogator.interrogate(image)\r\n  File \"/content/stable-diffusion-webui/modules/interrogate.py\", line 163, in interrogate\r\n    res += \"<error>\"\r\nTypeError: unsupported operand type(s) for +=: 'NoneType' and 'str'\r\n```", "created_at": "2022-09-28T20:22:42Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 1126, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-1126", "issue_numbers": ["1013"], "base_commit": "c0b1177a3203091ca43f2d08f24dd821f1237612", "patch": "diff --git a/.gitignore b/.gitignore\nindex fa1ab43e7b3..69ea78c566e 100644\n--- a/.gitignore\n+++ b/.gitignore\n@@ -20,4 +20,5 @@ __pycache__\n /interrogate\n /user.css\n /.idea\n+notification.mp3\n /SwinIR\ndiff --git a/javascript/notification.js b/javascript/notification.js\nindex e8159a7e5b5..bdf614ada21 100644\n--- a/javascript/notification.js\n+++ b/javascript/notification.js\n@@ -25,6 +25,9 @@ onUiUpdate(function(){\n \n     lastHeadImg = headImg;\n \n+    // play notification sound if available\n+    gradioApp().querySelector('#audio_notification audio')?.play();\n+\n     if (document.hasFocus()) return;\n \n     // Multiple copies of the images are in the DOM when one is selected. Dedup with a Set to get the real number generated.\ndiff --git a/modules/ui.py b/modules/ui.py\nindex d2402e28dd2..efd467088e6 100644\n--- a/modules/ui.py\n+++ b/modules/ui.py\n@@ -564,13 +564,13 @@ def create_ui(txt2img, img2img, run_extras, run_pnginfo):\n                     with gr.TabItem('Inpaint', id='inpaint'):\r\n                         init_img_with_mask = gr.Image(label=\"Image for inpainting with mask\",  show_label=False, elem_id=\"img2maskimg\", source=\"upload\", interactive=True, type=\"pil\", tool=\"sketch\", image_mode=\"RGBA\")\r\n \r\n-                        init_img_inpaint = gr.Image(label=\"Image for img2img\", show_label=False, source=\"upload\", interactive=True, type=\"pil\", visible=False)\r\n-                        init_mask_inpaint = gr.Image(label=\"Mask\", source=\"upload\", interactive=True, type=\"pil\", visible=False)\r\n+                        init_img_inpaint = gr.Image(label=\"Image for img2img\", show_label=False, source=\"upload\", interactive=True, type=\"pil\", visible=False, elem_id=\"img_inpaint_base\")\r\n+                        init_mask_inpaint = gr.Image(label=\"Mask\", source=\"upload\", interactive=True, type=\"pil\", visible=False, elem_id=\"img_inpaint_mask\")\r\n \r\n                         mask_blur = gr.Slider(label='Mask blur', minimum=0, maximum=64, step=1, value=4)\r\n \r\n                         with gr.Row():\r\n-                            mask_mode = gr.Radio(label=\"Mask mode\", show_label=False, choices=[\"Draw mask\", \"Upload mask\"], type=\"index\", value=\"Draw mask\")\r\n+                            mask_mode = gr.Radio(label=\"Mask mode\", show_label=False, choices=[\"Draw mask\", \"Upload mask\"], type=\"index\", value=\"Draw mask\", elem_id=\"mask_mode\")\r\n                             inpainting_mask_invert = gr.Radio(label='Masking mode', show_label=False, choices=['Inpaint masked', 'Inpaint not masked'], value='Inpaint masked', type=\"index\")\r\n \r\n                         inpainting_fill = gr.Radio(label='Masked content', choices=['fill', 'original', 'latent noise', 'latent nothing'], value='fill', type=\"index\")\r\n@@ -970,6 +970,9 @@ def run_settings(*args):\n             for interface, label, ifid in interfaces:\r\n                 with gr.TabItem(label, id=ifid):\r\n                     interface.render()\r\n+        \r\n+        if os.path.exists(os.path.join(script_path, \"notification.mp3\")):\r\n+            audio_notification = gr.Audio(interactive=False, value=os.path.join(script_path, \"notification.mp3\"), elem_id=\"audio_notification\", visible=False)\r\n \r\n         text_settings = gr.Textbox(elem_id=\"settings_json\", value=lambda: opts.dumpjson(), visible=False)\r\n         settings_submit.click(\r\n", "test_patch": "", "problem_statement": "Option for sound effect/notification upon job completion\nProblem:\r\nI often alt-tab and work on other things while the images are generating. Then it's usually half an hour passed when I realize that the job is done.    \r\n\r\nSolution request:\r\nOption for sound effect/notification upon job completion.\n", "hints_text": "I believe notification is already built in. If you go to settings screen and scroll to the bottom, there is a button to prompt for browser notification.\nSound effect would still be a nice option. Especially for those of us who access it over HTTP and can't use browser notifications.", "created_at": "2022-09-26T20:59:13Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 1125, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-1125", "issue_numbers": ["736"], "base_commit": "78d6aef30249530d8cc0b5023b2f8bf8c86f8446", "patch": "diff --git a/scripts/img2imgalt.py b/scripts/img2imgalt.py\nindex 7b4ba24476f..0ef137f7daa 100644\n--- a/scripts/img2imgalt.py\n+++ b/scripts/img2imgalt.py\n@@ -59,7 +59,55 @@ def find_noise_for_image(p, cond, uncond, cfg_scale, steps):\n     return x / x.std()\r\n \r\n \r\n-Cached = namedtuple(\"Cached\", [\"noise\", \"cfg_scale\", \"steps\", \"latent\", \"original_prompt\", \"original_negative_prompt\"])\r\n+Cached = namedtuple(\"Cached\", [\"noise\", \"cfg_scale\", \"steps\", \"latent\", \"original_prompt\", \"original_negative_prompt\", \"sigma_adjustment\"])\r\n+\r\n+\r\n+# Based on changes suggested by briansemrau in https://github.com/AUTOMATIC1111/stable-diffusion-webui/issues/736\r\n+def find_noise_for_image_sigma_adjustment(p, cond, uncond, cfg_scale, steps):\r\n+    x = p.init_latent\r\n+\r\n+    s_in = x.new_ones([x.shape[0]])\r\n+    dnw = K.external.CompVisDenoiser(shared.sd_model)\r\n+    sigmas = dnw.get_sigmas(steps).flip(0)\r\n+\r\n+    shared.state.sampling_steps = steps\r\n+\r\n+    for i in trange(1, len(sigmas)):\r\n+        shared.state.sampling_step += 1\r\n+\r\n+        x_in = torch.cat([x] * 2)\r\n+        sigma_in = torch.cat([sigmas[i - 1] * s_in] * 2)\r\n+        cond_in = torch.cat([uncond, cond])\r\n+\r\n+        c_out, c_in = [K.utils.append_dims(k, x_in.ndim) for k in dnw.get_scalings(sigma_in)]\r\n+\r\n+        if i == 1:\r\n+            t = dnw.sigma_to_t(torch.cat([sigmas[i] * s_in] * 2))\r\n+        else:\r\n+            t = dnw.sigma_to_t(sigma_in)\r\n+\r\n+        eps = shared.sd_model.apply_model(x_in * c_in, t, cond=cond_in)\r\n+        denoised_uncond, denoised_cond = (x_in + eps * c_out).chunk(2)\r\n+\r\n+        denoised = denoised_uncond + (denoised_cond - denoised_uncond) * cfg_scale\r\n+\r\n+        if i == 1:\r\n+            d = (x - denoised) / (2 * sigmas[i])\r\n+        else:\r\n+            d = (x - denoised) / sigmas[i - 1]\r\n+\r\n+        dt = sigmas[i] - sigmas[i - 1]\r\n+        x = x + d * dt\r\n+\r\n+        sd_samplers.store_latent(x)\r\n+\r\n+        # This shouldn't be necessary, but solved some VRAM issues\r\n+        del x_in, sigma_in, cond_in, c_out, c_in, t,\r\n+        del eps, denoised_uncond, denoised_cond, denoised, d, dt\r\n+\r\n+    shared.state.nextjob()\r\n+\r\n+    return x / sigmas[-1]\r\n \r\n \r\n class Script(scripts.Script):\r\n@@ -78,9 +126,10 @@ def ui(self, is_img2img):\n         cfg = gr.Slider(label=\"Decode CFG scale\", minimum=0.0, maximum=15.0, step=0.1, value=1.0)\r\n         st = gr.Slider(label=\"Decode steps\", minimum=1, maximum=150, step=1, value=50)\r\n         randomness = gr.Slider(label=\"Randomness\", minimum=0.0, maximum=1.0, step=0.01, value=0.0)\r\n-        return [original_prompt, original_negative_prompt, cfg, st, randomness]\r\n+        sigma_adjustment = gr.Checkbox(label=\"Sigma adjustment for finding noise for image\", value=False)\r\n+        return [original_prompt, original_negative_prompt, cfg, st, randomness, sigma_adjustment]\r\n \r\n-    def run(self, p, original_prompt, original_negative_prompt, cfg, st, randomness):\r\n+    def run(self, p, original_prompt, original_negative_prompt, cfg, st, randomness, sigma_adjustment):\r\n         p.batch_size = 1\r\n         p.batch_count = 1\r\n \r\n@@ -88,7 +137,10 @@ def run(self, p, original_prompt, original_negative_prompt, cfg, st, randomness)\n         def sample_extra(conditioning, unconditional_conditioning, seeds, subseeds, subseed_strength):\r\n             lat = (p.init_latent.cpu().numpy() * 10).astype(int)\r\n \r\n-            same_params = self.cache is not None and self.cache.cfg_scale == cfg and self.cache.steps == st and self.cache.original_prompt == original_prompt and self.cache.original_negative_prompt == original_negative_prompt\r\n+            same_params = self.cache is not None and self.cache.cfg_scale == cfg and self.cache.steps == st \\\r\n+                                and self.cache.original_prompt == original_prompt \\\r\n+                                and self.cache.original_negative_prompt == original_negative_prompt \\\r\n+                                and self.cache.sigma_adjustment == sigma_adjustment\r\n             same_everything = same_params and self.cache.latent.shape == lat.shape and np.abs(self.cache.latent-lat).sum() < 100\r\n \r\n             if same_everything:\r\n@@ -97,8 +149,11 @@ def sample_extra(conditioning, unconditional_conditioning, seeds, subseeds, subs\n                 shared.state.job_count += 1\r\n                 cond = p.sd_model.get_learned_conditioning(p.batch_size * [original_prompt])\r\n                 uncond = p.sd_model.get_learned_conditioning(p.batch_size * [original_negative_prompt])\r\n-                rec_noise = find_noise_for_image(p, cond, uncond, cfg, st)\r\n-                self.cache = Cached(rec_noise, cfg, st, lat, original_prompt, original_negative_prompt)\r\n+                if sigma_adjustment:\r\n+                    rec_noise = find_noise_for_image_sigma_adjustment(p, cond, uncond, cfg, st)\r\n+                else:\r\n+                    rec_noise = find_noise_for_image(p, cond, uncond, cfg, st)\r\n+                self.cache = Cached(rec_noise, cfg, st, lat, original_prompt, original_negative_prompt, sigma_adjustment)\r\n \r\n             rand_noise = processing.create_random_tensors(p.init_latent.shape[1:], [p.seed + x + 1 for x in range(p.init_latent.shape[0])])\r\n             \r\n@@ -121,6 +176,7 @@ def sample_extra(conditioning, unconditional_conditioning, seeds, subseeds, subs\n         p.extra_generation_params[\"Decode CFG scale\"] = cfg\r\n         p.extra_generation_params[\"Decode steps\"] = st\r\n         p.extra_generation_params[\"Randomness\"] = randomness\r\n+        p.extra_generation_params[\"Sigma Adjustment\"] = sigma_adjustment\r\n \r\n         processed = processing.process_images(p)\r\n \r\n", "test_patch": "", "problem_statement": "Img2Img alt should divide by sigma[-1], not std\nhttps://github.com/AUTOMATIC1111/stable-diffusion-webui/blob/53651696dc1492f3b7cc009d64a55532bc787aa7/scripts/img2imgalt.py#L59\r\n\r\nWhile the noise used in stable diffusion is generated from a normal distribution, that doesn't mean that it is always perfectly normal. Normalizing the reverse-generated noise results in incorrect saturation in the output image.\r\n\r\nI've been able to solve the saturation issue by dividing by the first sigma. I spent some time verifying the img2imgalt technique by attempting to re-derive it. Dividing by the first sigma appears to be the correct approach.\r\n\r\nSee my contribution to another repo here:\r\ncode: https://github.com/sd-webui/stable-diffusion-webui/blob/17748cbc9c34df44d0381c42e4f0fe1903089438/scripts/sd_utils.py#L525\r\noriginal pr: https://github.com/sd-webui/stable-diffusion-webui/pull/1070/files#diff-2e278c1b9a8c0e308b8272729de19c973ac24710b3467dfb9c877db5d1cf7a3f\r\n\r\n\r\nEdit:\r\nThe changes by MartinCairnsSQL below are _also_ required. They're from the original author's gist.\r\nhttps://github.com/AUTOMATIC1111/stable-diffusion-webui/issues/736#issuecomment-1252538516\n", "hints_text": "Dumb/random question, but why does it only work with Euler, and would it be possible to make it work with Euler_A/something else?\nIt only works with euler because the code is just the euler sampler in reverse. To support other samplers, they have to be similarly derived. It shouldn't be too hard for some of them, but it's still not super easy.\nThis just gives me blurry images for settings that used to work. What settings reproduce original images by the author with your code?\nIt's gives me blurry images too\nI had a look at the diffs in the method and found that the variables sigma_in, t & d also have changes.  I made the changes to a copy of the script and tested a few images and found the original version had too much contrast for the same set of prompts other than that the sigma change didn't blur the image with the extra changes.\r\n\r\n[img2imgalt_sigma.zip](https://github.com/AUTOMATIC1111/stable-diffusion-webui/files/9609118/img2imgalt_sigma.zip)\r\n\r\nhttps://github.com/AUTOMATIC1111/stable-diffusion-webui/blob/c2775528c9e5336b37987832795621fa49ff97d3/scripts/img2imgalt.py#L35\r\n```\r\n        sigma_in = torch.cat([sigmas[i - 1] * s_in] * 2)\r\n```\r\n\r\nhttps://github.com/AUTOMATIC1111/stable-diffusion-webui/blob/c2775528c9e5336b37987832795621fa49ff97d3/scripts/img2imgalt.py#L39\r\n```\r\n        if i == 1:\r\n            t = dnw.sigma_to_t(torch.cat([sigmas[i] * s_in] * 2))\r\n        else:\r\n            t = dnw.sigma_to_t(sigma_in)\r\n```\r\n\r\nhttps://github.com/AUTOMATIC1111/stable-diffusion-webui/blob/c2775528c9e5336b37987832795621fa49ff97d3/scripts/img2imgalt.py#L46\r\n```\r\n        if i == 1:\r\n            d = (x - denoised) / (2 * sigmas[i])\r\n        else:\r\n            d = (x - denoised) / sigmas[i - 1]\r\n```\nThese changes look good.\r\n\r\nWould it be possible to combine img2imgalt with masked inpaint in some way?\r\nSeparating the original prompt from the img2imgalt prompt we are essentially trying to tell the program which parts are the ones we want changed. But sometimes it is hard for the program to key in on those areas. Trying to change the hair color it would only do parts of the hair, and I would have to throw the settings too far out of whack, producing jibbirish in the process, in order to encapsulate the full hair. If I could do masked assist, or even automatically using a tool as txt2mask, would that not yield a better outcome?", "created_at": "2022-09-26T20:30:40Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 1112, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-1112", "issue_numbers": ["981"], "base_commit": "78d6aef30249530d8cc0b5023b2f8bf8c86f8446", "patch": "diff --git a/javascript/dragdrop.js b/javascript/dragdrop.js\nindex c01f66e2dd5..5aac57f77b9 100644\n--- a/javascript/dragdrop.js\n+++ b/javascript/dragdrop.js\n@@ -68,13 +68,19 @@ window.addEventListener('paste', e => {\n     if ( ! isValidImageList( files ) ) {\n         return;\n     }\n-    [...gradioApp().querySelectorAll('input[type=file][accept=\"image/x-png,image/gif,image/jpeg\"]')]\n-        .filter(input => !input.matches('.\\\\!hidden input[type=file]'))\n-        .forEach(input => {\n-            input.files = files;\n-            input.dispatchEvent(new Event('change'))\n-        });\n-    [...gradioApp().querySelectorAll('[data-testid=\"image\"]')]\n-        .filter(imgWrap => !imgWrap.closest('.\\\\!hidden'))\n-        .forEach(imgWrap => dropReplaceImage( imgWrap, files ));\n+\n+    const visibleImageFields = [...gradioApp().querySelectorAll('[data-testid=\"image\"]')]\n+        .filter(el => uiElementIsVisible(el));\n+    if ( ! visibleImageFields.length ) {\n+        return;\n+    }\n+    \n+    const firstFreeImageField = visibleImageFields\n+        .filter(el => el.querySelector('input[type=file]'))?.[0];\n+\n+    dropReplaceImage(\n+        firstFreeImageField ?\n+        firstFreeImageField :\n+        visibleImageFields[visibleImageFields.length - 1]\n+    , files );\n });\ndiff --git a/script.js b/script.js\nindex 7f26e23bd1f..cf9896053ac 100644\n--- a/script.js\n+++ b/script.js\n@@ -39,3 +39,24 @@ document.addEventListener(\"DOMContentLoaded\", function() {\n     });\n     mutationObserver.observe( gradioApp(), { childList:true, subtree:true })\n });\n+\n+/**\n+ * checks that a UI element is not in another hidden element or tab content\n+ */\n+function uiElementIsVisible(el) {\n+    let isVisible = !el.closest('.\\\\!hidden');\n+    if ( ! isVisible ) {\n+        return false;\n+    }\n+\n+    while( isVisible = el.closest('.tabitem')?.style.display !== 'none' ) {\n+        if ( ! isVisible ) {\n+            return false;\n+        } else if ( el.parentElement ) {\n+            el = el.parentElement\n+        } else {\n+            break;\n+        }\n+    }\n+    return isVisible;\n+}\n\\ No newline at end of file\n", "test_patch": "", "problem_statement": "Pasting the mask doesn't work anymore, it pastes as the input image AND the mask, used to work\n**Describe the bug**\r\nWhen doing inpainting and uploading a mask, pasting the mask from another software (say PS) will replace both the input image and the mask\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Go to img2img inpainting\r\n2. Click on upload mask\r\n3. Create a mask in photoshop and copy the image content\r\n4. Paste the mask in the img2img inpainting tab\r\n5. Observe as the mask is copied in both fields, the input and the mask, regardless if there was another image already in either of those fields\r\n\r\n**Expected behavior**\r\nIt use to be that once the input image was in place, pasting another image would put it in the mask field (if upload mask was selected)\r\n\r\n**Desktop (please complete the following information):**\r\n - OS: Windows 10\r\n - Brave\r\n - Commit revision a2bea2f97aab6ef60afe6534611e646f66226868\r\n\n", "hints_text": "this doesn't seem to have been intended by the drag-drop/paste code, if it ever functioned like this it would have been down to the internals of Gradio.\r\n\r\n@Connum this ring any bells with you?\nThere's a tricky work around, if you paste the input image, then go to edit the crop, it's going to lock it in place and pasting the mask will only go into the mask field (careful not to scroll up or anything in the input field else you'll just mess with the image crop & position)\r\n\r\nI may be wrong but I think this could have happened when they fixed the \"drag & drop/paste doesn't swap the current image with the new one\" thingy\nIt's unfortunately a bit more difficult to fix than I anticipated at first, because of the timing of uploads and UI state changes, but I'll see if I can come up with something in the next few days (given that nobody beats me to it, which everyone's welcome to do!)", "created_at": "2022-09-26T16:17:37Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 1031, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-1031", "issue_numbers": ["802"], "base_commit": "50ae19acf6003708390ae6696e833cf596621882", "patch": "diff --git a/javascript/hints.js b/javascript/hints.js\nindex b43f7bbd66d..ed79796f95e 100644\n--- a/javascript/hints.js\n+++ b/javascript/hints.js\n@@ -57,14 +57,13 @@ titles = {\n \n     \"Interrogate\": \"Reconstruct prompt from existing image and put it into the prompt field.\",\n \n-    \"Images filename pattern\": \"Use following tags to define how filenames for images are chosen: [steps], [cfg], [prompt], [prompt_spaces], [width], [height], [styles], [sampler], [seed], [model_hash], [prompt_words], [date]; leave empty for default.\",\n-    \"Directory name pattern\": \"Use following tags to define how subdirectories for images and grids are chosen: [steps], [cfg], [prompt], [prompt_spaces], [width], [height], [styles], [sampler], [seed], [model_hash], [prompt_words], [date]; leave empty for default.\",\n+    \"Images filename pattern\": \"Use following tags to define how filenames for images are chosen: [steps], [cfg], [prompt], [prompt_spaces], [width], [height], [styles], [sampler], [seed], [model_hash], [prompt_words], [date], [job_timestamp]; leave empty for default.\",\n+    \"Directory name pattern\": \"Use following tags to define how subdirectories for images and grids are chosen: [steps], [cfg], [prompt], [prompt_spaces], [width], [height], [styles], [sampler], [seed], [model_hash], [prompt_words], [date], [job_timestamp]; leave empty for default.\",\n     \"Max prompt words\": \"Set the maximum number of words to be used in the [prompt_words] option; ATTENTION: If the words are too long, they may exceed the maximum length of the file path that the system can handle\",\n \n     \"Loopback\": \"Process an image, use it as an input, repeat.\",\n     \"Loops\": \"How many times to repeat processing an image and using it as input for the next iteration\",\n \n-\n     \"Style 1\": \"Style to apply; styles have components for both positive and negative prompts and apply to both\",\n     \"Style 2\": \"Style to apply; styles have components for both positive and negative prompts and apply to both\",\n     \"Apply style\": \"Insert selected styles into prompt fields\",\ndiff --git a/modules/images.py b/modules/images.py\nindex 642cde36a0f..ae0e6304321 100644\n--- a/modules/images.py\n+++ b/modules/images.py\n@@ -295,6 +295,7 @@ def apply_filename_pattern(x, p, seed, prompt):\n \r\n     x = x.replace(\"[model_hash]\", shared.sd_model.sd_model_hash)\r\n     x = x.replace(\"[date]\", datetime.date.today().isoformat())\r\n+    x = x.replace(\"[job_timestamp]\", shared.state.job_timestamp)\r\n \r\n     if cmd_opts.hide_ui_dir_config:\r\n         x = re.sub(r'^[\\\\/]+|\\.{2,}[\\\\/]+|[\\\\/]+\\.{2,}', '', x)\r\ndiff --git a/modules/shared.py b/modules/shared.py\nindex 1ce6eefcca0..c32da110dba 100644\n--- a/modules/shared.py\n+++ b/modules/shared.py\n@@ -4,6 +4,7 @@\n import os\r\n import gradio as gr\r\n import tqdm\r\n+import datetime\r\n \r\n import modules.artists\r\n from modules.paths import script_path, sd_path\r\n@@ -65,6 +66,7 @@ class State:\n     job = \"\"\r\n     job_no = 0\r\n     job_count = 0\r\n+    job_timestamp = 0\r\n     sampling_step = 0\r\n     sampling_steps = 0\r\n     current_latent = None\r\n@@ -78,6 +80,8 @@ def nextjob(self):\n         self.job_no += 1\r\n         self.sampling_step = 0\r\n         self.current_image_sampling_step = 0\r\n+    def get_job_timestamp(self):\r\n+        return datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\r\n \r\n \r\n state = State()\r\ndiff --git a/webui.py b/webui.py\nindex 64c9d462eaa..9ea5f5a3228 100644\n--- a/webui.py\n+++ b/webui.py\n@@ -50,6 +50,7 @@ def f(*args, **kwargs):\n         shared.state.sampling_step = 0\r\n         shared.state.job_count = -1\r\n         shared.state.job_no = 0\r\n+        shared.state.job_timestamp = shared.state.get_job_timestamp()\r\n         shared.state.current_latent = None\r\n         shared.state.current_image = None\r\n         shared.state.current_image_sampling_step = 0\r\n", "test_patch": "", "problem_statement": "Request: something akin to a batch ID for folder name pattern\n**Is your feature request related to a problem? Please describe.**\r\nI've been experimenting with the wildcards script linked in the wiki lately.\r\nIt's wonderful but unfortunately, it's wreaking havoc on the folder structure in my outputs/txt2img-images folder since every single image of a batch is likely to have a different prompt. \r\n\r\nAnother problem this feature request would solve is when you run the same prompt (or at least similar start) multiple times. With the current filename patterns available, multiple batches are all sorted in the same folder. This is, admittedly, not a huge annoyance to most users, but it can be an issue or at least unexpected behavior when your prompt from today gets sorted into a folder from 3 days ago because you reused a prompt.\r\n\r\n**Describe the solution you'd like**\r\nGenerate an ID when starting a batch and make that available as a folder name pattern.  \r\nI don't care much about how that ID is formatted. I'd be fine with a random string of, let's say, 5 hexadecimal characters.\r\n\r\n**Describe alternatives you've considered**\r\nI don't believe there is a suitable alternative that allows a user to have images grouped by batch that does not have the potential to include images from other batches accidentally. At least not with the current file/directory patterns available.\r\n\r\n**Additional context**\r\nAll of these are from just one wildcard batch of 20, all containing just a single image. \r\n![](https://i.imgur.com/8CXxtsO.png)\r\n\r\nMy output directory pattern is set up in such a way that I have a folder for every day. As you can see, the subfolder just for *today* has >1000 subfolders. It's hard to keep track of things like this.\r\n![](https://i.imgur.com/6u2v0Hu.png)\r\n\n", "hints_text": "", "created_at": "2022-09-25T11:15:04Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 881, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-881", "issue_numbers": ["855"], "base_commit": "e16b9dc8192bc42cd13c7bdfc7d5719c102bc295", "patch": "diff --git a/javascript/progressbar.js b/javascript/progressbar.js\nindex 20b4dc624f6..ce0f0df35e9 100644\n--- a/javascript/progressbar.js\n+++ b/javascript/progressbar.js\n@@ -3,7 +3,7 @@ global_progressbar = null\n \n onUiUpdate(function(){\n     progressbar = gradioApp().getElementById('progressbar')\n-    progressDiv = gradioApp().querySelectorAll('.progressDiv').length > 0;\n+    progressDiv = gradioApp().querySelectorAll('#progressSpan').length > 0;\n     interrupt = gradioApp().getElementById('interrupt')\n \tif(progressbar!= null && progressbar != global_progressbar){\n \t    global_progressbar = progressbar\n@@ -40,7 +40,7 @@ function requestMoreProgress(){\n     if(btn==null) return;\n \n     btn.click();\n-    progressDiv = gradioApp().querySelectorAll('.progressDiv').length > 0;\n+    progressDiv = gradioApp().querySelectorAll('#progressSpan').length > 0;\n     if(progressDiv){\n         interrupt.style.display = \"block\"\n     }\ndiff --git a/modules/ui.py b/modules/ui.py\nindex 036f2ed3a00..233663d85c0 100644\n--- a/modules/ui.py\n+++ b/modules/ui.py\n@@ -200,7 +200,7 @@ def check_progress_call():\n         else:\r\n             preview_visibility = gr_show(True)\r\n \r\n-    return f\"<span style='display: none'>{time.time()}</span><p>{progressbar}</p>\", preview_visibility, image\r\n+    return f\"<span id='progressSpan' style='display: none'>{time.time()}</span><p>{progressbar}</p>\", preview_visibility, image\r\n \r\n \r\n def check_progress_call_initial():\r\n", "test_patch": "", "problem_statement": "Generate button doesn't change to Interrupt button when \"show progressbar\" is disabled in settings.\n**Describe the bug**\r\nWith \"show progressbar\" off in settings, the Generate button doesn't change when generation begins and interrupt doesn't work.\r\n\r\nWith \"show progressbar\" on in settings, the Generate button changes to the Interrupt button and works as expected.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Go to Settings tab\r\n2. Uncheck \"show progressbar\"\r\n3. Go to txt2img tab.\r\n4. Click Generate button\r\n5. Note that Generate button doesn't change to Interrupt button, leaving user with no way to interrupt generation.\r\n\r\n**Desktop (please complete the following information):**\r\n - OS: Windows 10\r\n - Browser Firefox\r\n - Commit revision a213d3a21c9e37297fdcb2c2b48bd24290a479cf\r\n\r\n\r\n\n", "hints_text": "", "created_at": "2022-09-22T20:28:36Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 867, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-867", "issue_numbers": ["668"], "base_commit": "75b90a5e403b7037430b6c9db956844d2d1c1c4d", "patch": "diff --git a/javascript/imageMaskFix.js b/javascript/imageMaskFix.js\nnew file mode 100644\nindex 00000000000..3d77bfe9b45\n--- /dev/null\n+++ b/javascript/imageMaskFix.js\n@@ -0,0 +1,45 @@\n+/**\n+ * temporary fix for https://github.com/AUTOMATIC1111/stable-diffusion-webui/issues/668\n+ * @see https://github.com/gradio-app/gradio/issues/1721\n+ */\n+window.addEventListener( 'resize', () => imageMaskResize());\n+function imageMaskResize() {\n+    const canvases = gradioApp().querySelectorAll('#img2maskimg .touch-none canvas');\n+    if ( ! canvases.length ) {\n+    canvases_fixed = false;\n+    window.removeEventListener( 'resize', imageMaskResize );\n+    return;\n+    }\n+\n+    const wrapper = canvases[0].closest('.touch-none');\n+    const previewImage = wrapper.previousElementSibling;\n+\n+    if ( ! previewImage.complete ) {\n+        previewImage.addEventListener( 'load', () => imageMaskResize());\n+        return;\n+    }\n+\n+    const w = previewImage.width;\n+    const h = previewImage.height;\n+    const nw = previewImage.naturalWidth;\n+    const nh = previewImage.naturalHeight;\n+    const portrait = nh > nw;\n+    const factor = portrait;\n+\n+    const wW = Math.min(w, portrait ? h/nh*nw : w/nw*nw);\n+    const wH = Math.min(h, portrait ? h/nh*nh : w/nw*nh);\n+\n+    wrapper.style.width = `${wW}px`;\n+    wrapper.style.height = `${wH}px`;\n+    wrapper.style.left = `${(w-wW)/2}px`;\n+    wrapper.style.top = `${(h-wH)/2}px`;\n+\n+    canvases.forEach( c => {\n+        c.style.width = c.style.height = '';\n+        c.style.maxWidth = '100%';\n+        c.style.maxHeight = '100%';\n+        c.style.objectFit = 'contain';\n+    });\n+ }\n+  \n+ onUiUpdate(() => imageMaskResize());\n\\ No newline at end of file\n", "test_patch": "", "problem_statement": "Misplaced inpaint mask in ultrawide images\nIf I inpaint in an image with 960x384 pixels, the mask is misplaced somehow.\r\n\r\n![grafik](https://user-images.githubusercontent.com/17240204/190919531-fda72a97-353d-4491-aa41-f778f1c03fcc.png)\r\n\r\nIn the result, the marked area is inpainted. Not the face. I tried a lot of different parameters, but can't influence it.\r\n\r\n![grafik](https://user-images.githubusercontent.com/17240204/190919598-1013dcc5-5252-4d85-b5b3-ac06a6aa27f3.png)\r\n\r\n\n", "hints_text": "I've noticed similar results. It works fine for e.g. 768x512, but certain resolutions lead to coordinate system missmatches between what's masked and what's used when inpainting.\nJust found this happens to me with source images that are 768x512, so it being ultra-wide is not required. It seems to resize the mask, because if I draw a vertical line 1/5 and 4/5 across the image, the rightmost one will be just to the left of the centre of the image.\r\n\r\nEDIT: I just realised that the \"canvas\" for drawing the inpaint area extends quite a way to the right of the input image. I think the issue is that if your browser is not wide enough, the input image is scaled down but the mask canvas is not.\r\n\r\nEDIT #2: Zooming out fixes the issue, so the above seems to be the case.\nGood hint. Would be nice ti get a fix for that, so that the mask is also scaled with the window.\nI think I can confirm this assumtion.\r\nThe mask layer is not scaled down to the image size.\r\nI can even paint the mask on the right output window.\r\n\r\n![grafik](https://user-images.githubusercontent.com/17240204/191115413-6210569f-a5bb-4cc8-9e08-9f874a8cd26c.png)\r\n\r\nYou can see the two masked areas and the result in the small output window in the same picture. Filled the mask with latent noise in 1 sampling step made it clearly visible.\r\nThe output windows is also way to small.\r\n\nWe could get around this just by uploading a mask instead of drawing one.\r\n\r\nUnfortunately, I don't see any information on how to actually create a mask image for uploading.\n> Unfortunately, I don't see any information on how to actually create a mask image for uploading.\r\n\r\nIt's just a black and white image (no grey or alpha, if I understand correctly), created in the image editing software of your choice, where white is the part you want to change.\r\n\r\nEDIT: Just noticed you can make part of the image not opaque and that will be regarded as part of the mask when you upload it:\r\n\r\n> erase a part of picture in external editor and upload a transparent picture. Any even slightly transparent areas will become part of the mask\r\n\r\nThough I'm not sure how that works with \"original\" fill pattern (does it still provide the RGB, for example).\nUsing external masks works, but the workflow is a lot slower than just using the unbuilt feature.\nIn the meantime I'm just cutting out lower res squares of the area I want to inpaint in Photoshop and then pasting them back into the image.\nI tried to figure out what the problem is and noticed two things:\r\n\r\n1. If you set this in style.css, the display doesn't overflow into the right area anymore.\r\n\r\n`#img2maskimg .h-60{\r\n    height: 30rem;\r\n    overflow: hidden !important;\r\n}\r\n`\r\n\r\n2. If you first load an image of 512x512px, and then a wider one, the yellow marked values don't get updated.\r\n![grafik](https://user-images.githubusercontent.com/17240204/191587634-fb8f1f56-990d-4e60-abc7-b140355ed625.png)\r\n\r\nIf you correct them by hand in the devconsole, it's better.\r\n\r\nTwo things that help improve the situation, but the mask is still off in the final picture. I am still searching for a reason.\r\nTell me if anyone has an idea please. ;)\nI'm in the process of figuring this out. Here's a quick-fix you can do to be able to draw accurate masks again:\r\n\r\nCreate a `user.css` file in the root directory with the following content:\r\n```css\r\n[data-testid=\"image\"] .touch-none canvas {\r\n    width: 100% !important;\r\n    object-fit: contain;\r\n}\r\n```\r\nthen restart the webui.\r\n\r\nThere's an offset between the mouse and the \"brush\" on the canvas, but ignoring one's actual mouse cursor, it's possible to draw the mask.\n@AUTOMATIC1111 does this functionality come completely out of gradio? Because I couldn't find any reference to the masks/canvasess in the code of this repo.\nIt seems like they are aware over at gradio that something's not right with the inpainting functionality and are working on an overhaul: https://github.com/gradio-app/gradio/issues/1721\r\nIt would still be nice to have a temporary fix until it's fixed in gradio itself, and I'll see if I can come up with something.", "created_at": "2022-09-22T17:44:00Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 773, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-773", "issue_numbers": ["771"], "base_commit": "1578859305f3593fa213309e505905e607a0e52c", "patch": "diff --git a/javascript/dragdrop.js b/javascript/dragdrop.js\nindex 29e26766e88..c01f66e2dd5 100644\n--- a/javascript/dragdrop.js\n+++ b/javascript/dragdrop.js\n@@ -10,22 +10,34 @@ function dropReplaceImage( imgWrap, files ) {\n     }\n \n     imgWrap.querySelector('.modify-upload button + button, .touch-none + div button + button')?.click();\n-    window.requestAnimationFrame( () => {\n+    const callback = () => {\n         const fileInput = imgWrap.querySelector('input[type=\"file\"]');\n         if ( fileInput ) {\n             fileInput.files = files;\n             fileInput.dispatchEvent(new Event('change'));   \n         }\n-    });\n-}\n-\n-function pressClearBtn(hoverElems) {\n-    //Find all buttons hovering over the image box\n-    let btns = Array.from(hoverElems.querySelectorAll(\"button\"))\n-\n-    //Press the last btn which will be the X button\n-    if (btns.length)\n-        btns[btns.length-1].click()\n+    };\n+    \n+    if ( imgWrap.closest('#pnginfo_image') ) {\n+        // special treatment for PNG Info tab, wait for fetch request to finish\n+        const oldFetch = window.fetch;\n+        window.fetch = async (input, options) => {\n+            const response = await oldFetch(input, options);\n+            if ( 'api/predict/' === input ) {\n+                const content = await response.text();\n+                window.fetch = oldFetch;\n+                window.requestAnimationFrame( () => callback() );\n+                return new Response(content, {\n+                    status: response.status,\n+                    statusText: response.statusText,\n+                    headers: response.headers\n+                })\n+            }\n+            return response;\n+        };        \n+    } else {\n+        window.requestAnimationFrame( () => callback() );\n+    }\n }\n \n window.document.addEventListener('dragover', e => {\n@@ -36,13 +48,7 @@ window.document.addEventListener('dragover', e => {\n     }\n     e.stopPropagation();\n     e.preventDefault();\n-\n-    if (e.dataTransfer) \n-         e.dataTransfer.dropEffect = 'copy';   \n-        \n-    //If is gr.Interface clear image on hover\n-    if (target.previousElementSibling)\n-        pressClearBtn(target.previousElementSibling)\n+    e.dataTransfer.dropEffect = 'copy';\n });\n \n window.document.addEventListener('drop', e => {\ndiff --git a/modules/ui.py b/modules/ui.py\nindex 0d428b5b330..e290b3ebb94 100644\n--- a/modules/ui.py\n+++ b/modules/ui.py\n@@ -797,7 +797,7 @@ def apply_mode(mode, uploadmask):\n     pnginfo_interface = gr.Interface(\r\n         wrap_gradio_call(run_pnginfo),\r\n         inputs=[\r\n-            gr.Image(label=\"Source\", source=\"upload\", interactive=True, type=\"pil\"),\r\n+            gr.Image(elem_id=\"pnginfo_image\", label=\"Source\", source=\"upload\", interactive=True, type=\"pil\"),\r\n         ],\r\n         outputs=[\r\n             gr.HTML(),\r\n", "test_patch": "", "problem_statement": "Images being cleared on dragover\nWith 9035afb, dragging over an image will trigger the reset of that image. I don't find that a logical UI behaviour from a UX point of view. I might accidentally drag something over the browser window and involuntarily lose the current image. \r\n\r\n@trufty, would you mind explaining why that change was made?\r\nIf it was merely to fix the issue that in the PNG Info tab the info was not updated when replacing the image via paste or drop, I have already prepared a solution for that without clearing the image on dragover. In that case I'd make a PR that reverses that change and implements the (in my view) better fix for that issue.\n", "hints_text": "", "created_at": "2022-09-20T23:59:54Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 737, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-737", "issue_numbers": ["662"], "base_commit": "1ebb5cc6696b66968851fb60034e4e707654aa10", "patch": "diff --git a/launch.py b/launch.py\nindex 197e656bdd7..42e45226d3c 100644\n--- a/launch.py\n+++ b/launch.py\n@@ -11,7 +11,7 @@\n \r\n python = sys.executable\r\n git = os.environ.get('GIT', \"git\")\r\n-torch_command = os.environ.get('TORCH_COMMAND', \"pip install torch==1.12.1+cu113 --extra-index-url https://download.pytorch.org/whl/cu113\")\r\n+torch_command = os.environ.get('TORCH_COMMAND', \"pip install torch==1.12.1+cu113 torchvision==0.13.1+cu113 --extra-index-url https://download.pytorch.org/whl/cu113\")\r\n requirements_file = os.environ.get('REQS_FILE', \"requirements_versions.txt\")\r\n commandline_args = os.environ.get('COMMANDLINE_ARGS', \"\")\r\n \r\n@@ -92,8 +92,9 @@ def git_clone(url, dir, name, commithash=None):\n print(f\"Python {sys.version}\")\r\n print(f\"Commit hash: {commit}\")\r\n \r\n-if not is_installed(\"torch\"):\r\n-    run(f'\"{python}\" -m {torch_command}', \"Installing torch\", \"Couldn't install torch\")\r\n+\r\n+if not is_installed(\"torch\") or not is_installed(\"torchvision\"):\r\n+    run(f'\"{python}\" -m {torch_command}', \"Installing torch and torchvision\", \"Couldn't install torch\")\r\n \r\n run_python(\"import torch; assert torch.cuda.is_available(), 'Torch is not able to use GPU'\")\r\n \r\ndiff --git a/webui.sh b/webui.sh\nold mode 100644\nnew mode 100755\nindex cb00f832959..4534f149247\n--- a/webui.sh\n+++ b/webui.sh\n@@ -41,12 +41,6 @@ then\n     venv_dir=\"venv\"\n fi\n \n-# install command for torch\n-if [[ -z \"${TORCH_COMMAND}\" ]]\n-then\n-    export TORCH_COMMAND=\"pip install torch==1.12.1+cu113 --extra-index-url https://download.pytorch.org/whl/cu113\"\n-fi\n-\n # Do not reinstall existing pip packages on Debian/Ubuntu\n export PIP_IGNORE_INSTALLED=0\n \n", "test_patch": "", "problem_statement": "GFPGAN restore faces error\nUsing GFPGAN restore faces gives following error\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/x/stable-diff/stable-diffusion-webui/modules/ui.py\", line 128, in f\r\n    res = list(func(*args, **kwargs))\r\n  File \"/home/x/stable-diff/stable-diffusion-webui/webui.py\", line 55, in f\r\n    res = func(*args, **kwargs)\r\n  File \"/home/x/stable-diff/stable-diffusion-webui/modules/txt2img.py\", line 39, in txt2img\r\n    processed = process_images(p)\r\n  File \"/home/x/stable-diff/stable-diffusion-webui/modules/processing.py\", line 314, in process_images\r\n    x_sample = modules.face_restoration.restore_faces(x_sample)\r\n  File \"/home/x/stable-diff/stable-diffusion-webui/modules/face_restoration.py\", line 19, in restore_faces\r\n    return face_restorer.restore(np_image)\r\n  File \"/home/x/stable-diff/stable-diffusion-webui/modules/codeformer_model.py\", line 79, in restore\r\n    self.face_helper.get_face_landmarks_5(only_center_face=False, resize=640, eye_dist_threshold=5)\r\n  File \"/home/x/stable-diff/stable-diffusion-webui/repositories/CodeFormer/facelib/utils/face_restoration_helper.py\", line 151, in get_face_landmarks_5\r\n    bboxes = self.face_det.detect_faces(input_img)\r\n  File \"/home/x/stable-diff/stable-diffusion-webui/repositories/CodeFormer/facelib/detection/retinaface/retinaface.py\", line 231, in detect_faces\r\n    keep = py_cpu_nms(bounding_boxes, nms_threshold)\r\n  File \"/home/x/stable-diff/stable-diffusion-webui/repositories/CodeFormer/facelib/detection/retinaface/retinaface_utils.py\", line 41, in py_cpu_nms\r\n    keep = torchvision.ops.nms(\r\n  File \"/home/x/.local/lib/python3.10/site-packages/torchvision/ops/boxes.py\", line 40, in nms\r\n    _assert_has_ops()\r\n  File \"/home/x/.local/lib/python3.10/site-packages/torchvision/extension.py\", line 33, in _assert_has_ops\r\n    raise RuntimeError(\r\nRuntimeError: Couldn't load custom C++ ops. This can happen if your PyTorch and torchvision versions are incompatible, or if you had errors while compiling torchvision from source. For further information on the compatible versions, check https://github.com/pytorch/vision#installation for the compatibility matrix. Please check your PyTorch version with torch.__version__ and your torchvision version with torchvision.__version__ and verify if they are compatible, and if not please reinstall torchvision so that it matches your PyTorch install.\r\n\r\n\r\nRunning: python -c \"import torch; import torchvision; print(torch.__version__); print(torchvision.__version__)\"\r\n\r\nGives the following results:\r\n1.12.1+cu113\r\n0.13.1+cu102\r\n\r\n\r\non Latest Arch Linux.\r\n\r\nGFPGAN works without issues in this similar tool: https://github.com/cmdr2/stable-diffusion-ui\r\n\r\n\n", "hints_text": "I'm seeing this as well. also on arch linux. \r\nI was getting this with GFPGAN and codeformer :scream: \nadding the correct version of torchvision to `requirements_version.txt` and running webui.sh seemed to fix it, but i needed to install cuda and cudnn from pacman\nI fixed this (Arch Linux) by setting the torch version to `+cu102` (instead of `+cu113`), which makes it match the subversion of torchvision.\r\n\r\nYou may be able to fix an existing installation by changing into the base directory and issuing the following commands.\r\n```\r\nsource venv/bin/activate\r\n pip3 install -Iv torch  --extra-index-url https://download.pytorch.org/whl/cu102\r\n```\r\n\r\nThis will activate the python virtual environment sd-webgui runs in and pip should download cu102 and uninstall the later version (e.g. cu113).\n> I fixed this (Arch Linux) by setting the torch version to `+cu102` (instead of `+cu113`), which makes it match the subversion of torchvision.\r\n> \r\n> You may be able to fix an existing installation by changing into the base directory and issuing the following commands.\r\n> \r\n> ```\r\n> source venv/bin/activate\r\n>  pip3 install -Iv torch  --extra-index-url https://download.pytorch.org/whl/cu102\r\n> ```\r\n> \r\n> This will activate the python virtual environment sd-webgui runs in and pip should download cu102 and uninstall the later version (e.g. cu113).\r\n\r\nThanks, i did this with cu113 and it is now restoring faces. \r\n`source venv/bin/activate`\r\n`pip3 install torchvision==0.13.1+cu113 --extra-index-url https://download.pytorch.org/whl/cu113\r\n`\r\n\n> Thanks, i did this with cu113 and it is now restoring faces.\r\n\r\nGlad to hear that it worked for you! Also, useful to know that upgrading `torchvision`, rather than downgrading `torch` worked as well, so thanks for confirming that.", "created_at": "2022-09-20T03:46:45Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 673, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-673", "issue_numbers": ["274"], "base_commit": "21086e60a9e4ad6f677ccc7719be651356c18a2e", "patch": "diff --git a/modules/shared.py b/modules/shared.py\nindex eb4db55aa1c..e74a00be5d4 100644\n--- a/modules/shared.py\n+++ b/modules/shared.py\n@@ -49,6 +49,7 @@\n parser.add_argument(\"--gradio-auth\", type=str, help='set gradio authentication like \"username:password\"; or comma-delimit multiple like \"u1:p1,u2:p2,u3:p3\"', default=None)\r\n parser.add_argument(\"--opt-channelslast\", action='store_true', help=\"change memory type for stable diffusion to channels last\")\r\n parser.add_argument(\"--styles-file\", type=str, help=\"filename to use for styles\", default=os.path.join(script_path, 'styles.csv'))\r\n+parser.add_argument(\"--autolaunch\", action=argparse.BooleanOptionalAction, help=\"open the webui URL in the system's default browser upon launch\", default=False)\r\n cmd_opts = parser.parse_args()\r\n \r\n if cmd_opts.opt_split_attention:\r\ndiff --git a/webui.py b/webui.py\nindex ff8997dbba1..81ed1df4496 100644\n--- a/webui.py\n+++ b/webui.py\n@@ -89,6 +89,7 @@ def sigint_handler(sig, frame):\n         server_port=cmd_opts.port,\r\n         debug=cmd_opts.gradio_debug,\r\n         auth=[tuple(cred.split(':')) for cred in cmd_opts.gradio_auth.strip('\"').split(',')] if cmd_opts.gradio_auth else None,\r\n+        inbrowser=cmd_opts.autolaunch,\r\n     )\r\n \r\n \r\n", "test_patch": "", "problem_statement": "[Feature Request] Automatic link opening\nA parameter that allows you to automatically open a UI link in the browser would be very convenient!\n", "hints_text": "", "created_at": "2022-09-18T19:03:43Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 672, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-672", "issue_numbers": ["649"], "base_commit": "21086e60a9e4ad6f677ccc7719be651356c18a2e", "patch": "diff --git a/javascript/dragdrop.js b/javascript/dragdrop.js\nindex 0dddea8836b..4cd05151143 100644\n--- a/javascript/dragdrop.js\n+++ b/javascript/dragdrop.js\n@@ -9,7 +9,7 @@ function dropReplaceImage( imgWrap, files ) {\n         return;\n     }\n \n-    imgWrap.querySelector('.modify-upload button + button')?.click();\n+    imgWrap.querySelector('.modify-upload button + button, .touch-none + div button + button')?.click();\n     window.requestAnimationFrame( () => {\n         const fileInput = imgWrap.querySelector('input[type=\"file\"]');\n         if ( fileInput ) {\n", "test_patch": "", "problem_statement": "Pasting or dropping inpaint image does not replace existing image\n**Describe the bug**\r\nPasting or dropping inpaint image does not replace existing image (see [comment in #645](https://github.com/AUTOMATIC1111/stable-diffusion-webui/issues/645#issuecomment-1250230557)).\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Go to img2img\r\n2. Switch to \"Inpaint a part of image\"\r\n3. Select an image\r\n4. Try to drop a different image or paste from clipboard\r\n\r\n**Expected behavior**\r\nImage is replaced, just as for any other image as implemented with #628\r\n\r\n**Additional context**\r\nMight look into this myself as soon as I find the time\r\n\n", "hints_text": "", "created_at": "2022-09-18T18:02:04Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 651, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-651", "issue_numbers": ["646"], "base_commit": "83a65919bb2af35c0d47cbb47b8db2ac233e86ce", "patch": "diff --git a/modules/memmon.py b/modules/memmon.py\nindex f2cac841fa5..9fb9b687caf 100644\n--- a/modules/memmon.py\n+++ b/modules/memmon.py\n@@ -22,6 +22,13 @@ def __init__(self, name, device, opts):\n         self.run_flag = threading.Event()\n         self.data = defaultdict(int)\n \n+        try:\n+            torch.cuda.mem_get_info()\n+            torch.cuda.memory_stats(self.device)\n+        except Exception as e:  # AMD or whatever\n+            print(f\"Warning: caught exception '{e}', memory monitor disabled\")\n+            self.disabled = True\n+\n     def run(self):\n         if self.disabled:\n             return\n@@ -62,13 +69,14 @@ def monitor(self):\n         self.run_flag.set()\n \n     def read(self):\n-        free, total = torch.cuda.mem_get_info()\n-        self.data[\"total\"] = total\n-\n-        torch_stats = torch.cuda.memory_stats(self.device)\n-        self.data[\"active_peak\"] = torch_stats[\"active_bytes.all.peak\"]\n-        self.data[\"reserved_peak\"] = torch_stats[\"reserved_bytes.all.peak\"]\n-        self.data[\"system_peak\"] = total - self.data[\"min_free\"]\n+        if not self.disabled:\n+            free, total = torch.cuda.mem_get_info()\n+            self.data[\"total\"] = total\n+\n+            torch_stats = torch.cuda.memory_stats(self.device)\n+            self.data[\"active_peak\"] = torch_stats[\"active_bytes.all.peak\"]\n+            self.data[\"reserved_peak\"] = torch_stats[\"reserved_bytes.all.peak\"]\n+            self.data[\"system_peak\"] = total - self.data[\"min_free\"]\n \n         return self.data\n \ndiff --git a/modules/ui.py b/modules/ui.py\nindex 202f45518b8..451ad253175 100644\n--- a/modules/ui.py\n+++ b/modules/ui.py\n@@ -119,7 +119,9 @@ def save_files(js_data, images, index):\n \r\n def wrap_gradio_call(func):\r\n     def f(*args, **kwargs):\r\n-        shared.mem_mon.monitor()\r\n+        run_memmon = opts.memmon_poll_rate > 0 and not shared.mem_mon.disabled\r\n+        if run_memmon:\r\n+            shared.mem_mon.monitor()\r\n         t = time.perf_counter()\r\n \r\n         try:\r\n@@ -136,17 +138,20 @@ def f(*args, **kwargs):\n \r\n         elapsed = time.perf_counter() - t\r\n \r\n-        mem_stats = {k: -(v//-(1024*1024)) for k,v in shared.mem_mon.stop().items()}\r\n-        active_peak = mem_stats['active_peak']\r\n-        reserved_peak = mem_stats['reserved_peak']\r\n-        sys_peak = '?' if opts.memmon_poll_rate <= 0 else mem_stats['system_peak']\r\n-        sys_total = mem_stats['total']\r\n-        sys_pct = '?' if opts.memmon_poll_rate <= 0 else round(sys_peak/sys_total * 100, 2)\r\n-        vram_tooltip = \"Torch active: Peak amount of VRAM used by Torch during generation, excluding cached data.&#013;\" \\\r\n-                       \"Torch reserved: Peak amount of VRAM allocated by Torch, including all active and cached data.&#013;\" \\\r\n-                       \"Sys VRAM: Peak amount of VRAM allocation across all applications / total GPU VRAM (peak utilization%).\"\r\n-\r\n-        vram_html = '' if opts.memmon_poll_rate == 0 else f\"<p class='vram' title='{vram_tooltip}'>Torch active/reserved: {active_peak}/{reserved_peak} MiB, <wbr>Sys VRAM: {sys_peak}/{sys_total} MiB ({sys_pct}%)</p>\"\r\n+        if run_memmon:\r\n+            mem_stats = {k: -(v//-(1024*1024)) for k, v in shared.mem_mon.stop().items()}\r\n+            active_peak = mem_stats['active_peak']\r\n+            reserved_peak = mem_stats['reserved_peak']\r\n+            sys_peak = mem_stats['system_peak']\r\n+            sys_total = mem_stats['total']\r\n+            sys_pct = round(sys_peak/max(sys_total, 1) * 100, 2)\r\n+            vram_tooltip = \"Torch active: Peak amount of VRAM used by Torch during generation, excluding cached data.&#013;\" \\\r\n+                           \"Torch reserved: Peak amount of VRAM allocated by Torch, including all active and cached data.&#013;\" \\\r\n+                           \"Sys VRAM: Peak amount of VRAM allocation across all applications / total GPU VRAM (peak utilization%).\"\r\n+\r\n+            vram_html = f\"<p class='vram' title='{vram_tooltip}'>Torch active/reserved: {active_peak}/{reserved_peak} MiB, <wbr>Sys VRAM: {sys_peak}/{sys_total} MiB ({sys_pct}%)</p>\"\r\n+        else:\r\n+            vram_html = ''\r\n \r\n         # last item is always HTML\r\n         res[-1] += f\"<div class='performance'><p class='time'>Time taken: <wbr>{elapsed:.2f}s</p>{vram_html}</div>\"\r\n", "test_patch": "", "problem_statement": "New VRAM Monitoring throws errors on an AMD powered install\nAs the title says, after updating to the latest branch, stable diffusion stopped worked. After some debugging and rudimentary coding around, removing any references to the Memmon.py allows for the webui to work as usual again.\r\n\r\nPerhaps an option to outright disable the memory monitoring?\n", "hints_text": "What's the error?\r\nThere's a setting for `VRAM usage polls per second during generation. Set to 0 to disable.`, but depending on what AMD doesn't support, it's likely to still error, since there's still portions of code that get called even with polling off. I can fix it so 0 turns more code off.\nSorry for the sparse details. Tomorrow I'll test a little more and come back to you with some more precise data rather than 'there's an error lol'. But basically It makes a call for a function in torch that doesn't actually exist (in the AMD compatible version anyway). The program actually still technically works, (as in images are still generated) it's just that it bugs out the UI, preventing you from seeing the result unless you actually go into the outputs folder. ", "created_at": "2022-09-18T10:04:40Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 648, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-648", "issue_numbers": ["645"], "base_commit": "3f29aa791bceea0b0de99263dd0a7f08c0551549", "patch": "diff --git a/javascript/dragdrop.js b/javascript/dragdrop.js\nindex a3605bdc7c8..0dddea8836b 100644\n--- a/javascript/dragdrop.js\n+++ b/javascript/dragdrop.js\n@@ -54,5 +54,6 @@ window.addEventListener('paste', e => {\n             input.dispatchEvent(new Event('change'))\n         });\n     [...gradioApp().querySelectorAll('[data-testid=\"image\"]')]\n+        .filter(imgWrap => !imgWrap.closest('.\\\\!hidden'))\n         .forEach(imgWrap => dropReplaceImage( imgWrap, files ));\n });\n", "test_patch": "", "problem_statement": "Ctrl+V into img2img no longer functions as of 7e779382\n**Describe the bug**\r\nAs of now, trying to Ctrl+V an image into img2img produces following error, when you try to \"Generate\":\r\n```\r\nTraceback (most recent call last):\r\n  File \"...\\auto1111-stable-diffusion\\venv\\lib\\site-packages\\gradio\\routes.py\", line 268, in run_predict\r\n    output = await app.blocks.process_api(\r\n  File \"...\\auto1111-stable-diffusion\\venv\\lib\\site-packages\\gradio\\blocks.py\", line 745, in process_api\r\n    inputs = self.preprocess_data(fn_index, inputs, state)\r\n  File \"...\\auto1111-stable-diffusion\\venv\\lib\\site-packages\\gradio\\blocks.py\", line 612, in preprocess_data\r\n    processed_input.append(block.preprocess(raw_input[i]))\r\n  File \"...\\auto1111-stable-diffusion\\venv\\lib\\site-packages\\gradio\\components.py\", line 1342, in preprocess\r\n    mask_im = processing_utils.decode_base64_to_image(mask)\r\n  File \"...\\auto1111-stable-diffusion\\venv\\lib\\site-packages\\gradio\\processing_utils.py\", line 28, in decode_base64_to_image\r\n    content = encoding.split(\";\")[1]\r\nAttributeError: 'NoneType' object has no attribute 'split'\r\n```\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Go to google images and copy any image through right click menu\r\n2. launch stable diffusion, go to img2img tab, press Ctrl+V, then press \"generate\"\r\n3. See error in console\r\n\r\n**Expected behavior**\r\nNo error in console\r\n\r\n**Desktop (please complete the following information):**\r\n - OS: Windows 10\r\n - Browser Chrome\r\n - Commit revision 7e77938230d4fefb6edccdba0b80b61d8416673e\r\n\r\n**Additional context**\r\nI've investigated this with git-bisect, the problem appears to have been introduced by commit 88e315a8d5a879b5a0c171bcc7ba6a29f87cd664\r\n```\r\ncommit 88e315a8d5a879b5a0c171bcc7ba6a29f87cd664\r\nDate:   Sat Sep 17 21:09:47 2022 +0200\r\n\r\n    Replace existing images when pasting from clipboard or dropping a new one (fixes #573)\r\n```\r\n\n", "hints_text": "Not sure if this is related to our PR #628, but as it worked in our tests before making the PR and it's stated that it only stopped working with a later commit after it was merged, I don't think so.\r\n\r\nEdit: Could it be a browser specific issue? Which browser is being used and does it work using a different one? \n@48design that's the commit git-bisect landed at, and I can confirm that it crashes reliably on that specific commit. whether it is caused by the commit itself or interaction with previous commit during merge, I don't know. I've made a fork out of commit e9a9764e002158e943ca519da4e7e476cc50810d and it works fine now. That commit is right before the one I reported in history.\r\n\r\nThe browser is 64 bit chrome 105.0.5195.127, and it worked fine with every version before that.\r\n\r\nI think it is somehow related to gallery code, because I actually witnessed the same error message ONCE on e9a9764e002158e943ca519da4e7e476cc50810d, and the gallery failed to update when that happened. However, starting with 88e315a8d5a879b5a0c171bcc7ba6a29f87cd664 , img2img is simply unusable for me as it crashes on any attempt to generate. At least for me.\nThanks for the info, I'll look into it but probably tomorrow!\r\nDoes the issue occur when using drag and drop as well, or only with pasting from the clipboard?\nOn my machine:\r\nFirefox 104.0.2: ctrl+v never worked with any revision.\r\n\r\nChrome 105.0.5195.127: ctrl+v worked before 88e315a8d5a879b5a0c171bcc7ba6a29f87cd664 (but not if image is already there), worked fully right after the commit, and works with the current version after my js file split.\r\n\r\nIt also pastes into all three image boxes img2img image, inpainting drawing image, and inpainting mask image.\r\n\r\nIt also does not allow you to drag into inpainting drawing image if there already is an image in it.\n@48design \r\nTested it a bit more.\r\n* Ctrl+V does not work in firefox.\r\n* Dragging image from firefox to chrome does not work.\r\n* Dragging image from chrome to chrome produces the same error\r\n* Dragging image from filesystem (explorer) into chrome produces the same error.\r\n* Deleting \"venv\" and launching webui.cmd to redownload venv does not fix it.\r\n\r\nI'll sit on e9a9764e002158e943ca519da4e7e476cc50810d meanwhile.\r\n\r\n------\r\n\r\nCould it be somehow locale related? \r\nRight, also, Python version is 3.10.6\n> It also pastes into all three image boxes img2img image, inpainting drawing image, and inpainting mask image.\r\n\r\nThat was a good hint! That triggered the inpaint image being updated but without any mask drawn, which results in this error. (In dependent from the pasting, so if you click generate without a mask image, that error will come up). I'll post a PR to fix the pasting and will also open a new issue for the other bug.", "created_at": "2022-09-18T09:42:58Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 628, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-628", "issue_numbers": ["573"], "base_commit": "23a0ec04c005957091ab35c26c4c31485e75d146", "patch": "diff --git a/script.js b/script.js\nindex 113d4335117..e533d084833 100644\n--- a/script.js\n+++ b/script.js\n@@ -326,12 +326,51 @@ function submit(){\n     return res\n }\n \n-window.addEventListener('paste', e => {\n-    const files = e.clipboardData.files;\n-    if (!files || files.length !== 1) {\n+function isValidImageList( files ) {\n+    return files && files?.length === 1 && ['image/png', 'image/gif', 'image/jpeg'].includes(files[0].type);\n+}\n+\n+function dropReplaceImage( imgWrap, files ) {\n+    if ( ! isValidImageList( files ) ) {\n         return;\n     }\n-    if (!['image/png', 'image/gif', 'image/jpeg'].includes(files[0].type)) {\n+\n+    imgWrap.querySelector('.modify-upload button + button')?.click();\n+    window.requestAnimationFrame( () => {\n+        const fileInput = imgWrap.querySelector('input[type=\"file\"]');\n+        if ( fileInput ) {\n+            fileInput.files = files;\n+            fileInput.dispatchEvent(new Event('change'));   \n+        }\n+    });\n+}\n+\n+window.document.addEventListener('dragover', e => {\n+    const target = e.composedPath()[0];\n+    const imgWrap = target.closest('[data-testid=\"image\"]');\n+    if ( !imgWrap ) {\n+        return;\n+    }\n+    e.stopPropagation();\n+    e.preventDefault();\n+    e.dataTransfer.dropEffect = 'copy';\n+});\n+\n+window.document.addEventListener('drop', e => {\n+    const target = e.composedPath()[0];\n+    const imgWrap = target.closest('[data-testid=\"image\"]');\n+    if ( !imgWrap ) {\n+        return;\n+    }\n+    e.stopPropagation();\n+    e.preventDefault();\n+    const files = e.dataTransfer.files;\n+    dropReplaceImage( imgWrap, files );\n+});\n+\n+window.addEventListener('paste', e => {\n+    const files = e.clipboardData.files;\n+    if ( ! isValidImageList( files ) ) {\n         return;\n     }\n     [...gradioApp().querySelectorAll('input[type=file][accept=\"image/x-png,image/gif,image/jpeg\"]')]\n@@ -340,6 +379,8 @@ window.addEventListener('paste', e => {\n             input.files = files;\n             input.dispatchEvent(new Event('change'))\n         });\n+    [...gradioApp().querySelectorAll('[data-testid=\"image\"]')]\n+        .forEach(imgWrap => dropReplaceImage( imgWrap, files ));\n });\n \n function ask_for_style_name(_, prompt_text, negative_prompt_text) {\n", "test_patch": "", "problem_statement": "Drag and drop should replace existing picture\nRight now, in the Extras and PNG info tabs, if you drag and drop another picture on an existing picture, it opens a new tab in the browser, instead of replacing the picture like you would expect.\r\nYou always have to remove the old picture first, which is a little uncomfortable to be honest.\r\nPlease make new drag and dropped pictures just replace previous ones.\r\n\r\n(Thanks for implementing the batch process in the Extras tab recently.)\r\n\n", "hints_text": "I think this is a limitation [from Gradio](https://gradio.app/docs/#image), I wonder if we can try to add some code in script.js file to detect a drag and drop event to clear the image automatically", "created_at": "2022-09-17T19:13:16Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 417, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-417", "issue_numbers": ["306"], "base_commit": "85b97cc49c4766cb47306e71e552871a0791ea29", "patch": "diff --git a/modules/extras.py b/modules/extras.py\nindex cb083544397..55a782c7c1d 100644\n--- a/modules/extras.py\n+++ b/modules/extras.py\n@@ -69,7 +69,7 @@ def upscale(image, scaler_index, resize):\n     while len(cached_images) > 2:\r\n         del cached_images[next(iter(cached_images.keys()))]\r\n \r\n-    images.save_image(image, outpath, \"\", None, info=info, extension=opts.samples_format, short_filename=True, no_prompt=True, pnginfo_section_name=\"extras\", existing_info=existing_pnginfo)\r\n+    images.save_image(image, path=outpath, basename=\"\", seed=None, prompt=None, extension=opts.samples_format, info=info, short_filename=True, no_prompt=True, grid=False, pnginfo_section_name=\"extras\", existing_info=existing_pnginfo)\r\n \r\n     return image, plaintext_to_html(info), ''\r\n \r\ndiff --git a/modules/images.py b/modules/images.py\nindex 50b0e099cb0..cc608b8e6b8 100644\n--- a/modules/images.py\n+++ b/modules/images.py\n@@ -280,10 +280,7 @@ def apply_filename_pattern(x, p, seed, prompt):\n     return x\r\n \r\n \r\n-def save_image(image, path, basename, seed=None, prompt=None, extension='png', info=None, short_filename=False, no_prompt=False, pnginfo_section_name='parameters', p=None, existing_info=None):\r\n-    # would be better to add this as an argument in future, but will do for now\r\n-    is_a_grid = basename != \"\"\r\n-\r\n+def save_image(image, path, basename, seed=None, prompt=None, extension='png', info=None, short_filename=False, no_prompt=False, grid=False, pnginfo_section_name='parameters', p=None, existing_info=None):\r\n     if short_filename or prompt is None or seed is None:\r\n         file_decoration = \"\"\r\n     elif opts.save_to_dirs:\r\n@@ -307,7 +304,7 @@ def save_image(image, path, basename, seed=None, prompt=None, extension='png', i\n     else:\r\n         pnginfo = None\r\n \r\n-    save_to_dirs = (is_a_grid and opts.grid_save_to_dirs) or (not is_a_grid and opts.save_to_dirs)\r\n+    save_to_dirs = (grid and opts.grid_save_to_dirs) or (not grid and opts.save_to_dirs and not no_prompt)\r\n \r\n     if save_to_dirs:\r\n         dirname = apply_filename_pattern(opts.directories_filename_pattern or \"[prompt_words]\", p, seed, prompt)\r\ndiff --git a/modules/img2img.py b/modules/img2img.py\nindex 70c99e33bc2..08e15911cc4 100644\n--- a/modules/img2img.py\n+++ b/modules/img2img.py\n@@ -96,7 +96,7 @@ def img2img(prompt: str, negative_prompt: str, prompt_style: str, init_img, init\n \r\n         grid = images.image_grid(history, batch_size, rows=1)\r\n \r\n-        images.save_image(grid, p.outpath_grids, \"grid\", initial_seed, prompt, opts.grid_format, info=info, short_filename=not opts.grid_extended_filename, p=p)\r\n+        images.save_image(grid, p.outpath_grids, \"grid\", initial_seed, prompt, opts.grid_format, info=info, short_filename=not opts.grid_extended_filename, grid=True, p=p)\r\n \r\n         processed = Processed(p, history, initial_seed, initial_info)\r\n \r\ndiff --git a/modules/processing.py b/modules/processing.py\nindex f33560eebf1..ef25d43df71 100644\n--- a/modules/processing.py\n+++ b/modules/processing.py\n@@ -321,7 +321,7 @@ def infotext(iteration=0, position_in_batch=0):\n                 output_images.insert(0, grid)\r\n \r\n             if opts.grid_save:\r\n-                images.save_image(grid, p.outpath_grids, \"grid\", all_seeds[0], all_prompts[0], opts.grid_format, info=infotext(), short_filename=not opts.grid_extended_filename, p=p)\r\n+                images.save_image(grid, p.outpath_grids, \"grid\", all_seeds[0], all_prompts[0], opts.grid_format, info=infotext(), short_filename=not opts.grid_extended_filename, grid=True, p=p)\r\n \r\n     devices.torch_gc()\r\n     return Processed(p, output_images, all_seeds[0], infotext())\r\ndiff --git a/scripts/prompt_matrix.py b/scripts/prompt_matrix.py\nindex aaece054c01..e49c9b205f9 100644\n--- a/scripts/prompt_matrix.py\n+++ b/scripts/prompt_matrix.py\n@@ -82,6 +82,6 @@ def run(self, p, put_at_start):\n         processed.images.insert(0, grid)\r\n \r\n         if opts.grid_save:\r\n-            images.save_image(processed.images[0], p.outpath_grids, \"prompt_matrix\", prompt=original_prompt, seed=processed.seed, p=p)\r\n+            images.save_image(processed.images[0], p.outpath_grids, \"prompt_matrix\", prompt=original_prompt, seed=processed.seed, grid=True, p=p)\r\n \r\n         return processed\r\ndiff --git a/scripts/xy_grid.py b/scripts/xy_grid.py\nindex dd6db81cc44..c459c2644a1 100644\n--- a/scripts/xy_grid.py\n+++ b/scripts/xy_grid.py\n@@ -192,6 +192,6 @@ def cell(x, y):\n         )\r\n \r\n         if opts.grid_save:\r\n-            images.save_image(processed.images[0], p.outpath_grids, \"xy_grid\", prompt=p.prompt, seed=processed.seed, p=p)\r\n+            images.save_image(processed.images[0], p.outpath_grids, \"xy_grid\", prompt=p.prompt, seed=processed.seed, grid=True, p=p)\r\n \r\n         return processed\r\n", "test_patch": "", "problem_statement": "[BUG] Not Working As Intended - create a directory with name derived from the prompt\n![image](https://user-images.githubusercontent.com/39418801/189551507-9e830688-a752-4660-af28-b4c8a181da6a.png)\r\n![image](https://user-images.githubusercontent.com/39418801/189551511-b586c344-c998-44b5-8985-853b7ad20669.png)\r\n![image](https://user-images.githubusercontent.com/39418801/189551520-50b57170-2483-47cc-b20e-258157faa05c.png)\r\n\r\nWhen run it does not save the grid or matrix under the `Prompt + Style content` as it does for images.\r\n\r\nPreferably this would be configurable like the file name and I could have it be `Prompt + Style name` for both.\n", "hints_text": "My private implementation just had `p.unstyled_prompt = p.prompt` before `modules.styles.apply_style(p, shared.prompt_styles[p.prompt_style])`\nOr you could have the `apply_style()` run when you are doing the samples filename format \n![image](https://user-images.githubusercontent.com/39418801/189553210-139ec0d7-b6c2-4207-9e31-c0a49cc413fd.png)\r\nIt also fails to set the name for batch grids\nbatch grids go in the image folder\r\nx/y grids go in just prompt no style\ncheck if this fixes it\nIs there a way to save the prompt before style as a variable, and the style name.\n[Feature Request] A toggle of including style in prompt could be included, and a tag for style name added.\n![image](https://user-images.githubusercontent.com/39418801/189796734-2b697473-50b2-4fcd-b5ee-2a79a146dd46.png)\r\nfolder name doesnt work for extras\r\nthis is so split in code i think it should be a func for all txt img extras and grid\ni can't reproduce that\nOn latest pull: \r\n![image](https://user-images.githubusercontent.com/39418801/189991175-7a684771-1c7b-46b3-a5e7-c7d20beca47e.png)\r\n![image](https://user-images.githubusercontent.com/39418801/189991245-f72d3815-cd2a-4e52-b39c-59f47a19ac11.png)\r\n![image](https://user-images.githubusercontent.com/39418801/189991353-0f199eaf-5e7c-4794-8efd-d08ed2034bfe.png)\r\n\nImma try and fix it. https://github.com/JustAnOkapi/stable-diffusion-webui/tree/dir-extras-fix\nextras shouldnt be affected by the save in folder option ", "created_at": "2022-09-13T20:32:30Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 378, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-378", "issue_numbers": ["368"], "base_commit": "fa8be8acd62894bfc96da985326fda3208266468", "patch": "diff --git a/modules/images.py b/modules/images.py\nindex ddd310a25ea..fc9a01130b6 100644\n--- a/modules/images.py\n+++ b/modules/images.py\n@@ -299,7 +299,7 @@ def save_image(image, path, basename, seed=None, prompt=None, extension='png', i\n \r\n         if existing_info is not None:\r\n             for k, v in existing_info.items():\r\n-                pnginfo.add_text(k, v)\r\n+                pnginfo.add_text(k, str(v))\r\n \r\n         pnginfo.add_text(pnginfo_section_name, info)\r\n     else:\r\n", "test_patch": "", "problem_statement": "Error in extras tab when resizing images\n**Describe the bug**\r\nGet error \"AttributeError: 'int' object has no attribute 'encode'\" on trying to upscale an image in the \"extras\" tab. \r\n\r\n**To Reproduce**\r\n1. Go to 'extras\r\n2. Click on lanczos, upload image, and press generate\r\n3. See error on the right\r\n\r\n**Expected behavior**\r\nNo error\r\n\r\n**Screenshots**\r\n![Screen Shot 2022-09-12 at 8 35 53 PM](https://user-images.githubusercontent.com/760865/189782451-411f6374-3220-405f-a1bb-ff4caac1ac68.png)\r\n\r\n**Desktop (please complete the following information):**\r\n - OS: MacOS\r\n - Browser: Safari\r\n - Commit revision: 19a817d97dad1b2ce58e70cadc35ccba5cf1130e\r\n\r\n**Additional context**\r\nServing from a linux box.\r\n\n", "hints_text": "Same error. Same steps to reproduce.\r\nOS: Windows 10\r\nBrowser: Chrome\r\nCommit revision: [fa8be8a](https://github.com/AUTOMATIC1111/stable-diffusion-webui/commit/fa8be8acd62894bfc96da985326fda3208266468)\nAgree, not only lanczos is broken, other upscalers not working as well\r\n\r\nUPD: it is not working when you manually upload an image from PC, but working if image is provided by pressing \"Send to extras\" button\r\n\r\nUPD2: I also got a bit different error\r\n\r\n`AttributeError: 'tuple' object has no attribute 'encode'`\r\n\r\nOS: Windows 10\r\nBrowser: Chrome\r\nCommit revision: [fa8be8a](https://github.com/AUTOMATIC1111/stable-diffusion-webui/commit/fa8be8acd62894bfc96da985326fda3208266468)\nsame issue\r\n\r\nw11, firefox", "created_at": "2022-09-13T04:35:12Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 129, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-129", "issue_numbers": ["57"], "base_commit": "296d012423f8d1862a63680443bb88b7d904ba4e", "patch": "diff --git a/webui.bat b/webui.bat\nindex 055a19b0419..0de2ab88d0f 100644\n--- a/webui.bat\n+++ b/webui.bat\n@@ -35,7 +35,7 @@ echo Unable to create venv in directory %VENV_DIR%\n goto :show_stdout_stderr\r\n \r\n :activate_venv\r\n-set PYTHON=%~dp0%VENV_DIR%\\Scripts\\Python.exe\r\n+set PYTHON=\"%~dp0%VENV_DIR%\\Scripts\\Python.exe\"\r\n %PYTHON% --version\r\n echo venv %PYTHON%\r\n goto :install_torch\r\n", "test_patch": "", "problem_statement": "If folder has space in name, the bat is not working\nFyi, if you try and set it up, set the name of the main folder \"stable-diffusion\" not \"stable diffusion\" as it fails the install\n", "hints_text": "", "created_at": "2022-09-07T22:31:56Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 60, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-60", "issue_numbers": ["58"], "base_commit": "595c827bd31773cc98eb6e87b11090960a32b2a2", "patch": "diff --git a/modules/scripts.py b/modules/scripts.py\nindex be348a70481..37a236827c4 100644\n--- a/modules/scripts.py\n+++ b/modules/scripts.py\n@@ -29,6 +29,9 @@ def describe(self):\n \r\n \r\n def load_scripts(basedir):\r\n+    if not os.path.exists(basedir):\r\n+        return\r\n+\r\n     for filename in os.listdir(basedir):\r\n         path = os.path.join(basedir, filename)\r\n \r\n", "test_patch": "", "problem_statement": "FileNotFoundError after new update\nGetting a FileNotFoundError: [WinError 3] The system cannot find the path specified: 'C:\\\\Users\\\\admin\\\\stable-diffusion-webui\\\\scripts' after the new update. \r\n\r\nNot exactly good at all the coding stuff, using it just fine yesterday but I downloaded the repo instead of git clone, for the sake of easier update I started a new installation by git cloning into user folder and the installation went well but ran into this while launching through webui.py.\r\n\r\nPython 3.10.6\r\nvenv C:\\Users\\admin\\stable-diffusion-webui\\venv\\Scripts\\Python.exe\r\nLaunching webui.py...\r\nLoading model from C:\\Users\\admin\\stable-diffusion-webui\\model.ckpt\r\nGlobal Step: 470000\r\nLatentDiffusion: Running in eps-prediction mode\r\nDiffusionWrapper has 859.52 M params.\r\nmaking attention of type 'vanilla' with 512 in_channels\r\nWorking with z of shape (1, 4, 32, 32) = 4096 dimensions.\r\nmaking attention of type 'vanilla' with 512 in_channels\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\admin\\stable-diffusion-webui\\webui.py\", line 135, in <module>\r\n    modules.scripts.load_scripts(os.path.join(script_path, \"scripts\"))\r\n  File \"C:\\Users\\admin\\stable-diffusion-webui\\modules\\scripts.py\", line 32, in load_scripts\r\n    for filename in os.listdir(basedir):\r\nFileNotFoundError: [WinError 3] The system cannot find the path specified: 'C:\\\\Users\\\\admin\\\\stable-diffusion-webui\\\\scripts'\n", "hints_text": "Happens to me now too.", "created_at": "2022-09-03T16:11:15Z"}
{"repo": "AUTOMATIC1111/stable-diffusion-webui", "pull_number": 11, "instance_id": "AUTOMATIC1111__stable-diffusion-webui-11", "issue_numbers": ["9"], "base_commit": "21bcbb945ead36cd9b969a1a957ce5ad754b6bad", "patch": "diff --git a/webui.py b/webui.py\nindex 2751d75adf4..b3375e98a16 100644\n--- a/webui.py\n+++ b/webui.py\n@@ -678,6 +678,7 @@ def infotext():\n                     x_sample = x_sample.astype(np.uint8)\r\n \r\n                     if use_GFPGAN and GFPGAN is not None:\r\n+                        torch_gc()\r\n                         cropped_faces, restored_faces, restored_img = GFPGAN.enhance(x_sample, has_aligned=False, only_center_face=False, paste_back=True)\r\n                         x_sample = restored_img\r\n \r\n", "test_patch": "", "problem_statement": "Fix faces using GFPGAN + high batch count tends to fail 6GB VRAM\n6GB is already on a knife's edge, and this seems to push it over. However it doesn't do it every single time, and I have not seen it happen when using batch count 1, even when hitting submit repeatedly.\r\n\r\nRuntimeError: CUDA out of memory. Tried to allocate 1024.00 MiB (GPU 0; 6.00 GiB total capacity; 3.30 GiB already allocated; 0 bytes free; 4.60 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n", "hints_text": "so just rename gfpgan folder and its solved", "created_at": "2022-08-26T01:33:44Z"}
