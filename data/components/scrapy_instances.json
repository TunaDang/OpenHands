[
  {
    "repo": "scrapy/scrapy",
    "pull_number": 6388,
    "instance_id": "scrapy__scrapy-6388",
    "issue_numbers": [
      "6383"
    ],
    "base_commit": "2b9e32f1ca491340148e6a1918d1df70443823e6",
    "patch": "diff --git a/scrapy/contracts/__init__.py b/scrapy/contracts/__init__.py\nindex b300b8457fc..27bc2fcbaf9 100644\n--- a/scrapy/contracts/__init__.py\n+++ b/scrapy/contracts/__init__.py\n@@ -120,7 +120,8 @@ def extract_contracts(self, method: Callable) -> List[Contract]:\n \n             if line.startswith(\"@\"):\n                 m = re.match(r\"@(\\w+)\\s*(.*)\", line)\n-                assert m is not None\n+                if m is None:\n+                    continue\n                 name, args = m.groups()\n                 args = re.split(r\"\\s+\", args)\n \n",
    "test_patch": "diff --git a/tests/test_contracts.py b/tests/test_contracts.py\nindex 1459e0b5fd5..c9c12f0d804 100644\n--- a/tests/test_contracts.py\n+++ b/tests/test_contracts.py\n@@ -182,6 +182,19 @@ def custom_form(self, response):\n         \"\"\"\n         pass\n \n+    def invalid_regex(self, response):\n+        \"\"\"method with invalid regex\n+        @ Scrapy is awsome\n+        \"\"\"\n+        pass\n+\n+    def invalid_regex_with_valid_contract(self, response):\n+        \"\"\"method with invalid regex\n+        @ scrapy is awsome\n+        @url http://scrapy.org\n+        \"\"\"\n+        pass\n+\n \n class CustomContractSuccessSpider(Spider):\n     name = \"custom_contract_success_spider\"\n@@ -385,6 +398,21 @@ def test_scrapes(self):\n         message = \"ContractFail: Missing fields: name, url\"\n         assert message in self.results.failures[-1][-1]\n \n+    def test_regex(self):\n+        spider = TestSpider()\n+        response = ResponseMock()\n+\n+        # invalid regex\n+        request = self.conman.from_method(spider.invalid_regex, self.results)\n+        self.should_succeed()\n+\n+        # invalid regex with valid contract\n+        request = self.conman.from_method(\n+            spider.invalid_regex_with_valid_contract, self.results\n+        )\n+        self.should_succeed()\n+        request.callback(response)\n+\n     def test_custom_contracts(self):\n         self.conman.from_spider(CustomContractSuccessSpider(), self.results)\n         self.should_succeed()\n",
    "problem_statement": "Error handling in contract parsing\nWe found that there is no proper handling for unmatched regexes in `\u200escrapy.contracts.ContractsManager.extract_contracts()`, so e.g. calling `from_method()` on a method with `@ foo` in the docstring produces an unhandled exception. I think we should just skip lines that don't match.\n",
    "hints_text": "@wRAR May I work on this issue?\nOf course",
    "created_at": "2024-06-03T12:47:28Z",
    "version": "2.11",
    "PASS_TO_PASS": [],
    "FAIL_TO_PASS": [
      "tests/test_contracts.py"
    ]
  },
  {
    "repo": "scrapy/scrapy",
    "pull_number": 6324,
    "instance_id": "scrapy__scrapy-6324",
    "issue_numbers": [
      "6323"
    ],
    "base_commit": "a5da77d01dccbc91206d053396fb5b80e1a6b15b",
    "patch": "diff --git a/scrapy/spiders/__init__.py b/scrapy/spiders/__init__.py\nindex 72c2aaba7f5..0a653bd4155 100644\n--- a/scrapy/spiders/__init__.py\n+++ b/scrapy/spiders/__init__.py\n@@ -22,6 +22,7 @@\n \n     from scrapy.crawler import Crawler\n     from scrapy.settings import BaseSettings\n+    from scrapy.utils.log import SpiderLoggerAdapter\n \n \n class Spider(object_ref):\n@@ -42,9 +43,11 @@ def __init__(self, name: Optional[str] = None, **kwargs: Any):\n             self.start_urls: List[str] = []\n \n     @property\n-    def logger(self) -> logging.LoggerAdapter:\n+    def logger(self) -> SpiderLoggerAdapter:\n+        from scrapy.utils.log import SpiderLoggerAdapter\n+\n         logger = logging.getLogger(self.name)\n-        return logging.LoggerAdapter(logger, {\"spider\": self})\n+        return SpiderLoggerAdapter(logger, {\"spider\": self})\n \n     def log(self, message: Any, level: int = logging.DEBUG, **kw: Any) -> None:\n         \"\"\"Log the given message at the given log level\ndiff --git a/scrapy/utils/log.py b/scrapy/utils/log.py\nindex 2a38f151a16..430a91e9592 100644\n--- a/scrapy/utils/log.py\n+++ b/scrapy/utils/log.py\n@@ -4,7 +4,17 @@\n import sys\n from logging.config import dictConfig\n from types import TracebackType\n-from typing import TYPE_CHECKING, Any, List, Optional, Tuple, Type, Union, cast\n+from typing import (\n+    TYPE_CHECKING,\n+    Any,\n+    List,\n+    MutableMapping,\n+    Optional,\n+    Tuple,\n+    Type,\n+    Union,\n+    cast,\n+)\n \n from twisted.python import log as twisted_log\n from twisted.python.failure import Failure\n@@ -238,3 +248,16 @@ def logformatter_adapter(logkws: dict) -> Tuple[int, str, dict]:\n     args = logkws if not logkws.get(\"args\") else logkws[\"args\"]\n \n     return (level, message, args)\n+\n+\n+class SpiderLoggerAdapter(logging.LoggerAdapter):\n+    def process(\n+        self, msg: str, kwargs: MutableMapping[str, Any]\n+    ) -> Tuple[str, MutableMapping[str, Any]]:\n+        \"\"\"Method that augments logging with additional 'extra' data\"\"\"\n+        if isinstance(kwargs.get(\"extra\"), MutableMapping):\n+            kwargs[\"extra\"].update(self.extra)\n+        else:\n+            kwargs[\"extra\"] = self.extra\n+\n+        return msg, kwargs\n",
    "test_patch": "diff --git a/tests/spiders.py b/tests/spiders.py\nindex 94969db993d..ea419afbdac 100644\n--- a/tests/spiders.py\n+++ b/tests/spiders.py\n@@ -4,6 +4,7 @@\n \n import asyncio\n import time\n+from typing import Optional\n from urllib.parse import urlencode\n \n from twisted.internet import defer\n@@ -78,6 +79,28 @@ def errback(self, failure):\n         self.t2_err = time.time()\n \n \n+class LogSpider(MetaSpider):\n+    name = \"log_spider\"\n+\n+    def log_debug(self, message: str, extra: Optional[dict] = None):\n+        self.logger.debug(message, extra=extra)\n+\n+    def log_info(self, message: str, extra: Optional[dict] = None):\n+        self.logger.info(message, extra=extra)\n+\n+    def log_warning(self, message: str, extra: Optional[dict] = None):\n+        self.logger.warning(message, extra=extra)\n+\n+    def log_error(self, message: str, extra: Optional[dict] = None):\n+        self.logger.error(message, extra=extra)\n+\n+    def log_critical(self, message: str, extra: Optional[dict] = None):\n+        self.logger.critical(message, extra=extra)\n+\n+    def parse(self, response):\n+        pass\n+\n+\n class SlowSpider(DelaySpider):\n     name = \"slow\"\n \ndiff --git a/tests/test_utils_log.py b/tests/test_utils_log.py\nindex eae744df5e4..a8d0808222e 100644\n--- a/tests/test_utils_log.py\n+++ b/tests/test_utils_log.py\n@@ -1,18 +1,26 @@\n+import json\n import logging\n+import re\n import sys\n import unittest\n+from io import StringIO\n+from typing import Any, Dict, Mapping, MutableMapping\n+from unittest import TestCase\n \n+import pytest\n from testfixtures import LogCapture\n from twisted.python.failure import Failure\n \n from scrapy.extensions import telnet\n from scrapy.utils.log import (\n     LogCounterHandler,\n+    SpiderLoggerAdapter,\n     StreamLogger,\n     TopLevelFormatter,\n     failure_to_exc_info,\n )\n from scrapy.utils.test import get_crawler\n+from tests.spiders import LogSpider\n \n \n class FailureToExcInfoTest(unittest.TestCase):\n@@ -106,3 +114,180 @@ def test_redirect(self):\n         with LogCapture() as log:\n             print(\"test log msg\")\n         log.check((\"test\", \"ERROR\", \"test log msg\"))\n+\n+\n+@pytest.mark.parametrize(\n+    (\"base_extra\", \"log_extra\", \"expected_extra\"),\n+    (\n+        (\n+            {\"spider\": \"test\"},\n+            {\"extra\": {\"log_extra\": \"info\"}},\n+            {\"extra\": {\"log_extra\": \"info\", \"spider\": \"test\"}},\n+        ),\n+        (\n+            {\"spider\": \"test\"},\n+            {\"extra\": None},\n+            {\"extra\": {\"spider\": \"test\"}},\n+        ),\n+        (\n+            {\"spider\": \"test\"},\n+            {\"extra\": {\"spider\": \"test2\"}},\n+            {\"extra\": {\"spider\": \"test\"}},\n+        ),\n+    ),\n+)\n+def test_spider_logger_adapter_process(\n+    base_extra: Mapping[str, Any], log_extra: MutableMapping, expected_extra: Dict\n+):\n+    logger = logging.getLogger(\"test\")\n+    spider_logger_adapter = SpiderLoggerAdapter(logger, base_extra)\n+\n+    log_message = \"test_log_message\"\n+    result_message, result_kwargs = spider_logger_adapter.process(\n+        log_message, log_extra\n+    )\n+\n+    assert result_message == log_message\n+    assert result_kwargs == expected_extra\n+\n+\n+class LoggingTestCase(TestCase):\n+    def setUp(self):\n+        self.log_stream = StringIO()\n+        handler = logging.StreamHandler(self.log_stream)\n+        logger = logging.getLogger(\"log_spider\")\n+        logger.addHandler(handler)\n+        logger.setLevel(logging.DEBUG)\n+        self.handler = handler\n+        self.logger = logger\n+        self.spider = LogSpider()\n+\n+    def tearDown(self):\n+        self.logger.removeHandler(self.handler)\n+\n+    def test_debug_logging(self):\n+        log_message = \"Foo message\"\n+        self.spider.log_debug(log_message)\n+        log_contents = self.log_stream.getvalue()\n+\n+        assert log_contents == f\"{log_message}\\n\"\n+\n+    def test_info_logging(self):\n+        log_message = \"Bar message\"\n+        self.spider.log_info(log_message)\n+        log_contents = self.log_stream.getvalue()\n+\n+        assert log_contents == f\"{log_message}\\n\"\n+\n+    def test_warning_logging(self):\n+        log_message = \"Baz message\"\n+        self.spider.log_warning(log_message)\n+        log_contents = self.log_stream.getvalue()\n+\n+        assert log_contents == f\"{log_message}\\n\"\n+\n+    def test_error_logging(self):\n+        log_message = \"Foo bar message\"\n+        self.spider.log_error(log_message)\n+        log_contents = self.log_stream.getvalue()\n+\n+        assert log_contents == f\"{log_message}\\n\"\n+\n+    def test_critical_logging(self):\n+        log_message = \"Foo bar baz message\"\n+        self.spider.log_critical(log_message)\n+        log_contents = self.log_stream.getvalue()\n+\n+        assert log_contents == f\"{log_message}\\n\"\n+\n+\n+class LoggingWithExtraTestCase(TestCase):\n+    def setUp(self):\n+        self.log_stream = StringIO()\n+        handler = logging.StreamHandler(self.log_stream)\n+        formatter = logging.Formatter(\n+            '{\"levelname\": \"%(levelname)s\", \"message\": \"%(message)s\", \"spider\": \"%(spider)s\", \"important_info\": \"%(important_info)s\"}'\n+        )\n+        handler.setFormatter(formatter)\n+        logger = logging.getLogger(\"log_spider\")\n+        logger.addHandler(handler)\n+        logger.setLevel(logging.DEBUG)\n+        self.handler = handler\n+        self.logger = logger\n+        self.spider = LogSpider()\n+        self.regex_pattern = re.compile(r\"^<LogSpider\\s'log_spider'\\sat\\s[^>]+>$\")\n+\n+    def tearDown(self):\n+        self.logger.removeHandler(self.handler)\n+\n+    def test_debug_logging(self):\n+        log_message = \"Foo message\"\n+        extra = {\"important_info\": \"foo\"}\n+        self.spider.log_debug(log_message, extra)\n+        log_contents = self.log_stream.getvalue()\n+        log_contents = json.loads(log_contents)\n+\n+        assert log_contents[\"levelname\"] == \"DEBUG\"\n+        assert log_contents[\"message\"] == log_message\n+        assert self.regex_pattern.match(log_contents[\"spider\"])\n+        assert log_contents[\"important_info\"] == extra[\"important_info\"]\n+\n+    def test_info_logging(self):\n+        log_message = \"Bar message\"\n+        extra = {\"important_info\": \"bar\"}\n+        self.spider.log_info(log_message, extra)\n+        log_contents = self.log_stream.getvalue()\n+        log_contents = json.loads(log_contents)\n+\n+        assert log_contents[\"levelname\"] == \"INFO\"\n+        assert log_contents[\"message\"] == log_message\n+        assert self.regex_pattern.match(log_contents[\"spider\"])\n+        assert log_contents[\"important_info\"] == extra[\"important_info\"]\n+\n+    def test_warning_logging(self):\n+        log_message = \"Baz message\"\n+        extra = {\"important_info\": \"baz\"}\n+        self.spider.log_warning(log_message, extra)\n+        log_contents = self.log_stream.getvalue()\n+        log_contents = json.loads(log_contents)\n+\n+        assert log_contents[\"levelname\"] == \"WARNING\"\n+        assert log_contents[\"message\"] == log_message\n+        assert self.regex_pattern.match(log_contents[\"spider\"])\n+        assert log_contents[\"important_info\"] == extra[\"important_info\"]\n+\n+    def test_error_logging(self):\n+        log_message = \"Foo bar message\"\n+        extra = {\"important_info\": \"foo bar\"}\n+        self.spider.log_error(log_message, extra)\n+        log_contents = self.log_stream.getvalue()\n+        log_contents = json.loads(log_contents)\n+\n+        assert log_contents[\"levelname\"] == \"ERROR\"\n+        assert log_contents[\"message\"] == log_message\n+        assert self.regex_pattern.match(log_contents[\"spider\"])\n+        assert log_contents[\"important_info\"] == extra[\"important_info\"]\n+\n+    def test_critical_logging(self):\n+        log_message = \"Foo bar baz message\"\n+        extra = {\"important_info\": \"foo bar baz\"}\n+        self.spider.log_critical(log_message, extra)\n+        log_contents = self.log_stream.getvalue()\n+        log_contents = json.loads(log_contents)\n+\n+        assert log_contents[\"levelname\"] == \"CRITICAL\"\n+        assert log_contents[\"message\"] == log_message\n+        assert self.regex_pattern.match(log_contents[\"spider\"])\n+        assert log_contents[\"important_info\"] == extra[\"important_info\"]\n+\n+    def test_overwrite_spider_extra(self):\n+        log_message = \"Foo message\"\n+        extra = {\"important_info\": \"foo\", \"spider\": \"shouldn't change\"}\n+        self.spider.log_error(log_message, extra)\n+        log_contents = self.log_stream.getvalue()\n+        log_contents = json.loads(log_contents)\n+\n+        assert log_contents[\"levelname\"] == \"ERROR\"\n+        assert log_contents[\"message\"] == log_message\n+        assert self.regex_pattern.match(log_contents[\"spider\"])\n+        assert log_contents[\"important_info\"] == extra[\"important_info\"]\n",
    "problem_statement": "Spider.logger not logging custom extra information\nI noticed the implicit behavior of the Spider.logger: when logging with extra, extra ultimately do not end up in the log because they are overwritten by default `process` method of [LoggerAdapter](https://github.com/scrapy/scrapy/blob/master/scrapy/spiders/__init__.py#L47)\r\n\r\nCurrent logic:\r\n```py\r\n>>> self.logger.info(\"test log\", extra={\"test\": \"very important information\"})\r\n{\"message\": \"test log\", \"spider\": \"spider_name\"}\r\n```\r\n\r\n\r\n\r\nExpected logic:\r\n```py\r\n>>> self.logger.info(\"test log\", extra={\"test\": \"very important information\"})\r\n{\"message\": \"test log\", \"spider\": \"spider_name\", \"test\": \"very important information\"}\r\n```\r\n\r\n\n",
    "hints_text": "",
    "created_at": "2024-04-27T16:58:30Z",
    "version": "2.11",
    "PASS_TO_PASS": [],
    "FAIL_TO_PASS": [
      "tests/test_utils_log.py::test_spider_logger_adapter_process[base_extra0-log_extra0-expected_extra0]",
      "tests/test_utils_log.py::test_spider_logger_adapter_process[base_extra1-log_extra1-expected_extra1]",
      "tests/test_utils_log.py::test_spider_logger_adapter_process[base_extra2-log_extra2-expected_extra2]",
      "tests/test_utils_log.py::LoggingTestCase::test_critical_logging",
      "tests/test_utils_log.py::LoggingTestCase::test_debug_logging",
      "tests/test_utils_log.py::LoggingTestCase::test_error_logging",
      "tests/test_utils_log.py::LoggingTestCase::test_info_logging",
      "tests/test_utils_log.py::LoggingTestCase::test_warning_logging",
      "tests/test_utils_log.py::LoggingWithExtraTestCase::test_debug_logging",
      "tests/test_utils_log.py::LoggingWithExtraTestCase::test_info_logging",
      "tests/test_utils_log.py::LoggingWithExtraTestCase::test_warning_logging",
      "tests/test_utils_log.py::LoggingWithExtraTestCase::test_error_logging",
      "tests/test_utils_log.py::LoggingWithExtraTestCase::test_critical_logging",
      "tests/test_utils_log.py::LoggingWithExtraTestCase::test_overwrite_spider_extra"
    ]
  },
  {
    "repo": "scrapy/scrapy",
    "pull_number": 5950,
    "instance_id": "scrapy__scrapy-5950",
    "issue_numbers": [
      "5992"
    ],
    "base_commit": "510574216d70ec84d75639ebcda360834a992e47",
    "patch": "diff --git a/docs/index.rst b/docs/index.rst\nindex 5404969e02e..8798aebd132 100644\n--- a/docs/index.rst\n+++ b/docs/index.rst\n@@ -222,6 +222,7 @@ Extending Scrapy\n    :hidden:\n \n    topics/architecture\n+   topics/addons\n    topics/downloader-middleware\n    topics/spider-middleware\n    topics/extensions\n@@ -235,6 +236,9 @@ Extending Scrapy\n :doc:`topics/architecture`\n     Understand the Scrapy architecture.\n \n+:doc:`topics/addons`\n+    Enable and configure third-party extensions.\n+\n :doc:`topics/downloader-middleware`\n     Customize how pages get requested and downloaded.\n \ndiff --git a/docs/topics/addons.rst b/docs/topics/addons.rst\nnew file mode 100644\nindex 00000000000..1bf2172bd40\n--- /dev/null\n+++ b/docs/topics/addons.rst\n@@ -0,0 +1,193 @@\n+.. _topics-addons:\n+\n+=======\n+Add-ons\n+=======\n+\n+Scrapy's add-on system is a framework which unifies managing and configuring\n+components that extend Scrapy's core functionality, such as middlewares,\n+extensions, or pipelines. It provides users with a plug-and-play experience in\n+Scrapy extension management, and grants extensive configuration control to\n+developers.\n+\n+\n+Activating and configuring add-ons\n+==================================\n+\n+During :class:`~scrapy.crawler.Crawler` initialization, the list of enabled\n+add-ons is read from your ``ADDONS`` setting.\n+\n+The ``ADDONS`` setting is a dict in which every key is an add-on class or its\n+import path and the value is its priority.\n+\n+This is an example where two add-ons are enabled in a project's\n+``settings.py``::\n+\n+    ADDONS = {\n+        'path.to.someaddon': 0,\n+        SomeAddonClass: 1,\n+    }\n+\n+\n+Writing your own add-ons\n+========================\n+\n+Add-ons are Python classes that include the following method:\n+\n+.. method:: update_settings(settings)\n+\n+    This method is called during the initialization of the\n+    :class:`~scrapy.crawler.Crawler`. Here, you should perform dependency checks\n+    (e.g. for external Python libraries) and update the\n+    :class:`~scrapy.settings.Settings` object as wished, e.g. enable components\n+    for this add-on or set required configuration of other extensions.\n+\n+    :param settings: The settings object storing Scrapy/component configuration\n+    :type settings: :class:`~scrapy.settings.Settings`\n+\n+They can also have the following method:\n+\n+.. classmethod:: from_crawler(cls, crawler)\n+   :noindex:\n+\n+   If present, this class method is called to create an add-on instance\n+   from a :class:`~scrapy.crawler.Crawler`. It must return a new instance\n+   of the add-on. The crawler object provides access to all Scrapy core\n+   components like settings and signals; it is a way for the add-on to access\n+   them and hook its functionality into Scrapy.\n+\n+   :param crawler: The crawler that uses this add-on\n+   :type crawler: :class:`~scrapy.crawler.Crawler`\n+\n+The settings set by the add-on should use the ``addon`` priority (see\n+:ref:`populating-settings` and :func:`scrapy.settings.BaseSettings.set`)::\n+\n+    class MyAddon:\n+        def update_settings(self, settings):\n+            settings.set(\"DNSCACHE_ENABLED\", True, \"addon\")\n+\n+This allows users to override these settings in the project or spider\n+configuration. This is not possible with settings that are mutable objects,\n+such as the dict that is a value of :setting:`ITEM_PIPELINES`. In these cases\n+you can provide an add-on-specific setting that governs whether the add-on will\n+modify :setting:`ITEM_PIPELINES`::\n+\n+    class MyAddon:\n+        def update_settings(self, settings):\n+            if settings.getbool(\"MYADDON_ENABLE_PIPELINE\"):\n+                settings[\"ITEM_PIPELINES\"][\"path.to.mypipeline\"] = 200\n+\n+If the ``update_settings`` method raises\n+:exc:`scrapy.exceptions.NotConfigured`, the add-on will be skipped. This makes\n+it easy to enable an add-on only when some conditions are met.\n+\n+Fallbacks\n+---------\n+\n+Some components provided by add-ons need to fall back to \"default\"\n+implementations, e.g. a custom download handler needs to send the request that\n+it doesn't handle via the default download handler, or a stats collector that\n+includes some additional processing but otherwise uses the default stats\n+collector. And it's possible that a project needs to use several custom\n+components of the same type, e.g. two custom download handlers that support\n+different kinds of custom requests and still need to use the default download\n+handler for other requests. To make such use cases easier to configure, we\n+recommend that such custom components should be written in the following way:\n+\n+1. The custom component (e.g. ``MyDownloadHandler``) shouldn't inherit from the\n+   default Scrapy one (e.g.\n+   ``scrapy.core.downloader.handlers.http.HTTPDownloadHandler``), but instead\n+   be able to load the class of the fallback component from a special setting\n+   (e.g. ``MY_FALLBACK_DOWNLOAD_HANDLER``), create an instance of it and use\n+   it.\n+2. The add-ons that include these components should read the current value of\n+   the default setting (e.g. ``DOWNLOAD_HANDLERS``) in their\n+   ``update_settings()`` methods, save that value into the fallback setting\n+   (``MY_FALLBACK_DOWNLOAD_HANDLER`` mentioned earlier) and set the default\n+   setting to the component provided by the add-on (e.g.\n+   ``MyDownloadHandler``). If the fallback setting is already set by the user,\n+   they shouldn't change it.\n+3. This way, if there are several add-ons that want to modify the same setting,\n+   all of them will fallback to the component from the previous one and then to\n+   the Scrapy default. The order of that depends on the priority order in the\n+   ``ADDONS`` setting.\n+\n+\n+Add-on examples\n+===============\n+\n+Set some basic configuration:\n+\n+.. code-block:: python\n+\n+    class MyAddon:\n+        def update_settings(self, settings):\n+            settings[\"ITEM_PIPELINES\"][\"path.to.mypipeline\"] = 200\n+            settings.set(\"DNSCACHE_ENABLED\", True, \"addon\")\n+\n+Check dependencies:\n+\n+.. code-block:: python\n+\n+    class MyAddon:\n+        def update_settings(self, settings):\n+            try:\n+                import boto\n+            except ImportError:\n+                raise NotConfigured(\"MyAddon requires the boto library\")\n+            ...\n+\n+Access the crawler instance:\n+\n+.. code-block:: python\n+\n+    class MyAddon:\n+        def __init__(self, crawler) -> None:\n+            super().__init__()\n+            self.crawler = crawler\n+\n+        @classmethod\n+        def from_crawler(cls, crawler):\n+            return cls(crawler)\n+\n+        def update_settings(self, settings):\n+            ...\n+\n+Use a fallback component:\n+\n+.. code-block:: python\n+\n+    from scrapy.core.downloader.handlers.http import HTTPDownloadHandler\n+\n+\n+    FALLBACK_SETTING = \"MY_FALLBACK_DOWNLOAD_HANDLER\"\n+\n+\n+    class MyHandler:\n+        lazy = False\n+\n+        def __init__(self, settings, crawler):\n+            dhcls = load_object(settings.get(FALLBACK_SETTING))\n+            self._fallback_handler = create_instance(\n+                dhcls,\n+                settings=None,\n+                crawler=crawler,\n+            )\n+\n+        def download_request(self, request, spider):\n+            if request.meta.get(\"my_params\"):\n+                # handle the request\n+                ...\n+            else:\n+                return self._fallback_handler.download_request(request, spider)\n+\n+\n+    class MyAddon:\n+        def update_settings(self, settings):\n+            if not settings.get(FALLBACK_SETTING):\n+                settings.set(\n+                    FALLBACK_SETTING,\n+                    settings.getwithbase(\"DOWNLOAD_HANDLERS\")[\"https\"],\n+                    \"addon\",\n+                )\n+            settings[\"DOWNLOAD_HANDLERS\"][\"https\"] = MyHandler\ndiff --git a/docs/topics/api.rst b/docs/topics/api.rst\nindex 26834487998..16c28405cfb 100644\n--- a/docs/topics/api.rst\n+++ b/docs/topics/api.rst\n@@ -137,6 +137,7 @@ Settings API\n         SETTINGS_PRIORITIES = {\n             \"default\": 0,\n             \"command\": 10,\n+            \"addon\": 15,\n             \"project\": 20,\n             \"spider\": 30,\n             \"cmdline\": 40,\ndiff --git a/docs/topics/settings.rst b/docs/topics/settings.rst\nindex 3e06d84f90b..602ab587d7e 100644\n--- a/docs/topics/settings.rst\n+++ b/docs/topics/settings.rst\n@@ -40,8 +40,9 @@ precedence:\n  1. Command line options (most precedence)\n  2. Settings per-spider\n  3. Project settings module\n- 4. Default settings per-command\n- 5. Default global settings (less precedence)\n+ 4. Settings set by add-ons\n+ 5. Default settings per-command\n+ 6. Default global settings (less precedence)\n \n The population of these settings sources is taken care of internally, but a\n manual handling is possible using API calls. See the\n@@ -89,7 +90,13 @@ project, it's where most of your custom settings will be populated. For a\n standard Scrapy project, this means you'll be adding or changing the settings\n in the ``settings.py`` file created for your project.\n \n-4. Default settings per-command\n+4. Settings set by add-ons\n+--------------------------\n+\n+:ref:`Add-ons <topics-addons>` can modify settings. They should do this with\n+this priority, though this is not enforced.\n+\n+5. Default settings per-command\n -------------------------------\n \n Each :doc:`Scrapy tool </topics/commands>` command can have its own default\n@@ -97,7 +104,7 @@ settings, which override the global default settings. Those custom command\n settings are specified in the ``default_settings`` attribute of the command\n class.\n \n-5. Default global settings\n+6. Default global settings\n --------------------------\n \n The global defaults are located in the ``scrapy.settings.default_settings``\n@@ -201,6 +208,16 @@ to any particular component. In that case the module of that component will be\n shown, typically an extension, middleware or pipeline. It also means that the\n component must be enabled in order for the setting to have any effect.\n \n+.. setting:: ADDONS\n+\n+ADDONS\n+------\n+\n+Default: ``{}``\n+\n+A dict containing paths to the add-ons enabled in your project and their\n+priorities. For more information, see :ref:`topics-addons`.\n+\n .. setting:: AWS_ACCESS_KEY_ID\n \n AWS_ACCESS_KEY_ID\n@@ -964,7 +981,6 @@ some of them need to be enabled through a setting.\n For more information See the :ref:`extensions user guide  <topics-extensions>`\n and the :ref:`list of available extensions <topics-extensions-ref>`.\n \n-\n .. setting:: FEED_TEMPDIR\n \n FEED_TEMPDIR\ndiff --git a/scrapy/addons.py b/scrapy/addons.py\nnew file mode 100644\nindex 00000000000..02dd4fde85b\n--- /dev/null\n+++ b/scrapy/addons.py\n@@ -0,0 +1,54 @@\n+import logging\n+from typing import TYPE_CHECKING, Any, List\n+\n+from scrapy.exceptions import NotConfigured\n+from scrapy.settings import Settings\n+from scrapy.utils.conf import build_component_list\n+from scrapy.utils.misc import create_instance, load_object\n+\n+if TYPE_CHECKING:\n+    from scrapy.crawler import Crawler\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+class AddonManager:\n+    \"\"\"This class facilitates loading and storing :ref:`topics-addons`.\"\"\"\n+\n+    def __init__(self, crawler: \"Crawler\") -> None:\n+        self.crawler: \"Crawler\" = crawler\n+        self.addons: List[Any] = []\n+\n+    def load_settings(self, settings: Settings) -> None:\n+        \"\"\"Load add-ons and configurations from a settings object.\n+\n+        This will load the add-on for every add-on path in the\n+        ``ADDONS`` setting and execute their ``update_settings`` methods.\n+\n+        :param settings: The :class:`~scrapy.settings.Settings` object from \\\n+            which to read the add-on configuration\n+        :type settings: :class:`~scrapy.settings.Settings`\n+        \"\"\"\n+        enabled: List[Any] = []\n+        for clspath in build_component_list(settings[\"ADDONS\"]):\n+            try:\n+                addoncls = load_object(clspath)\n+                addon = create_instance(\n+                    addoncls, settings=settings, crawler=self.crawler\n+                )\n+                addon.update_settings(settings)\n+                self.addons.append(addon)\n+            except NotConfigured as e:\n+                if e.args:\n+                    logger.warning(\n+                        \"Disabled %(clspath)s: %(eargs)s\",\n+                        {\"clspath\": clspath, \"eargs\": e.args[0]},\n+                        extra={\"crawler\": self.crawler},\n+                    )\n+        logger.info(\n+            \"Enabled addons:\\n%(addons)s\",\n+            {\n+                \"addons\": enabled,\n+            },\n+            extra={\"crawler\": self.crawler},\n+        )\ndiff --git a/scrapy/crawler.py b/scrapy/crawler.py\nindex 69ff07bb719..bf69cee2626 100644\n--- a/scrapy/crawler.py\n+++ b/scrapy/crawler.py\n@@ -18,6 +18,7 @@\n from zope.interface.verify import verifyClass\n \n from scrapy import Spider, signals\n+from scrapy.addons import AddonManager\n from scrapy.core.engine import ExecutionEngine\n from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.extension import ExtensionManager\n@@ -68,6 +69,9 @@ def __init__(\n         self.settings: Settings = settings.copy()\n         self.spidercls.update_settings(self.settings)\n \n+        self.addons: AddonManager = AddonManager(self)\n+        self.addons.load_settings(self.settings)\n+\n         self.signals: SignalManager = SignalManager(self)\n \n         self.stats: StatsCollector = load_object(self.settings[\"STATS_CLASS\"])(self)\ndiff --git a/scrapy/middleware.py b/scrapy/middleware.py\nindex 03e92b56506..04b838d2d11 100644\n--- a/scrapy/middleware.py\n+++ b/scrapy/middleware.py\n@@ -46,10 +46,9 @@ def from_settings(cls, settings: Settings, crawler=None):\n                 enabled.append(clspath)\n             except NotConfigured as e:\n                 if e.args:\n-                    clsname = clspath.split(\".\")[-1]\n                     logger.warning(\n-                        \"Disabled %(clsname)s: %(eargs)s\",\n-                        {\"clsname\": clsname, \"eargs\": e.args[0]},\n+                        \"Disabled %(clspath)s: %(eargs)s\",\n+                        {\"clspath\": clspath, \"eargs\": e.args[0]},\n                         extra={\"crawler\": crawler},\n                     )\n \ndiff --git a/scrapy/settings/__init__.py b/scrapy/settings/__init__.py\nindex 8b3bdbabe27..0f5cf85acc0 100644\n--- a/scrapy/settings/__init__.py\n+++ b/scrapy/settings/__init__.py\n@@ -9,6 +9,7 @@\n SETTINGS_PRIORITIES = {\n     \"default\": 0,\n     \"command\": 10,\n+    \"addon\": 15,\n     \"project\": 20,\n     \"spider\": 30,\n     \"cmdline\": 40,\ndiff --git a/scrapy/settings/default_settings.py b/scrapy/settings/default_settings.py\nindex a4cb555bd9d..ef1b7ea99b4 100644\n--- a/scrapy/settings/default_settings.py\n+++ b/scrapy/settings/default_settings.py\n@@ -17,6 +17,8 @@\n from importlib import import_module\n from pathlib import Path\n \n+ADDONS = {}\n+\n AJAXCRAWL_ENABLED = False\n \n ASYNCIO_EVENT_LOOP = None\ndiff --git a/scrapy/utils/misc.py b/scrapy/utils/misc.py\nindex 70187ba748a..b3c28da9239 100644\n--- a/scrapy/utils/misc.py\n+++ b/scrapy/utils/misc.py\n@@ -67,7 +67,7 @@ def load_object(path: Union[str, Callable]) -> Any:\n         if callable(path):\n             return path\n         raise TypeError(\n-            \"Unexpected argument type, expected string \" f\"or object, got: {type(path)}\"\n+            f\"Unexpected argument type, expected string or object, got: {type(path)}\"\n         )\n \n     try:\ndiff --git a/sep/sep-021.rst b/sep/sep-021.rst\ndeleted file mode 100644\nindex e8affa94332..00000000000\n--- a/sep/sep-021.rst\n+++ /dev/null\n@@ -1,113 +0,0 @@\n-=======  ===================\n-SEP      21\n-Title    Add-ons\n-Author   Pablo Hoffman\n-Created  2014-02-14\n-Status   Draft\n-=======  ===================\n-\n-================\n-SEP-021: Add-ons\n-================\n-\n-This proposal introduces add-ons, a unified way to manage Scrapy extensions,\n-middlewares and pipelines.\n-\n-Scrapy currently supports many hooks and mechanisms for extending its\n-functionality, but no single entry point for enabling and configuring them.\n-Instead, the hooks are spread over:\n-\n-* Spider middlewares (SPIDER_MIDDLEWARES)\n-* Downloader middlewares (DOWNLOADER_MIDDLEWARES)\n-* Downloader handlers (DOWNLOADER_HANDLERS)\n-* Item pipelines (ITEM_PIPELINES)\n-* Feed exporters and storages (FEED_EXPORTERS, FEED_STORAGES)\n-* Overridable components (DUPEFILTER_CLASS, STATS_CLASS, SCHEDULER, SPIDER_MANAGER_CLASS, ITEM_PROCESSOR, etc)\n-* Generic extensions (EXTENSIONS)\n-* CLI commands (COMMANDS_MODULE)\n-\n-One problem of this approach is that enabling an extension often requires\n-modifying many settings, often in a coordinated way, which is complex and error\n-prone. Add-ons are meant to fix this by providing a simple mechanism for\n-enabling extensions.\n-\n-Design goals and non-goals\n-==========================\n-\n-Goals:\n-\n-* simple to manage: adding or removing extensions should be just a matter of\n-  adding or removing lines in a ``scrapy.cfg`` file\n-* backward compatibility with enabling extension the \"old way\" (i.e. modifying\n-  settings directly)\n-\n-Non-goals:\n-\n-* a way to publish, distribute or discover extensions (use pypi for that)\n-\n-\n-Managing add-ons\n-================\n-\n-Add-ons are defined in the ``scrapy.cfg`` file, inside the ``[addons]``\n-section.\n-\n-To enable the \"httpcache\" addon, either shipped with Scrapy or in the Python\n-search path, create an entry for it in your ``scrapy.cfg``, like this::\n-\n-    [addons]\n-    httpcache = \n-\n-You may also specify the full path to an add-on (which may be either a .py file\n-or a folder containing __init__.py)::\n-\n-    [addons]\n-    mongodb_pipeline = /path/to/mongodb_pipeline.py\n-\n-\n-Writing add-ons\n-===============\n-\n-Add-ons are Python modules that implement the following callbacks.\n-\n-addon_configure\n----------------\n-\n-Receives the Settings object and modifies it to enable the required components.\n-If it raises an exception, Scrapy will print it and exit.\n-\n-Examples:\n-\n-.. code-block:: python\n-\n-    def addon_configure(settings):\n-        settings.overrides[\"DOWNLOADER_MIDDLEWARES\"].update(\n-            {\n-                \"scrapy.contrib.downloadermiddleware.httpcache.HttpCacheMiddleware\": 900,\n-            }\n-        )\n-\n-.. code-block:: python\n-\n-    def addon_configure(settings):\n-        try:\n-            import boto\n-        except ImportError:\n-            raise RuntimeError(\"boto library is required\")\n-\n-\n-crawler_ready\n--------------\n-\n-``crawler_ready`` receives a Crawler object after it has been initialized and\n-is meant to be used to perform post-initialization checks like making sure the\n-extension and its dependencies were configured properly. If it raises an\n-exception, Scrapy will print and exit.\n-\n-Examples:\n-\n-.. code-block:: python\n-\n-    def crawler_ready(crawler):\n-        if \"some.other.addon\" not in crawler.extensions.enabled:\n-            raise RuntimeError(\"Some other addon is required to use this addon\")\n",
    "test_patch": "diff --git a/tests/test_addons.py b/tests/test_addons.py\nnew file mode 100644\nindex 00000000000..5d053ed52d9\n--- /dev/null\n+++ b/tests/test_addons.py\n@@ -0,0 +1,158 @@\n+import itertools\n+import unittest\n+from typing import Any, Dict\n+\n+from scrapy import Spider\n+from scrapy.crawler import Crawler, CrawlerRunner\n+from scrapy.exceptions import NotConfigured\n+from scrapy.settings import BaseSettings, Settings\n+from scrapy.utils.test import get_crawler\n+\n+\n+class SimpleAddon:\n+    def update_settings(self, settings):\n+        pass\n+\n+\n+def get_addon_cls(config: Dict[str, Any]) -> type:\n+    class AddonWithConfig:\n+        def update_settings(self, settings: BaseSettings):\n+            settings.update(config, priority=\"addon\")\n+\n+    return AddonWithConfig\n+\n+\n+class CreateInstanceAddon:\n+    def __init__(self, crawler: Crawler) -> None:\n+        super().__init__()\n+        self.crawler = crawler\n+        self.config = crawler.settings.getdict(\"MYADDON\")\n+\n+    @classmethod\n+    def from_crawler(cls, crawler: Crawler):\n+        return cls(crawler)\n+\n+    def update_settings(self, settings):\n+        settings.update(self.config, \"addon\")\n+\n+\n+class AddonTest(unittest.TestCase):\n+    def test_update_settings(self):\n+        settings = BaseSettings()\n+        settings.set(\"KEY1\", \"default\", priority=\"default\")\n+        settings.set(\"KEY2\", \"project\", priority=\"project\")\n+        addon_config = {\"KEY1\": \"addon\", \"KEY2\": \"addon\", \"KEY3\": \"addon\"}\n+        testaddon = get_addon_cls(addon_config)()\n+        testaddon.update_settings(settings)\n+        self.assertEqual(settings[\"KEY1\"], \"addon\")\n+        self.assertEqual(settings[\"KEY2\"], \"project\")\n+        self.assertEqual(settings[\"KEY3\"], \"addon\")\n+\n+\n+class AddonManagerTest(unittest.TestCase):\n+    def test_load_settings(self):\n+        settings_dict = {\n+            \"ADDONS\": {\"tests.test_addons.SimpleAddon\": 0},\n+        }\n+        crawler = get_crawler(settings_dict=settings_dict)\n+        manager = crawler.addons\n+        self.assertIsInstance(manager.addons[0], SimpleAddon)\n+\n+    def test_notconfigured(self):\n+        class NotConfiguredAddon:\n+            def update_settings(self, settings):\n+                raise NotConfigured()\n+\n+        settings_dict = {\n+            \"ADDONS\": {NotConfiguredAddon: 0},\n+        }\n+        crawler = get_crawler(settings_dict=settings_dict)\n+        manager = crawler.addons\n+        self.assertFalse(manager.addons)\n+\n+    def test_load_settings_order(self):\n+        # Get three addons with different settings\n+        addonlist = []\n+        for i in range(3):\n+            addon = get_addon_cls({\"KEY1\": i})\n+            addon.number = i\n+            addonlist.append(addon)\n+        # Test for every possible ordering\n+        for ordered_addons in itertools.permutations(addonlist):\n+            expected_order = [a.number for a in ordered_addons]\n+            settings = {\"ADDONS\": {a: i for i, a in enumerate(ordered_addons)}}\n+            crawler = get_crawler(settings_dict=settings)\n+            manager = crawler.addons\n+            self.assertEqual([a.number for a in manager.addons], expected_order)\n+            self.assertEqual(crawler.settings.getint(\"KEY1\"), expected_order[-1])\n+\n+    def test_create_instance(self):\n+        settings_dict = {\n+            \"ADDONS\": {\"tests.test_addons.CreateInstanceAddon\": 0},\n+            \"MYADDON\": {\"MYADDON_KEY\": \"val\"},\n+        }\n+        crawler = get_crawler(settings_dict=settings_dict)\n+        manager = crawler.addons\n+        self.assertIsInstance(manager.addons[0], CreateInstanceAddon)\n+        self.assertEqual(crawler.settings.get(\"MYADDON_KEY\"), \"val\")\n+\n+    def test_settings_priority(self):\n+        config = {\n+            \"KEY\": 15,  # priority=addon\n+        }\n+        settings_dict = {\n+            \"ADDONS\": {get_addon_cls(config): 1},\n+        }\n+        crawler = get_crawler(settings_dict=settings_dict)\n+        self.assertEqual(crawler.settings.getint(\"KEY\"), 15)\n+\n+        settings = Settings(settings_dict)\n+        settings.set(\"KEY\", 0, priority=\"default\")\n+        runner = CrawlerRunner(settings)\n+        crawler = runner.create_crawler(Spider)\n+        self.assertEqual(crawler.settings.getint(\"KEY\"), 15)\n+\n+        settings_dict = {\n+            \"KEY\": 20,  # priority=project\n+            \"ADDONS\": {get_addon_cls(config): 1},\n+        }\n+        settings = Settings(settings_dict)\n+        settings.set(\"KEY\", 0, priority=\"default\")\n+        runner = CrawlerRunner(settings)\n+        crawler = runner.create_crawler(Spider)\n+        self.assertEqual(crawler.settings.getint(\"KEY\"), 20)\n+\n+    def test_fallback_workflow(self):\n+        FALLBACK_SETTING = \"MY_FALLBACK_DOWNLOAD_HANDLER\"\n+\n+        class AddonWithFallback:\n+            def update_settings(self, settings):\n+                if not settings.get(FALLBACK_SETTING):\n+                    settings.set(\n+                        FALLBACK_SETTING,\n+                        settings.getwithbase(\"DOWNLOAD_HANDLERS\")[\"https\"],\n+                        \"addon\",\n+                    )\n+                settings[\"DOWNLOAD_HANDLERS\"][\"https\"] = \"AddonHandler\"\n+\n+        settings_dict = {\n+            \"ADDONS\": {AddonWithFallback: 1},\n+        }\n+        crawler = get_crawler(settings_dict=settings_dict)\n+        self.assertEqual(\n+            crawler.settings.getwithbase(\"DOWNLOAD_HANDLERS\")[\"https\"], \"AddonHandler\"\n+        )\n+        self.assertEqual(\n+            crawler.settings.get(FALLBACK_SETTING),\n+            \"scrapy.core.downloader.handlers.http.HTTPDownloadHandler\",\n+        )\n+\n+        settings_dict = {\n+            \"ADDONS\": {AddonWithFallback: 1},\n+            \"DOWNLOAD_HANDLERS\": {\"https\": \"UserHandler\"},\n+        }\n+        crawler = get_crawler(settings_dict=settings_dict)\n+        self.assertEqual(\n+            crawler.settings.getwithbase(\"DOWNLOAD_HANDLERS\")[\"https\"], \"AddonHandler\"\n+        )\n+        self.assertEqual(crawler.settings.get(FALLBACK_SETTING), \"UserHandler\")\ndiff --git a/tests/test_middleware.py b/tests/test_middleware.py\nindex 00ff746ee5a..a42c7b3d1e2 100644\n--- a/tests/test_middleware.py\n+++ b/tests/test_middleware.py\n@@ -39,7 +39,7 @@ def close_spider(self, spider):\n         pass\n \n     def __init__(self):\n-        raise NotConfigured\n+        raise NotConfigured(\"foo\")\n \n \n class TestMiddlewareManager(MiddlewareManager):\ndiff --git a/tests/test_utils_deprecate.py b/tests/test_utils_deprecate.py\nindex 2d9210410d4..eedb6f6af9c 100644\n--- a/tests/test_utils_deprecate.py\n+++ b/tests/test_utils_deprecate.py\n@@ -296,3 +296,7 @@ def test_unmatched_path_stays_the_same(self):\n             output = update_classpath(\"scrapy.unmatched.Path\")\n         self.assertEqual(output, \"scrapy.unmatched.Path\")\n         self.assertEqual(len(w), 0)\n+\n+    def test_returns_nonstring(self):\n+        for notastring in [None, True, [1, 2, 3], object()]:\n+            self.assertEqual(update_classpath(notastring), notastring)\n",
    "problem_statement": "NotConfigured logging breaks when the component is added by class object\nAs the log message for components that raise `NotConfigured` with a message assumes `clsname` is an import path string, it raises an AttributeError when it's a class instance. https://github.com/scrapy/scrapy/blob/bddbbc522aef00dc150e479e6288041cee2e95c9/scrapy/middleware.py#L49\n",
    "hints_text": "",
    "created_at": "2023-06-14T14:17:09Z",
    "version": "2.9",
    "PASS_TO_PASS": [],
    "FAIL_TO_PASS": [
      "tests/test_addons.py::AddonTest::test_update_settings",
      "tests/test_addons.py::AddonManagerTest::test_create_instance",
      "tests/test_addons.py::AddonManagerTest::test_fallback_workflow",
      "tests/test_addons.py::AddonManagerTest::test_load_settings",
      "tests/test_addons.py::AddonManagerTest::test_load_settings_order",
      "tests/test_addons.py::AddonManagerTest::test_notconfigured",
      "tests/test_addons.py::AddonManagerTest::test_settings_priority",
      "tests/test_utils_deprecate.py::UpdateClassPathTest::test_returns_nonstring"
    ]
  },
  {
    "repo": "scrapy/scrapy",
    "pull_number": 6608,
    "instance_id": "scrapy__scrapy-6608",
    "issue_numbers": [
      "6603"
    ],
    "base_commit": "b6d69e389576c137a33d14c9e9319891dce68442",
    "patch": "diff --git a/docs/topics/settings.rst b/docs/topics/settings.rst\nindex 76904a26ef0..ea12a7f8447 100644\n--- a/docs/topics/settings.rst\n+++ b/docs/topics/settings.rst\n@@ -418,6 +418,38 @@ This setting also affects :setting:`DOWNLOAD_DELAY` and\n :ref:`topics-autothrottle`: if :setting:`CONCURRENT_REQUESTS_PER_IP`\n is non-zero, download delay is enforced per IP, not per domain.\n \n+.. setting:: DEFAULT_DROPITEM_LOG_LEVEL\n+\n+DEFAULT_DROPITEM_LOG_LEVEL\n+--------------------------\n+\n+Default: ``\"WARNING\"``\n+\n+Default :ref:`log level <levels>` of messages about dropped items.\n+\n+When an item is dropped by raising :exc:`scrapy.exceptions.DropItem` from the\n+:func:`process_item` method of an :ref:`item pipeline <topics-item-pipeline>`,\n+a message is logged, and by default its log level is the one configured in this\n+setting.\n+\n+You may specify this log level as an integer (e.g. ``20``), as a log level\n+constant (e.g. ``logging.INFO``) or as a string with the name of a log level\n+constant (e.g. ``\"INFO\"``).\n+\n+When writing an item pipeline, you can force a different log level by setting\n+:attr:`scrapy.exceptions.DropItem.log_level` in your\n+:exc:`scrapy.exceptions.DropItem` exception. For example:\n+\n+.. code-block:: python\n+\n+   from scrapy.exceptions import DropItem\n+\n+\n+   class MyPipeline:\n+       def process_item(self, item, spider):\n+           if not item.get(\"price\"):\n+               raise DropItem(\"Missing price data\", log_level=\"INFO\")\n+           return item\n \n .. setting:: DEFAULT_ITEM_CLASS\n \ndiff --git a/scrapy/exceptions.py b/scrapy/exceptions.py\nindex 96566ba864f..f37f881a7da 100644\n--- a/scrapy/exceptions.py\n+++ b/scrapy/exceptions.py\n@@ -5,6 +5,8 @@\n new exceptions here without documenting them there.\n \"\"\"\n \n+from __future__ import annotations\n+\n from typing import Any\n \n # Internal\n@@ -58,6 +60,10 @@ def __init__(self, *, fail: bool = True):\n class DropItem(Exception):\n     \"\"\"Drop item from the item pipeline\"\"\"\n \n+    def __init__(self, message: str, log_level: str | None = None):\n+        super().__init__(message)\n+        self.log_level = log_level\n+\n \n class NotSupported(Exception):\n     \"\"\"Indicates a feature or method is not supported\"\"\"\ndiff --git a/scrapy/logformatter.py b/scrapy/logformatter.py\nindex 544f4adfe42..eff81d28fc6 100644\n--- a/scrapy/logformatter.py\n+++ b/scrapy/logformatter.py\n@@ -120,8 +120,12 @@ def dropped(\n         spider: Spider,\n     ) -> LogFormatterResult:\n         \"\"\"Logs a message when an item is dropped while it is passing through the item pipeline.\"\"\"\n+        if (level := getattr(exception, \"log_level\", None)) is None:\n+            level = spider.crawler.settings[\"DEFAULT_DROPITEM_LOG_LEVEL\"]\n+        if isinstance(level, str):\n+            level = getattr(logging, level)\n         return {\n-            \"level\": logging.WARNING,\n+            \"level\": level,\n             \"msg\": DROPPEDMSG,\n             \"args\": {\n                 \"exception\": exception,\ndiff --git a/scrapy/settings/default_settings.py b/scrapy/settings/default_settings.py\nindex 0bbde118e95..7ef365f686d 100644\n--- a/scrapy/settings/default_settings.py\n+++ b/scrapy/settings/default_settings.py\n@@ -49,6 +49,8 @@\n COOKIES_ENABLED = True\n COOKIES_DEBUG = False\n \n+DEFAULT_DROPITEM_LOG_LEVEL = \"WARNING\"\n+\n DEFAULT_ITEM_CLASS = \"scrapy.item.Item\"\n \n DEFAULT_REQUEST_HEADERS = {\n",
    "test_patch": "diff --git a/tests/test_logformatter.py b/tests/test_logformatter.py\nindex 5a92521cc3f..0d1e82f0b5a 100644\n--- a/tests/test_logformatter.py\n+++ b/tests/test_logformatter.py\n@@ -1,5 +1,7 @@\n+import logging\n import unittest\n \n+import pytest\n from testfixtures import LogCapture\n from twisted.internet import defer\n from twisted.python.failure import Failure\n@@ -26,6 +28,7 @@ class LogFormatterTestCase(unittest.TestCase):\n     def setUp(self):\n         self.formatter = LogFormatter()\n         self.spider = Spider(\"default\")\n+        self.spider.crawler = get_crawler()\n \n     def test_crawled_with_referer(self):\n         req = Request(\"http://www.example.com\")\n@@ -68,6 +71,62 @@ def test_dropped(self):\n         assert all(isinstance(x, str) for x in lines)\n         self.assertEqual(lines, [\"Dropped: \\u2018\", \"{}\"])\n \n+    def test_dropitem_default_log_level(self):\n+        item = {}\n+        exception = DropItem(\"Test drop\")\n+        response = Response(\"http://www.example.com\")\n+        spider = Spider(\"foo\")\n+        spider.crawler = get_crawler(Spider)\n+\n+        logkws = self.formatter.dropped(item, exception, response, spider)\n+        self.assertEqual(logkws[\"level\"], logging.WARNING)\n+\n+        spider.crawler.settings.frozen = False\n+        spider.crawler.settings[\"DEFAULT_DROPITEM_LOG_LEVEL\"] = logging.INFO\n+        spider.crawler.settings.frozen = True\n+        logkws = self.formatter.dropped(item, exception, response, spider)\n+        self.assertEqual(logkws[\"level\"], logging.INFO)\n+\n+        spider.crawler.settings.frozen = False\n+        spider.crawler.settings[\"DEFAULT_DROPITEM_LOG_LEVEL\"] = \"INFO\"\n+        spider.crawler.settings.frozen = True\n+        logkws = self.formatter.dropped(item, exception, response, spider)\n+        self.assertEqual(logkws[\"level\"], logging.INFO)\n+\n+        spider.crawler.settings.frozen = False\n+        spider.crawler.settings[\"DEFAULT_DROPITEM_LOG_LEVEL\"] = 10\n+        spider.crawler.settings.frozen = True\n+        logkws = self.formatter.dropped(item, exception, response, spider)\n+        self.assertEqual(logkws[\"level\"], logging.DEBUG)\n+\n+        spider.crawler.settings.frozen = False\n+        spider.crawler.settings[\"DEFAULT_DROPITEM_LOG_LEVEL\"] = 0\n+        spider.crawler.settings.frozen = True\n+        logkws = self.formatter.dropped(item, exception, response, spider)\n+        self.assertEqual(logkws[\"level\"], logging.NOTSET)\n+\n+        unsupported_value = object()\n+        spider.crawler.settings.frozen = False\n+        spider.crawler.settings[\"DEFAULT_DROPITEM_LOG_LEVEL\"] = unsupported_value\n+        spider.crawler.settings.frozen = True\n+        logkws = self.formatter.dropped(item, exception, response, spider)\n+        self.assertEqual(logkws[\"level\"], unsupported_value)\n+\n+        with pytest.raises(TypeError):\n+            logging.log(logkws[\"level\"], \"message\")\n+\n+    def test_dropitem_custom_log_level(self):\n+        item = {}\n+        response = Response(\"http://www.example.com\")\n+\n+        exception = DropItem(\"Test drop\", log_level=\"INFO\")\n+        logkws = self.formatter.dropped(item, exception, response, self.spider)\n+        self.assertEqual(logkws[\"level\"], logging.INFO)\n+\n+        exception = DropItem(\"Test drop\", log_level=\"ERROR\")\n+        logkws = self.formatter.dropped(item, exception, response, self.spider)\n+        self.assertEqual(logkws[\"level\"], logging.ERROR)\n+\n     def test_item_error(self):\n         # In practice, the complete traceback is shown by passing the\n         # 'exc_info' argument to the logging function\n@@ -145,6 +204,7 @@ class LogformatterSubclassTest(LogFormatterTestCase):\n     def setUp(self):\n         self.formatter = LogFormatterSubclass()\n         self.spider = Spider(\"default\")\n+        self.spider.crawler = get_crawler(Spider)\n \n     def test_crawled_with_referer(self):\n         req = Request(\"http://www.example.com\")\n",
    "problem_statement": "Allow changing the log severity of DropItem\nBy default, when an item is dropped, a `WARNING` message is logged. [It would be nice to make that more flexible](https://github.com/zytedata/zyte-common-items/pull/126#discussion_r1900692613).\n",
    "hints_text": "",
    "created_at": "2025-01-05T20:32:00Z",
    "version": "2.12",
    "PASS_TO_PASS": [],
    "FAIL_TO_PASS": [
      "tests/test_logformatter.py::LogFormatterTestCase::test_dropitem_default_log_level",
      "tests/test_logformatter.py::LogFormatterTestCase::test_dropitem_custom_log_level"
    ]
  },
  {
    "repo": "scrapy/scrapy",
    "pull_number": 6606,
    "instance_id": "scrapy__scrapy-6606",
    "issue_numbers": [
      "6600"
    ],
    "base_commit": "98ba61256deceba7b04b938a97005258f4ef5c66",
    "patch": "diff --git a/scrapy/cmdline.py b/scrapy/cmdline.py\nindex 065adccfb29..b08fd34095c 100644\n--- a/scrapy/cmdline.py\n+++ b/scrapy/cmdline.py\n@@ -96,10 +96,9 @@ def _get_project_only_cmds(settings: BaseSettings) -> set[str]:\n \n \n def _pop_command_name(argv: list[str]) -> str | None:\n-    for i, arg in enumerate(argv[1:]):\n-        if not arg.startswith(\"-\"):\n-            del argv[i]\n-            return arg\n+    for i in range(1, len(argv)):\n+        if not argv[i].startswith(\"-\"):\n+            return argv.pop(i)\n     return None\n \n \n",
    "test_patch": "diff --git a/tests/test_commands.py b/tests/test_commands.py\nindex 1aae3222e5c..50f09304333 100644\n--- a/tests/test_commands.py\n+++ b/tests/test_commands.py\n@@ -23,7 +23,7 @@\n from twisted.trial import unittest\n \n import scrapy\n-from scrapy.cmdline import _print_unknown_command_msg\n+from scrapy.cmdline import _pop_command_name, _print_unknown_command_msg\n from scrapy.commands import ScrapyCommand, ScrapyHelpFormatter, view\n from scrapy.commands.startproject import IGNORE\n from scrapy.settings import Settings\n@@ -1163,3 +1163,29 @@ def test_help_messages(self):\n         for command in self.commands:\n             _, out, _ = self.proc(command, \"-h\")\n             self.assertIn(\"Usage\", out)\n+\n+\n+class PopCommandNameTest(unittest.TestCase):\n+    def test_valid_command(self):\n+        argv = [\"scrapy\", \"crawl\", \"my_spider\"]\n+        command = _pop_command_name(argv)\n+        self.assertEqual(command, \"crawl\")\n+        self.assertEqual(argv, [\"scrapy\", \"my_spider\"])\n+\n+    def test_no_command(self):\n+        argv = [\"scrapy\"]\n+        command = _pop_command_name(argv)\n+        self.assertIsNone(command)\n+        self.assertEqual(argv, [\"scrapy\"])\n+\n+    def test_option_before_command(self):\n+        argv = [\"scrapy\", \"-h\", \"crawl\"]\n+        command = _pop_command_name(argv)\n+        self.assertEqual(command, \"crawl\")\n+        self.assertEqual(argv, [\"scrapy\", \"-h\"])\n+\n+    def test_option_after_command(self):\n+        argv = [\"scrapy\", \"crawl\", \"-h\"]\n+        command = _pop_command_name(argv)\n+        self.assertEqual(command, \"crawl\")\n+        self.assertEqual(argv, [\"scrapy\", \"-h\"])\n",
    "problem_statement": "Investigate off-by-1 in `scrapy.cmdline._pop_command_name()`\nIt looks like `del argv[i]` removes the wrong item in `scrapy.cmdline._pop_command_name()` but as we don't seem to see any problems because of this it's worth investigating what exactly happens here and either fixing or refactoring the code.\n",
    "hints_text": "Hi. I looked into `scrapy.cmdline._pop_command_name()` and the goal is to extract the command name from `argv `list. The `i` variable in the function is used to track the position in the original `argv `list and it's value is 0. However, since `argv[1:]` is a slice of `argv`, it starts at index 0 of the slice, not the original list, so when `del argv[i]` is executed, it deletes the wrong element because `i` does not align with the original list's index. I created a sample scrapy project [using this tutorial](https://docs.scrapy.org/en/master/intro/tutorial.html) and ran the command `scrapy crawl quotes`. I can see that the value of argv at the start is a list `['C:\\\\Users\\\\Hp\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\Scripts\\\\scrapy', 'crawl', 'quotes']`. It is expected that the `'crawl'` should be removed from `argv` and but after the for loop executes, `scrapy` is removed from the list and the value of `argv` is `['crawl', 'quotes']`. Screenshot below for reference.\r\n![image](https://github.com/user-attachments/assets/084f3f48-9d59-431b-bc90-1eaff5ec1fc9)\r\n\r\nAs the `for `loop iterates over `argv[1:]`, the value of `arg` is `'crawl' `and the `return arg` line returns `'crawl'`. So even though the wrong value was removed from `argv` due to incorrect index being used, the value returned by `_pop_command_name()` is the expected value.\r\n\r\nTo fix the issue, we need to ensure that the index `i` refers to the correct position in the original `argv` list. I can work on refactoring the code. However, I noticed that the function implementation for `scrapy.cmdline._pop_command_name()` in the master branch is different from what I see from the installed version of scrapy v2.12.0. I see there is a branch 2.12. For which branch should the PR be submitted?\nThanks for the analysis. As `argv` is modified in-place and used after `_pop_command_name()` is called (`parser.parse_known_args(args=argv[1:])`, I wonder why it works even though `argv` is incorrect. I guess `parse_known_args()` only cares about dash-prefixed argv entries, but it's possible that a dash-prefixed one will be deleted because of off-by-1 and then the behavior will be unexpected. It would be nice to find such cases. `scrapy -h crawl` doesn't work while `scrapy crawl -h` does, which may be the only way to break this, and `scrapy -h` says `scrapy <command> [options] [args]` which suggests the options can't go before the command name. If it's indeed the only way to break this, and the fix fixes such commands, it would still be actually useful.\r\n\r\n> For which branch should the PR be submitted?\r\n\r\nAlways for master.\n> it's possible that a dash-prefixed one will be deleted because of off-by-1 and then the behavior will be unexpected.\r\n\r\nThat's correct. If I use the command `scrapy crawl quote`, then the `parser.parse_known_args(args=argv[1:])` works even if `argv` is incorrect because the value passed to `parse_known_args` is `argv[1:]` which will always be `'quote'`, irrespective of whether `scrapy` or `crawl` is popped from the original arg list, and since `_pop_command_name` unintentionally returns the correct command, the program runs without an error. Same goes for `scrapy crawl -h`. However, if I try the command `scrapy -h crawl`, then `-h` gets deleted from the arg list and the value returned by `_pop_command_name` is `'crawl'`, which instructs the program to run the crawl command. As the line `parser.parse_known_args(args=argv[1:])` is executed, the value of `argv[1:]` also remains `'crawl'` and since there is no spider with the name 'crawl' in my scrapy project, the program exits with an error. Will work on fixing it and submit a PR today.\r\n\r\nI have a question unrelated to the problem. I would like to understand why the implementation of scrapy.cmdline._pop_command_name() from the scrapy package I installed from pip is different from the master branch. I thought that master branch contains the latest changes that should also be present in the latest package installed from pip.\n> I would like to understand why the implementation of scrapy.cmdline._pop_command_name() from the scrapy package I installed from pip is different from the master branch. \r\n\r\nBecause it was changed in master after 2.12.0 was released.\r\n\r\n> I thought that master branch contains the latest changes that should also be present in the latest package installed from pip.\r\n\r\nWell no, it contains changes that were not released yet.",
    "created_at": "2025-01-04T10:41:31Z",
    "version": "2.12",
    "PASS_TO_PASS": [],
    "FAIL_TO_PASS": [
      "tests/test_commands.py::PopCommandNameTest::test_valid_command",
      "tests/test_commands.py::PopCommandNameTest::test_no_command",
      "tests/test_commands.py::PopCommandNameTest::test_option_before_command",
      "tests/test_commands.py::PopCommandNameTest::test_option_after_command"
    ]
  },
  {
    "repo": "scrapy/scrapy",
    "pull_number": 6050,
    "instance_id": "scrapy__scrapy-6050",
    "issue_numbers": [
      "6049"
    ],
    "base_commit": "dba37674e6eaa6c2030c8eb35ebf8127cd488062",
    "patch": "diff --git a/scrapy/downloadermiddlewares/retry.py b/scrapy/downloadermiddlewares/retry.py\nindex 205bb48b103..380623cea3e 100644\n--- a/scrapy/downloadermiddlewares/retry.py\n+++ b/scrapy/downloadermiddlewares/retry.py\n@@ -24,9 +24,8 @@\n retry_logger = getLogger(__name__)\n \n \n-class BackwardsCompatibilityMetaclass(type):\n-    @property\n-    def EXCEPTIONS_TO_RETRY(cls):\n+def backwards_compatibility_getattr(self, name):\n+    if name == \"EXCEPTIONS_TO_RETRY\":\n         warnings.warn(\n             \"Attribute RetryMiddleware.EXCEPTIONS_TO_RETRY is deprecated. \"\n             \"Use the RETRY_EXCEPTIONS setting instead.\",\n@@ -37,6 +36,13 @@ def EXCEPTIONS_TO_RETRY(cls):\n             load_object(x) if isinstance(x, str) else x\n             for x in Settings().getlist(\"RETRY_EXCEPTIONS\")\n         )\n+    raise AttributeError(\n+        f\"{self.__class__.__name__!r} object has no attribute {name!r}\"\n+    )\n+\n+\n+class BackwardsCompatibilityMetaclass(type):\n+    __getattr__ = backwards_compatibility_getattr\n \n \n def get_retry_request(\n@@ -137,15 +143,14 @@ def __init__(self, settings):\n         )\n         self.priority_adjust = settings.getint(\"RETRY_PRIORITY_ADJUST\")\n \n-        if not hasattr(\n-            self, \"EXCEPTIONS_TO_RETRY\"\n-        ):  # If EXCEPTIONS_TO_RETRY is not \"overriden\"\n+        try:\n+            self.exceptions_to_retry = self.__getattribute__(\"EXCEPTIONS_TO_RETRY\")\n+        except AttributeError:\n+            # If EXCEPTIONS_TO_RETRY is not \"overridden\"\n             self.exceptions_to_retry = tuple(\n                 load_object(x) if isinstance(x, str) else x\n                 for x in settings.getlist(\"RETRY_EXCEPTIONS\")\n             )\n-        else:\n-            self.exceptions_to_retry = self.EXCEPTIONS_TO_RETRY\n \n     @classmethod\n     def from_crawler(cls, crawler):\n@@ -175,3 +180,5 @@ def _retry(self, request, reason, spider):\n             max_retry_times=max_retry_times,\n             priority_adjust=priority_adjust,\n         )\n+\n+    __getattr__ = backwards_compatibility_getattr\n",
    "test_patch": "diff --git a/tests/test_downloadermiddleware_retry.py b/tests/test_downloadermiddleware_retry.py\nindex 97ae1e29a27..66117584052 100644\n--- a/tests/test_downloadermiddleware_retry.py\n+++ b/tests/test_downloadermiddleware_retry.py\n@@ -122,7 +122,7 @@ def test_exception_to_retry_added(self):\n         req = Request(f\"http://www.scrapytest.org/{exc.__name__}\")\n         self._test_retry_exception(req, exc(\"foo\"), mw)\n \n-    def test_exception_to_retry_customMiddleware(self):\n+    def test_exception_to_retry_custom_middleware(self):\n         exc = ValueError\n \n         with warnings.catch_warnings(record=True) as warns:\n@@ -138,6 +138,21 @@ class MyRetryMiddleware(RetryMiddleware):\n         assert isinstance(req, Request)\n         self.assertEqual(req.meta[\"retry_times\"], 1)\n \n+    def test_exception_to_retry_custom_middleware_self(self):\n+        class MyRetryMiddleware(RetryMiddleware):\n+            def process_exception(self, request, exception, spider):\n+                if isinstance(exception, self.EXCEPTIONS_TO_RETRY):\n+                    return self._retry(request, exception, spider)\n+\n+        exc = OSError\n+        mw2 = MyRetryMiddleware.from_crawler(self.crawler)\n+        req = Request(f\"http://www.scrapytest.org/{exc.__name__}\")\n+        with warnings.catch_warnings(record=True) as warns:\n+            req = mw2.process_exception(req, exc(\"foo\"), self.spider)\n+        assert isinstance(req, Request)\n+        self.assertEqual(req.meta[\"retry_times\"], 1)\n+        self.assertEqual(len(warns), 1)\n+\n     def _test_retry_exception(self, req, exception, mw=None):\n         if mw is None:\n             mw = self.mw\n",
    "problem_statement": "`downloadermiddlewares.retry.BackwardsCompatibilityMetaclass` does not provide backward compatibility for middleware instances\n# Description\r\n\r\nPreviously, `EXCEPTIONS_TO_RETRY` was an attribute of `RetryMiddleware`. This allows:\r\n- `RetryMiddleware` subclasses could access `EXCEPTIONS_TO_RETRY` via `cls.EXCEPTIONS_TO_RETRY`.\r\n- `RetryMiddleware` instances and instances of its subclasses could access `EXCEPTIONS_TO_RETRY` via `self.EXCEPTIONS_TO_RETRY`.\r\n\r\nIn 2.10 `EXCEPTIONS_TO_RETRY` was removed and added as a property to `BackwardsCompatibilityMetaclass`. This added compatibility only for the first point.\r\n\r\n# Steps to Reproduce\r\n\r\n```python\r\nclass MyRetryMiddleware(RetryMiddleware):\r\n\r\n    def process_exception(self, request, exception, spider):\r\n        if isinstance(exception, self.EXCEPTIONS_TO_RETRY) and not request.meta.get('dont_retry', False):\r\n            # update request\r\n            return self._retry(request, exception, spider)\r\n```\r\n\r\n# Expected behavior\r\n\r\nA warning about `EXCEPTIONS_TO_RETRY` deprecation.\r\n\r\n# Actual behavior\r\n\r\nAttributeError: 'MyRetryMiddleware' object has no attribute 'EXCEPTIONS_TO_RETRY'\r\n\r\n# Versions\r\n\r\n```\r\nScrapy       : 2.10.1\r\nlxml         : 4.9.3.0\r\nlibxml2      : 2.10.3\r\ncssselect    : 1.2.0\r\nparsel       : 1.8.1\r\nw3lib        : 2.1.2\r\nTwisted      : 22.10.0\r\nPython       : 3.11.4 (tags/v3.11.4:d2340ef, Jun  7 2023, 05:45:37) [MSC v.1934 64 bit (AMD64)]\r\npyOpenSSL    : 23.2.0 (OpenSSL 3.1.2 1 Aug 2023)\r\ncryptography : 41.0.3\r\nPlatform     : Windows-10-10.0.19044-SP0\r\n```\r\n\n",
    "hints_text": "The fix is actually very tricky. Here is a sample snippet:\r\n```python\r\nDEPRECATED_ATTRIBUTE = 'A'\r\n\r\n\r\ndef __getattr__(self, item):\r\n    if item == DEPRECATED_ATTRIBUTE:\r\n        return 'A does not exist'\r\n\r\n    raise AttributeError(f'{self.__class__.__name__!r} object has no attribute {item!r}')\r\n\r\n\r\nclass Meta(type):\r\n    __getattr__ = __getattr__\r\n\r\n\r\nclass Data(metaclass=Meta):\r\n    def __init__(self):\r\n        try:\r\n            self.a = self.__getattribute__(DEPRECATED_ATTRIBUTE)\r\n        except AttributeError:\r\n            self.a = 'a here'\r\n\r\n    __getattr__ = __getattr__\r\n\r\n\r\nclass DataDefineA(Data):\r\n    A = 1\r\n\r\n\r\nclass DataNoA(Data): pass\r\n\r\n\r\nprint(Data.A)\r\nprint(Data().A)\r\nprint(Data().a)\r\n\r\nprint(DataDefineA.A)\r\nprint(DataDefineA().A)\r\nprint(DataDefineA().a)\r\n\r\nprint(DataNoA.A)\r\nprint(DataNoA().A)\r\nprint(DataNoA().a)\r\n```\r\n\r\nOutput:\r\n```\r\nA does not exist\r\nA does not exist\r\na here\r\n1\r\n1\r\n1\r\nA does not exist\r\nA does not exist\r\na here\r\n```\r\n\r\nSuch solution achieves both compatibility issues. In addition, if a subclass or its instance defines attribute `A`, then it will be assigned to attribute `a`.\r\n\r\nRewritten `retry.py`:\r\n\r\n```python\r\nDEPRECATED_ATTRIBUTE = 'EXCEPTIONS_TO_RETRY'\r\n\r\n\r\ndef backwards_compatibility_getattr(self, item):\r\n    if item == DEPRECATED_ATTRIBUTE:\r\n        warnings.warn(\r\n            f\"Attribute RetryMiddleware.{DEPRECATED_ATTRIBUTE} is deprecated. \"\r\n            \"Use the RETRY_EXCEPTIONS setting instead.\",\r\n            ScrapyDeprecationWarning,\r\n            stacklevel=2,\r\n            )\r\n\r\n        return tuple(\r\n            load_object(x) if isinstance(x, str) else x\r\n            for x in Settings().getlist(\"RETRY_EXCEPTIONS\")\r\n            )\r\n\r\n    raise AttributeError(f'{self.__class__.__name__!r} object has no attribute {item!r}')\r\n\r\n\r\nclass BackwardsCompatibilityMetaclass(type):\r\n    __getattr__ = backwards_compatibility_getattr\r\n\r\n\r\nclass RetryMiddleware(metaclass=BackwardsCompatibilityMetaclass):\r\n    def __init__(self, settings):\r\n        if not settings.getbool(\"RETRY_ENABLED\"):\r\n            raise NotConfigured\r\n        self.max_retry_times = settings.getint(\"RETRY_TIMES\")\r\n        self.retry_http_codes = set(\r\n            int(x) for x in settings.getlist(\"RETRY_HTTP_CODES\")\r\n        )\r\n        self.priority_adjust = settings.getint(\"RETRY_PRIORITY_ADJUST\")\r\n\r\n        try:\r\n            self.exceptions_to_retry = self.__getattribute__(DEPRECATED_ATTRIBUTE)\r\n        except AttributeError:\r\n            # If EXCEPTIONS_TO_RETRY is not \"overridden\"\r\n            self.exceptions_to_retry = tuple(\r\n                load_object(x) if isinstance(x, str) else x\r\n                for x in settings.getlist(\"RETRY_EXCEPTIONS\")\r\n                )\r\n\r\n    __getattr__ = backwards_compatibility_getattr\r\n```",
    "created_at": "2023-09-14T12:08:06Z",
    "version": "2.10",
    "PASS_TO_PASS": [],
    "FAIL_TO_PASS": [
      "tests/test_downloadermiddleware_retry.py::RetryTest::test_exception_to_retry_custom_middleware_self"
    ]
  },
  {
    "repo": "scrapy/scrapy",
    "pull_number": 6013,
    "instance_id": "scrapy__scrapy-6013",
    "issue_numbers": [
      "6011"
    ],
    "base_commit": "7c497688f8e20339d766573ae6ce2e7782beb1da",
    "patch": "diff --git a/scrapy/settings/__init__.py b/scrapy/settings/__init__.py\nindex bc82cc098ac..ba9727bacf5 100644\n--- a/scrapy/settings/__init__.py\n+++ b/scrapy/settings/__init__.py\n@@ -238,7 +238,7 @@ def getdict(\n     def getdictorlist(\n         self,\n         name: _SettingsKeyT,\n-        default: Union[Dict[Any, Any], List[Any], None] = None,\n+        default: Union[Dict[Any, Any], List[Any], Tuple[Any], None] = None,\n     ) -> Union[Dict[Any, Any], List[Any]]:\n         \"\"\"Get a setting value as either a :class:`dict` or a :class:`list`.\n \n@@ -271,6 +271,8 @@ def getdictorlist(\n                 return value_loaded\n             except ValueError:\n                 return value.split(\",\")\n+        if isinstance(value, tuple):\n+            return list(value)\n         assert isinstance(value, (dict, list))\n         return copy.deepcopy(value)\n \n",
    "test_patch": "diff --git a/.github/workflows/tests-ubuntu.yml b/.github/workflows/tests-ubuntu.yml\nindex 54b3fbaa2c0..c2b6866286e 100644\n--- a/.github/workflows/tests-ubuntu.yml\n+++ b/.github/workflows/tests-ubuntu.yml\n@@ -48,13 +48,13 @@ jobs:\n           env:\n             TOXENV: botocore\n \n-        - python-version: \"3.12.0-beta.4\"\n+        - python-version: \"3.12.0-rc.1\"\n           env:\n             TOXENV: py\n-        - python-version: \"3.12.0-beta.4\"\n+        - python-version: \"3.12.0-rc.1\"\n           env:\n             TOXENV: asyncio\n-        - python-version: \"3.12.0-beta.4\"\n+        - python-version: \"3.12.0-rc.1\"\n           env:\n             TOXENV: extra-deps\n \ndiff --git a/tests/test_feedexport.py b/tests/test_feedexport.py\nindex 42fa25b1df8..6b82974fada 100644\n--- a/tests/test_feedexport.py\n+++ b/tests/test_feedexport.py\n@@ -1321,6 +1321,17 @@ def test_export_dicts(self):\n         yield self.assertExportedCsv(items, [\"foo\", \"egg\"], rows_csv)\n         yield self.assertExportedJsonLines(items, rows_jl)\n \n+    @defer.inlineCallbacks\n+    def test_export_tuple(self):\n+        items = [\n+            {\"foo\": \"bar1\", \"egg\": \"spam1\"},\n+            {\"foo\": \"bar2\", \"egg\": \"spam2\", \"baz\": \"quux\"},\n+        ]\n+\n+        settings = {\"FEED_EXPORT_FIELDS\": (\"foo\", \"baz\")}\n+        rows = [{\"foo\": \"bar1\", \"baz\": \"\"}, {\"foo\": \"bar2\", \"baz\": \"quux\"}]\n+        yield self.assertExported(items, [\"foo\", \"baz\"], rows, settings=settings)\n+\n     @defer.inlineCallbacks\n     def test_export_feed_export_fields(self):\n         # FEED_EXPORT_FIELDS option allows to order export fields\n",
    "problem_statement": "AssertionError   scrapy.settings in getdictorlist\n<!--\r\n\r\nThanks for taking an interest in Scrapy!\r\n\r\nIf you have a question that starts with \"How to...\", please see the Scrapy Community page: https://scrapy.org/community/.\r\nThe GitHub issue tracker's purpose is to deal with bug reports and feature requests for the project itself.\r\n\r\nKeep in mind that by filing an issue, you are expected to comply with Scrapy's Code of Conduct, including treating everyone with respect: https://github.com/scrapy/scrapy/blob/master/CODE_OF_CONDUCT.md\r\n\r\nThe following is a suggested template to structure your issue, you can find more guidelines at https://doc.scrapy.org/en/latest/contributing.html#reporting-bugs\r\n\r\n-->\r\n\r\n### Description\r\n\r\nAssertionError after upgrade to scrapy==2.10.0\r\n\r\n### Steps to Reproduce\r\n\r\n1. in settings.py:\r\n    ```python\r\n    FEED_EXPORT_FIELDS = tuple( ### if change to list - Ok\r\n      're_num idgood num code title price artikul valuta url_id url_rsp '\r\n      'is_auto_valuta code_nohash url_item'.split())\r\n    ```\r\n\r\n**Actual behavior:** [What actually happens]\r\n```\r\nTraceback (most recent call last):\r\n  File \"scrapy\", line 8, in <module>\r\n    sys.exit(execute())\r\n  File \"scrapy/cmdline.py\", line 161, in execute\r\n    _run_print_help(parser, _run_command, cmd, args, opts)\r\n  File \"scrapy/cmdline.py\", line 114, in _run_print_help\r\n    func(*a, **kw)\r\n  File \"scrapy/cmdline.py\", line 169, in _run_command\r\n    cmd.run(args, opts)\r\n  File \"scrapy/commands/crawl.py\", line 23, in run\r\n    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\r\n  File \"scrapy/crawler.py\", line 244, in crawl\r\n    crawler = self.create_crawler(crawler_or_spidercls)\r\n  File \"scrapy/crawler.py\", line 280, in create_crawler\r\n    return self._create_crawler(crawler_or_spidercls)\r\n  File \"scrapy/crawler.py\", line 363, in _create_crawler\r\n    return Crawler(spidercls, self.settings, init_reactor=init_reactor)\r\n  File \"scrapy/crawler.py\", line 119, in __init__\r\n    self.extensions: ExtensionManager = ExtensionManager.from_crawler(self)\r\n  File \"scrapy/middleware.py\", line 67, in from_crawler\r\n    return cls.from_settings(crawler.settings, crawler)\r\n  File \"scrapy/middleware.py\", line 44, in from_settings\r\n    mw = create_instance(mwcls, settings, crawler)\r\n  File \"scrapy/utils/misc.py\", line 188, in create_instance\r\n    instance = objcls.from_crawler(crawler, *args, **kwargs)\r\n  File \"scrapy/extensions/feedexport.py\", line 398, in from_crawler\r\n    exporter = cls(crawler)\r\n  File \"scrapy/extensions/feedexport.py\", line 436, in __init__\r\n    self.feeds[uri] = feed_complete_default_values_from_settings(\r\n  File \"scrapy/utils/conf.py\", line 136, in feed_complete_default_values_from_settings\r\n    out.setdefault(\"fields\", settings.getdictorlist(\"FEED_EXPORT_FIELDS\") or None)\r\n  File \"scrapy/settings/__init__.py\", line 274, in getdictorlist\r\n    assert isinstance(value, (dict, list))\r\nAssertionError\r\n```\r\n**Reproduces how often:** [What percentage of the time does it reproduce?]\r\n\r\n### Versions\r\n\r\n```\r\n$ scrapy version --verbose\r\nScrapy       : 2.10.0\r\nlxml         : 4.9.3.0\r\nlibxml2      : 2.10.3\r\ncssselect    : 1.2.0\r\nparsel       : 1.8.1\r\nw3lib        : 2.1.2\r\nTwisted      : 22.10.0\r\nPython       : 3.11.4 (main, Jun  8 2023, 09:53:06) [GCC 11.3.0]\r\npyOpenSSL    : 23.2.0 (OpenSSL 3.1.2 1 Aug 2023)\r\ncryptography : 41.0.3\r\nPlatform     : Linux-6.2.0-26-generic-x86_64-with-glibc2.35\r\n```\r\n\r\n### Additional context\r\n\r\nAny additional information, configuration, data or output from commands that might be necessary to reproduce or understand the issue. Please try not to include screenshots of code or the command line, paste the contents as text instead. You can use [GitHub Flavored Markdown](https://help.github.com/en/articles/creating-and-highlighting-code-blocks) to make the text look better.\r\n\n",
    "hints_text": "This doesn't seem to be a bug but intentional behavior, why would you need to pass a tuple object?\n@wRAR I tested locally and indeed `tuple` object on <2.10 works to filtering out fields, should we consider this a bug and put tuple object in assertion list?\nInitially `getlist()` was used to handle `FEED_EXPORT_FIELDS`, later `getdictorlist()` was added and used there (this isn't what was changed in 2.10.0, I'm explaining the behavior difference between the two functions). But `getlist()` uses `list()` while `getdictorlist()` doesn't. So both functions shouldn't return tuple but convert it to a list. It makes sense to check if `value` is not a dict and convert it to a list in that case.",
    "created_at": "2023-08-13T06:45:53Z",
    "version": "2.10",
    "PASS_TO_PASS": [],
    "FAIL_TO_PASS": [
      "tests/test_feedexport.py::FeedExportTest::test_export_tuple"
    ]
  },
  {
    "repo": "scrapy/scrapy",
    "pull_number": 5993,
    "instance_id": "scrapy__scrapy-5993",
    "issue_numbers": [
      "5726"
    ],
    "base_commit": "06ebdee35dc7ab5fa86c5076db341eae33485c37",
    "patch": "diff --git a/scrapy/utils/conf.py b/scrapy/utils/conf.py\nindex 43a8b65a5c7..1889f757190 100644\n--- a/scrapy/utils/conf.py\n+++ b/scrapy/utils/conf.py\n@@ -50,11 +50,17 @@ def _validate_values(compdict):\n                     \"please provide a real number or None instead\"\n                 )\n \n-    if isinstance(custom, (list, tuple)):\n-        _check_components(custom)\n-        return type(custom)(convert(c) for c in custom)\n-\n     if custom is not None:\n+        warnings.warn(\n+            \"The 'custom' attribute of build_component_list() is deprecated. \"\n+            \"Please merge its value into 'compdict' manually or change your \"\n+            \"code to use Settings.getwithbase().\",\n+            category=ScrapyDeprecationWarning,\n+            stacklevel=2,\n+        )\n+        if isinstance(custom, (list, tuple)):\n+            _check_components(custom)\n+            return type(custom)(convert(c) for c in custom)\n         compdict.update(custom)\n \n     _validate_values(compdict)\n",
    "test_patch": "diff --git a/tests/test_utils_conf.py b/tests/test_utils_conf.py\nindex 78ed9a7c9a7..dc3f01d574f 100644\n--- a/tests/test_utils_conf.py\n+++ b/tests/test_utils_conf.py\n@@ -1,6 +1,8 @@\n import unittest\n import warnings\n \n+import pytest\n+\n from scrapy.exceptions import ScrapyDeprecationWarning, UsageError\n from scrapy.settings import BaseSettings, Settings\n from scrapy.utils.conf import (\n@@ -21,44 +23,45 @@ def test_build_dict(self):\n     def test_backward_compatible_build_dict(self):\n         base = {\"one\": 1, \"two\": 2, \"three\": 3, \"five\": 5, \"six\": None}\n         custom = {\"two\": None, \"three\": 8, \"four\": 4}\n-        self.assertEqual(\n-            build_component_list(base, custom, convert=lambda x: x),\n-            [\"one\", \"four\", \"five\", \"three\"],\n-        )\n+        with pytest.warns(ScrapyDeprecationWarning, match=\"The 'custom' attribute\"):\n+            self.assertEqual(\n+                build_component_list(base, custom, convert=lambda x: x),\n+                [\"one\", \"four\", \"five\", \"three\"],\n+            )\n \n     def test_return_list(self):\n         custom = [\"a\", \"b\", \"c\"]\n-        self.assertEqual(\n-            build_component_list(None, custom, convert=lambda x: x), custom\n-        )\n+        with pytest.warns(ScrapyDeprecationWarning, match=\"The 'custom' attribute\"):\n+            self.assertEqual(\n+                build_component_list(None, custom, convert=lambda x: x), custom\n+            )\n \n     def test_map_dict(self):\n         custom = {\"one\": 1, \"two\": 2, \"three\": 3}\n-        self.assertEqual(\n-            build_component_list({}, custom, convert=lambda x: x.upper()),\n-            [\"ONE\", \"TWO\", \"THREE\"],\n-        )\n+        with pytest.warns(ScrapyDeprecationWarning, match=\"The 'custom' attribute\"):\n+            self.assertEqual(\n+                build_component_list({}, custom, convert=lambda x: x.upper()),\n+                [\"ONE\", \"TWO\", \"THREE\"],\n+            )\n \n     def test_map_list(self):\n         custom = [\"a\", \"b\", \"c\"]\n-        self.assertEqual(\n-            build_component_list(None, custom, lambda x: x.upper()), [\"A\", \"B\", \"C\"]\n-        )\n+        with pytest.warns(ScrapyDeprecationWarning, match=\"The 'custom' attribute\"):\n+            self.assertEqual(\n+                build_component_list(None, custom, lambda x: x.upper()), [\"A\", \"B\", \"C\"]\n+            )\n \n     def test_duplicate_components_in_dict(self):\n         duplicate_dict = {\"one\": 1, \"two\": 2, \"ONE\": 4}\n-        self.assertRaises(\n-            ValueError,\n-            build_component_list,\n-            {},\n-            duplicate_dict,\n-            convert=lambda x: x.lower(),\n-        )\n+        with self.assertRaises(ValueError):\n+            with pytest.warns(ScrapyDeprecationWarning, match=\"The 'custom' attribute\"):\n+                build_component_list({}, duplicate_dict, convert=lambda x: x.lower())\n \n     def test_duplicate_components_in_list(self):\n         duplicate_list = [\"a\", \"b\", \"a\"]\n         with self.assertRaises(ValueError) as cm:\n-            build_component_list(None, duplicate_list, convert=lambda x: x)\n+            with pytest.warns(ScrapyDeprecationWarning, match=\"The 'custom' attribute\"):\n+                build_component_list(None, duplicate_list, convert=lambda x: x)\n         self.assertIn(str(duplicate_list), str(cm.exception))\n \n     def test_duplicate_components_in_basesettings(self):\n@@ -76,9 +79,8 @@ def test_duplicate_components_in_basesettings(self):\n         )\n         # Same priority raises ValueError\n         duplicate_bs.set(\"ONE\", duplicate_bs[\"ONE\"], priority=20)\n-        self.assertRaises(\n-            ValueError, build_component_list, duplicate_bs, convert=lambda x: x.lower()\n-        )\n+        with self.assertRaises(ValueError):\n+            build_component_list(duplicate_bs, convert=lambda x: x.lower())\n \n     def test_valid_numbers(self):\n         # work well with None and numeric values\n@@ -92,15 +94,9 @@ def test_valid_numbers(self):\n         self.assertEqual(build_component_list(d, convert=lambda x: x), [\"b\", \"c\", \"a\"])\n         # raise exception for invalid values\n         d = {\"one\": \"5\"}\n-        self.assertRaises(ValueError, build_component_list, {}, d, convert=lambda x: x)\n-        d = {\"one\": \"1.0\"}\n-        self.assertRaises(ValueError, build_component_list, {}, d, convert=lambda x: x)\n-        d = {\"one\": [1, 2, 3]}\n-        self.assertRaises(ValueError, build_component_list, {}, d, convert=lambda x: x)\n-        d = {\"one\": {\"a\": \"a\", \"b\": 2}}\n-        self.assertRaises(ValueError, build_component_list, {}, d, convert=lambda x: x)\n-        d = {\"one\": \"lorem ipsum\"}\n-        self.assertRaises(ValueError, build_component_list, {}, d, convert=lambda x: x)\n+        with self.assertRaises(ValueError):\n+            with pytest.warns(ScrapyDeprecationWarning, match=\"The 'custom' attribute\"):\n+                build_component_list({}, d, convert=lambda x: x)\n \n \n class UtilsConfTestCase(unittest.TestCase):\n",
    "problem_statement": "Backward compatibility in utils.conf.build_component_list\nThere is some code from 2015 in `scrapy.utils.conf.build_component_list` marked as \"Backward compatibility for old (base, custom) call signature\", which was added in #1586. I couldn't understand after a quick glance why is it \"backward compatibility\" but if it's something deprecated we should deprecare it properly with a message, and if it's a properly supported code path we should remove the comments.\n",
    "hints_text": "Actually, as #1586 \"is a partial reversal of #1149\", the `(base, custom, convert=update_classpath)` signature is the oldest one, #1149 replaced it with the `(compdict, convert=update_classpath)` one and #1586 restored the support for both. So I think we should properly deprecate the old signature and later remove the support for it.\nI've tentatively added this to the next milestone so that we could deprecate this earlier and remove this earlier, but it shouldn't be a blocker.\nHi @wRAR, thank you for the information. In that case should we proceed to merge the linked PR's commit and then move ahead with the deprecation process?\nYeah, I think it's fine to merge it (but not close this issue) as the status of that code is now documented here.",
    "created_at": "2023-08-02T09:00:03Z",
    "version": "2.9",
    "PASS_TO_PASS": [],
    "FAIL_TO_PASS": [
      "tests/test_utils_conf.py::BuildComponentListTest::test_backward_compatible_build_dict",
      "tests/test_utils_conf.py::BuildComponentListTest::test_return_list",
      "tests/test_utils_conf.py::BuildComponentListTest::test_map_dict",
      "tests/test_utils_conf.py::BuildComponentListTest::test_map_list",
      "tests/test_utils_conf.py::BuildComponentListTest::test_duplicate_components_in_dict",
      "tests/test_utils_conf.py::BuildComponentListTest::test_duplicate_components_in_list",
      "tests/test_utils_conf.py::BuildComponentListTest::test_duplicate_components_in_basesettings",
      "tests/test_utils_conf.py::BuildComponentListTest::test_valid_numbers"
    ]
  },
  {
    "repo": "scrapy/scrapy",
    "pull_number": 5952,
    "instance_id": "scrapy__scrapy-5952",
    "issue_numbers": [
      "3090",
      "3111"
    ],
    "base_commit": "58300e066fc330892fb7ccbeceef6d8ac4efc4dc",
    "patch": "diff --git a/scrapy/exporters.py b/scrapy/exporters.py\nindex 4538c9ee10f..8254ea63ef1 100644\n--- a/scrapy/exporters.py\n+++ b/scrapy/exporters.py\n@@ -133,6 +133,13 @@ def _beautify_newline(self):\n         if self.indent is not None:\n             self.file.write(b\"\\n\")\n \n+    def _add_comma_after_first(self):\n+        if self.first_item:\n+            self.first_item = False\n+        else:\n+            self.file.write(b\",\")\n+            self._beautify_newline()\n+\n     def start_exporting(self):\n         self.file.write(b\"[\")\n         self._beautify_newline()\n@@ -142,14 +149,10 @@ def finish_exporting(self):\n         self.file.write(b\"]\")\n \n     def export_item(self, item):\n-        if self.first_item:\n-            self.first_item = False\n-        else:\n-            self.file.write(b\",\")\n-            self._beautify_newline()\n         itemdict = dict(self._get_serialized_fields(item))\n-        data = self.encoder.encode(itemdict)\n-        self.file.write(to_bytes(data, self.encoding))\n+        data = to_bytes(self.encoder.encode(itemdict), self.encoding)\n+        self._add_comma_after_first()\n+        self.file.write(data)\n \n \n class XmlItemExporter(BaseItemExporter):\n",
    "test_patch": "diff --git a/tests/test_exporters.py b/tests/test_exporters.py\nindex 63bebcf7a26..cb24ddd8ecf 100644\n--- a/tests/test_exporters.py\n+++ b/tests/test_exporters.py\n@@ -599,6 +599,20 @@ def test_two_items(self):\n     def test_two_dict_items(self):\n         self.assertTwoItemsExported(ItemAdapter(self.i).asdict())\n \n+    def test_two_items_with_failure_between(self):\n+        i1 = TestItem(name=\"Joseph\\xa3\", age=\"22\")\n+        i2 = TestItem(\n+            name=\"Maria\", age=1j\n+        )  # Invalid datetimes didn't consistently fail between Python versions\n+        i3 = TestItem(name=\"Jesus\", age=\"44\")\n+        self.ie.start_exporting()\n+        self.ie.export_item(i1)\n+        self.assertRaises(TypeError, self.ie.export_item, i2)\n+        self.ie.export_item(i3)\n+        self.ie.finish_exporting()\n+        exported = json.loads(to_unicode(self.output.getvalue()))\n+        self.assertEqual(exported, [dict(i1), dict(i3)])\n+\n     def test_nested_item(self):\n         i1 = self.item_class(name=\"Joseph\\xa3\", age=\"22\")\n         i2 = self.item_class(name=\"Maria\", age=i1)\n@@ -637,6 +651,24 @@ def test_nonstring_types_item(self):\n         self.assertEqual(exported, [item])\n \n \n+class JsonItemExporterToBytesTest(BaseItemExporterTest):\n+    def _get_exporter(self, **kwargs):\n+        kwargs[\"encoding\"] = \"latin\"\n+        return JsonItemExporter(self.output, **kwargs)\n+\n+    def test_two_items_with_failure_between(self):\n+        i1 = TestItem(name=\"Joseph\", age=\"22\")\n+        i2 = TestItem(name=\"\\u263a\", age=\"11\")\n+        i3 = TestItem(name=\"Jesus\", age=\"44\")\n+        self.ie.start_exporting()\n+        self.ie.export_item(i1)\n+        self.assertRaises(UnicodeEncodeError, self.ie.export_item, i2)\n+        self.ie.export_item(i3)\n+        self.ie.finish_exporting()\n+        exported = json.loads(to_unicode(self.output.getvalue(), encoding=\"latin\"))\n+        self.assertEqual(exported, [dict(i1), dict(i3)])\n+\n+\n class JsonItemExporterDataclassTest(JsonItemExporterTest):\n     item_class = TestDataClass\n     custom_field_item_class = CustomFieldDataclass\n",
    "problem_statement": "JsonItemExporter puts lone comma in the output if encoder fails\nIf `JsonItemExporter` is unable to encode the item, it still writes a delimiter (comma) to the output file. Here is a sample spider:\r\n\r\n```\r\n# -*- coding: utf-8 -*-\r\nimport datetime\r\nimport scrapy\r\n\r\nclass DummySpider(scrapy.Spider):\r\n    name = 'dummy'\r\n    start_urls = ['http://example.org/']\r\n\r\n    def parse(self, response):\r\n        yield {'date': datetime.date(2018, 1, 1)}\r\n        yield {'date': datetime.date(1234, 1, 1)}\r\n        yield {'date': datetime.date(2019, 1, 1)})\r\n```\r\n\r\nEncoding the second items fails:\r\n\r\n```\r\n2018-01-25 09:05:57 [scrapy.utils.signal] ERROR: Error caught on signal handler: <bound method ?.item_scraped of <scrapy.extensions.feedexport.FeedExporter object at 0x7fcbfbd81250>>\r\nTraceback (most recent call last):\r\n  File \"/home/pasmen/SW/Python/data-sci-env/local/lib/python2.7/site-packages/twisted/internet/defer.py\", line 150, in maybeDeferred\r\n    result = f(*args, **kw)\r\n  File \"/home/pasmen/SW/Python/data-sci-env/local/lib/python2.7/site-packages/pydispatch/robustapply.py\", line 55, in robustApply\r\n    return receiver(*arguments, **named)\r\n  File \"/home/pasmen/SW/Python/data-sci-env/local/lib/python2.7/site-packages/scrapy/extensions/feedexport.py\", line 224, in item_scraped\r\n    slot.exporter.export_item(item)\r\n  File \"/home/pasmen/SW/Python/data-sci-env/local/lib/python2.7/site-packages/scrapy/exporters.py\", line 130, in export_item\r\n    data = self.encoder.encode(itemdict)\r\n  File \"/usr/lib/python2.7/json/encoder.py\", line 207, in encode\r\n    chunks = self.iterencode(o, _one_shot=True)\r\n  File \"/usr/lib/python2.7/json/encoder.py\", line 270, in iterencode\r\n    return _iterencode(o, 0)\r\n  File \"/home/pasmen/SW/Python/data-sci-env/local/lib/python2.7/site-packages/scrapy/utils/serialize.py\", line 22, in default\r\n    return o.strftime(self.DATE_FORMAT)\r\nValueError: year=1234 is before 1900; the datetime strftime() methods require year >= 1900\r\n```\r\n\r\nThe output looks like this:\r\n\r\n```\r\n[\r\n{\"date\": \"2018-01-01\"},\r\n,\r\n{\"date\": \"2019-01-01\"}\r\n]\r\n```\r\n\r\nThis seems not to be a valid JSON file as e.g. `json.load()` and `jq` fail to parse it.\r\n\r\nI think the problem is in [`export_item`](https://github.com/scrapy/scrapy/blob/108f8c4fd20a47bb94e010e9c6296f7ed9fdb2bd/scrapy/exporters.py#L123) method of `JsonItemExporter` class where it outputs the comma before decoding the item. The correct approach would be to try to decode the item (possibly with other needed operations) and perform the write atomically.\nPartial fix for #3090 - only addresses JSON feeds.\nQuick fix for #3090, but the general problem of failure-during-multistep-serialisation also probably affects our XML exporter. A more broad fix would be to buffer changes and then \"commit\" them once everything has succeeded, but that's a bigger change than I felt comfortable writing without some discussion. :)\n",
    "hints_text": "Thanks @tlinhart - will take a look at making a fix to that right away. :)\n",
    "created_at": "2023-06-15T07:06:58Z",
    "version": "2.9",
    "PASS_TO_PASS": [],
    "FAIL_TO_PASS": [
      "tests/test_exporters.py::JsonItemExporterTest::test_two_items_with_failure_between",
      "tests/test_exporters.py::JsonItemExporterToBytesTest::test_two_items_with_failure_between"
    ]
  },
  {
    "repo": "scrapy/scrapy",
    "pull_number": 5929,
    "instance_id": "scrapy__scrapy-5929",
    "issue_numbers": [
      "5929"
    ],
    "base_commit": "52c072640aa61884de05214cb1bdda07c2a87bef",
    "patch": "diff --git a/docs/topics/downloader-middleware.rst b/docs/topics/downloader-middleware.rst\nindex 7665a901a7e..a8e5b23bf92 100644\n--- a/docs/topics/downloader-middleware.rst\n+++ b/docs/topics/downloader-middleware.rst\n@@ -915,6 +915,7 @@ settings (see the settings documentation for more info):\n * :setting:`RETRY_ENABLED`\n * :setting:`RETRY_TIMES`\n * :setting:`RETRY_HTTP_CODES`\n+* :setting:`RETRY_EXCEPTIONS`\n \n .. reqmeta:: dont_retry\n \n@@ -966,6 +967,37 @@ In some cases you may want to add 400 to :setting:`RETRY_HTTP_CODES` because\n it is a common code used to indicate server overload. It is not included by\n default because HTTP specs say so.\n \n+.. setting:: RETRY_EXCEPTIONS\n+\n+RETRY_EXCEPTIONS\n+^^^^^^^^^^^^^^^^\n+\n+Default::\n+\n+    [\n+        'twisted.internet.defer.TimeoutError',\n+        'twisted.internet.error.TimeoutError',\n+        'twisted.internet.error.DNSLookupError',\n+        'twisted.internet.error.ConnectionRefusedError',\n+        'twisted.internet.error.ConnectionDone',\n+        'twisted.internet.error.ConnectError',\n+        'twisted.internet.error.ConnectionLost',\n+        'twisted.internet.error.TCPTimedOutError',\n+        'twisted.web.client.ResponseFailed',\n+        IOError,\n+        'scrapy.core.downloader.handlers.http11.TunnelError',\n+    ]\n+\n+List of exceptions to retry.\n+\n+Each list entry may be an exception type or its import path as a string.\n+\n+An exception will not be caught when the exception type is not in\n+:setting:`RETRY_EXCEPTIONS` or when the maximum number of retries for a request\n+has been exceeded (see :setting:`RETRY_TIMES`). To learn about uncaught\n+exception propagation, see\n+:meth:`~scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception`.\n+\n .. setting:: RETRY_PRIORITY_ADJUST\n \n RETRY_PRIORITY_ADJUST\ndiff --git a/scrapy/downloadermiddlewares/retry.py b/scrapy/downloadermiddlewares/retry.py\nindex 081642a4b81..50cbc3111a1 100644\n--- a/scrapy/downloadermiddlewares/retry.py\n+++ b/scrapy/downloadermiddlewares/retry.py\n@@ -9,31 +9,36 @@\n Failed pages are collected on the scraping process and rescheduled at the end,\n once the spider has finished crawling all regular (non failed) pages.\n \"\"\"\n+import warnings\n from logging import Logger, getLogger\n from typing import Optional, Union\n \n-from twisted.internet import defer\n-from twisted.internet.error import (\n-    ConnectError,\n-    ConnectionDone,\n-    ConnectionLost,\n-    ConnectionRefusedError,\n-    DNSLookupError,\n-    TCPTimedOutError,\n-    TimeoutError,\n-)\n-from twisted.web.client import ResponseFailed\n-\n-from scrapy.core.downloader.handlers.http11 import TunnelError\n-from scrapy.exceptions import NotConfigured\n+from scrapy.exceptions import NotConfigured, ScrapyDeprecationWarning\n from scrapy.http.request import Request\n+from scrapy.settings import Settings\n from scrapy.spiders import Spider\n+from scrapy.utils.misc import load_object\n from scrapy.utils.python import global_object_name\n from scrapy.utils.response import response_status_message\n \n retry_logger = getLogger(__name__)\n \n \n+class BackwardsCompatibilityMetaclass(type):\n+    @property\n+    def EXCEPTIONS_TO_RETRY(cls):\n+        warnings.warn(\n+            \"Attribute RetryMiddleware.EXCEPTIONS_TO_RETRY is deprecated. \"\n+            \"Use the RETRY_EXCEPTIONS setting instead.\",\n+            ScrapyDeprecationWarning,\n+            stacklevel=2,\n+        )\n+        return tuple(\n+            load_object(x) if isinstance(x, str) else x\n+            for x in Settings().getlist(\"RETRY_EXCEPTIONS\")\n+        )\n+\n+\n def get_retry_request(\n     request: Request,\n     *,\n@@ -121,23 +126,7 @@ def parse(self, response):\n     return None\n \n \n-class RetryMiddleware:\n-    # IOError is raised by the HttpCompression middleware when trying to\n-    # decompress an empty response\n-    EXCEPTIONS_TO_RETRY = (\n-        defer.TimeoutError,\n-        TimeoutError,\n-        DNSLookupError,\n-        ConnectionRefusedError,\n-        ConnectionDone,\n-        ConnectError,\n-        ConnectionLost,\n-        TCPTimedOutError,\n-        ResponseFailed,\n-        IOError,\n-        TunnelError,\n-    )\n-\n+class RetryMiddleware(metaclass=BackwardsCompatibilityMetaclass):\n     def __init__(self, settings):\n         if not settings.getbool(\"RETRY_ENABLED\"):\n             raise NotConfigured\n@@ -147,6 +136,16 @@ def __init__(self, settings):\n         )\n         self.priority_adjust = settings.getint(\"RETRY_PRIORITY_ADJUST\")\n \n+        if not hasattr(\n+            self, \"EXCEPTIONS_TO_RETRY\"\n+        ):  # If EXCEPTIONS_TO_RETRY is not \"overriden\"\n+            self.exceptions_to_retry = tuple(\n+                load_object(x) if isinstance(x, str) else x\n+                for x in settings.getlist(\"RETRY_EXCEPTIONS\")\n+            )\n+        else:\n+            self.exceptions_to_retry = self.EXCEPTIONS_TO_RETRY\n+\n     @classmethod\n     def from_crawler(cls, crawler):\n         return cls(crawler.settings)\n@@ -160,7 +159,7 @@ def process_response(self, request, response, spider):\n         return response\n \n     def process_exception(self, request, exception, spider):\n-        if isinstance(exception, self.EXCEPTIONS_TO_RETRY) and not request.meta.get(\n+        if isinstance(exception, self.exceptions_to_retry) and not request.meta.get(\n             \"dont_retry\", False\n         ):\n             return self._retry(request, exception, spider)\ndiff --git a/scrapy/settings/default_settings.py b/scrapy/settings/default_settings.py\nindex 260ec1701c7..89837b4abf3 100644\n--- a/scrapy/settings/default_settings.py\n+++ b/scrapy/settings/default_settings.py\n@@ -258,6 +258,21 @@\n RETRY_TIMES = 2  # initial response + 2 retries = 3 requests\n RETRY_HTTP_CODES = [500, 502, 503, 504, 522, 524, 408, 429]\n RETRY_PRIORITY_ADJUST = -1\n+RETRY_EXCEPTIONS = [\n+    \"twisted.internet.defer.TimeoutError\",\n+    \"twisted.internet.error.TimeoutError\",\n+    \"twisted.internet.error.DNSLookupError\",\n+    \"twisted.internet.error.ConnectionRefusedError\",\n+    \"twisted.internet.error.ConnectionDone\",\n+    \"twisted.internet.error.ConnectError\",\n+    \"twisted.internet.error.ConnectionLost\",\n+    \"twisted.internet.error.TCPTimedOutError\",\n+    \"twisted.web.client.ResponseFailed\",\n+    # IOError is raised by the HttpCompression middleware when trying to\n+    # decompress an empty response\n+    IOError,\n+    \"scrapy.core.downloader.handlers.http11.TunnelError\",\n+]\n \n ROBOTSTXT_OBEY = False\n ROBOTSTXT_PARSER = \"scrapy.robotstxt.ProtegoRobotParser\"\n",
    "test_patch": "diff --git a/tests/test_downloadermiddleware_retry.py b/tests/test_downloadermiddleware_retry.py\nindex 63bd618489b..97ae1e29a27 100644\n--- a/tests/test_downloadermiddleware_retry.py\n+++ b/tests/test_downloadermiddleware_retry.py\n@@ -1,5 +1,6 @@\n import logging\n import unittest\n+import warnings\n \n from testfixtures import LogCapture\n from twisted.internet import defer\n@@ -15,6 +16,7 @@\n from scrapy.downloadermiddlewares.retry import RetryMiddleware, get_retry_request\n from scrapy.exceptions import IgnoreRequest\n from scrapy.http import Request, Response\n+from scrapy.settings.default_settings import RETRY_EXCEPTIONS\n from scrapy.spiders import Spider\n from scrapy.utils.test import get_crawler\n \n@@ -110,19 +112,48 @@ def test_twistederrors(self):\n             == 2\n         )\n \n-    def _test_retry_exception(self, req, exception):\n+    def test_exception_to_retry_added(self):\n+        exc = ValueError\n+        settings_dict = {\n+            \"RETRY_EXCEPTIONS\": list(RETRY_EXCEPTIONS) + [exc],\n+        }\n+        crawler = get_crawler(Spider, settings_dict=settings_dict)\n+        mw = RetryMiddleware.from_crawler(crawler)\n+        req = Request(f\"http://www.scrapytest.org/{exc.__name__}\")\n+        self._test_retry_exception(req, exc(\"foo\"), mw)\n+\n+    def test_exception_to_retry_customMiddleware(self):\n+        exc = ValueError\n+\n+        with warnings.catch_warnings(record=True) as warns:\n+\n+            class MyRetryMiddleware(RetryMiddleware):\n+                EXCEPTIONS_TO_RETRY = RetryMiddleware.EXCEPTIONS_TO_RETRY + (exc,)\n+\n+            self.assertEqual(len(warns), 1)\n+\n+        mw2 = MyRetryMiddleware.from_crawler(self.crawler)\n+        req = Request(f\"http://www.scrapytest.org/{exc.__name__}\")\n+        req = mw2.process_exception(req, exc(\"foo\"), self.spider)\n+        assert isinstance(req, Request)\n+        self.assertEqual(req.meta[\"retry_times\"], 1)\n+\n+    def _test_retry_exception(self, req, exception, mw=None):\n+        if mw is None:\n+            mw = self.mw\n+\n         # first retry\n-        req = self.mw.process_exception(req, exception, self.spider)\n+        req = mw.process_exception(req, exception, self.spider)\n         assert isinstance(req, Request)\n         self.assertEqual(req.meta[\"retry_times\"], 1)\n \n         # second retry\n-        req = self.mw.process_exception(req, exception, self.spider)\n+        req = mw.process_exception(req, exception, self.spider)\n         assert isinstance(req, Request)\n         self.assertEqual(req.meta[\"retry_times\"], 2)\n \n         # discard it\n-        req = self.mw.process_exception(req, exception, self.spider)\n+        req = mw.process_exception(req, exception, self.spider)\n         self.assertEqual(req, None)\n \n \n",
    "problem_statement": "Configurable exception list retry middleware 2701\nResolves https://github.com/scrapy/scrapy/issues/2701, closes https://github.com/scrapy/scrapy/pull/2984, closes https://github.com/scrapy/scrapy/pull/3334.\n",
    "hints_text": "",
    "created_at": "2023-05-09T15:17:38Z",
    "version": "2.9",
    "PASS_TO_PASS": [],
    "FAIL_TO_PASS": [
      "tests/test_downloadermiddleware_retry.py::RetryTest::test_exception_to_retry_added",
      "tests/test_downloadermiddleware_retry.py::RetryTest::test_exception_to_retry_customMiddleware",
      "tests/test_downloadermiddleware_retry.py::RetryTest::test_twistederrors"
    ]
  },
  {
    "repo": "scrapy/scrapy",
    "pull_number": 5917,
    "instance_id": "scrapy__scrapy-5917",
    "issue_numbers": [
      "5914"
    ],
    "base_commit": "b50c032ee9a75d1c9b42f1126637fdc655b141a8",
    "patch": "diff --git a/scrapy/http/response/text.py b/scrapy/http/response/text.py\nindex 73bb811dedb..d580a7876ee 100644\n--- a/scrapy/http/response/text.py\n+++ b/scrapy/http/response/text.py\n@@ -100,11 +100,13 @@ def urljoin(self, url):\n     @memoizemethod_noargs\n     def _headers_encoding(self):\n         content_type = self.headers.get(b\"Content-Type\", b\"\")\n-        return http_content_type_encoding(to_unicode(content_type))\n+        return http_content_type_encoding(to_unicode(content_type, encoding=\"latin-1\"))\n \n     def _body_inferred_encoding(self):\n         if self._cached_benc is None:\n-            content_type = to_unicode(self.headers.get(b\"Content-Type\", b\"\"))\n+            content_type = to_unicode(\n+                self.headers.get(b\"Content-Type\", b\"\"), encoding=\"latin-1\"\n+            )\n             benc, ubody = html_to_unicode(\n                 content_type,\n                 self.body,\ndiff --git a/scrapy/responsetypes.py b/scrapy/responsetypes.py\nindex f01e9096ccd..58884f21a13 100644\n--- a/scrapy/responsetypes.py\n+++ b/scrapy/responsetypes.py\n@@ -51,7 +51,9 @@ def from_content_type(self, content_type, content_encoding=None):\n         header\"\"\"\n         if content_encoding:\n             return Response\n-        mimetype = to_unicode(content_type).split(\";\")[0].strip().lower()\n+        mimetype = (\n+            to_unicode(content_type, encoding=\"latin-1\").split(\";\")[0].strip().lower()\n+        )\n         return self.from_mimetype(mimetype)\n \n     def from_content_disposition(self, content_disposition):\n",
    "test_patch": "diff --git a/tests/test_http_response.py b/tests/test_http_response.py\nindex dbc9f1feff9..a05b702aa71 100644\n--- a/tests/test_http_response.py\n+++ b/tests/test_http_response.py\n@@ -448,6 +448,13 @@ def test_encoding(self):\n             body=codecs.BOM_UTF8 + b\"\\xc2\\xa3\",\n             headers={\"Content-type\": [\"text/html; charset=cp1251\"]},\n         )\n+        r9 = self.response_class(\n+            \"http://www.example.com\",\n+            body=b\"\\x80\",\n+            headers={\n+                \"Content-type\": [b\"application/x-download; filename=\\x80dummy.txt\"]\n+            },\n+        )\n \n         self.assertEqual(r1._headers_encoding(), \"utf-8\")\n         self.assertEqual(r2._headers_encoding(), None)\n@@ -458,9 +465,12 @@ def test_encoding(self):\n         self.assertEqual(r4._headers_encoding(), None)\n         self.assertEqual(r5._headers_encoding(), None)\n         self.assertEqual(r8._headers_encoding(), \"cp1251\")\n+        self.assertEqual(r9._headers_encoding(), None)\n         self.assertEqual(r8._declared_encoding(), \"utf-8\")\n+        self.assertEqual(r9._declared_encoding(), None)\n         self._assert_response_encoding(r5, \"utf-8\")\n         self._assert_response_encoding(r8, \"utf-8\")\n+        self._assert_response_encoding(r9, \"cp1252\")\n         assert (\n             r4._body_inferred_encoding() is not None\n             and r4._body_inferred_encoding() != \"ascii\"\n@@ -470,6 +480,7 @@ def test_encoding(self):\n         self._assert_response_values(r3, \"iso-8859-1\", \"\\xa3\")\n         self._assert_response_values(r6, \"gb18030\", \"\\u2015\")\n         self._assert_response_values(r7, \"gb18030\", \"\\u2015\")\n+        self._assert_response_values(r9, \"cp1252\", \"\u20ac\")\n \n         # TextResponse (and subclasses) must be passed a encoding when instantiating with unicode bodies\n         self.assertRaises(\ndiff --git a/tests/test_responsetypes.py b/tests/test_responsetypes.py\nindex 85996051830..6e1ed82f0c2 100644\n--- a/tests/test_responsetypes.py\n+++ b/tests/test_responsetypes.py\n@@ -42,6 +42,7 @@ def test_from_content_type(self):\n             (\"application/octet-stream\", Response),\n             (\"application/x-json; encoding=UTF8;charset=UTF-8\", TextResponse),\n             (\"application/json-amazonui-streaming;charset=UTF-8\", TextResponse),\n+            (b\"application/x-download; filename=\\x80dummy.txt\", Response),\n         ]\n         for source, cls in mappings:\n             retcls = responsetypes.from_content_type(source)\n",
    "problem_statement": "Exception with non-UTF-8 Content-Type\n<!--\r\n\r\nThanks for taking an interest in Scrapy!\r\n\r\nIf you have a question that starts with \"How to...\", please see the Scrapy Community page: https://scrapy.org/community/.\r\nThe GitHub issue tracker's purpose is to deal with bug reports and feature requests for the project itself.\r\n\r\nKeep in mind that by filing an issue, you are expected to comply with Scrapy's Code of Conduct, including treating everyone with respect: https://github.com/scrapy/scrapy/blob/master/CODE_OF_CONDUCT.md\r\n\r\nThe following is a suggested template to structure your issue, you can find more guidelines at https://doc.scrapy.org/en/latest/contributing.html#reporting-bugs\r\n\r\n-->\r\n\r\n### Description\r\n\r\nI am trying to download a document from the link http://pravo.gov.ru/proxy/ips/?savertf=&link_id=0&nd=128284801&intelsearch=&firstDoc=1&page=all\r\nEverything works fine in the browser, but when I try to automate this process through scrapy, everything break down.\r\n### Steps to Reproduce\r\n\r\n1.Create new spider `scrapy genspider test pravo.gov.ru`\r\n2. Paste code\r\n```\r\nimport scrapy\r\n\r\n\r\nclass TestSpider(scrapy.Spider):\r\n    name = \"test\"\r\n    allowed_domains = [\"pravo.gov.ru\"]\r\n    start_urls = [\"http://pravo.gov.ru/proxy/ips/\"]\r\n\r\n    def parse(self, response):\r\n        yield scrapy.Request(\r\n            self.start_urls[0] + f'?savertf=&link_id=0&nd=128284801&intelsearch=&firstDoc=1&page=all',\r\n            callback=self.test_parse,\r\n            encoding='cp1251') #The same behavior without this line\r\n           \r\n\r\n    def test_parse(self, response):\r\n        pass\r\n```\r\n\r\n4. run\r\n***OR***\r\nrun `scrapy fetch http://pravo.gov.ru/proxy/ips/?savertf=&link_id=0&nd=128284801&intelsearch=&firstDoc=1&page=all\"\"`\r\n**Expected behavior:** The document on the link is downloaded\r\n**Actual behavior:** \r\n2023-04-28 00:07:35 [scrapy.core.scraper] ERROR: Error downloading <GET http://pravo.gov.ru/proxy/ips/?savertf=&link_id=0&nd=128284801&intelsearch=&fir\r\nstDoc=1&page=all>\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python310\\site-packages\\twisted\\internet\\defer.py\", line 1693, in _inlineCallbacks\r\n    result = context.run(\r\n  File \"C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python310\\site-packages\\twisted\\python\\failure.py\", line 518, in throwExceptionIntoGenerator\r\n    return g.throw(self.type, self.value, self.tb)\r\n  File \"C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python310\\site-packages\\scrapy\\core\\downloader\\middleware.py\", line 52, in process_request\r\n    return (yield download_func(request=request, spider=spider))\r\n  File \"C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python310\\site-packages\\twisted\\internet\\defer.py\", line 892, in _runCallbacks\r\n    current.result = callback(  # type: ignore[misc]\r\n  File \"C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python310\\site-packages\\scrapy\\core\\downloader\\handlers\\http11.py\", line 501, in _cb_bodydone\r\n    respcls = responsetypes.from_args(headers=headers, url=url, body=result[\"body\"])\r\n  File \"C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python310\\site-packages\\scrapy\\responsetypes.py\", line 113, in from_args\r\n    cls = self.from_headers(headers)\r\n  File \"C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python310\\site-packages\\scrapy\\responsetypes.py\", line 75, in from_headers\r\n    cls = self.from_content_type(\r\n  File \"C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python310\\site-packages\\scrapy\\responsetypes.py\", line 55, in from_content_type\r\n    mimetype = to_unicode(content_type).split(\";\")[0].strip().lower()\r\n  File \"C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python310\\site-packages\\scrapy\\utils\\python.py\", line 97, in to_unicode\r\n    return text.decode(encoding, errors)\r\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0xcf in position 33: invalid continuation byte\r\n\r\n**Reproduces how often:** 100% runs\r\n\r\n### Versions\r\n\r\nScrapy       : 2.8.0\r\nlxml         : 4.9.2.0\r\nlibxml2      : 2.9.12\r\ncssselect    : 1.2.0\r\nparsel       : 1.8.1\r\nw3lib        : 2.1.1\r\nTwisted      : 22.10.0\r\nPython       : 3.10.9 | packaged by Anaconda, Inc. | (main, Mar  1 2023, 18:18:15) [MSC v.1916 64 bit (AMD64)]\r\npyOpenSSL    : 23.1.1 (OpenSSL 3.1.0 14 Mar 2023)\r\ncryptography : 40.0.2\r\nPlatform     : Windows-10-10.0.19044-SP0\r\n### Additional context\r\n\r\nThese documents are encoded in cp1251 encoding, which is clearly indicated in their headers :  \r\nContent-Transfer-Encoding: quoted-printable\r\nContent-Type: text/html; charset=\"windows-1251\"\r\nThe same behavior when trying to save a file using FilesPipeline\n",
    "hints_text": "> which is clearly indicated in their headers\r\n\r\nI can't confirm this. E.g. this is from curl:\r\n\r\n```\r\n< HTTP/1.1 200 OK\r\n< Server: nginx\r\n< Date: Fri, 28 Apr 2023 07:50:55 GMT\r\n< Content-Type: application/x-download; filename=\ufffd-593-24_04_2023.rtf\r\n< Content-Length: 15961\r\n< Connection: keep-alive\r\n< Content-Disposition: attachment; filename=\ufffd-593-24_04_2023.rtf\r\n```\r\n\r\nI see the same in a browser.\n> > which is clearly indicated in their headers\r\n> \r\n> I can't confirm this. E.g. this is from curl:\r\n> \r\n> ```\r\n> < HTTP/1.1 200 OK\r\n> < Server: nginx\r\n> < Date: Fri, 28 Apr 2023 07:50:55 GMT\r\n> < Content-Type: application/x-download; filename=\ufffd-593-24_04_2023.rtf\r\n> < Content-Length: 15961\r\n> < Connection: keep-alive\r\n> < Content-Disposition: attachment; filename=\ufffd-593-24_04_2023.rtf\r\n> ```\r\n> \r\n> I see the same in a browser.\r\n\r\nI wrote about the headers of the file itself:\r\n```\r\n------=_NextPart_01CAD650.0093E2A0\r\nContent-Location: file:///C:/B1334631/001.htm\r\nContent-Transfer-Encoding: quoted-printable\r\nContent-Type: text/html; charset=\"windows-1251\"\r\n```\r\nIf I use the requests library in python, then the response from the server comes without any errors.\r\n```\r\nimport requests\r\nresponse = requests.get('http://pravo.gov.ru/proxy/ips/?savertf=&link_id=5&nd=128284801&&page=all')\r\nprint(response.content.decode('cp1251'))\r\nprint(response.headers)\r\n```\r\n`MIME-Version: 1.0\r\nContent-Type: multipart/related; boundary=\"----=_NextPart_01CAD650.0093E2A0\"\r\n..................................................................................\r\n`\r\n`\r\n{'Server': 'nginx', 'Date': 'Fri, 28 Apr 2023 08:51:06 GMT', 'Content-Type': 'application/x-download; filename=\u00cf-593-24_04_2023.rtf', 'Content-Length': '15961', 'Connection': 'keep-alive', 'Content-Disposition': 'attachment; filename=\u00cf-593-24_04_2023.rtf'}\r\n`\r\nI don't understand why exactly an encoding error occurs when trying to make a scrapy request \r\n`scrapy.Request('http://pravo.gov.ru/proxy/ips/?savertf=&link_id=5&nd=128284801&&page=all')\r\n`\n> I wrote about the headers of the file itself:\r\n\r\nI don't think these matter to any of the libraries you mentioned. Even your `requests` sample prints them as a part of the response body.\r\n\r\n> If I use the requests library in python, then the response from the server comes without any errors.\r\n\r\nSure, it doesn't try to decode the response, unlike Scrapy.\r\n\r\nThe _actual_ problem is that the `Content-Type` header value is in CP1251 (I guess?): the actual exception happens when trying to get the response MIME type and the code assumes it's in UTF-8 while it's actually `b'application/x-download; filename=\\xcf-593-24_04_2023.rtf'`. We can specify the latin1 encoding instead of the implicit utf-8 when converting the `Content-Type` header value to `str` (`scrapy.http.response.text.TextResponse._headers_encoding()`, `scrapy.http.response.text.TextResponse._body_inferred_encoding()`, `scrapy.responsetypes.ResponseTypes.from_content_type()`) and this will prevent the exceptions in this case, I don't think we can do anything better. I also don't think we need the file name from this header value (or from the `Content-Disposition` header value which has the same problem) as we don't support `Content-Disposition: attachment`, and return the raw response body directly so we don't need to guess the encoding for unquoted non-ASCII file names.\nI use this code to download documents from this site:\r\n```\r\nyield scrapy.Request(\r\n          self.start_urls[0] + f'?savertf=&link_id=5&nd=128284801&&page=all',\r\n          callback=self.test_parse,\r\n          meta={'download_file': True}\r\n      )\r\n\r\n\r\n```\r\n```\r\nclass FileDownloaderMiddleware:\r\n    def process_request(self, request, spider):\r\n        if request.meta.get('download_file'):\r\n            response = requests.get(request.url)\r\n            response.raise_for_status()\r\n            return scrapy.http.Response(\r\n                url=request.url,\r\n                body=response.content,\r\n                headers=response.headers,\r\n            )\r\n        return None\r\n\r\n```",
    "created_at": "2023-05-02T15:56:07Z",
    "version": "2.8",
    "PASS_TO_PASS": [],
    "FAIL_TO_PASS": [
      "tests/test_http_response.py::TextResponseTest::test_encoding",
      "tests/test_responsetypes.py::ResponseTypesTest::test_from_content_type"
    ]
  },
  {
    "repo": "scrapy/scrapy",
    "pull_number": 5885,
    "instance_id": "scrapy__scrapy-5885",
    "issue_numbers": [
      "5872"
    ],
    "base_commit": "96033ce5a7f857942e3c6d488c8aab5b4aa03295",
    "patch": "diff --git a/scrapy/utils/python.py b/scrapy/utils/python.py\nindex fc50e0f1240..818fa5d6b21 100644\n--- a/scrapy/utils/python.py\n+++ b/scrapy/utils/python.py\n@@ -174,33 +174,33 @@ def binary_is_text(data):\n \n \n def get_func_args(func, stripself=False):\n-    \"\"\"Return the argument name list of a callable\"\"\"\n-    if inspect.isfunction(func):\n-        spec = inspect.getfullargspec(func)\n-        func_args = spec.args + spec.kwonlyargs\n-    elif inspect.isclass(func):\n-        return get_func_args(func.__init__, True)\n-    elif inspect.ismethod(func):\n-        return get_func_args(func.__func__, True)\n-    elif inspect.ismethoddescriptor(func):\n-        return []\n-    elif isinstance(func, partial):\n-        return [\n-            x\n-            for x in get_func_args(func.func)[len(func.args) :]\n-            if not (func.keywords and x in func.keywords)\n-        ]\n-    elif hasattr(func, \"__call__\"):\n-        if inspect.isroutine(func):\n-            return []\n-        if getattr(func, \"__name__\", None) == \"__call__\":\n-            return []\n-        return get_func_args(func.__call__, True)\n+    \"\"\"Return the argument name list of a callable object\"\"\"\n+    if not callable(func):\n+        raise TypeError(f\"func must be callable, got '{type(func).__name__}'\")\n+\n+    args = []\n+    try:\n+        sig = inspect.signature(func)\n+    except ValueError:\n+        return args\n+\n+    if isinstance(func, partial):\n+        partial_args = func.args\n+        partial_kw = func.keywords\n+\n+        for name, param in sig.parameters.items():\n+            if param.name in partial_args:\n+                continue\n+            if partial_kw and param.name in partial_kw:\n+                continue\n+            args.append(name)\n     else:\n-        raise TypeError(f\"{type(func)} is not callable\")\n-    if stripself:\n-        func_args.pop(0)\n-    return func_args\n+        for name in sig.parameters.keys():\n+            args.append(name)\n+\n+    if stripself and args and args[0] == \"self\":\n+        args = args[1:]\n+    return args\n \n \n def get_spec(func):\n",
    "test_patch": "diff --git a/tests/test_utils_python.py b/tests/test_utils_python.py\nindex 57f40c2e5fd..80d2e8da100 100644\n--- a/tests/test_utils_python.py\n+++ b/tests/test_utils_python.py\n@@ -235,20 +235,16 @@ def __call__(self, a, b, c):\n         self.assertEqual(get_func_args(partial_f3), [\"c\"])\n         self.assertEqual(get_func_args(cal), [\"a\", \"b\", \"c\"])\n         self.assertEqual(get_func_args(object), [])\n+        self.assertEqual(get_func_args(str.split, stripself=True), [\"sep\", \"maxsplit\"])\n+        self.assertEqual(get_func_args(\" \".join, stripself=True), [\"iterable\"])\n \n         if platform.python_implementation() == \"CPython\":\n-            # TODO: how do we fix this to return the actual argument names?\n-            self.assertEqual(get_func_args(str.split), [])\n-            self.assertEqual(get_func_args(\" \".join), [])\n+            # doesn't work on CPython: https://bugs.python.org/issue42785\n             self.assertEqual(get_func_args(operator.itemgetter(2)), [])\n         elif platform.python_implementation() == \"PyPy\":\n-            self.assertEqual(\n-                get_func_args(str.split, stripself=True), [\"sep\", \"maxsplit\"]\n-            )\n             self.assertEqual(\n                 get_func_args(operator.itemgetter(2), stripself=True), [\"obj\"]\n             )\n-            self.assertEqual(get_func_args(\" \".join, stripself=True), [\"iterable\"])\n \n     def test_without_none_values(self):\n         self.assertEqual(without_none_values([1, None, 3, 4]), [1, 3, 4])\n",
    "problem_statement": "get_func_args does not fully work in CPython\nAs [shown in tests](https://github.com/scrapy/scrapy/blob/ada917307844950a81226f020b596d5932187f6e/tests/test_utils_python.py#L240-L243), `get_func_args` does not work in CPython with inputs like `str.split`, `\"\".join` or `itemgetter(2)`.\n",
    "hints_text": "If/when somebody works on this, please check if `get_func_args` can just be replaced by `inspect.signature` (which \"Accepts a wide range of Python callables, from plain functions and classes to functools.partial() objects.\" but wasn't available when `get_func_args` was written).\nI'm on it. ",
    "created_at": "2023-04-04T12:18:17Z",
    "version": "2.8",
    "PASS_TO_PASS": [],
    "FAIL_TO_PASS": [
      "tests/test_utils_python.py::UtilsPythonTestCase::test_get_func_args"
    ]
  },
  {
    "repo": "scrapy/scrapy",
    "pull_number": 5847,
    "instance_id": "scrapy__scrapy-5847",
    "issue_numbers": [
      "872",
      "5633"
    ],
    "base_commit": "9e0bfc4a3dc92ac7f0fc0aff6254374045733299",
    "patch": "diff --git a/docs/topics/feed-exports.rst b/docs/topics/feed-exports.rst\nindex b4ac93b1d7f..89968a5f6da 100644\n--- a/docs/topics/feed-exports.rst\n+++ b/docs/topics/feed-exports.rst\n@@ -572,9 +572,12 @@ to ``.json`` or ``.xml``.\n FEED_STORE_EMPTY\n ----------------\n \n-Default: ``False``\n+Default: ``True``\n \n Whether to export empty feeds (i.e. feeds with no items).\n+If ``False``, and there are no items to export, no new files are created and \n+existing files are not modified, even if the :ref:`overwrite feed option \n+<feed-options>` is enabled.\n \n .. setting:: FEED_STORAGES\n \ndiff --git a/scrapy/extensions/feedexport.py b/scrapy/extensions/feedexport.py\nindex bcf0b779a7d..d088450a790 100644\n--- a/scrapy/extensions/feedexport.py\n+++ b/scrapy/extensions/feedexport.py\n@@ -277,8 +277,6 @@ def _store_in_thread(self, file):\n class FeedSlot:\n     def __init__(\n         self,\n-        file,\n-        exporter,\n         storage,\n         uri,\n         format,\n@@ -286,9 +284,14 @@ def __init__(\n         batch_id,\n         uri_template,\n         filter,\n+        feed_options,\n+        spider,\n+        exporters,\n+        settings,\n+        crawler,\n     ):\n-        self.file = file\n-        self.exporter = exporter\n+        self.file = None\n+        self.exporter = None\n         self.storage = storage\n         # feed params\n         self.batch_id = batch_id\n@@ -297,15 +300,44 @@ def __init__(\n         self.uri_template = uri_template\n         self.uri = uri\n         self.filter = filter\n+        # exporter params\n+        self.feed_options = feed_options\n+        self.spider = spider\n+        self.exporters = exporters\n+        self.settings = settings\n+        self.crawler = crawler\n         # flags\n         self.itemcount = 0\n         self._exporting = False\n+        self._fileloaded = False\n \n     def start_exporting(self):\n+        if not self._fileloaded:\n+            self.file = self.storage.open(self.spider)\n+            if \"postprocessing\" in self.feed_options:\n+                self.file = PostProcessingManager(\n+                    self.feed_options[\"postprocessing\"], self.file, self.feed_options\n+                )\n+            self.exporter = self._get_exporter(\n+                file=self.file,\n+                format=self.feed_options[\"format\"],\n+                fields_to_export=self.feed_options[\"fields\"],\n+                encoding=self.feed_options[\"encoding\"],\n+                indent=self.feed_options[\"indent\"],\n+                **self.feed_options[\"item_export_kwargs\"],\n+            )\n+            self._fileloaded = True\n+\n         if not self._exporting:\n             self.exporter.start_exporting()\n             self._exporting = True\n \n+    def _get_instance(self, objcls, *args, **kwargs):\n+        return create_instance(objcls, self.settings, self.crawler, *args, **kwargs)\n+\n+    def _get_exporter(self, file, format, *args, **kwargs):\n+        return self._get_instance(self.exporters[format], file, *args, **kwargs)\n+\n     def finish_exporting(self):\n         if self._exporting:\n             self.exporter.finish_exporting()\n@@ -406,11 +438,16 @@ def get_file(slot_):\n                 return slot_.file.file\n             return slot_.file\n \n-        slot.finish_exporting()\n-        if not slot.itemcount and not slot.store_empty:\n-            # We need to call slot.storage.store nonetheless to get the file\n-            # properly closed.\n-            return defer.maybeDeferred(slot.storage.store, get_file(slot))\n+        if slot.itemcount:\n+            # Normal case\n+            slot.finish_exporting()\n+        elif slot.store_empty and slot.batch_id == 1:\n+            # Need to store the empty file\n+            slot.start_exporting()\n+            slot.finish_exporting()\n+        else:\n+            # In this case, the file is not stored, so no processing is required.\n+            return None\n \n         logmsg = f\"{slot.format} feed ({slot.itemcount} items) in: {slot.uri}\"\n         d = defer.maybeDeferred(slot.storage.store, get_file(slot))\n@@ -455,23 +492,7 @@ def _start_new_batch(self, batch_id, uri, feed_options, spider, uri_template):\n         :param uri_template: template of uri which contains %(batch_time)s or %(batch_id)d to create new uri\n         \"\"\"\n         storage = self._get_storage(uri, feed_options)\n-        file = storage.open(spider)\n-        if \"postprocessing\" in feed_options:\n-            file = PostProcessingManager(\n-                feed_options[\"postprocessing\"], file, feed_options\n-            )\n-\n-        exporter = self._get_exporter(\n-            file=file,\n-            format=feed_options[\"format\"],\n-            fields_to_export=feed_options[\"fields\"],\n-            encoding=feed_options[\"encoding\"],\n-            indent=feed_options[\"indent\"],\n-            **feed_options[\"item_export_kwargs\"],\n-        )\n         slot = FeedSlot(\n-            file=file,\n-            exporter=exporter,\n             storage=storage,\n             uri=uri,\n             format=feed_options[\"format\"],\n@@ -479,9 +500,12 @@ def _start_new_batch(self, batch_id, uri, feed_options, spider, uri_template):\n             batch_id=batch_id,\n             uri_template=uri_template,\n             filter=self.filters[uri_template],\n+            feed_options=feed_options,\n+            spider=spider,\n+            exporters=self.exporters,\n+            settings=self.settings,\n+            crawler=getattr(self, \"crawler\", None),\n         )\n-        if slot.store_empty:\n-            slot.start_exporting()\n         return slot\n \n     def item_scraped(self, item, spider):\n@@ -565,14 +589,6 @@ def _storage_supported(self, uri, feed_options):\n         else:\n             logger.error(\"Unknown feed storage scheme: %(scheme)s\", {\"scheme\": scheme})\n \n-    def _get_instance(self, objcls, *args, **kwargs):\n-        return create_instance(\n-            objcls, self.settings, getattr(self, \"crawler\", None), *args, **kwargs\n-        )\n-\n-    def _get_exporter(self, file, format, *args, **kwargs):\n-        return self._get_instance(self.exporters[format], file, *args, **kwargs)\n-\n     def _get_storage(self, uri, feed_options):\n         \"\"\"Fork of create_instance specific to feed storage classes\n \ndiff --git a/scrapy/settings/default_settings.py b/scrapy/settings/default_settings.py\nindex 260ec1701c7..ea63d35c52b 100644\n--- a/scrapy/settings/default_settings.py\n+++ b/scrapy/settings/default_settings.py\n@@ -141,7 +141,7 @@\n FEED_TEMPDIR = None\n FEEDS = {}\n FEED_URI_PARAMS = None  # a function to extend uri arguments\n-FEED_STORE_EMPTY = False\n+FEED_STORE_EMPTY = True\n FEED_EXPORT_ENCODING = None\n FEED_EXPORT_FIELDS = None\n FEED_STORAGES = {}\n",
    "test_patch": "diff --git a/tests/test_feedexport.py b/tests/test_feedexport.py\nindex b1059099a37..faf4bd22319 100644\n--- a/tests/test_feedexport.py\n+++ b/tests/test_feedexport.py\n@@ -726,10 +726,9 @@ def run_and_export(self, spider_cls, settings):\n                 yield crawler.crawl()\n \n             for file_path, feed_options in FEEDS.items():\n-                if not Path(file_path).exists():\n-                    continue\n-\n-                content[feed_options[\"format\"]] = Path(file_path).read_bytes()\n+                content[feed_options[\"format\"]] = (\n+                    Path(file_path).read_bytes() if Path(file_path).exists() else None\n+                )\n \n         finally:\n             for file_path in FEEDS.keys():\n@@ -946,9 +945,10 @@ def test_export_no_items_not_store_empty(self):\n                 \"FEEDS\": {\n                     self._random_temp_filename(): {\"format\": fmt},\n                 },\n+                \"FEED_STORE_EMPTY\": False,\n             }\n             data = yield self.exported_no_data(settings)\n-            self.assertEqual(b\"\", data[fmt])\n+            self.assertEqual(None, data[fmt])\n \n     @defer.inlineCallbacks\n     def test_start_finish_exporting_items(self):\n@@ -1064,8 +1064,7 @@ def test_export_no_items_multiple_feeds(self):\n         with LogCapture() as log:\n             yield self.exported_no_data(settings)\n \n-        print(log)\n-        self.assertEqual(str(log).count(\"Storage.store is called\"), 3)\n+        self.assertEqual(str(log).count(\"Storage.store is called\"), 0)\n \n     @defer.inlineCallbacks\n     def test_export_multiple_item_classes(self):\n@@ -1732,10 +1731,9 @@ def run_and_export(self, spider_cls, settings):\n                 yield crawler.crawl()\n \n             for file_path, feed_options in FEEDS.items():\n-                if not Path(file_path).exists():\n-                    continue\n-\n-                content[str(file_path)] = Path(file_path).read_bytes()\n+                content[str(file_path)] = (\n+                    Path(file_path).read_bytes() if Path(file_path).exists() else None\n+                )\n \n         finally:\n             for file_path in FEEDS.keys():\n@@ -2236,6 +2234,9 @@ def run_and_export(self, spider_cls, settings):\n \n             for path, feed in FEEDS.items():\n                 dir_name = Path(path).parent\n+                if not dir_name.exists():\n+                    content[feed[\"format\"]] = []\n+                    continue\n                 for file in sorted(dir_name.iterdir()):\n                     content[feed[\"format\"]].append(file.read_bytes())\n         finally:\n@@ -2419,10 +2420,11 @@ def test_export_no_items_not_store_empty(self):\n                     / self._file_mark: {\"format\": fmt},\n                 },\n                 \"FEED_EXPORT_BATCH_ITEM_COUNT\": 1,\n+                \"FEED_STORE_EMPTY\": False,\n             }\n             data = yield self.exported_no_data(settings)\n             data = dict(data)\n-            self.assertEqual(b\"\", data[fmt][0])\n+            self.assertEqual(0, len(data[fmt]))\n \n     @defer.inlineCallbacks\n     def test_export_no_items_store_empty(self):\n@@ -2536,9 +2538,6 @@ def test_batch_item_count_feeds_setting(self):\n             for expected_batch, got_batch in zip(expected, data[fmt]):\n                 self.assertEqual(expected_batch, got_batch)\n \n-    @pytest.mark.skipif(\n-        sys.platform == \"win32\", reason=\"Odd behaviour on file creation/output\"\n-    )\n     @defer.inlineCallbacks\n     def test_batch_path_differ(self):\n         \"\"\"\n@@ -2560,7 +2559,7 @@ def test_batch_path_differ(self):\n             \"FEED_EXPORT_BATCH_ITEM_COUNT\": 1,\n         }\n         data = yield self.exported_data(items, settings)\n-        self.assertEqual(len(items), len([_ for _ in data[\"json\"] if _]))\n+        self.assertEqual(len(items), len(data[\"json\"]))\n \n     @defer.inlineCallbacks\n     def test_stats_batch_file_success(self):\n@@ -2647,7 +2646,7 @@ def parse(self, response):\n             crawler = get_crawler(TestSpider, settings)\n             yield crawler.crawl()\n \n-        self.assertEqual(len(CustomS3FeedStorage.stubs), len(items) + 1)\n+        self.assertEqual(len(CustomS3FeedStorage.stubs), len(items))\n         for stub in CustomS3FeedStorage.stubs[:-1]:\n             stub.assert_no_pending_responses()\n \n",
    "problem_statement": "FileFeedStorage creates empty file when no items are scraped\nWhen no items are scraped, the corresponding file is created none the less, because it is created in by the `storage.open` call in `FeedExporter.open_spider`. This behavior ignores the the setting of `FEED_STORE_EMPTY` when using file export.\n\nMy proposal for this would be to add a `cleanup` method to the `IFeedStorage` interface. Then  `FeedExporter.close_spider` can call that method before returning in case `slot.itemcount` is zero and `self.store_empty` is `False`. `cleanup` could also be called internally from the `store` methods of the `IFeedStorage` implementations.\n\nAddress BatchDeliveriesTest.test_batch_path_differ failures on Windows\nAfter https://github.com/scrapy/scrapy/pull/5632, the only remaining test failure seems to be that of `BatchDeliveriesTest.test_batch_path_differ`, which seems to be failing in Windows for Python 3.7 (and pinned dependencies), Python 3.9+, but not Python 3.8, for some reason.\r\n\r\nIt is also weird that the error seems to suggest an extra item is yielded (4 != 3) in those scenarios. I cannot imagine what could cause an extra item yield in a specific platform.\n",
    "hints_text": "@gbirke I have met the same problem, there are so many empty files when you save data to files, and your proposal sounds like a good idea to me.  `IFeedStorage` need to be added a new method called in `close_spider` of `FeedExporter`\n\nI'm having a similar issue, where the `JSONItemExporter` will write out the opening `[` when calling it's `start_exporting` method but if there are no items scraped, it's `finish_exporting` method never gets called, and the closing `]` is never added, so any resources consuming that JSON file will throw errors because of the invalid JSON format. \n\nThe bug has not been resolved yet. I don't believe the resolution proposed in the PR #2258 was sufficient. Although it may fix the issue of broken JSON, the problem of FileFeedStorage creating empty files remains.\r\nThis is because FileFeedStorage.open open the actual file within it, which generates the file even if there is no writing happening.\r\nIn my opinion, one of the following is necessary:\r\n1. Only open the file when the first item is received, similar to how start_exporting works.\r\n2. Make FileFeedStorage use a TemporaryFile like BlockingFeedStorage does.\nSo I hope this issue will be reopened.\nhttps://github.com/scrapy/scrapy/pull/916#issuecomment-247939197\r\n> The problem with removing file is that it doesn't play nicely with the default behavior of appending to the result.\r\n\r\nWhile removing the file may cause problems with appending, I think delaying the direct opening of the file would  be able to work well with the appending behavior and prevent the creation of an empty file.\nSince there doesn't seem to be much demand for the patch, I am thinking of making my own patch. However, I am not sure which of the two solutions suggested earlier is better. \r\nCreating a temporary file and writing to it seems like unnecessary overhead. On the other hand, opening the file for the first time at the moment of writing and trying to write to it would require a major change in logic and look bad.\nMy gut says postponing file creation until there is an item to write is the way to go, however ugly it might be. But my opinion may change depending on how ugly things can get :sweat_smile: \nLet's see how the return value of IFeedStorage.open() is used.\r\nhttps://github.com/scrapy/scrapy/blob/8fbebfa943c3352f5ba49f46531a6ccdd0b52b60/scrapy/extensions/feedexport.py#L426-L450\r\nHere is the first problem.\r\nThe _FeedSlot is a private class within feedexporter, so it is easy to fix. ItemExporter, on the other hand, is a class outside of the module. I don't yet know the inner workings of scrapy, so I don't know how much modification to the source code would be required to change the interface of the ItemExporter.\nThen it seems like a simple answer to let _FeedSlot manage whether the file and ItemExporter are loaded.\r\nThe _FeedSlot is given the necessary settings to open the file and initialize the ItemExporter in the constructor, and then processes them when the file or ItemExporter is first requested.\nI have finished creating the fix and am checking for conflicts with the test, but in doing so I am not sure if this should be done as a \"fix\".\r\nIs this something that should be a destructive change to FEED_STORE_EMPTY as a bug...? Or should I create a new option to not export files when empty feeds?\r\n\r\nAt least the docs.\r\n\r\n> FEED_STORE_EMPTY\r\n> Default: False\r\n> Whether to export empty feeds (i.e. feeds with no items).\r\n\r\nI feel this part is one of the reasons for this confusion.\r\nWhat does \"Whether\" mean?\r\nWrite out an empty feed or nothing?\r\nDoes \"Nothing\" mean no file is created? Does it mean an empty file is created?\nTo me the documentation reads clearly enough: an empty feed is a feed with no items, and `FEED_STORE_EMPTY=False` should cause such feeds to not be exported.\r\n\r\nThat doesn\u2019t mean we cannot improve it, e.g. by explicitly saying that a file without items is not even created. But it means that it feels like considering the creation of a file a bug when `FEED_STORE_EMPTY=False` seems OK to me.\nIf the current behavior is a bug, then I feel it is more necessary to fix the behavior of other features as well.\r\nThis fix results in the command line option -O or --overwrite-output no longer creating an empty file, which causes the test to fail.\r\nIt is easy to modify the test to a test case such that the item exists, but I feel that is not testing an edge case.\r\nWhen fixing this Issue as a bug, it feels more natural to implicitly set FEED_STORE_EMPTY=True when --output or --overwrite-output, or to set the default for FEED_STORE_EMPTY to True. What would you suggest?\r\n\n> It feels more natural to implicitly set FEED_STORE_EMPTY=True when --output or --overwrite-output, or to set the default for FEED_STORE_EMPTY to True. What would you suggest?\r\n\r\nThat\u2019s a very good question, and I am not entirely sure of what would be best. I think it may make sense to do that, but only for `-O`, not for `-o`, and in any case it should be possible to override with `-s FEED_STORE_EMPTY=False`.\r\n\r\nChanging the global default may not be good, since it would be a backward incompatible case\u2026 unless there is no single scenario where this bug does not happen, in which case `FEED_STORE_EMPTY=True` has been the de-facto default at least since the bug was introduced, and making it the actual default as we fix the bug would be *less* backward-incompatible.\r\n\r\n@kmike @wRAR Any thoughts?\n>  unless there is no single scenario where this bug does not happen, in which case FEED_STORE_EMPTY=True has been the de-facto default at least since the bug was introduced,\r\n\r\nA modest change in behavior will occur.\r\nWhereas before a completely empty file was generated, that modification will now generate a formatted empty representation, e.g., [] for json, xml declaration and empty elements for xml, etc.\r\nHowever, my personal opinion is that empty files were never really expected in the first place.\nApparently, I can indirectly solve #5633 and directly solve [the issue mentioned in this PR](https://github.com/scrapy/scrapy/pull/5639#issuecomment-1259655714) while solving this issue.\r\nThe creation of empty files during batch testing is essentially the same problem as this issue, and could be solved at the same time by adding an exception to match the batch process.\r\nSince the empty file creation will no longer occur, there is no need for Windows exception handling related to this behavior.\n",
    "created_at": "2023-03-08T14:14:54Z",
    "version": "2.9",
    "PASS_TO_PASS": [],
    "FAIL_TO_PASS": [
      "tests/test_feedexport.py::FeedExportTest::test_export_no_items_not_store_empty",
      "tests/test_feedexport.py::BatchDeliveriesTest::test_export_no_items_not_store_empty",
      "tests/test_feedexport.py::BatchDeliveriesTest::test_batch_path_differ",
      "tests/test_feedexport.py::BatchDeliveriesTest::test_s3_export"
    ]
  },
  {
    "repo": "scrapy/scrapy",
    "pull_number": 5833,
    "instance_id": "scrapy__scrapy-5833",
    "issue_numbers": [
      "960",
      "960",
      "5735"
    ],
    "base_commit": "d60b4edd11436e61284615ec7ce89f8ac7e46d9a",
    "patch": "diff --git a/docs/topics/feed-exports.rst b/docs/topics/feed-exports.rst\nindex eef0bb5ca89..5eea6aaf9cd 100644\n--- a/docs/topics/feed-exports.rst\n+++ b/docs/topics/feed-exports.rst\n@@ -101,12 +101,12 @@ The storages backends supported out of the box are:\n \n -   :ref:`topics-feed-storage-fs`\n -   :ref:`topics-feed-storage-ftp`\n--   :ref:`topics-feed-storage-s3` (requires botocore_)\n+-   :ref:`topics-feed-storage-s3` (requires boto3_)\n -   :ref:`topics-feed-storage-gcs` (requires `google-cloud-storage`_)\n -   :ref:`topics-feed-storage-stdout`\n \n Some storage backends may be unavailable if the required external libraries are\n-not available. For example, the S3 backend is only available if the botocore_\n+not available. For example, the S3 backend is only available if the boto3_\n library is installed.\n \n \n@@ -193,7 +193,7 @@ The feeds are stored on `Amazon S3`_.\n \n     -   ``s3://aws_key:aws_secret@mybucket/path/to/export.csv``\n \n--   Required external libraries: `botocore`_ >= 1.4.87\n+-   Required external libraries: `boto3`_ >= 1.20.0\n \n The AWS credentials can be passed as user/password in the URI, or they can be\n passed through the following settings:\n@@ -779,6 +779,6 @@ source spider in the feed URI:\n \n .. _URIs: https://en.wikipedia.org/wiki/Uniform_Resource_Identifier\n .. _Amazon S3: https://aws.amazon.com/s3/\n-.. _botocore: https://github.com/boto/botocore\n+.. _boto3: https://github.com/boto/boto3\n .. _Canned ACL: https://docs.aws.amazon.com/AmazonS3/latest/dev/acl-overview.html#canned-acl\n .. _Google Cloud Storage: https://cloud.google.com/storage/\ndiff --git a/scrapy/extensions/feedexport.py b/scrapy/extensions/feedexport.py\nindex cd26b577896..83849ca1361 100644\n--- a/scrapy/extensions/feedexport.py\n+++ b/scrapy/extensions/feedexport.py\n@@ -30,6 +30,13 @@\n \n logger = logging.getLogger(__name__)\n \n+try:\n+    import boto3  # noqa: F401\n+\n+    IS_BOTO3_AVAILABLE = True\n+except ImportError:\n+    IS_BOTO3_AVAILABLE = False\n+\n \n def build_storage(builder, uri, *args, feed_options=None, preargs=(), **kwargs):\n     argument_names = get_func_args(builder)\n@@ -173,16 +180,38 @@ def __init__(\n         self.keyname = u.path[1:]  # remove first \"/\"\n         self.acl = acl\n         self.endpoint_url = endpoint_url\n-        import botocore.session\n-\n-        session = botocore.session.get_session()\n-        self.s3_client = session.create_client(\n-            \"s3\",\n-            aws_access_key_id=self.access_key,\n-            aws_secret_access_key=self.secret_key,\n-            aws_session_token=self.session_token,\n-            endpoint_url=self.endpoint_url,\n-        )\n+\n+        if IS_BOTO3_AVAILABLE:\n+            import boto3.session\n+\n+            session = boto3.session.Session()\n+\n+            self.s3_client = session.client(\n+                \"s3\",\n+                aws_access_key_id=self.access_key,\n+                aws_secret_access_key=self.secret_key,\n+                aws_session_token=self.session_token,\n+                endpoint_url=self.endpoint_url,\n+            )\n+        else:\n+            warnings.warn(\n+                \"`botocore` usage has been deprecated for S3 feed \"\n+                \"export, please use `boto3` to avoid problems\",\n+                category=ScrapyDeprecationWarning,\n+            )\n+\n+            import botocore.session\n+\n+            session = botocore.session.get_session()\n+\n+            self.s3_client = session.create_client(\n+                \"s3\",\n+                aws_access_key_id=self.access_key,\n+                aws_secret_access_key=self.secret_key,\n+                aws_session_token=self.session_token,\n+                endpoint_url=self.endpoint_url,\n+            )\n+\n         if feed_options and feed_options.get(\"overwrite\", True) is False:\n             logger.warning(\n                 \"S3 does not support appending to files. To \"\n@@ -205,10 +234,16 @@ def from_crawler(cls, crawler, uri, *, feed_options=None):\n \n     def _store_in_thread(self, file):\n         file.seek(0)\n-        kwargs = {\"ACL\": self.acl} if self.acl else {}\n-        self.s3_client.put_object(\n-            Bucket=self.bucketname, Key=self.keyname, Body=file, **kwargs\n-        )\n+        if IS_BOTO3_AVAILABLE:\n+            kwargs = {\"ExtraArgs\": {\"ACL\": self.acl}} if self.acl else {}\n+            self.s3_client.upload_fileobj(\n+                Bucket=self.bucketname, Key=self.keyname, Fileobj=file, **kwargs\n+            )\n+        else:\n+            kwargs = {\"ACL\": self.acl} if self.acl else {}\n+            self.s3_client.put_object(\n+                Bucket=self.bucketname, Key=self.keyname, Body=file, **kwargs\n+            )\n         file.close()\n \n \ndiff --git a/tox.ini b/tox.ini\nindex 5a9d9cf29a6..80fc287355a 100644\n--- a/tox.ini\n+++ b/tox.ini\n@@ -18,8 +18,6 @@ deps =\n     mitmproxy >= 4.0.4, < 8; python_version < '3.9' and implementation_name != 'pypy'\n     # newer markupsafe is incompatible with deps of old mitmproxy (which we get on Python 3.7 and lower)\n     markupsafe < 2.1.0; python_version < '3.8' and implementation_name != 'pypy'\n-    # Extras\n-    botocore>=1.4.87\n passenv =\n     S3_TEST_FILE_URI\n     AWS_ACCESS_KEY_ID\n@@ -90,11 +88,6 @@ deps =\n \n     # mitmproxy 4.0.4+ requires upgrading some of the pinned dependencies\n     # above, hence we do not install it in pinned environments at the moment\n-\n-    # Extras\n-    botocore==1.4.87\n-    google-cloud-storage==1.29.0\n-    Pillow==7.1.0\n setenv =\n     _SCRAPY_PINNED=true\n install_command =\n@@ -121,14 +114,26 @@ setenv =\n basepython = python3\n deps =\n     {[testenv]deps}\n-    boto\n+    boto3\n     google-cloud-storage\n     # Twisted[http2] currently forces old mitmproxy because of h2 version\n     # restrictions in their deps, so we need to pin old markupsafe here too.\n     markupsafe < 2.1.0\n     robotexclusionrulesparser\n-    Pillow>=4.0.0\n-    Twisted[http2]>=17.9.0\n+    Pillow\n+    Twisted[http2]\n+\n+[testenv:extra-deps-pinned]\n+basepython = python3.7\n+deps =\n+    {[pinned]deps}\n+    boto3==1.20.0\n+    google-cloud-storage==1.29.0\n+    Pillow==7.1.0\n+    robotexclusionrulesparser==1.6.2\n+install_command = {[pinned]install_command}\n+setenv =\n+    {[pinned]setenv}\n \n [testenv:asyncio]\n commands =\n@@ -187,3 +192,24 @@ deps = {[docs]deps}\n setenv = {[docs]setenv}\n commands =\n     sphinx-build -W -b linkcheck . {envtmpdir}/linkcheck\n+\n+\n+# Run S3 tests with botocore installed but without boto3.\n+\n+[testenv:botocore]\n+deps =\n+    {[testenv]deps}\n+    botocore>=1.4.87\n+commands =\n+    pytest --cov=scrapy --cov-report=xml --cov-report= {posargs:tests -k s3}\n+\n+[testenv:botocore-pinned]\n+basepython = python3.7\n+deps =\n+    {[pinned]deps}\n+    botocore==1.4.87\n+install_command = {[pinned]install_command}\n+setenv =\n+    {[pinned]setenv}\n+commands =\n+    pytest --cov=scrapy --cov-report=xml --cov-report= {posargs:tests -k s3}\n",
    "test_patch": "diff --git a/.github/workflows/tests-ubuntu.yml b/.github/workflows/tests-ubuntu.yml\nindex 8fcf90a1814..96b26a1f89a 100644\n--- a/.github/workflows/tests-ubuntu.yml\n+++ b/.github/workflows/tests-ubuntu.yml\n@@ -37,10 +37,19 @@ jobs:\n         - python-version: pypy3.7\n           env:\n             TOXENV: pypy3-pinned\n+        - python-version: 3.7.13\n+          env:\n+            TOXENV: extra-deps-pinned\n+        - python-version: 3.7.13\n+          env:\n+            TOXENV: botocore-pinned\n \n         - python-version: \"3.11\"\n           env:\n             TOXENV: extra-deps\n+        - python-version: \"3.11\"\n+          env:\n+            TOXENV: botocore\n \n     steps:\n     - uses: actions/checkout@v3\ndiff --git a/tests/test_feedexport.py b/tests/test_feedexport.py\nindex eafe1b3342f..7df3e6dd3d3 100644\n--- a/tests/test_feedexport.py\n+++ b/tests/test_feedexport.py\n@@ -35,6 +35,7 @@\n from scrapy.exceptions import NotConfigured, ScrapyDeprecationWarning\n from scrapy.exporters import CsvItemExporter, JsonItemExporter\n from scrapy.extensions.feedexport import (\n+    IS_BOTO3_AVAILABLE,\n     BlockingFeedStorage,\n     FeedExporter,\n     FileFeedStorage,\n@@ -235,8 +236,10 @@ def test_invalid_folder(self):\n \n \n class S3FeedStorageTest(unittest.TestCase):\n-    def test_parse_credentials(self):\n+    def setUp(self):\n         skip_if_no_boto()\n+\n+    def test_parse_credentials(self):\n         aws_credentials = {\n             \"AWS_ACCESS_KEY_ID\": \"settings_key\",\n             \"AWS_SECRET_ACCESS_KEY\": \"settings_secret\",\n@@ -272,8 +275,6 @@ def test_parse_credentials(self):\n \n     @defer.inlineCallbacks\n     def test_store(self):\n-        skip_if_no_boto()\n-\n         settings = {\n             \"AWS_ACCESS_KEY_ID\": \"access_key\",\n             \"AWS_SECRET_ACCESS_KEY\": \"secret_key\",\n@@ -285,30 +286,39 @@ def test_store(self):\n         verifyObject(IFeedStorage, storage)\n \n         file = mock.MagicMock()\n-        from botocore.stub import Stubber\n-\n-        with Stubber(storage.s3_client) as stub:\n-            stub.add_response(\n-                \"put_object\",\n-                expected_params={\n-                    \"Body\": file,\n-                    \"Bucket\": bucket,\n-                    \"Key\": key,\n-                },\n-                service_response={},\n-            )\n \n+        if IS_BOTO3_AVAILABLE:\n+            storage.s3_client = mock.MagicMock()\n             yield storage.store(file)\n-\n-            stub.assert_no_pending_responses()\n             self.assertEqual(\n-                file.method_calls,\n-                [\n-                    mock.call.seek(0),\n-                    # The call to read does not happen with Stubber\n-                    mock.call.close(),\n-                ],\n+                storage.s3_client.upload_fileobj.call_args,\n+                mock.call(Bucket=bucket, Key=key, Fileobj=file),\n             )\n+        else:\n+            from botocore.stub import Stubber\n+\n+            with Stubber(storage.s3_client) as stub:\n+                stub.add_response(\n+                    \"put_object\",\n+                    expected_params={\n+                        \"Body\": file,\n+                        \"Bucket\": bucket,\n+                        \"Key\": key,\n+                    },\n+                    service_response={},\n+                )\n+\n+                yield storage.store(file)\n+\n+                stub.assert_no_pending_responses()\n+                self.assertEqual(\n+                    file.method_calls,\n+                    [\n+                        mock.call.seek(0),\n+                        # The call to read does not happen with Stubber\n+                        mock.call.close(),\n+                    ],\n+                )\n \n     def test_init_without_acl(self):\n         storage = S3FeedStorage(\"s3://mybucket/export.csv\", \"access_key\", \"secret_key\")\n@@ -391,8 +401,7 @@ def test_from_crawler_with_endpoint_url(self):\n         self.assertEqual(storage.endpoint_url, \"https://example.com\")\n \n     @defer.inlineCallbacks\n-    def test_store_botocore_without_acl(self):\n-        skip_if_no_boto()\n+    def test_store_without_acl(self):\n         storage = S3FeedStorage(\n             \"s3://mybucket/export.csv\",\n             \"access_key\",\n@@ -404,11 +413,18 @@ def test_store_botocore_without_acl(self):\n \n         storage.s3_client = mock.MagicMock()\n         yield storage.store(BytesIO(b\"test file\"))\n-        self.assertNotIn(\"ACL\", storage.s3_client.put_object.call_args[1])\n+        if IS_BOTO3_AVAILABLE:\n+            acl = (\n+                storage.s3_client.upload_fileobj.call_args[1]\n+                .get(\"ExtraArgs\", {})\n+                .get(\"ACL\")\n+            )\n+        else:\n+            acl = storage.s3_client.put_object.call_args[1].get(\"ACL\")\n+        self.assertIsNone(acl)\n \n     @defer.inlineCallbacks\n-    def test_store_botocore_with_acl(self):\n-        skip_if_no_boto()\n+    def test_store_with_acl(self):\n         storage = S3FeedStorage(\n             \"s3://mybucket/export.csv\", \"access_key\", \"secret_key\", \"custom-acl\"\n         )\n@@ -418,9 +434,11 @@ def test_store_botocore_with_acl(self):\n \n         storage.s3_client = mock.MagicMock()\n         yield storage.store(BytesIO(b\"test file\"))\n-        self.assertEqual(\n-            storage.s3_client.put_object.call_args[1].get(\"ACL\"), \"custom-acl\"\n-        )\n+        if IS_BOTO3_AVAILABLE:\n+            acl = storage.s3_client.upload_fileobj.call_args[1][\"ExtraArgs\"][\"ACL\"]\n+        else:\n+            acl = storage.s3_client.put_object.call_args[1][\"ACL\"]\n+        self.assertEqual(acl, \"custom-acl\")\n \n     def test_overwrite_default(self):\n         with LogCapture() as log:\n@@ -888,15 +906,10 @@ def test_stats_file_failed(self):\n     @defer.inlineCallbacks\n     def test_stats_multiple_file(self):\n         settings = {\n-            \"AWS_ACCESS_KEY_ID\": \"access_key\",\n-            \"AWS_SECRET_ACCESS_KEY\": \"secret_key\",\n             \"FEEDS\": {\n                 printf_escape(path_to_url(str(self._random_temp_filename()))): {\n                     \"format\": \"json\",\n                 },\n-                \"s3://bucket/key/foo.csv\": {\n-                    \"format\": \"csv\",\n-                },\n                 \"stdout:\": {\n                     \"format\": \"xml\",\n                 },\n@@ -908,18 +921,12 @@ def test_stats_multiple_file(self):\n         self.assertIn(\n             \"feedexport/success_count/FileFeedStorage\", crawler.stats.get_stats()\n         )\n-        self.assertIn(\n-            \"feedexport/success_count/S3FeedStorage\", crawler.stats.get_stats()\n-        )\n         self.assertIn(\n             \"feedexport/success_count/StdoutFeedStorage\", crawler.stats.get_stats()\n         )\n         self.assertEqual(\n             crawler.stats.get_value(\"feedexport/success_count/FileFeedStorage\"), 1\n         )\n-        self.assertEqual(\n-            crawler.stats.get_value(\"feedexport/success_count/S3FeedStorage\"), 1\n-        )\n         self.assertEqual(\n             crawler.stats.get_value(\"feedexport/success_count/StdoutFeedStorage\"), 1\n         )\n@@ -2535,7 +2542,6 @@ def test_stats_batch_file_success(self):\n     @defer.inlineCallbacks\n     def test_s3_export(self):\n         skip_if_no_boto()\n-\n         bucket = \"mybucket\"\n         items = [\n             self.MyItem({\"foo\": \"bar1\", \"egg\": \"spam1\"}),\n@@ -2707,6 +2713,9 @@ class S3FeedStoragePreFeedOptionsTest(unittest.TestCase):\n \n     maxDiff = None\n \n+    def setUp(self):\n+        skip_if_no_boto()\n+\n     def test_init(self):\n         settings_dict = {\n             \"FEED_URI\": \"file:///tmp/foobar\",\ndiff --git a/tests/test_pipeline_files.py b/tests/test_pipeline_files.py\nindex 9701e5d4eeb..e0bcfcfeabb 100644\n--- a/tests/test_pipeline_files.py\n+++ b/tests/test_pipeline_files.py\n@@ -225,12 +225,16 @@ def file_path(self, request, response=None, info=None, item=None):\n \n \n class FilesPipelineTestCaseFieldsMixin:\n+    def setUp(self):\n+        self.tempdir = mkdtemp()\n+\n+    def tearDown(self):\n+        rmtree(self.tempdir)\n+\n     def test_item_fields_default(self):\n         url = \"http://www.example.com/files/1.txt\"\n         item = self.item_class(name=\"item1\", file_urls=[url])\n-        pipeline = FilesPipeline.from_settings(\n-            Settings({\"FILES_STORE\": \"s3://example/files/\"})\n-        )\n+        pipeline = FilesPipeline.from_settings(Settings({\"FILES_STORE\": self.tempdir}))\n         requests = list(pipeline.get_media_requests(item, None))\n         self.assertEqual(requests[0].url, url)\n         results = [(True, {\"url\": url})]\n@@ -245,7 +249,7 @@ def test_item_fields_override_settings(self):\n         pipeline = FilesPipeline.from_settings(\n             Settings(\n                 {\n-                    \"FILES_STORE\": \"s3://example/files/\",\n+                    \"FILES_STORE\": self.tempdir,\n                     \"FILES_URLS_FIELD\": \"custom_file_urls\",\n                     \"FILES_RESULT_FIELD\": \"custom_files\",\n                 }\n",
    "problem_statement": "S3 Feed Export throws boto error: \"Connection Reset By Peer\"\nPosted details on SO: http://stackoverflow.com/questions/27131693/scrapyd-s3-feed-export-connection-reset-by-peer\n\nS3 Feed Export throws boto error: \"Connection Reset By Peer\"\nPosted details on SO: http://stackoverflow.com/questions/27131693/scrapyd-s3-feed-export-connection-reset-by-peer\n\nS3 backend can't handle uploads larger than 5GB \n### Description\r\n\r\nWhen feeds larger than 5GB are sent using AWS S3 backend, I'm receiving the follow error:\r\n\r\n```bash\r\n2022-11-24 18:45:31 [scrapy.extensions.feedexport] ERROR: Error storing csv feed (55 items) in: s3://scrapy-test/large_export.csv\r\nTraceback (most recent call last):\r\n  File \"/Users/ogabrielsantos/crawler-scrapy/venv/lib/python3.10/site-packages/twisted/python/threadpool.py\", line 244, in inContext\r\n    result = inContext.theWork()  # type: ignore[attr-defined]\r\n  File \"/Users/ogabrielsantos/crawler-scrapy/venv/lib/python3.10/site-packages/twisted/python/threadpool.py\", line 260, in <lambda>\r\n    inContext.theWork = lambda: context.call(  # type: ignore[attr-defined]\r\n  File \"/Users/ogabrielsantos/crawler-scrapy/venv/lib/python3.10/site-packages/twisted/python/context.py\", line 117, in callWithContext\r\n    return self.currentContext().callWithContext(ctx, func, *args, **kw)\r\n  File \"/Users/ogabrielsantos/crawler-scrapy/venv/lib/python3.10/site-packages/twisted/python/context.py\", line 82, in callWithContext\r\n    return func(*args, **kw)\r\n  File \"/Users/ogabrielsantos/crawler-scrapy/venv/lib/python3.10/site-packages/scrapy/extensions/feedexport.py\", line 196, in _store_in_thread\r\n    self.s3_client.put_object(\r\n  File \"/Users/ogabrielsantos/crawler-scrapy/venv/lib/python3.10/site-packages/botocore/client.py\", line 530, in _api_call\r\n    return self._make_api_call(operation_name, kwargs)\r\n  File \"/Users/ogabrielsantos/crawler-scrapy/venv/lib/python3.10/site-packages/botocore/client.py\", line 960, in _make_api_call\r\n    raise error_class(parsed_response, operation_name)\r\nbotocore.exceptions.ClientError: An error occurred (EntityTooLarge) when calling the PutObject operation: Your proposed upload exceeds the maximum allowed size\r\n```\r\n\r\n### Steps to Reproduce\r\n\r\nI've write a minimum exemple code to simulate this issue:\r\n\r\n```python\r\nfrom scrapy.spiders import Spider\r\n\r\n\r\nclass LargeExportSpider(Spider):\r\n    name = \"large_export\"\r\n    start_urls = [\"http://news.ycombinator.com/\"]\r\n    custom_settings = {\r\n        \"FEEDS\": {\r\n            \"s3://scrapy-test/large_export.csv\": {\r\n                \"format\": \"csv\",\r\n            },\r\n        },\r\n    }\r\n\r\n    def parse(self, response, **kwargs):\r\n        text = \"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque iaculis odio efficitur, ultricies\"\r\n\r\n        for _ in range(0, 55):  # creates a 5.3GB csv file\r\n            yield {\"name\": \"John Doe\", \"text\": text * 1000000}\r\n\r\n```\r\n\r\n### Versions\r\n`scrapy version --verbose`:\r\n```bash\r\nScrapy       : 2.7.1\r\nlxml         : 4.9.1.0\r\nlibxml2      : 2.9.13\r\ncssselect    : 1.2.0\r\nparsel       : 1.7.0\r\nw3lib        : 2.0.1\r\nTwisted      : 22.10.0\r\nPython       : 3.10.8 (main, Oct 13 2022, 09:48:40) [Clang 14.0.0 (clang-1400.0.29.102)]\r\npyOpenSSL    : 22.1.0 (OpenSSL 3.0.5 5 Jul 2022)\r\ncryptography : 38.0.1\r\nPlatform     : macOS-13.0.1-arm64-arm-64bit\r\n```\r\n\r\n`requirements.txt`:\r\n```\r\nbotocore==1.29.16\r\nScrapy==2.7.1\r\n```\r\n\r\n### Additional context\r\n\r\nDoing some investigation, I've seen that `S3FeedStorage` uses `put_object` which, as [per documentation](https://docs.aws.amazon.com/AmazonS3/latest/userguide/upload-objects.html), has a limit of 5GB per uploaded object:\r\n\r\nhttps://github.com/scrapy/scrapy/blob/6ded3cf4cd134b615239babe28bb28c3ff524b05/scrapy/extensions/feedexport.py#L196\r\n\r\nLooks like `boto3` [already have an upload method](https://boto3.amazonaws.com/v1/documentation/api/1.16.53/guide/s3-uploading-files.html) which handles multipart files, but scrapy relies on `botocore`.\n",
    "hints_text": "Scrapy uses boto for feed exports, so it is likely a boto issue (the one you've linked at SO, https://github.com/boto/boto/issues/2207). Do you know a workaround?\n\nTo send big feed files to S3 and avoid this bug, the fix is to use multipart upload. PR #1559 used to implement this for boto2.\n\nIt would be great if someone can resurrect this WIP #1559\n\nLooking at the upstream API, it seems like implementing this change is a matter of:\r\n- Documenting the need to install boto3 for S3 support, rather than botocore.\r\n- Implement code that uploads using boto3\u2019s method instead of botocore to upload files. It seems the interfaces are similar.\r\n- If boto3 is not installed, but botocore is, fall back to the current implementation, and log a deprecation warning.\nhttps://github.com/scrapy/scrapy/pull/4077 is an interesting approach, which starts uploading to S3 right away, rather than storing the whole output on disk first, and then uploading it.\r\n\r\nBut I would rather have [a simpler approach](https://github.com/scrapy/scrapy/issues/960#issuecomment-1326872050) than none. We can always implement the https://github.com/scrapy/scrapy/pull/4077 approach afterwards.\nSee also the [workaround](https://github.com/scrapy/scrapy/issues/5735#issuecomment-1327465852) by @ogabrielsantos.\nI'd like to have a go at [the simpler approach](https://github.com/scrapy/scrapy/issues/960#issuecomment-1326872050), if that's alright.\nScrapy uses boto for feed exports, so it is likely a boto issue (the one you've linked at SO, https://github.com/boto/boto/issues/2207). Do you know a workaround?\n\nTo send big feed files to S3 and avoid this bug, the fix is to use multipart upload. PR #1559 used to implement this for boto2.\n\nIt would be great if someone can resurrect this WIP #1559\n\nLooking at the upstream API, it seems like implementing this change is a matter of:\r\n- Documenting the need to install boto3 for S3 support, rather than botocore.\r\n- Implement code that uploads using boto3\u2019s method instead of botocore to upload files. It seems the interfaces are similar.\r\n- If boto3 is not installed, but botocore is, fall back to the current implementation, and log a deprecation warning.\nhttps://github.com/scrapy/scrapy/pull/4077 is an interesting approach, which starts uploading to S3 right away, rather than storing the whole output on disk first, and then uploading it.\r\n\r\nBut I would rather have [a simpler approach](https://github.com/scrapy/scrapy/issues/960#issuecomment-1326872050) than none. We can always implement the https://github.com/scrapy/scrapy/pull/4077 approach afterwards.\nSee also the [workaround](https://github.com/scrapy/scrapy/issues/5735#issuecomment-1327465852) by @ogabrielsantos.\nI'd like to have a go at [the simpler approach](https://github.com/scrapy/scrapy/issues/960#issuecomment-1326872050), if that's alright.\nSounds related to https://github.com/scrapy/scrapy/issues/960 (at the very least, the solution seems to be the same)\nI'm workarounding it until an official fix, this way:\r\n\r\n`requirements.txt`:\r\n```\r\nboto3==1.26.16\r\n```\r\n\r\n`crawlers/feed.py`:\r\n```python\r\nclass LargeS3FeedStorage(S3FeedStorage):\r\n    def __init__(\r\n        self,\r\n        uri,\r\n        access_key=None,\r\n        secret_key=None,\r\n        acl=None,\r\n        endpoint_url=None,\r\n        *,\r\n        feed_options=None,\r\n        session_token=None,\r\n    ):\r\n        super().__init__(\r\n            uri, access_key, secret_key, acl, endpoint_url, feed_options=feed_options, session_token=session_token\r\n        )\r\n\r\n        self.s3_client = boto3.client(\r\n            \"s3\",\r\n            aws_access_key_id=self.access_key,\r\n            aws_secret_access_key=self.secret_key,\r\n            aws_session_token=self.session_token,\r\n            endpoint_url=self.endpoint_url,\r\n        )\r\n\r\n    def _store_in_thread(self, file):\r\n        file.seek(0)\r\n        self.s3_client.upload_file(Filename=file.file.name, Bucket=self.bucketname, Key=self.keyname)\r\n        file.close()\r\n```\r\n\r\n`settings.py`:\r\n```python\r\nFEED_STORAGES = {\r\n    \"s3\": \"crawlers.feed.LargeS3FeedStorage\",\r\n}\r\n```\r\n\nOOPS. Accidentally closed by private repository merge. Reopening.",
    "created_at": "2023-02-17T15:16:03Z",
    "version": "2.8",
    "PASS_TO_PASS": [],
    "FAIL_TO_PASS": [
      "tests/test_feedexport.py::FeedExportTest::test_stats_multiple_file",
      "tests/test_feedexport.py::S3FeedStorageTest::test_parse_credentials",
      "tests/test_feedexport.py::S3FeedStorageTest::test_store",
      "tests/test_feedexport.py::S3FeedStorageTest::test_store_without_acl",
      "tests/test_feedexport.py::S3FeedStorageTest::test_store_with_acl",
      "tests/test_pipeline_files.py::FilesPipelineTestCaseFieldsDict::test_item_fields_default",
      "tests/test_pipeline_files.py::FilesPipelineTestCaseFieldsDict::test_item_fields_override_settings",
      "tests/test_pipeline_files.py::FilesPipelineTestCaseFieldsItem::test_item_fields_default",
      "tests/test_pipeline_files.py::FilesPipelineTestCaseFieldsItem::test_item_fields_override_settings",
      "tests/test_pipeline_files.py::FilesPipelineTestCaseFieldsDataClass::test_item_fields_default",
      "tests/test_pipeline_files.py::FilesPipelineTestCaseFieldsDataClass::test_item_fields_override_settings",
      "tests/test_pipeline_files.py::FilesPipelineTestCaseFieldsAttrsItem::test_item_fields_default",
      "tests/test_pipeline_files.py::FilesPipelineTestCaseFieldsAttrsItem::test_item_fields_override_settings"
    ]
  },
  {
    "repo": "scrapy/scrapy",
    "pull_number": 5808,
    "instance_id": "scrapy__scrapy-5808",
    "issue_numbers": [
      "3553",
      "3558"
    ],
    "base_commit": "9411cf4e708ea60c7a6972a6804334f2a799e5c6",
    "patch": "diff --git a/docs/topics/commands.rst b/docs/topics/commands.rst\nindex 54fd5d66311..106045fc073 100644\n--- a/docs/topics/commands.rst\n+++ b/docs/topics/commands.rst\n@@ -238,9 +238,6 @@ genspider\n \n Create a new spider in the current folder or in the current project's ``spiders`` folder, if called from inside a project. The ``<name>`` parameter is set as the spider's ``name``, while ``<domain or URL>`` is used to generate the ``allowed_domains`` and ``start_urls`` spider's attributes.\n \n-.. note:: Even if an HTTPS URL is specified, the protocol used in\n-          ``start_urls`` is always HTTP. This is a known issue: :issue:`3553`.\n-\n Usage example::\n \n     $ scrapy genspider -l\ndiff --git a/scrapy/commands/genspider.py b/scrapy/commands/genspider.py\nindex c1565a13848..68cbe8ff608 100644\n--- a/scrapy/commands/genspider.py\n+++ b/scrapy/commands/genspider.py\n@@ -31,6 +31,14 @@ def extract_domain(url):\n     return o.netloc\n \n \n+def verify_url_scheme(url):\n+    \"\"\"Check url for scheme and insert https if none found.\"\"\"\n+    parsed = urlparse(url)\n+    if parsed.scheme == \"\" and parsed.netloc == \"\":\n+        parsed = urlparse(\"//\" + url)._replace(scheme=\"https\")\n+    return parsed.geturl()\n+\n+\n class Command(ScrapyCommand):\n     requires_project = False\n     default_settings = {\"LOG_ENABLED\": False}\n@@ -91,7 +99,7 @@ def run(self, args, opts):\n             raise UsageError()\n \n         name, url = args[0:2]\n-        domain = extract_domain(url)\n+        url = verify_url_scheme(url)\n         module = sanitize_module_name(name)\n \n         if self.settings.get(\"BOT_NAME\") == module:\n@@ -103,18 +111,20 @@ def run(self, args, opts):\n \n         template_file = self._find_template(opts.template)\n         if template_file:\n-            self._genspider(module, name, domain, opts.template, template_file)\n+            self._genspider(module, name, url, opts.template, template_file)\n             if opts.edit:\n                 self.exitcode = os.system(f'scrapy edit \"{name}\"')\n \n-    def _genspider(self, module, name, domain, template_name, template_file):\n+    def _genspider(self, module, name, url, template_name, template_file):\n         \"\"\"Generate the spider module, based on the given template\"\"\"\n         capitalized_module = \"\".join(s.capitalize() for s in module.split(\"_\"))\n+        domain = extract_domain(url)\n         tvars = {\n             \"project_name\": self.settings.get(\"BOT_NAME\"),\n             \"ProjectName\": string_camelcase(self.settings.get(\"BOT_NAME\")),\n             \"module\": module,\n             \"name\": name,\n+            \"url\": url,\n             \"domain\": domain,\n             \"classname\": f\"{capitalized_module}Spider\",\n         }\ndiff --git a/scrapy/templates/spiders/basic.tmpl b/scrapy/templates/spiders/basic.tmpl\nindex d3ba19553a7..20e777271ee 100644\n--- a/scrapy/templates/spiders/basic.tmpl\n+++ b/scrapy/templates/spiders/basic.tmpl\n@@ -4,7 +4,7 @@ import scrapy\n class $classname(scrapy.Spider):\n     name = \"$name\"\n     allowed_domains = [\"$domain\"]\n-    start_urls = [\"http://$domain/\"]\n+    start_urls = [\"$url\"]\n \n     def parse(self, response):\n         pass\ndiff --git a/scrapy/templates/spiders/crawl.tmpl b/scrapy/templates/spiders/crawl.tmpl\nindex 2e467e63224..36d05e43a21 100644\n--- a/scrapy/templates/spiders/crawl.tmpl\n+++ b/scrapy/templates/spiders/crawl.tmpl\n@@ -6,7 +6,7 @@ from scrapy.spiders import CrawlSpider, Rule\n class $classname(CrawlSpider):\n     name = \"$name\"\n     allowed_domains = [\"$domain\"]\n-    start_urls = [\"http://$domain/\"]\n+    start_urls = [\"$url\"]\n \n     rules = (Rule(LinkExtractor(allow=r\"Items/\"), callback=\"parse_item\", follow=True),)\n \ndiff --git a/scrapy/templates/spiders/csvfeed.tmpl b/scrapy/templates/spiders/csvfeed.tmpl\nindex ce9c1dd202a..fe96878dc5d 100644\n--- a/scrapy/templates/spiders/csvfeed.tmpl\n+++ b/scrapy/templates/spiders/csvfeed.tmpl\n@@ -4,7 +4,7 @@ from scrapy.spiders import CSVFeedSpider\n class $classname(CSVFeedSpider):\n     name = \"$name\"\n     allowed_domains = [\"$domain\"]\n-    start_urls = [\"http://$domain/feed.csv\"]\n+    start_urls = [\"$url\"]\n     #headers = [\"id\", \"name\", \"description\", \"image_link\"]\n     #delimiter = \"\\t\"\n \ndiff --git a/scrapy/templates/spiders/xmlfeed.tmpl b/scrapy/templates/spiders/xmlfeed.tmpl\nindex 6b50e4cf465..ac62d78d1c7 100644\n--- a/scrapy/templates/spiders/xmlfeed.tmpl\n+++ b/scrapy/templates/spiders/xmlfeed.tmpl\n@@ -4,7 +4,7 @@ from scrapy.spiders import XMLFeedSpider\n class $classname(XMLFeedSpider):\n     name = \"$name\"\n     allowed_domains = [\"$domain\"]\n-    start_urls = [\"http://$domain/feed.xml\"]\n+    start_urls = [\"$url\"]\n     iterator = \"iternodes\"  # you can change this; see the docs\n     itertag = \"item\"  # change it accordingly\n \n",
    "test_patch": "diff --git a/tests/test_commands.py b/tests/test_commands.py\nindex 00ddcdd3ee6..014f50e92e5 100644\n--- a/tests/test_commands.py\n+++ b/tests/test_commands.py\n@@ -541,7 +541,7 @@ def test_url(self, url=\"test.com\", domain=\"test.com\"):\n             ).group(1),\n         )\n         self.assertEqual(\n-            f\"http://{domain}/\",\n+            f\"https://{domain}\",\n             self.find_in_file(\n                 Path(self.proj_mod_path, \"spiders\", \"test_name.py\"),\n                 r\"start_urls\\s*=\\s*\\[['\\\"](.+)['\\\"]\\]\",\n@@ -549,13 +549,64 @@ def test_url(self, url=\"test.com\", domain=\"test.com\"):\n         )\n \n     def test_url_schema(self):\n-        self.test_url(\"http://test.com\", \"test.com\")\n+        self.test_url(\"https://test.com\", \"test.com\")\n \n-    def test_url_path(self):\n-        self.test_url(\"test.com/some/other/page\", \"test.com\")\n+    def test_template_start_urls(\n+        self, url=\"test.com\", expected=\"https://test.com\", template=\"basic\"\n+    ):\n+        self.assertEqual(\n+            0, self.call(\"genspider\", \"-t\", template, \"--force\", \"test_name\", url)\n+        )\n+        self.assertEqual(\n+            expected,\n+            self.find_in_file(\n+                Path(self.proj_mod_path, \"spiders\", \"test_name.py\"),\n+                r\"start_urls\\s*=\\s*\\[['\\\"](.+)['\\\"]\\]\",\n+            ).group(1),\n+        )\n+\n+    def test_genspider_basic_start_urls(self):\n+        self.test_template_start_urls(\"https://test.com\", \"https://test.com\", \"basic\")\n+        self.test_template_start_urls(\"http://test.com\", \"http://test.com\", \"basic\")\n+        self.test_template_start_urls(\n+            \"http://test.com/other/path\", \"http://test.com/other/path\", \"basic\"\n+        )\n+        self.test_template_start_urls(\n+            \"test.com/other/path\", \"https://test.com/other/path\", \"basic\"\n+        )\n \n-    def test_url_schema_path(self):\n-        self.test_url(\"https://test.com/some/other/page\", \"test.com\")\n+    def test_genspider_crawl_start_urls(self):\n+        self.test_template_start_urls(\"https://test.com\", \"https://test.com\", \"crawl\")\n+        self.test_template_start_urls(\"http://test.com\", \"http://test.com\", \"crawl\")\n+        self.test_template_start_urls(\n+            \"http://test.com/other/path\", \"http://test.com/other/path\", \"crawl\"\n+        )\n+        self.test_template_start_urls(\n+            \"test.com/other/path\", \"https://test.com/other/path\", \"crawl\"\n+        )\n+        self.test_template_start_urls(\"test.com\", \"https://test.com\", \"crawl\")\n+\n+    def test_genspider_xmlfeed_start_urls(self):\n+        self.test_template_start_urls(\n+            \"https://test.com/feed.xml\", \"https://test.com/feed.xml\", \"xmlfeed\"\n+        )\n+        self.test_template_start_urls(\n+            \"http://test.com/feed.xml\", \"http://test.com/feed.xml\", \"xmlfeed\"\n+        )\n+        self.test_template_start_urls(\n+            \"test.com/feed.xml\", \"https://test.com/feed.xml\", \"xmlfeed\"\n+        )\n+\n+    def test_genspider_csvfeed_start_urls(self):\n+        self.test_template_start_urls(\n+            \"https://test.com/feed.csv\", \"https://test.com/feed.csv\", \"csvfeed\"\n+        )\n+        self.test_template_start_urls(\n+            \"http://test.com/feed.xml\", \"http://test.com/feed.xml\", \"csvfeed\"\n+        )\n+        self.test_template_start_urls(\n+            \"test.com/feed.csv\", \"https://test.com/feed.csv\", \"csvfeed\"\n+        )\n \n \n class GenspiderStandaloneCommandTest(ProjectTest):\n",
    "problem_statement": "Genspider prepend `http://` without checking it in the `domain`\ngenspider prepend http:// But when i enter address like https://example.com it becomes http://https://example.com that, when run scrapy crawl throws an error.\r\nWhat it should do, it should first check the receiving domain than take decision according to the passing domain whether it needs a http:// or nothing.\nFix [#3553] Add_http_if_no_scheme in domain in genspider\nFixed [#3553] \r\nbefore passing the domain, use built-in function from utils.url.add_http_if_no_scheme to add the http:// before the domain.\r\nUpdated template files by removing hard coded values.\n",
    "hints_text": "Hello everybody! I would like to contribute to Scrapy, specifically with this issue and I already fork the repo and made the changes but before issuing a pull request I would like to discuss it and reach to an agreement.\r\n\r\nI made the changes over 5 files:\r\n- **scrapy/commands/genspider.py**. Where `urllib.parse.urlparse` is being used on the domain parameter and prepend `http` scheme in the case is not there. But, this is a very tricky situation because of the command syntax, which asks for a domain and not an URL, so we're just putting a very big bet that it should be HTTP but what about if we ask for a URL instead of a domain, then the contradiction is solved right away since we can extract the domain without making any assumption. Don't you think fellas?\r\n- **Each template file**. In order to receive domain and URL values.\r\nI checked the test just for tests/test_commands.py, I didn't add test because I don't know how to make a test for this particular scenario where two attributes are set based on user input on a class created through a template. If someone has an idea how it could be made please I'm all ears\r\n\r\nLastly and maybe more important, I don't know how to try the code it's my first time and I don't have any idea how to execute it. So any help or guide would be really appreciated.\r\n\r\nThank you in advance and happy coding!!!! =D\n@ambarmendez I would suggest you open a pull request already, discussions over actual code are usually easier.\r\n\r\nAlso, make sure you check out previous pull requests (#3554, #3558). Feedback there, specially @kmike\u2019s, could answer some of your points.\nThank you @Gallaecio! Let's see how it goes =D\nNow URLs are supported as input: https://github.com/scrapy/scrapy/pull/4439\r\n\r\nHowever, we still simply extract the domain from them. Which means that the input protocol is not respected for `start_urls`, and instead it is always forced to be HTTP (as opposed to, for example, HTTPS).\nhttps://github.com/scrapy/scrapy/pull/3558 seems to aim to address the `start_urls` issue. The pull request needs some work, but if not resumed, maybe it can be taken as inspiration for alternative implementations.\nHi @Gallaecio, can I be assigned to work on this issue?\n@msenior85 No need. Feel free to open a pull request and include `Fixes #3553` in its description so that it shows up in this thread.\n# [Codecov](https://codecov.io/gh/scrapy/scrapy/pull/3558?src=pr&el=h1) Report\n> Merging [#3558](https://codecov.io/gh/scrapy/scrapy/pull/3558?src=pr&el=desc) into [master](https://codecov.io/gh/scrapy/scrapy/commit/094dde6fdb1b03351888e437828af5da03f46352?src=pr&el=desc) will **decrease** coverage by `0.01%`.\n> The diff coverage is `76.92%`.\n\n\n```diff\n@@            Coverage Diff             @@\n##           master    #3558      +/-   ##\n==========================================\n- Coverage   84.48%   84.47%   -0.02%     \n==========================================\n  Files         167      167              \n  Lines        9405     9416      +11     \n  Branches     1397     1399       +2     \n==========================================\n+ Hits         7946     7954       +8     \n- Misses       1201     1202       +1     \n- Partials      258      260       +2\n```\n\n| [Impacted Files](https://codecov.io/gh/scrapy/scrapy/pull/3558?src=pr&el=tree) | Coverage \u0394 | |\n|---|---|---|\n| [scrapy/commands/genspider.py](https://codecov.io/gh/scrapy/scrapy/pull/3558/diff?src=pr&el=tree#diff-c2NyYXB5L2NvbW1hbmRzL2dlbnNwaWRlci5weQ==) | `82.47% <76.92%> (-1.25%)` | :arrow_down: |\n",
    "created_at": "2023-01-27T08:45:12Z",
    "version": "2.8",
    "PASS_TO_PASS": [],
    "FAIL_TO_PASS": [
      "tests/test_commands.py::GenspiderCommandTest::test_url",
      "tests/test_commands.py::GenspiderCommandTest::test_url_schema",
      "tests/test_commands.py::GenspiderCommandTest::test_template_start_urls",
      "tests/test_commands.py::GenspiderCommandTest::test_genspider_basic_start_urls",
      "tests/test_commands.py::GenspiderCommandTest::test_genspider_crawl_start_urls",
      "tests/test_commands.py::GenspiderCommandTest::test_genspider_xmlfeed_start_urls",
      "tests/test_commands.py::GenspiderCommandTest::test_genspider_csvfeed_start_urls"
    ]
  },
  {
    "repo": "scrapy/scrapy",
    "pull_number": 5691,
    "instance_id": "scrapy__scrapy-5691",
    "issue_numbers": [
      "5323"
    ],
    "base_commit": "b33244e2f0d877b8911f949308222db0b076d665",
    "patch": "diff --git a/scrapy/utils/misc.py b/scrapy/utils/misc.py\nindex 1221b39b229..4d4fb9600f0 100644\n--- a/scrapy/utils/misc.py\n+++ b/scrapy/utils/misc.py\n@@ -226,7 +226,14 @@ def returns_none(return_node):\n         return value is None or isinstance(value, ast.NameConstant) and value.value is None\n \n     if inspect.isgeneratorfunction(callable):\n-        code = re.sub(r\"^[\\t ]+\", \"\", inspect.getsource(callable))\n+        src = inspect.getsource(callable)\n+        pattern = re.compile(r\"(^[\\t ]+)\")\n+        code = pattern.sub(\"\", src)\n+\n+        match = pattern.match(src)  # finds indentation\n+        if match:\n+            code = re.sub(f\"\\n{match.group(0)}\", \"\\n\", code)  # remove indentation\n+\n         tree = ast.parse(code)\n         for node in walk_callable(tree):\n             if isinstance(node, ast.Return) and not returns_none(node):\n",
    "test_patch": "diff --git a/tests/test_utils_misc/test_return_with_argument_inside_generator.py b/tests/test_utils_misc/test_return_with_argument_inside_generator.py\nindex 1c85ca35369..72277d70184 100644\n--- a/tests/test_utils_misc/test_return_with_argument_inside_generator.py\n+++ b/tests/test_utils_misc/test_return_with_argument_inside_generator.py\n@@ -165,6 +165,89 @@ def l2():\n             warn_on_generator_with_return_value(None, l2)\n             self.assertEqual(len(w), 0)\n \n+    def test_generators_return_none_with_decorator(self):\n+        def decorator(func):\n+            def inner_func():\n+                func()\n+            return inner_func\n+\n+        @decorator\n+        def f3():\n+            yield 1\n+            return None\n+\n+        @decorator\n+        def g3():\n+            yield 1\n+            return\n+\n+        @decorator\n+        def h3():\n+            yield 1\n+\n+        @decorator\n+        def i3():\n+            yield 1\n+            yield from generator_that_returns_stuff()\n+\n+        @decorator\n+        def j3():\n+            yield 1\n+\n+            def helper():\n+                return 0\n+\n+            yield helper()\n+\n+        @decorator\n+        def k3():\n+            \"\"\"\n+docstring\n+            \"\"\"\n+            url = \"\"\"\n+https://example.org\n+        \"\"\"\n+            yield url\n+            return\n+\n+        @decorator\n+        def l3():\n+            return\n+\n+        assert not is_generator_with_return_value(top_level_return_none)\n+        assert not is_generator_with_return_value(f3)\n+        assert not is_generator_with_return_value(g3)\n+        assert not is_generator_with_return_value(h3)\n+        assert not is_generator_with_return_value(i3)\n+        assert not is_generator_with_return_value(j3)  # not recursive\n+        assert not is_generator_with_return_value(k3)  # not recursive\n+        assert not is_generator_with_return_value(l3)\n+\n+        with warnings.catch_warnings(record=True) as w:\n+            warn_on_generator_with_return_value(None, top_level_return_none)\n+            self.assertEqual(len(w), 0)\n+        with warnings.catch_warnings(record=True) as w:\n+            warn_on_generator_with_return_value(None, f3)\n+            self.assertEqual(len(w), 0)\n+        with warnings.catch_warnings(record=True) as w:\n+            warn_on_generator_with_return_value(None, g3)\n+            self.assertEqual(len(w), 0)\n+        with warnings.catch_warnings(record=True) as w:\n+            warn_on_generator_with_return_value(None, h3)\n+            self.assertEqual(len(w), 0)\n+        with warnings.catch_warnings(record=True) as w:\n+            warn_on_generator_with_return_value(None, i3)\n+            self.assertEqual(len(w), 0)\n+        with warnings.catch_warnings(record=True) as w:\n+            warn_on_generator_with_return_value(None, j3)\n+            self.assertEqual(len(w), 0)\n+        with warnings.catch_warnings(record=True) as w:\n+            warn_on_generator_with_return_value(None, k3)\n+            self.assertEqual(len(w), 0)\n+        with warnings.catch_warnings(record=True) as w:\n+            warn_on_generator_with_return_value(None, l3)\n+            self.assertEqual(len(w), 0)\n+\n     @mock.patch(\"scrapy.utils.misc.is_generator_with_return_value\", new=_indentation_error)\n     def test_indentation_error(self):\n         with warnings.catch_warnings(record=True) as w:\n",
    "problem_statement": "Warning: Unable to determine whether or not callable is a generator with a return value [unable to parse decorated class methods]\n### Description\r\n\r\nIf you have a decorated spider method, `scrapy.utils.misc` [throws a warning](https://github.com/scrapy/scrapy/blob/master/scrapy/utils/misc.py#L240-L264) saying that it cannot determine if the callable is a generator with a return value.\r\n\r\n`ast.parse()` fails [here](https://github.com/scrapy/scrapy/blob/master/scrapy/utils/misc.py#L228-L230) when called with a decorated method.\r\n\r\n### Steps to Reproduce\r\n\r\nI just copied the logic from `misc.py` and used it to analyze a class with the same overall code structure:\r\n\r\n```python\r\nimport re\r\nimport ast\r\nimport inspect\r\n\r\nclass Foo:\r\n    @classmethod\r\n    def func(self):\r\n        \"\"\"\r\n        Description of func\r\n        \"\"\"  \r\n        return\r\n\r\ncode = re.sub(r\"^[\\t ]+\", \"\", inspect.getsource(Foo.func))\r\ntree = ast.parse(code)\r\n```\r\n\r\n```\r\npython3 test.py\r\n> IndentationError: unexpected indent\r\n```\r\n\r\nThe regex replacement isn't accounting for a possible decorator, so the code ends up looking like:\r\n\r\n```\r\n@classmethod\r\n    def func(self):\r\n        \"\"\"\r\n        Description of func\r\n        \"\"\"  \r\n        return\r\n```\r\n\r\n**Expected behavior:** I'd like to be able to use decorated methods without dealing with noisy logs.\r\n\r\n**Actual behavior:** My container logs are filled with tons of warning messages. The only workaround is to avoid the usage of decorators.\r\n\r\n**Reproduces how often:** 100% of the time to my knowledge\r\n\r\n### Versions\r\n\r\n```\r\n$ scrapy version --verbose\r\n\r\nScrapy       : 2.5.1\r\nlxml         : 4.6.4.0\r\nlibxml2      : 2.9.10\r\ncssselect    : 1.1.0\r\nparsel       : 1.6.0\r\nw3lib        : 1.22.0\r\nTwisted      : 21.7.0\r\nPython       : 3.8.2 (default, Dec 21 2020, 15:06:04) - [Clang 12.0.0 (clang-1200.0.32.29)]\r\npyOpenSSL    : 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021)\r\ncryptography : 35.0.0\r\nPlatform     : macOS-10.15.7-x86_64-i386-64bit\r\n```\r\n\r\n### Additional context\r\n\r\nThis is my first time filing a Scrapy issue. I'm happy to add more context if necessary, and apologies in advance if this has already been discussed elsewhere (fwiw I couldn't find anything).\n",
    "hints_text": "When this features was introduced in #3869, it used `dedent`, which would have avoided this issue. In #4935, this was changed to a simple regex, because apparently some people write code like:\r\n\r\n```python\r\nclass Bob:\r\n    def doit(self):\r\n        \"\"\"\r\nthis line is flush left\r\n        \"\"\"\r\n        if True:\r\n            yield 1234\r\n```\r\n\r\nAnyway, to support both decorators and non-indented heredocs, then the regex needs to be made smarter.\n```python test.py\r\nimport re\r\nimport ast\r\nimport inspect\r\n\r\nclass Foo:\r\n    @classmethod\r\n    def func(self):\r\n        \"\"\"\r\n        Description of func\r\n        \"\"\"  \r\n        return\r\n\r\n    @classmethod\r\n    def test(self):\r\n        \"\"\"\r\n    Description of test\r\n        \"\"\"\r\n        return\r\n\r\n\r\ndef parse(func):\r\n    src = inspect.getsource(func)\r\n    pattern = r\"^[\\t ]+\"\r\n    matches = re.findall(pattern, src)  # finds indentation\r\n    code = re.sub(pattern, \"\", src)\r\n    if matches:\r\n        code = re.sub(f\"\\n{matches[0]}\", \"\\n\", code)  # remove indentation\r\n\r\n    tree = ast.parse(code)\r\n\r\nparse(Foo.func)\r\nparse(Foo.test)\r\nparse(parse)\r\n```\r\n\r\nFinding indentation of callable and removing it works.",
    "created_at": "2022-10-25T03:26:33Z",
    "version": "2.7",
    "PASS_TO_PASS": [],
    "FAIL_TO_PASS": [
      "tests/test_utils_misc/test_return_with_argument_inside_generator.py::UtilsMiscPy3TestCase::test_generators_return_none_with_decorator"
    ]
  },
  {
    "repo": "scrapy/scrapy",
    "pull_number": 5689,
    "instance_id": "scrapy__scrapy-5689",
    "issue_numbers": [
      "5685",
      "5685"
    ],
    "base_commit": "92be5ba2572ec14e2580abe12d276e8aa24247b6",
    "patch": "diff --git a/scrapy/utils/defer.py b/scrapy/utils/defer.py\nindex 8fcf31cab54..38aefd6d02a 100644\n--- a/scrapy/utils/defer.py\n+++ b/scrapy/utils/defer.py\n@@ -26,7 +26,7 @@\n from twisted.python.failure import Failure\n \n from scrapy.exceptions import IgnoreRequest\n-from scrapy.utils.reactor import is_asyncio_reactor_installed\n+from scrapy.utils.reactor import is_asyncio_reactor_installed, get_asyncio_event_loop_policy\n \n \n def defer_fail(_failure: Failure) -> Deferred:\n@@ -269,7 +269,8 @@ def deferred_from_coro(o) -> Any:\n             return ensureDeferred(o)\n         else:\n             # wrapping the coroutine into a Future and then into a Deferred, this requires AsyncioSelectorReactor\n-            return Deferred.fromFuture(asyncio.ensure_future(o))\n+            event_loop = get_asyncio_event_loop_policy().get_event_loop()\n+            return Deferred.fromFuture(asyncio.ensure_future(o, loop=event_loop))\n     return o\n \n \n@@ -320,7 +321,8 @@ async def parse(self, response):\n                 d = treq.get('https://example.com/additional')\n                 additional_response = await deferred_to_future(d)\n     \"\"\"\n-    return d.asFuture(asyncio.get_event_loop())\n+    policy = get_asyncio_event_loop_policy()\n+    return d.asFuture(policy.get_event_loop())\n \n \n def maybe_deferred_to_future(d: Deferred) -> Union[Deferred, Future]:\ndiff --git a/scrapy/utils/reactor.py b/scrapy/utils/reactor.py\nindex 652733ce8b3..ddf354d886e 100644\n--- a/scrapy/utils/reactor.py\n+++ b/scrapy/utils/reactor.py\n@@ -51,6 +51,19 @@ def __call__(self):\n         return self._func(*self._a, **self._kw)\n \n \n+def get_asyncio_event_loop_policy():\n+    policy = asyncio.get_event_loop_policy()\n+    if (\n+        sys.version_info >= (3, 8)\n+        and sys.platform == \"win32\"\n+        and not isinstance(policy, asyncio.WindowsSelectorEventLoopPolicy)\n+    ):\n+        policy = asyncio.WindowsSelectorEventLoopPolicy()\n+        asyncio.set_event_loop_policy(policy)\n+\n+    return policy\n+\n+\n def install_reactor(reactor_path, event_loop_path=None):\n     \"\"\"Installs the :mod:`~twisted.internet.reactor` with the specified\n     import path. Also installs the asyncio event loop with the specified import\n@@ -58,16 +71,14 @@ def install_reactor(reactor_path, event_loop_path=None):\n     reactor_class = load_object(reactor_path)\n     if reactor_class is asyncioreactor.AsyncioSelectorReactor:\n         with suppress(error.ReactorAlreadyInstalledError):\n-            if sys.version_info >= (3, 8) and sys.platform == \"win32\":\n-                policy = asyncio.get_event_loop_policy()\n-                if not isinstance(policy, asyncio.WindowsSelectorEventLoopPolicy):\n-                    asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())\n+            policy = get_asyncio_event_loop_policy()\n             if event_loop_path is not None:\n                 event_loop_class = load_object(event_loop_path)\n                 event_loop = event_loop_class()\n                 asyncio.set_event_loop(event_loop)\n             else:\n-                event_loop = asyncio.get_event_loop()\n+                event_loop = policy.get_event_loop()\n+\n             asyncioreactor.install(eventloop=event_loop)\n     else:\n         *module, _ = reactor_path.split(\".\")\n",
    "test_patch": "diff --git a/tests/test_utils_asyncio.py b/tests/test_utils_asyncio.py\nindex 295323e4daa..741c6a5051b 100644\n--- a/tests/test_utils_asyncio.py\n+++ b/tests/test_utils_asyncio.py\n@@ -1,3 +1,4 @@\n+import warnings\n from unittest import TestCase\n \n from pytest import mark\n@@ -13,5 +14,6 @@ def test_is_asyncio_reactor_installed(self):\n         self.assertEqual(is_asyncio_reactor_installed(), self.reactor_pytest == 'asyncio')\n \n     def test_install_asyncio_reactor(self):\n-        # this should do nothing\n-        install_reactor(\"twisted.internet.asyncioreactor.AsyncioSelectorReactor\")\n+        with warnings.catch_warnings(record=True) as w:\n+            install_reactor(\"twisted.internet.asyncioreactor.AsyncioSelectorReactor\")\n+            self.assertEqual(len(w), 0)\n",
    "problem_statement": "DeprecationWarning: There is no current event loop\n```\r\n/home/runner/work/scrapy/scrapy/scrapy/utils/reactor.py:70: DeprecationWarning: There is no current event loop\r\n  event_loop = asyncio.get_event_loop()\r\ntests/test_downloadermiddleware.py: 2 warnings\r\ntests/test_utils_asyncgen.py: 2 warnings\r\ntests/test_utils_defer.py: 4 warnings\r\ntests/test_utils_python.py: 2 warnings\r\ntests/test_utils_signal.py: 2 warnings\r\n  /home/runner/work/scrapy/scrapy/scrapy/utils/defer.py:272: DeprecationWarning: There is no current event loop\r\n    return Deferred.fromFuture(asyncio.ensure_future(o))\r\ntests/test_utils_asyncio.py::AsyncioTest::test_install_asyncio_reactor\r\n  /home/runner/work/scrapy/scrapy/scrapy/utils/reactor.py:70: DeprecationWarning: There is no current event loop\r\n    event_loop = asyncio.get_event_loop()\r\n```\r\n\r\nNot sure yet what does this imply and when can this break (this is from tests on 3.10 and 3.11).\nDeprecationWarning: There is no current event loop\n```\r\n/home/runner/work/scrapy/scrapy/scrapy/utils/reactor.py:70: DeprecationWarning: There is no current event loop\r\n  event_loop = asyncio.get_event_loop()\r\ntests/test_downloadermiddleware.py: 2 warnings\r\ntests/test_utils_asyncgen.py: 2 warnings\r\ntests/test_utils_defer.py: 4 warnings\r\ntests/test_utils_python.py: 2 warnings\r\ntests/test_utils_signal.py: 2 warnings\r\n  /home/runner/work/scrapy/scrapy/scrapy/utils/defer.py:272: DeprecationWarning: There is no current event loop\r\n    return Deferred.fromFuture(asyncio.ensure_future(o))\r\ntests/test_utils_asyncio.py::AsyncioTest::test_install_asyncio_reactor\r\n  /home/runner/work/scrapy/scrapy/scrapy/utils/reactor.py:70: DeprecationWarning: There is no current event loop\r\n    event_loop = asyncio.get_event_loop()\r\n```\r\n\r\nNot sure yet what does this imply and when can this break (this is from tests on 3.10 and 3.11).\n",
    "hints_text": "> asyncio.get_event_loop() now emits a deprecation warning if there is no running event loop. In the future it will be an alias of get_running_loop(). asyncio functions which implicitly create Future or Task objects now emit a deprecation warning if there is no running event loop and no explicit loop argument is passed\ncan I add an exception handling to this then? @wRAR \n@shashwata27 what kind of exception handling?\nMaybe we can just switch to [`get_running_loop()`](https://docs.python.org/3/library/asyncio-eventloop.html#asyncio.get_running_loop), it has been around since Python 3.7, which is our minimum Python version now.\r\n\r\n~~I wonder, though, if there is any case where we can get a `RuntimeError` when using it.~~ Looking at the deprecation warnings, they explicitly say that there is no running event loop, which would trigger a `RuntimeError` with `get_running_loop()`.\nFor:\r\n\r\nhttps://github.com/scrapy/scrapy/blob/82f25bc44acd2599115fa339967b436189eec9c1/scrapy/utils/reactor.py#L70\r\n\r\nA try-except like this may make sense:\r\n\r\n```python\r\ntry:\r\n    event_loop = asyncio.get_running_loop()\r\nexcept RuntimeError:\r\n    event_loop = asyncio.new_event_loop()\r\n```\nThis seems problematic, though:\r\n\r\nhttps://github.com/scrapy/scrapy/blob/52d93490f57d4b2284e3d95efa5c622ab8ad16cb/scrapy/utils/defer.py#L323 \nIn the latter, maybe we could start by switching to `get_running_loop()`, and see what breaks. It should really only be called if there is a running loop, as far as I can tell. If not, maybe we are doing something wrong.\n> asyncio.get_event_loop() now emits a deprecation warning if there is no running event loop. In the future it will be an alias of get_running_loop(). asyncio functions which implicitly create Future or Task objects now emit a deprecation warning if there is no running event loop and no explicit loop argument is passed\ncan I add an exception handling to this then? @wRAR \n@shashwata27 what kind of exception handling?\nMaybe we can just switch to [`get_running_loop()`](https://docs.python.org/3/library/asyncio-eventloop.html#asyncio.get_running_loop), it has been around since Python 3.7, which is our minimum Python version now.\r\n\r\n~~I wonder, though, if there is any case where we can get a `RuntimeError` when using it.~~ Looking at the deprecation warnings, they explicitly say that there is no running event loop, which would trigger a `RuntimeError` with `get_running_loop()`.\nFor:\r\n\r\nhttps://github.com/scrapy/scrapy/blob/82f25bc44acd2599115fa339967b436189eec9c1/scrapy/utils/reactor.py#L70\r\n\r\nA try-except like this may make sense:\r\n\r\n```python\r\ntry:\r\n    event_loop = asyncio.get_running_loop()\r\nexcept RuntimeError:\r\n    event_loop = asyncio.new_event_loop()\r\n```\nThis seems problematic, though:\r\n\r\nhttps://github.com/scrapy/scrapy/blob/52d93490f57d4b2284e3d95efa5c622ab8ad16cb/scrapy/utils/defer.py#L323 \nIn the latter, maybe we could start by switching to `get_running_loop()`, and see what breaks. It should really only be called if there is a running loop, as far as I can tell. If not, maybe we are doing something wrong.",
    "created_at": "2022-10-24T16:54:02Z",
    "version": "2.7",
    "PASS_TO_PASS": [],
    "FAIL_TO_PASS": [
      "tests/test_utils_asyncio.py::AsyncioTest::test_install_asyncio_reactor"
    ]
  },
  {
    "repo": "scrapy/scrapy",
    "pull_number": 5679,
    "instance_id": "scrapy__scrapy-5679",
    "issue_numbers": [
      "5590"
    ],
    "base_commit": "715c05d504d22e87935ae42cee55ee35b12c2ebd",
    "patch": "diff --git a/docs/topics/settings.rst b/docs/topics/settings.rst\nindex a711fd197ab..0b1ef71cfa3 100644\n--- a/docs/topics/settings.rst\n+++ b/docs/topics/settings.rst\n@@ -1642,6 +1642,11 @@ install the default reactor defined by Twisted for the current platform. This\n is to maintain backward compatibility and avoid possible problems caused by\n using a non-default reactor.\n \n+.. versionchanged:: VERSION\n+   The :command:`startproject` command now sets this setting to\n+   ``twisted.internet.asyncioreactor.AsyncioSelectorReactor`` in the generated\n+   ``settings.py`` file.\n+\n For additional information, see :doc:`core/howto/choosing-reactor`.\n \n \ndiff --git a/scrapy/templates/project/module/settings.py.tmpl b/scrapy/templates/project/module/settings.py.tmpl\nindex 5e541e2c0bb..c0c34e986cb 100644\n--- a/scrapy/templates/project/module/settings.py.tmpl\n+++ b/scrapy/templates/project/module/settings.py.tmpl\n@@ -89,3 +89,4 @@ ROBOTSTXT_OBEY = True\n \n # Set settings whose default value is deprecated to a future-proof value\n REQUEST_FINGERPRINTER_IMPLEMENTATION = 'VERSION'\n+TWISTED_REACTOR = 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'\n",
    "test_patch": "diff --git a/tests/test_commands.py b/tests/test_commands.py\nindex 76d5f3935b4..eaca41102b9 100644\n--- a/tests/test_commands.py\n+++ b/tests/test_commands.py\n@@ -689,8 +689,15 @@ def test_asyncio_enabled_true(self):\n         ])\n         self.assertIn(\"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\", log)\n \n-    def test_asyncio_enabled_false(self):\n+    def test_asyncio_enabled_default(self):\n         log = self.get_log(self.debug_log_spider, args=[])\n+        self.assertIn(\"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\", log)\n+\n+    def test_asyncio_enabled_false(self):\n+        log = self.get_log(self.debug_log_spider, args=[\n+            '-s', 'TWISTED_REACTOR=twisted.internet.selectreactor.SelectReactor'\n+        ])\n+        self.assertIn(\"Using reactor: twisted.internet.selectreactor.SelectReactor\", log)\n         self.assertNotIn(\"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\", log)\n \n     @mark.skipif(sys.implementation.name == 'pypy', reason='uvloop does not support pypy properly')\n",
    "problem_statement": "Make asyncio reactor a default\nShould we make 'twisted.internet.asyncioreactor.AsyncioSelectorReactor' a default? If so, should it be default in Scrapy 2.7, or is it too soon? We can start with making it default in the project template; it looks safe enough for Scrapy 2.7, or is it?\r\n\r\nThe reactor was introduced in Scrapy 2.0; I don't recall any particular issues reported about it. It'd be good to have it as a default eventually, because there are more an more Scrapy plugins which rely on asyncio libraries.\n",
    "hints_text": "Are there any big users/projects using the asyncio reactor @kmike?\n@pablohoffman packages like \r\n\r\n* https://github.com/scrapy-plugins/scrapy-playwright/, \r\n* https://github.com/scrapinghub/scrapy-autoextract, \r\n* https://github.com/scrapy-plugins/scrapy-zyte-api \r\n \r\nrequire asyncio reactor; any Scrapy project which uses one of these extensions needs to switch to ascynio reactor.\nRelated: #5103\nThanks. As far as I'm concerned, it makes sense, and changing the default project template is a good way to start.",
    "created_at": "2022-10-14T16:04:51Z",
    "version": "2.6",
    "PASS_TO_PASS": [],
    "FAIL_TO_PASS": [
      "tests/test_commands.py::RunSpiderCommandTest::test_asyncio_enabled_default",
      "tests/test_commands.py::RunSpiderCommandTest::test_asyncio_enabled_false"
    ]
  },
  {
    "repo": "scrapy/scrapy",
    "pull_number": 5611,
    "instance_id": "scrapy__scrapy-5611",
    "issue_numbers": [
      "5601"
    ],
    "base_commit": "52d93490f57d4b2284e3d95efa5c622ab8ad16cb",
    "patch": "diff --git a/scrapy/http/response/text.py b/scrapy/http/response/text.py\nindex 89516b9b63f..bfcde878dd0 100644\n--- a/scrapy/http/response/text.py\n+++ b/scrapy/http/response/text.py\n@@ -11,8 +11,13 @@\n from urllib.parse import urljoin\n \n import parsel\n-from w3lib.encoding import (html_body_declared_encoding, html_to_unicode,\n-                            http_content_type_encoding, resolve_encoding)\n+from w3lib.encoding import (\n+    html_body_declared_encoding,\n+    html_to_unicode,\n+    http_content_type_encoding,\n+    resolve_encoding,\n+    read_bom,\n+)\n from w3lib.html import strip_html5_whitespace\n \n from scrapy.http import Request\n@@ -60,6 +65,7 @@ def encoding(self):\n     def _declared_encoding(self):\n         return (\n             self._encoding\n+            or self._bom_encoding()\n             or self._headers_encoding()\n             or self._body_declared_encoding()\n         )\n@@ -117,6 +123,10 @@ def _auto_detect_fun(self, text):\n     def _body_declared_encoding(self):\n         return html_body_declared_encoding(self.body)\n \n+    @memoizemethod_noargs\n+    def _bom_encoding(self):\n+        return read_bom(self.body)[0]\n+\n     @property\n     def selector(self):\n         from scrapy.selector import Selector\n",
    "test_patch": "diff --git a/tests/test_http_response.py b/tests/test_http_response.py\nindex 2986f884fcd..5d67a5e74cc 100644\n--- a/tests/test_http_response.py\n+++ b/tests/test_http_response.py\n@@ -1,3 +1,4 @@\n+import codecs\n import unittest\n from unittest import mock\n \n@@ -358,6 +359,8 @@ def test_encoding(self):\n                                  headers={\"Content-type\": [\"text/html; charset=gb2312\"]})\n         r7 = self.response_class(\"http://www.example.com\", body=b\"\\xa8D\",\n                                  headers={\"Content-type\": [\"text/html; charset=gbk\"]})\n+        r8 = self.response_class(\"http://www.example.com\", body=codecs.BOM_UTF8 + b\"\\xc2\\xa3\",\n+                                 headers={\"Content-type\": [\"text/html; charset=cp1251\"]})\n \n         self.assertEqual(r1._headers_encoding(), \"utf-8\")\n         self.assertEqual(r2._headers_encoding(), None)\n@@ -367,7 +370,10 @@ def test_encoding(self):\n         self.assertEqual(r3._declared_encoding(), \"cp1252\")\n         self.assertEqual(r4._headers_encoding(), None)\n         self.assertEqual(r5._headers_encoding(), None)\n+        self.assertEqual(r8._headers_encoding(), \"cp1251\")\n+        self.assertEqual(r8._declared_encoding(), \"utf-8\")\n         self._assert_response_encoding(r5, \"utf-8\")\n+        self._assert_response_encoding(r8, \"utf-8\")\n         assert r4._body_inferred_encoding() is not None and r4._body_inferred_encoding() != 'ascii'\n         self._assert_response_values(r1, 'utf-8', \"\\xa3\")\n         self._assert_response_values(r2, 'utf-8', \"\\xa3\")\n",
    "problem_statement": "BOM should take precedence over Content-Type header when detecting the encoding\nCurrently Scrapy uses headers first to detect the encoding. But browsers actually put a higher priority for BOM; this is also in WHATWG [standard](https://html.spec.whatwg.org/multipage/parsing.html#determining-the-character-encoding). It can be checked e.g. by running this server, and opening URL in a browser - UTF-8 is used by browser, but cp1251 is used by Scrapy:\r\n\r\n```py\r\nimport codecs\r\nfrom http.server import BaseHTTPRequestHandler\r\nfrom http.server import HTTPServer\r\n\r\n\r\nclass HttpGetHandler(BaseHTTPRequestHandler):\r\n    def do_GET(self):\r\n        self.send_response(200)\r\n        self.send_header(\"Content-type\", \"text/html; charset=cp1251\")\r\n        self.end_headers()\r\n        self.wfile.write(codecs.BOM_UTF8)\r\n        self.wfile.write(\"<!DOCTYPE html>\".encode('utf8'))\r\n        self.wfile.write(\"\u041f\u0440\u0438\u0432\u0435\u0442!\".encode('utf8'))\r\n\r\n\r\nif __name__ == '__main__':\r\n    httpd = HTTPServer(('', 8000), HttpGetHandler)\r\n    httpd.serve_forever()\r\n```\r\n\r\nWhen opening this page in a browser, it shows \"\u041f\u0440\u0438\u0432\u0435\u0442!\".\r\n\r\nSpider code to check it:\r\n\r\n```py\r\nimport scrapy\r\nfrom scrapy.crawler import CrawlerProcess\r\n\r\n\r\nclass MySpider(scrapy.Spider):\r\n    name = \"tst\"\r\n\r\n    start_urls = [\"http://0.0.0.0:8000\"]\r\n\r\n    def parse(self, response):\r\n        return {\"encoding\": response.encoding, \"text\": response.text}\r\n\r\n\r\nif __name__ == '__main__':\r\n    process = CrawlerProcess()\r\n    process.crawl(MySpider)\r\n    process.start()\r\n```\r\n\r\nSpider outputs\r\n\r\n> {'encoding': 'cp1251', 'text': '\u043f\u00bb\u0457<!DOCTYPE html>\u0420\u045f\u0421\u0402\u0420\u0451\u0420\u0406\u0420\u00b5\u0421\u201a!'}\r\n\r\nSee also: https://github.com/scrapy/w3lib/issues/189 - it's a similar issue, but fixing it in w3lib is not enough to make it working in Scrapy.\n",
    "hints_text": "This is a mistake of target web page.I don't think scrapy could add some methods to detect encoding by checking web content's encoding.For example,if a web page is composed of some strings of  defferent encodings,so which is the right encoding ?\nWe already try to detect the encoding of the page.\nHey @simapple! Scrapy aims to have a behavior which is similar to web browsers.\r\n\r\n> For example,if a web page is composed of some strings of defferent encodings,so which is the right encoding ?\r\n\r\nReal-world browser behavior is documented in various WHATWG standards, so they have an answer for this. This BOM issue is not a random hack which only Scrapy would have, it's something which all browsers do, and something which is described in the standard.\nSorry,I was not very clear about WHATWG standard.Following the standard is always right.",
    "created_at": "2022-08-28T16:01:19Z",
    "version": "2.6",
    "PASS_TO_PASS": [],
    "FAIL_TO_PASS": [
      "tests/test_http_response.py::TextResponseTest::test_encoding"
    ]
  },
  {
    "repo": "scrapy/scrapy",
    "pull_number": 5581,
    "instance_id": "scrapy__scrapy-5581",
    "issue_numbers": [
      "5500"
    ],
    "base_commit": "d60b4edd11436e61284615ec7ce89f8ac7e46d9a",
    "patch": "diff --git a/scrapy/extensions/feedexport.py b/scrapy/extensions/feedexport.py\nindex cd26b577896..da1a88299ec 100644\n--- a/scrapy/extensions/feedexport.py\n+++ b/scrapy/extensions/feedexport.py\n@@ -383,13 +383,20 @@ def close_spider(self, spider):\n         return defer.DeferredList(deferred_list) if deferred_list else None\n \n     def _close_slot(self, slot, spider):\n+        def get_file(slot_):\n+            if isinstance(slot_.file, PostProcessingManager):\n+                slot_.file.close()\n+                return slot_.file.file\n+            return slot_.file\n+\n         slot.finish_exporting()\n         if not slot.itemcount and not slot.store_empty:\n             # We need to call slot.storage.store nonetheless to get the file\n             # properly closed.\n-            return defer.maybeDeferred(slot.storage.store, slot.file)\n+            return defer.maybeDeferred(slot.storage.store, get_file(slot))\n+\n         logmsg = f\"{slot.format} feed ({slot.itemcount} items) in: {slot.uri}\"\n-        d = defer.maybeDeferred(slot.storage.store, slot.file)\n+        d = defer.maybeDeferred(slot.storage.store, get_file(slot))\n \n         d.addCallback(\n             self._handle_store_success, logmsg, spider, type(slot.storage).__name__\n",
    "test_patch": "diff --git a/tests/test_feedexport.py b/tests/test_feedexport.py\nindex eafe1b3342f..3124d9d67b1 100644\n--- a/tests/test_feedexport.py\n+++ b/tests/test_feedexport.py\n@@ -1638,6 +1638,57 @@ def test_extend_kwargs(self):\n             data = yield self.exported_data(items, settings)\n             self.assertEqual(row[\"expected\"], data[feed_options[\"format\"]])\n \n+    @defer.inlineCallbacks\n+    def test_storage_file_no_postprocessing(self):\n+        @implementer(IFeedStorage)\n+        class Storage:\n+            def __init__(self, uri, *, feed_options=None):\n+                pass\n+\n+            def open(self, spider):\n+                Storage.open_file = tempfile.NamedTemporaryFile(prefix=\"feed-\")\n+                return Storage.open_file\n+\n+            def store(self, file):\n+                Storage.store_file = file\n+                file.close()\n+\n+        settings = {\n+            \"FEEDS\": {self._random_temp_filename(): {\"format\": \"jsonlines\"}},\n+            \"FEED_STORAGES\": {\"file\": Storage},\n+        }\n+        yield self.exported_no_data(settings)\n+        self.assertIs(Storage.open_file, Storage.store_file)\n+\n+    @defer.inlineCallbacks\n+    def test_storage_file_postprocessing(self):\n+        @implementer(IFeedStorage)\n+        class Storage:\n+            def __init__(self, uri, *, feed_options=None):\n+                pass\n+\n+            def open(self, spider):\n+                Storage.open_file = tempfile.NamedTemporaryFile(prefix=\"feed-\")\n+                return Storage.open_file\n+\n+            def store(self, file):\n+                Storage.store_file = file\n+                file.close()\n+\n+        settings = {\n+            \"FEEDS\": {\n+                self._random_temp_filename(): {\n+                    \"format\": \"jsonlines\",\n+                    \"postprocessing\": [\n+                        \"scrapy.extensions.postprocessing.GzipPlugin\",\n+                    ],\n+                },\n+            },\n+            \"FEED_STORAGES\": {\"file\": Storage},\n+        }\n+        yield self.exported_no_data(settings)\n+        self.assertIs(Storage.open_file, Storage.store_file)\n+\n \n class FeedPostProcessedExportsTest(FeedExportTestBase):\n     __test__ = True\n",
    "problem_statement": "Postprocessing feeds do not work for S3 feed storage\n<!--\r\n\r\nThanks for taking an interest in Scrapy!\r\n\r\nIf you have a question that starts with \"How to...\", please see the Scrapy Community page: https://scrapy.org/community/.\r\nThe GitHub issue tracker's purpose is to deal with bug reports and feature requests for the project itself.\r\n\r\nKeep in mind that by filing an issue, you are expected to comply with Scrapy's Code of Conduct, including treating everyone with respect: https://github.com/scrapy/scrapy/blob/master/CODE_OF_CONDUCT.md\r\n\r\nThe following is a suggested template to structure your issue, you can find more guidelines at https://doc.scrapy.org/en/latest/contributing.html#reporting-bugs\r\n\r\n-->\r\n\r\n### Description\r\n\r\nExample settings:\r\n```\r\nFEEDS = {\r\n    \"s3://base/file_%(batch_id)05d.gz\": {\r\n        \"format\": \"csv\",\r\n        \"postprocessing\": [GzipPlugin],\r\n        \"gzip_compresslevel\": 5,\r\n    },\r\n}\r\nFEED_EXPORT_BATCH_ITEM_COUNT = 50000\r\n```\r\nThis causes an exception:\r\n\r\n```python\r\n2022-05-14 01:02:12 [scrapy.extensions.feedexport] ERROR: Error storing csv feed (15 items) in: s3://base/file_00001.gz\r\nTraceback (most recent call last):\r\n  File \"/home/mariusz/Documents/Praca/venv_wgsn/lib/python3.10/site-packages/twisted/python/threadpool.py\", line 244, in inContext\r\n    result = inContext.theWork()  # type: ignore[attr-defined]\r\n  File \"/home/mariusz/Documents/Praca/venv_wgsn/lib/python3.10/site-packages/twisted/python/threadpool.py\", line 260, in <lambda>\r\n    inContext.theWork = lambda: context.call(  # type: ignore[attr-defined]\r\n  File \"/home/mariusz/Documents/Praca/venv_wgsn/lib/python3.10/site-packages/twisted/python/context.py\", line 117, in callWithContext\r\n    return self.currentContext().callWithContext(ctx, func, *args, **kw)\r\n  File \"/home/mariusz/Documents/Praca/venv_wgsn/lib/python3.10/site-packages/twisted/python/context.py\", line 82, in callWithContext\r\n    return func(*args, **kw)\r\n  File \"/home/mariusz/Documents/Praca/venv_wgsn/lib/python3.10/site-packages/scrapy/extensions/feedexport.py\", line 194, in _store_in_thread\r\n    file.seek(0)\r\nio.UnsupportedOperation: seek\r\n```\r\nApparently `scrapy.extensions.postprocessing.PostProcessingManager` doesn't fully implement file protocol. Adding this method to the class:\r\n```python\r\n    def seek(self, offset, whence=SEEK_SET):\r\n        return self.file.seek(offset, whence)\r\n```\r\nCause an exception in a different place:\r\n```python\r\nTraceback (most recent call last):\r\n  File \"/home/mariusz/Documents/Praca/venv_wgsn/lib/python3.10/site-packages/twisted/python/threadpool.py\", line 244, in inContext\r\n    result = inContext.theWork()  # type: ignore[attr-defined]\r\n  File \"/home/mariusz/Documents/Praca/venv_wgsn/lib/python3.10/site-packages/twisted/python/threadpool.py\", line 260, in <lambda>\r\n    inContext.theWork = lambda: context.call(  # type: ignore[attr-defined]\r\n  File \"/home/mariusz/Documents/Praca/venv_wgsn/lib/python3.10/site-packages/twisted/python/context.py\", line 117, in callWithContext\r\n    return self.currentContext().callWithContext(ctx, func, *args, **kw)\r\n  File \"/home/mariusz/Documents/Praca/venv_wgsn/lib/python3.10/site-packages/twisted/python/context.py\", line 82, in callWithContext\r\n    return func(*args, **kw)\r\n  File \"/home/mariusz/Documents/Praca/venv_wgsn/lib/python3.10/site-packages/scrapy/extensions/feedexport.py\", line 196, in _store_in_thread\r\n    self.s3_client.put_object(\r\n  File \"/home/mariusz/Documents/Praca/venv_wgsn/lib/python3.10/site-packages/botocore/client.py\", line 395, in _api_call\r\n    return self._make_api_call(operation_name, kwargs)\r\n  File \"/home/mariusz/Documents/Praca/venv_wgsn/lib/python3.10/site-packages/botocore/client.py\", line 695, in _make_api_call\r\n    request_dict = self._convert_to_request_dict(\r\n  File \"/home/mariusz/Documents/Praca/venv_wgsn/lib/python3.10/site-packages/botocore/client.py\", line 745, in _convert_to_request_dict\r\n    request_dict = self._serializer.serialize_to_request(\r\n  File \"/home/mariusz/Documents/Praca/venv_wgsn/lib/python3.10/site-packages/botocore/validate.py\", line 360, in serialize_to_request\r\n    raise ParamValidationError(report=report.generate_report())\r\nbotocore.exceptions.ParamValidationError: Parameter validation failed:\r\nInvalid type for parameter Body, value: <scrapy.extensions.postprocessing.PostProcessingManager object at 0x7f1f245c6920>, type: <class 'scrapy.extensions.postprocessing.PostProcessingManager'>, valid types: <class 'bytes'>, <class 'bytearray'>, file-like object\r\n```\r\nApparently `boto` excepts a `read()` method to be present as well ([here](https://github.com/boto/botocore/blob/develop/botocore/validate.py#L320-L332)).\r\n\r\nTried to add `read()` method to `scrapy.extensions.postprocessing.PostProcessingManager` as well but I only received an incomplete file. I think it's possible because `gzip.GzipFile` use some buffering so it only save full file when `close()` is called on it. Since `S3FeedStorage` uses internally `tempfile.NamedTemporaryFile`, this cause the file to disappear right after creation.\r\n\r\n`PostProcessingManager` needs to be refactored so it can handle `BlockingFeedStorage` correctly.\r\n\r\n### Versions\r\n\r\n```\r\n$ scrapy version --verbose\r\nScrapy       : 2.6.1\r\nlxml         : 4.8.0.0\r\nlibxml2      : 2.9.12\r\ncssselect    : 1.1.0\r\nparsel       : 1.6.0\r\nw3lib        : 1.22.0\r\nTwisted      : 22.2.0\r\nPython       : 3.10.4 (main, May 11 2022, 11:41:05) [GCC 11.0.1 20210417 (experimental) [master revision c1c86ab96c2:b6fb0ccbb48:8ae\r\npyOpenSSL    : 22.0.0 (OpenSSL 1.1.1m  14 Dec 2021)\r\ncryptography : 36.0.1\r\nPlatform     : Linux-5.11.0-16-generic-x86_64-with-glibc2.33\r\n```\r\n\r\n### Additional context\n",
    "hints_text": "I was able to fix this ad hoc for my project. This is what I did: \r\n\r\n1. `scrapy.extensions.postprocessing.PostProcessingManager` must pass all calls to its `seek` and `read` methods to the same methods inside its `file` attribute. \r\n1. In `GzipPlugin` its `gzipfile` attribute must be closed before trying to read form it. \r\n\r\nI was able to monkey patch everything with the following code:\r\n\r\n```python\r\n from scrapy.extensions.postprocessing import PostProcessingManager, GzipPlugin\r\n \r\n \r\n def read(self, *args, **kwargs):\r\n     return self.file.read(*args, **kwargs)\r\n \r\n \r\n def seek(self, *args, **kwargs):\r\n     # Only time seek is executed is when uploading the finished file\r\n     if hasattr(self.head_plugin, \"gzipfile\") and not self.head_plugin.gzipfile.closed:\r\n         self.head_plugin.gzipfile.flush()\r\n         # It should be safe to close at this point\r\n         self.head_plugin.gzipfile.close()\r\n \r\n     return self.file.seek(*args, **kwargs)\r\n \r\n \r\n def close(self):\r\n     # Gzip is already closed by PostProcessingManager.seek\r\n     self.file.close()\r\n \r\n \r\n PostProcessingManager.read = read\r\n PostProcessingManager.seek = seek\r\n GzipPlugin.close = close\r\n```\r\n\r\nHowever, this code assumes only GzipPlugin will be used and seek will only be called right before writting the file to s3. \r\nThis is not a good solution overall. A more generic solution should be designed thinking about every possible combination of all plugins.",
    "created_at": "2022-07-28T21:47:07Z",
    "version": "2.8",
    "PASS_TO_PASS": [],
    "FAIL_TO_PASS": [
      "tests/test_feedexport.py::FeedExportTest::test_storage_file_no_postprocessing",
      "tests/test_feedexport.py::FeedExportTest::test_storage_file_postprocessing"
    ]
  },
  {
    "repo": "scrapy/scrapy",
    "pull_number": 5526,
    "instance_id": "scrapy__scrapy-5526",
    "issue_numbers": [
      "5515"
    ],
    "base_commit": "e2769cfe484fa9bf3d4b36623471cc605393ec85",
    "patch": "diff --git a/scrapy/http/headers.py b/scrapy/http/headers.py\nindex 1a2b99b0a4e..9c03fe54f09 100644\n--- a/scrapy/http/headers.py\n+++ b/scrapy/http/headers.py\n@@ -1,3 +1,5 @@\n+from collections.abc import Mapping\n+\n from w3lib.http import headers_dict_to_raw\n from scrapy.utils.datatypes import CaselessDict\n from scrapy.utils.python import to_unicode\n@@ -10,6 +12,13 @@ def __init__(self, seq=None, encoding='utf-8'):\n         self.encoding = encoding\n         super().__init__(seq)\n \n+    def update(self, seq):\n+        seq = seq.items() if isinstance(seq, Mapping) else seq\n+        iseq = {}\n+        for k, v in seq:\n+            iseq.setdefault(self.normkey(k), []).extend(self.normvalue(v))\n+        super().update(iseq)\n+\n     def normkey(self, key):\n         \"\"\"Normalize key to bytes\"\"\"\n         return self._tobytes(key.title())\n@@ -86,4 +95,5 @@ def to_unicode_dict(self):\n \n     def __copy__(self):\n         return self.__class__(self)\n+\n     copy = __copy__\n",
    "test_patch": "diff --git a/tests/test_http_headers.py b/tests/test_http_headers.py\nindex 64ff7a73dbf..1ca93624794 100644\n--- a/tests/test_http_headers.py\n+++ b/tests/test_http_headers.py\n@@ -38,6 +38,12 @@ def test_multivalue(self):\n         self.assertEqual(h.getlist('X-Forwarded-For'), [b'ip1', b'ip2'])\n         assert h.getlist('X-Forwarded-For') is not hlist\n \n+    def test_multivalue_for_one_header(self):\n+        h = Headers(((\"a\", \"b\"), (\"a\", \"c\")))\n+        self.assertEqual(h[\"a\"], b\"c\")\n+        self.assertEqual(h.get(\"a\"), b\"c\")\n+        self.assertEqual(h.getlist(\"a\"), [b\"b\", b\"c\"])\n+\n     def test_encode_utf8(self):\n         h = Headers({'key': '\\xa3'}, encoding='utf-8')\n         key, val = dict(h).popitem()\n",
    "problem_statement": "Response.headers loses data on multiple values\nhttps://github.com/scrapy/scrapy/issues/1262 reported that by default `response.headers` would only expose the first value of a header e.g. when casted as a `dict`, acknowledging that `response.headers.getlist` could be used instead to get all values.\r\n\r\nI have just found out that the latter is not true:\r\n\r\n```python\r\n>>> from scrapy.http import Response\r\n>>> response = Response(\"https://example.com\", headers=((\"a\", \"b\"), (\"a\", \"c\")))\r\n>>> response.headers.getlist(\"a\")\r\n[b'c']\r\n```\r\n\r\nI could verify the issue happening as far back as Scrapy 1.6, so it does not look like a recent bug.\n",
    "hints_text": "",
    "created_at": "2022-06-11T19:24:16Z",
    "version": "2.6",
    "PASS_TO_PASS": [],
    "FAIL_TO_PASS": [
      "tests/test_http_headers.py::HeadersTest::test_multivalue_for_one_header"
    ]
  },
  {
    "repo": "scrapy/scrapy",
    "pull_number": 6542,
    "instance_id": "scrapy__scrapy-6542",
    "issue_numbers": [
      "6505"
    ],
    "base_commit": "ab5cb7c7d9e268b501009d991d97ca19b6f7fe96",
    "patch": "diff --git a/scrapy/contracts/__init__.py b/scrapy/contracts/__init__.py\nindex 9071395e3d9..3b4f932a014 100644\n--- a/scrapy/contracts/__init__.py\n+++ b/scrapy/contracts/__init__.py\n@@ -38,9 +38,7 @@ def add_pre_hook(self, request: Request, results: TestResult) -> Request:\n             assert cb is not None\n \n             @wraps(cb)\n-            def wrapper(  # pylint: disable=inconsistent-return-statements\n-                response: Response, **cb_kwargs: Any\n-            ) -> list[Any]:\n+            def wrapper(response: Response, **cb_kwargs: Any) -> list[Any]:\n                 try:\n                     results.startTest(self.testcase_pre)\n                     self.pre_process(response)\n@@ -51,13 +49,10 @@ def wrapper(  # pylint: disable=inconsistent-return-statements\n                     results.addError(self.testcase_pre, sys.exc_info())\n                 else:\n                     results.addSuccess(self.testcase_pre)\n-                finally:\n-                    cb_result = cb(response, **cb_kwargs)\n-                    if isinstance(cb_result, (AsyncGenerator, CoroutineType)):\n-                        raise TypeError(\"Contracts don't support async callbacks\")\n-                    return list(  # pylint: disable=return-in-finally\n-                        cast(Iterable[Any], iterate_spider_output(cb_result))\n-                    )\n+                cb_result = cb(response, **cb_kwargs)\n+                if isinstance(cb_result, (AsyncGenerator, CoroutineType)):\n+                    raise TypeError(\"Contracts don't support async callbacks\")\n+                return list(cast(Iterable[Any], iterate_spider_output(cb_result)))\n \n             request.callback = wrapper\n \n@@ -69,9 +64,7 @@ def add_post_hook(self, request: Request, results: TestResult) -> Request:\n             assert cb is not None\n \n             @wraps(cb)\n-            def wrapper(  # pylint: disable=inconsistent-return-statements\n-                response: Response, **cb_kwargs: Any\n-            ) -> list[Any]:\n+            def wrapper(response: Response, **cb_kwargs: Any) -> list[Any]:\n                 cb_result = cb(response, **cb_kwargs)\n                 if isinstance(cb_result, (AsyncGenerator, CoroutineType)):\n                     raise TypeError(\"Contracts don't support async callbacks\")\n@@ -86,8 +79,7 @@ def wrapper(  # pylint: disable=inconsistent-return-statements\n                     results.addError(self.testcase_post, sys.exc_info())\n                 else:\n                     results.addSuccess(self.testcase_post)\n-                finally:\n-                    return output  # pylint: disable=return-in-finally\n+                return output\n \n             request.callback = wrapper\n \n",
    "test_patch": "diff --git a/tests/test_contracts.py b/tests/test_contracts.py\nindex d578b3af450..b0cb92d12d9 100644\n--- a/tests/test_contracts.py\n+++ b/tests/test_contracts.py\n@@ -556,3 +556,61 @@ def test_inherited_contracts(self):\n \n         requests = self.conman.from_spider(spider, self.results)\n         self.assertTrue(requests)\n+\n+\n+class CustomFailContractPreProcess(Contract):\n+    name = \"test_contract\"\n+\n+    def pre_process(self, response):\n+        raise KeyboardInterrupt(\"Pre-process exception\")\n+\n+\n+class CustomFailContractPostProcess(Contract):\n+    name = \"test_contract\"\n+\n+    def post_process(self, response):\n+        raise KeyboardInterrupt(\"Post-process exception\")\n+\n+\n+class CustomContractPrePostProcess(unittest.TestCase):\n+\n+    def setUp(self):\n+        self.results = TextTestResult(stream=None, descriptions=False, verbosity=0)\n+\n+    def test_pre_hook_keyboard_interrupt(self):\n+        spider = TestSpider()\n+        response = ResponseMock()\n+        contract = CustomFailContractPreProcess(spider.returns_request)\n+        conman = ContractsManager([contract])\n+\n+        try:\n+            request = conman.from_method(spider.returns_request, self.results)\n+            contract.add_pre_hook(request, self.results)\n+            # Expect this to raise a KeyboardInterrupt\n+            request.callback(response, **request.cb_kwargs)\n+        except KeyboardInterrupt as e:\n+            self.assertEqual(str(e), \"Pre-process exception\")\n+        else:\n+            self.fail(\"KeyboardInterrupt not raised\")\n+\n+        self.assertFalse(self.results.failures)\n+        self.assertFalse(self.results.errors)\n+\n+    def test_post_hook_keyboard_interrupt(self):\n+        spider = TestSpider()\n+        response = ResponseMock()\n+        contract = CustomFailContractPostProcess(spider.returns_request)\n+        conman = ContractsManager([contract])\n+\n+        try:\n+            request = conman.from_method(spider.returns_request, self.results)\n+            contract.add_post_hook(request, self.results)\n+            # Expect this to raise a KeyboardInterrupt\n+            request.callback(response, **request.cb_kwargs)\n+        except KeyboardInterrupt as e:\n+            self.assertEqual(str(e), \"Post-process exception\")\n+        else:\n+            self.fail(\"KeyboardInterrupt not raised\")\n+\n+        self.assertFalse(self.results.failures)\n+        self.assertFalse(self.results.errors)\n",
    "problem_statement": "return in finally can swallow exception\n### Description\r\n\r\nThere are two places in `scrapy/contracts/__init__.py` where a `finally:` body contains a `return` statement, which would swallow any in-flight exception. \r\n\r\nThis means that if a `BaseException` (such as `KeyboardInterrupt`) is raised from the body, or any exception is raised from one of the `except:` clause, it will not propagate on as expected. \r\n\r\nThe pylint warning about this was suppressed in [this commit](https://github.com/scrapy/scrapy/commit/991121fa91aee4d428ae09e75427d4e91970a41b) but it doesn't seem like there was justification for that.\r\n\r\nThese are the two locations:\r\nhttps://github.com/scrapy/scrapy/blame/b4bad97eae6bcce790a626d244c63589f4134408/scrapy/contracts/__init__.py#L56\r\n\r\nhttps://github.com/scrapy/scrapy/blame/b4bad97eae6bcce790a626d244c63589f4134408/scrapy/contracts/__init__.py#L86\r\n\r\nSee also https://docs.python.org/3/tutorial/errors.html#defining-clean-up-actions.\n",
    "hints_text": "> If the finally clause executes a [break](https://docs.python.org/3/reference/simple_stmts.html#break), [continue](https://docs.python.org/3/reference/simple_stmts.html#continue) or [return](https://docs.python.org/3/reference/simple_stmts.html#return) statement, exceptions are not re-raised.\r\n\r\nTIL\nHey,  \r\nWhat about re-raising the issue with a general raise statement in every except block along with putting the return statement outside the finally block?  \r\nIf this solution seems promising, I'd like to contribute to the same. I would appreciate your insights.\nI don't remember why I silenced them :-/\nIs this issue still open?\n@divyranjan17 it is.\r\n\r\n@AdityaS8804 I don't think that makes sense to me.\nFrom a quick look, it seems to me that simply taking the return part out of finally, and put it after the try-except blocks, could be all that\u2019s needed. But we should also first write tests that detect the issues before we address those issues.\n> From a quick look, it seems to me that simply taking the return part out of finally, and put it after the try-except blocks, could be all that\u2019s needed.\r\n\r\nThis matches my first impression.\nIs there a way to reproduce a this failure?\n> Is there a way to reproduce a this failure?\r\n\r\nFor the first issue, for example, it seems like raising `KeyboardInterrupt` from an implementation of https://docs.scrapy.org/en/2.11/topics/contracts.html#scrapy.contracts.Contract.pre_process should see that exception raise, but will instead silence it.\n\r\nI can think of 3 ways to tackle this issue\r\n\r\n#### Solution 1: Using a Temporary Variable for Return Value\r\nWe can capture the callback result in a variable outside the `finally` block and then return it at the end of the function. By avoiding `return` inside `finally`, exceptions propagate naturally, allowing errors to be handled as expected.\r\n\r\n**Simply:**\r\n\r\n- Store the callback output in a variable (e.g., `cb_result`).\r\n- Avoid using `return` in the `finally` block.\r\n- Return `cb_result` at the end of the function, outside of any `try/finally` structure.\r\n\r\n```python\r\ncb_result = None\r\ntry:\r\n    cb_result = cb(response, **cb_kwargs)\r\nfinally:\r\n    pass  # Any final cleanup here\r\nreturn list(iterate_spider_output(cb_result))\r\n```\r\n\r\n#### Solution 2: Separating Error Logging and Result Processing\r\n- Create a helper function, `log_results()`, to handle logging outcomes.\r\n- Call `log_results()` within `try/except` blocks to process success or errors.\r\n- Return `cb_result` outside of the `try` block without `finally`.\r\n\r\n```python\r\ndef log_results(testcase, exception_info=None):\r\n    if exception_info:\r\n        # Log failure\r\n```\r\n\r\n#### Solution 3: Wrapping Return Values with Exception Handling\r\n- Define `process_result` to manage callback outputs while capturing exceptions.\r\n- Invoke `process_result` instead of a direct return in the callback.\r\n- Ensure all exception info is handled without using a return in `finally`.\r\n\r\n```python\r\ndef process_result(cb, response, **cb_kwargs):\r\n    try:\r\n        cb_result = cb(response, **cb_kwargs)\r\n    except Exception as exc:\r\n        log_error(exc)\r\n    return list(iterate_spider_output(cb_result))\r\n```\r\n\nIs this AI-generated?\nyes Sol.1 and Sol.3 were suggested by github copilot \nThat's unfortunate, especially as the way forward was already suggested earlier.\n[Here](https://github.com/scrapy/scrapy/issues/6505#issuecomment-2434584223), specifically.\n> [Here](https://github.com/scrapy/scrapy/issues/6505#issuecomment-2434584223), specifically.\r\n\r\nUnderstood",
    "created_at": "2024-11-14T03:19:30Z",
    "version": "2.11",
    "PASS_TO_PASS": [],
    "FAIL_TO_PASS": [
      "tests/test_contracts.py"
    ]
  },
  {
    "repo": "scrapy/scrapy",
    "pull_number": 6540,
    "instance_id": "scrapy__scrapy-6540",
    "issue_numbers": [
      "6534"
    ],
    "base_commit": "b042ad255db139adc740cd97047b6607889f9f1c",
    "patch": "diff --git a/docs/topics/email.rst b/docs/topics/email.rst\nindex d6a7ad354cb..8f7a2357a5a 100644\n--- a/docs/topics/email.rst\n+++ b/docs/topics/email.rst\n@@ -27,13 +27,13 @@ the standard ``__init__`` method:\n \n     mailer = MailSender()\n \n-Or you can instantiate it passing a Scrapy settings object, which will respect\n-the :ref:`settings <topics-email-settings>`:\n+Or you can instantiate it passing a :class:`scrapy.Crawler` instance, which\n+will respect the :ref:`settings <topics-email-settings>`:\n \n .. skip: start\n .. code-block:: python\n \n-    mailer = MailSender.from_settings(settings)\n+    mailer = MailSender.from_crawler(crawler)\n \n And here is how to use it to send an e-mail (without attachments):\n \n@@ -81,13 +81,13 @@ rest of the framework.\n     :param smtpssl: enforce using a secure SSL connection\n     :type smtpssl: bool\n \n-    .. classmethod:: from_settings(settings)\n+    .. classmethod:: from_crawler(crawler)\n \n-        Instantiate using a Scrapy settings object, which will respect\n-        :ref:`these Scrapy settings <topics-email-settings>`.\n+        Instantiate using a :class:`scrapy.Crawler` instance, which will\n+        respect :ref:`these Scrapy settings <topics-email-settings>`.\n \n-        :param settings: the e-mail recipients\n-        :type settings: :class:`scrapy.settings.Settings` object\n+        :param crawler: the crawler\n+        :type settings: :class:`scrapy.Crawler` object\n \n     .. method:: send(to, subject, body, cc=None, attachs=(), mimetype='text/plain', charset=None)\n \ndiff --git a/docs/topics/request-response.rst b/docs/topics/request-response.rst\nindex 7c15b67e8f3..710e2e1314e 100644\n--- a/docs/topics/request-response.rst\n+++ b/docs/topics/request-response.rst\n@@ -488,7 +488,7 @@ A request fingerprinter is a class that must implement the following method:\n    :param request: request to fingerprint\n    :type request: scrapy.http.Request\n \n-Additionally, it may also implement the following methods:\n+Additionally, it may also implement the following method:\n \n .. classmethod:: from_crawler(cls, crawler)\n    :noindex:\n@@ -504,13 +504,6 @@ Additionally, it may also implement the following methods:\n    :param crawler: crawler that uses this request fingerprinter\n    :type crawler: :class:`~scrapy.crawler.Crawler` object\n \n-.. classmethod:: from_settings(cls, settings)\n-\n-   If present, and ``from_crawler`` is not defined, this class method is called\n-   to create a request fingerprinter instance from a\n-   :class:`~scrapy.settings.Settings` object. It must return a new instance of\n-   the request fingerprinter.\n-\n .. currentmodule:: scrapy.http\n \n The :meth:`fingerprint` method of the default request fingerprinter,\ndiff --git a/scrapy/core/downloader/contextfactory.py b/scrapy/core/downloader/contextfactory.py\nindex f80f832a706..8e17eab9aa7 100644\n--- a/scrapy/core/downloader/contextfactory.py\n+++ b/scrapy/core/downloader/contextfactory.py\n@@ -21,6 +21,7 @@\n     ScrapyClientTLSOptions,\n     openssl_methods,\n )\n+from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.utils.misc import build_from_crawler, load_object\n \n if TYPE_CHECKING:\n@@ -69,6 +70,31 @@ def from_settings(\n         method: int = SSL.SSLv23_METHOD,\n         *args: Any,\n         **kwargs: Any,\n+    ) -> Self:\n+        warnings.warn(\n+            f\"{cls.__name__}.from_settings() is deprecated, use from_crawler() instead.\",\n+            category=ScrapyDeprecationWarning,\n+            stacklevel=2,\n+        )\n+        return cls._from_settings(settings, method, *args, **kwargs)\n+\n+    @classmethod\n+    def from_crawler(\n+        cls,\n+        crawler: Crawler,\n+        method: int = SSL.SSLv23_METHOD,\n+        *args: Any,\n+        **kwargs: Any,\n+    ) -> Self:\n+        return cls._from_settings(crawler.settings, method, *args, **kwargs)\n+\n+    @classmethod\n+    def _from_settings(\n+        cls,\n+        settings: BaseSettings,\n+        method: int = SSL.SSLv23_METHOD,\n+        *args: Any,\n+        **kwargs: Any,\n     ) -> Self:\n         tls_verbose_logging: bool = settings.getbool(\n             \"DOWNLOADER_CLIENT_TLS_VERBOSE_LOGGING\"\ndiff --git a/scrapy/dupefilters.py b/scrapy/dupefilters.py\nindex d37d2741a48..7b8eea135e7 100644\n--- a/scrapy/dupefilters.py\n+++ b/scrapy/dupefilters.py\n@@ -1,9 +1,11 @@\n from __future__ import annotations\n \n import logging\n+import warnings\n from pathlib import Path\n from typing import TYPE_CHECKING\n \n+from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.utils.job import job_dir\n from scrapy.utils.request import (\n     RequestFingerprinter,\n@@ -26,6 +28,15 @@\n class BaseDupeFilter:\n     @classmethod\n     def from_settings(cls, settings: BaseSettings) -> Self:\n+        warnings.warn(\n+            f\"{cls.__name__}.from_settings() is deprecated, use from_crawler() instead.\",\n+            category=ScrapyDeprecationWarning,\n+            stacklevel=2,\n+        )\n+        return cls()\n+\n+    @classmethod\n+    def from_crawler(cls, crawler: Crawler) -> Self:\n         return cls()\n \n     def request_seen(self, request: Request) -> bool:\n@@ -72,17 +83,31 @@ def from_settings(\n         *,\n         fingerprinter: RequestFingerprinterProtocol | None = None,\n     ) -> Self:\n-        debug = settings.getbool(\"DUPEFILTER_DEBUG\")\n-        return cls(job_dir(settings), debug, fingerprinter=fingerprinter)\n+        warnings.warn(\n+            f\"{cls.__name__}.from_settings() is deprecated, use from_crawler() instead.\",\n+            category=ScrapyDeprecationWarning,\n+            stacklevel=2,\n+        )\n+        return cls._from_settings(settings, fingerprinter=fingerprinter)\n \n     @classmethod\n     def from_crawler(cls, crawler: Crawler) -> Self:\n         assert crawler.request_fingerprinter\n-        return cls.from_settings(\n+        return cls._from_settings(\n             crawler.settings,\n             fingerprinter=crawler.request_fingerprinter,\n         )\n \n+    @classmethod\n+    def _from_settings(\n+        cls,\n+        settings: BaseSettings,\n+        *,\n+        fingerprinter: RequestFingerprinterProtocol | None = None,\n+    ) -> Self:\n+        debug = settings.getbool(\"DUPEFILTER_DEBUG\")\n+        return cls(job_dir(settings), debug, fingerprinter=fingerprinter)\n+\n     def request_seen(self, request: Request) -> bool:\n         fp = self.request_fingerprint(request)\n         if fp in self.fingerprints:\ndiff --git a/scrapy/extensions/feedexport.py b/scrapy/extensions/feedexport.py\nindex 6ab88dbb467..27f0b79ae01 100644\n--- a/scrapy/extensions/feedexport.py\n+++ b/scrapy/extensions/feedexport.py\n@@ -62,6 +62,11 @@ def build_storage(\n     preargs: Iterable[Any] = (),\n     **kwargs: Any,\n ) -> _StorageT:\n+    warnings.warn(\n+        \"scrapy.extensions.feedexport.build_storage() is deprecated, call the builder directly.\",\n+        category=ScrapyDeprecationWarning,\n+        stacklevel=2,\n+    )\n     kwargs[\"feed_options\"] = feed_options\n     return builder(*preargs, uri, *args, **kwargs)\n \n@@ -248,8 +253,7 @@ def from_crawler(\n         *,\n         feed_options: dict[str, Any] | None = None,\n     ) -> Self:\n-        return build_storage(\n-            cls,\n+        return cls(\n             uri,\n             access_key=crawler.settings[\"AWS_ACCESS_KEY_ID\"],\n             secret_key=crawler.settings[\"AWS_SECRET_ACCESS_KEY\"],\n@@ -323,10 +327,9 @@ def from_crawler(\n         *,\n         feed_options: dict[str, Any] | None = None,\n     ) -> Self:\n-        return build_storage(\n-            cls,\n+        return cls(\n             uri,\n-            crawler.settings.getbool(\"FEED_STORAGE_FTP_ACTIVE\"),\n+            use_active_mode=crawler.settings.getbool(\"FEED_STORAGE_FTP_ACTIVE\"),\n             feed_options=feed_options,\n         )\n \n@@ -407,15 +410,12 @@ def start_exporting(self) -> None:\n             self.exporter.start_exporting()\n             self._exporting = True\n \n-    def _get_instance(\n-        self, objcls: type[BaseItemExporter], *args: Any, **kwargs: Any\n-    ) -> BaseItemExporter:\n-        return build_from_crawler(objcls, self.crawler, *args, **kwargs)\n-\n     def _get_exporter(\n         self, file: IO[bytes], format: str, *args: Any, **kwargs: Any\n     ) -> BaseItemExporter:\n-        return self._get_instance(self.exporters[format], file, *args, **kwargs)\n+        return build_from_crawler(\n+            self.exporters[format], self.crawler, file, *args, **kwargs\n+        )\n \n     def finish_exporting(self) -> None:\n         if self._exporting:\n@@ -692,34 +692,8 @@ def _storage_supported(self, uri: str, feed_options: dict[str, Any]) -> bool:\n     def _get_storage(\n         self, uri: str, feed_options: dict[str, Any]\n     ) -> FeedStorageProtocol:\n-        \"\"\"Fork of create_instance specific to feed storage classes\n-\n-        It supports not passing the *feed_options* parameters to classes that\n-        do not support it, and issuing a deprecation warning instead.\n-        \"\"\"\n         feedcls = self.storages.get(urlparse(uri).scheme, self.storages[\"file\"])\n-        crawler = getattr(self, \"crawler\", None)\n-\n-        def build_instance(\n-            builder: type[FeedStorageProtocol], *preargs: Any\n-        ) -> FeedStorageProtocol:\n-            return build_storage(\n-                builder, uri, feed_options=feed_options, preargs=preargs\n-            )\n-\n-        instance: FeedStorageProtocol\n-        if crawler and hasattr(feedcls, \"from_crawler\"):\n-            instance = build_instance(feedcls.from_crawler, crawler)\n-            method_name = \"from_crawler\"\n-        elif hasattr(feedcls, \"from_settings\"):\n-            instance = build_instance(feedcls.from_settings, self.settings)\n-            method_name = \"from_settings\"\n-        else:\n-            instance = build_instance(feedcls)\n-            method_name = \"__new__\"\n-        if instance is None:\n-            raise TypeError(f\"{feedcls.__qualname__}.{method_name} returned None\")\n-        return instance\n+        return build_from_crawler(feedcls, self.crawler, uri, feed_options=feed_options)\n \n     def _get_uri_params(\n         self,\ndiff --git a/scrapy/extensions/memusage.py b/scrapy/extensions/memusage.py\nindex 73d864d5dc1..d7f810107bd 100644\n--- a/scrapy/extensions/memusage.py\n+++ b/scrapy/extensions/memusage.py\n@@ -48,7 +48,7 @@ def __init__(self, crawler: Crawler):\n         self.check_interval: float = crawler.settings.getfloat(\n             \"MEMUSAGE_CHECK_INTERVAL_SECONDS\"\n         )\n-        self.mail: MailSender = MailSender.from_settings(crawler.settings)\n+        self.mail: MailSender = MailSender.from_crawler(crawler)\n         crawler.signals.connect(self.engine_started, signal=signals.engine_started)\n         crawler.signals.connect(self.engine_stopped, signal=signals.engine_stopped)\n \ndiff --git a/scrapy/extensions/statsmailer.py b/scrapy/extensions/statsmailer.py\nindex 600eebcf2de..22162864205 100644\n--- a/scrapy/extensions/statsmailer.py\n+++ b/scrapy/extensions/statsmailer.py\n@@ -33,7 +33,7 @@ def from_crawler(cls, crawler: Crawler) -> Self:\n         recipients: list[str] = crawler.settings.getlist(\"STATSMAILER_RCPTS\")\n         if not recipients:\n             raise NotConfigured\n-        mail: MailSender = MailSender.from_settings(crawler.settings)\n+        mail: MailSender = MailSender.from_crawler(crawler)\n         assert crawler.stats\n         o = cls(crawler.stats, recipients, mail)\n         crawler.signals.connect(o.spider_closed, signal=signals.spider_closed)\ndiff --git a/scrapy/mail.py b/scrapy/mail.py\nindex ce7beb77307..3c40fea34c6 100644\n--- a/scrapy/mail.py\n+++ b/scrapy/mail.py\n@@ -7,6 +7,7 @@\n from __future__ import annotations\n \n import logging\n+import warnings\n from email import encoders as Encoders\n from email.mime.base import MIMEBase\n from email.mime.multipart import MIMEMultipart\n@@ -19,6 +20,7 @@\n from twisted.internet import ssl\n from twisted.internet.defer import Deferred\n \n+from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.utils.misc import arg_to_iter\n from scrapy.utils.python import to_bytes\n \n@@ -32,6 +34,7 @@\n     # typing.Self requires Python 3.11\n     from typing_extensions import Self\n \n+    from scrapy.crawler import Crawler\n     from scrapy.settings import BaseSettings\n \n \n@@ -72,6 +75,19 @@ def __init__(\n \n     @classmethod\n     def from_settings(cls, settings: BaseSettings) -> Self:\n+        warnings.warn(\n+            f\"{cls.__name__}.from_settings() is deprecated, use from_crawler() instead.\",\n+            category=ScrapyDeprecationWarning,\n+            stacklevel=2,\n+        )\n+        return cls._from_settings(settings)\n+\n+    @classmethod\n+    def from_crawler(cls, crawler: Crawler) -> Self:\n+        return cls._from_settings(crawler.settings)\n+\n+    @classmethod\n+    def _from_settings(cls, settings: BaseSettings) -> Self:\n         return cls(\n             smtphost=settings[\"MAIL_HOST\"],\n             mailfrom=settings[\"MAIL_FROM\"],\ndiff --git a/scrapy/middleware.py b/scrapy/middleware.py\nindex b6a4278952b..2b67dcd21a1 100644\n--- a/scrapy/middleware.py\n+++ b/scrapy/middleware.py\n@@ -2,12 +2,13 @@\n \n import logging\n import pprint\n+import warnings\n from collections import defaultdict, deque\n from typing import TYPE_CHECKING, Any, TypeVar, cast\n \n-from scrapy.exceptions import NotConfigured\n+from scrapy.exceptions import NotConfigured, ScrapyDeprecationWarning\n from scrapy.utils.defer import process_chain, process_parallel\n-from scrapy.utils.misc import build_from_crawler, build_from_settings, load_object\n+from scrapy.utils.misc import build_from_crawler, load_object\n \n if TYPE_CHECKING:\n     from collections.abc import Callable, Iterable\n@@ -20,7 +21,7 @@\n \n     from scrapy import Spider\n     from scrapy.crawler import Crawler\n-    from scrapy.settings import Settings\n+    from scrapy.settings import BaseSettings, Settings\n \n     _P = ParamSpec(\"_P\")\n \n@@ -50,8 +51,33 @@ def __init__(self, *middlewares: Any) -> None:\n     def _get_mwlist_from_settings(cls, settings: Settings) -> list[Any]:\n         raise NotImplementedError\n \n+    @staticmethod\n+    def _build_from_settings(objcls: type[_T], settings: BaseSettings) -> _T:\n+        if hasattr(objcls, \"from_settings\"):\n+            instance = objcls.from_settings(settings)  # type: ignore[attr-defined]\n+            method_name = \"from_settings\"\n+        else:\n+            instance = objcls()\n+            method_name = \"__new__\"\n+        if instance is None:\n+            raise TypeError(f\"{objcls.__qualname__}.{method_name} returned None\")\n+        return cast(_T, instance)\n+\n     @classmethod\n     def from_settings(cls, settings: Settings, crawler: Crawler | None = None) -> Self:\n+        warnings.warn(\n+            f\"{cls.__name__}.from_settings() is deprecated, use from_crawler() instead.\",\n+            category=ScrapyDeprecationWarning,\n+            stacklevel=2,\n+        )\n+        return cls._from_settings(settings, crawler)\n+\n+    @classmethod\n+    def from_crawler(cls, crawler: Crawler) -> Self:\n+        return cls._from_settings(crawler.settings, crawler)\n+\n+    @classmethod\n+    def _from_settings(cls, settings: Settings, crawler: Crawler | None = None) -> Self:\n         mwlist = cls._get_mwlist_from_settings(settings)\n         middlewares = []\n         enabled = []\n@@ -61,7 +87,7 @@ def from_settings(cls, settings: Settings, crawler: Crawler | None = None) -> Se\n                 if crawler is not None:\n                     mw = build_from_crawler(mwcls, crawler)\n                 else:\n-                    mw = build_from_settings(mwcls, settings)\n+                    mw = MiddlewareManager._build_from_settings(mwcls, settings)\n                 middlewares.append(mw)\n                 enabled.append(clspath)\n             except NotConfigured as e:\n@@ -82,10 +108,6 @@ def from_settings(cls, settings: Settings, crawler: Crawler | None = None) -> Se\n         )\n         return cls(*middlewares)\n \n-    @classmethod\n-    def from_crawler(cls, crawler: Crawler) -> Self:\n-        return cls.from_settings(crawler.settings, crawler)\n-\n     def _add_middleware(self, mw: Any) -> None:\n         if hasattr(mw, \"open_spider\"):\n             self.methods[\"open_spider\"].append(mw.open_spider)\ndiff --git a/scrapy/pipelines/files.py b/scrapy/pipelines/files.py\nindex 4a8639c220b..196b54acb7f 100644\n--- a/scrapy/pipelines/files.py\n+++ b/scrapy/pipelines/files.py\n@@ -12,6 +12,7 @@\n import logging\n import mimetypes\n import time\n+import warnings\n from collections import defaultdict\n from contextlib import suppress\n from ftplib import FTP\n@@ -24,16 +25,17 @@\n from twisted.internet.defer import Deferred, maybeDeferred\n from twisted.internet.threads import deferToThread\n \n-from scrapy.exceptions import IgnoreRequest, NotConfigured\n+from scrapy.exceptions import IgnoreRequest, NotConfigured, ScrapyDeprecationWarning\n from scrapy.http import Request, Response\n from scrapy.http.request import NO_CALLBACK\n from scrapy.pipelines.media import FileInfo, FileInfoOrError, MediaPipeline\n-from scrapy.settings import Settings\n+from scrapy.settings import BaseSettings, Settings\n from scrapy.utils.boto import is_botocore_available\n from scrapy.utils.datatypes import CaseInsensitiveDict\n+from scrapy.utils.deprecate import method_is_overridden\n from scrapy.utils.ftp import ftp_store_file\n from scrapy.utils.log import failure_to_exc_info\n-from scrapy.utils.python import to_bytes\n+from scrapy.utils.python import get_func_args, global_object_name, to_bytes\n from scrapy.utils.request import referer_str\n \n if TYPE_CHECKING:\n@@ -46,6 +48,7 @@\n     from typing_extensions import Self\n \n     from scrapy import Spider\n+    from scrapy.crawler import Crawler\n \n \n logger = logging.getLogger(__name__)\n@@ -443,12 +446,24 @@ def __init__(\n         store_uri: str | PathLike[str],\n         download_func: Callable[[Request, Spider], Response] | None = None,\n         settings: Settings | dict[str, Any] | None = None,\n+        *,\n+        crawler: Crawler | None = None,\n     ):\n         store_uri = _to_string(store_uri)\n         if not store_uri:\n             raise NotConfigured\n \n-        if isinstance(settings, dict) or settings is None:\n+        if crawler is not None:\n+            if settings is not None:\n+                warnings.warn(\n+                    f\"FilesPipeline.__init__() was called with a crawler instance and a settings instance\"\n+                    f\" when creating {global_object_name(self.__class__)}. The settings instance will be ignored\"\n+                    f\" and crawler.settings will be used. The settings argument will be removed in a future Scrapy version.\",\n+                    category=ScrapyDeprecationWarning,\n+                    stacklevel=2,\n+                )\n+            settings = crawler.settings\n+        elif isinstance(settings, dict) or settings is None:\n             settings = Settings(settings)\n         cls_name = \"FilesPipeline\"\n         self.store: FilesStoreProtocol = self._get_store(store_uri)\n@@ -467,10 +482,54 @@ def __init__(\n             resolve(\"FILES_RESULT_FIELD\"), self.FILES_RESULT_FIELD\n         )\n \n-        super().__init__(download_func=download_func, settings=settings)\n+        super().__init__(\n+            download_func=download_func,\n+            settings=settings if not crawler else None,\n+            crawler=crawler,\n+        )\n \n     @classmethod\n     def from_settings(cls, settings: Settings) -> Self:\n+        warnings.warn(\n+            f\"{cls.__name__}.from_settings() is deprecated, use from_crawler() instead.\",\n+            category=ScrapyDeprecationWarning,\n+            stacklevel=2,\n+        )\n+        return cls._from_settings(settings, None)\n+\n+    @classmethod\n+    def from_crawler(cls, crawler: Crawler) -> Self:\n+        if method_is_overridden(cls, FilesPipeline, \"from_settings\"):\n+            warnings.warn(\n+                f\"{global_object_name(cls)} overrides FilesPipeline.from_settings().\"\n+                f\" This method is deprecated and won't be called in future Scrapy versions,\"\n+                f\" please update your code so that it overrides from_crawler() instead.\",\n+                category=ScrapyDeprecationWarning,\n+            )\n+            o = cls.from_settings(crawler.settings)\n+            o._finish_init(crawler)\n+            return o\n+        return cls._from_settings(crawler.settings, crawler)\n+\n+    @classmethod\n+    def _from_settings(cls, settings: Settings, crawler: Crawler | None) -> Self:\n+        cls._update_stores(settings)\n+        store_uri = settings[\"FILES_STORE\"]\n+        if \"crawler\" in get_func_args(cls.__init__):\n+            o = cls(store_uri, crawler=crawler)\n+        else:\n+            o = cls(store_uri, settings=settings)\n+            if crawler:\n+                o._finish_init(crawler)\n+            warnings.warn(\n+                f\"{global_object_name(cls)}.__init__() doesn't take a crawler argument.\"\n+                \" This is deprecated and the argument will be required in future Scrapy versions.\",\n+                category=ScrapyDeprecationWarning,\n+            )\n+        return o\n+\n+    @classmethod\n+    def _update_stores(cls, settings: BaseSettings) -> None:\n         s3store: type[S3FilesStore] = cast(type[S3FilesStore], cls.STORE_SCHEMES[\"s3\"])\n         s3store.AWS_ACCESS_KEY_ID = settings[\"AWS_ACCESS_KEY_ID\"]\n         s3store.AWS_SECRET_ACCESS_KEY = settings[\"AWS_SECRET_ACCESS_KEY\"]\n@@ -494,9 +553,6 @@ def from_settings(cls, settings: Settings) -> Self:\n         ftp_store.FTP_PASSWORD = settings[\"FTP_PASSWORD\"]\n         ftp_store.USE_ACTIVE_MODE = settings.getbool(\"FEED_STORAGE_FTP_ACTIVE\")\n \n-        store_uri = settings[\"FILES_STORE\"]\n-        return cls(store_uri, settings=settings)\n-\n     def _get_store(self, uri: str) -> FilesStoreProtocol:\n         if Path(uri).is_absolute():  # to support win32 paths like: C:\\\\some\\dir\n             scheme = \"file\"\ndiff --git a/scrapy/pipelines/images.py b/scrapy/pipelines/images.py\nindex 2c4c9376e49..e86e7c4930e 100644\n--- a/scrapy/pipelines/images.py\n+++ b/scrapy/pipelines/images.py\n@@ -8,25 +8,19 @@\n \n import functools\n import hashlib\n+import warnings\n from contextlib import suppress\n from io import BytesIO\n-from typing import TYPE_CHECKING, Any, cast\n+from typing import TYPE_CHECKING, Any\n \n from itemadapter import ItemAdapter\n \n-from scrapy.exceptions import NotConfigured\n+from scrapy.exceptions import NotConfigured, ScrapyDeprecationWarning\n from scrapy.http import Request, Response\n from scrapy.http.request import NO_CALLBACK\n-from scrapy.pipelines.files import (\n-    FileException,\n-    FilesPipeline,\n-    FTPFilesStore,\n-    GCSFilesStore,\n-    S3FilesStore,\n-    _md5sum,\n-)\n+from scrapy.pipelines.files import FileException, FilesPipeline, _md5sum\n from scrapy.settings import Settings\n-from scrapy.utils.python import to_bytes\n+from scrapy.utils.python import get_func_args, global_object_name, to_bytes\n \n if TYPE_CHECKING:\n     from collections.abc import Callable, Iterable\n@@ -38,6 +32,7 @@\n     from typing_extensions import Self\n \n     from scrapy import Spider\n+    from scrapy.crawler import Crawler\n     from scrapy.pipelines.media import FileInfoOrError, MediaPipeline\n \n \n@@ -64,6 +59,8 @@ def __init__(\n         store_uri: str | PathLike[str],\n         download_func: Callable[[Request, Spider], Response] | None = None,\n         settings: Settings | dict[str, Any] | None = None,\n+        *,\n+        crawler: Crawler | None = None,\n     ):\n         try:\n             from PIL import Image\n@@ -74,9 +71,24 @@ def __init__(\n                 \"ImagesPipeline requires installing Pillow 4.0.0 or later\"\n             )\n \n-        super().__init__(store_uri, settings=settings, download_func=download_func)\n+        super().__init__(\n+            store_uri,\n+            settings=settings if not crawler else None,\n+            download_func=download_func,\n+            crawler=crawler,\n+        )\n \n-        if isinstance(settings, dict) or settings is None:\n+        if crawler is not None:\n+            if settings is not None:\n+                warnings.warn(\n+                    f\"ImagesPipeline.__init__() was called with a crawler instance and a settings instance\"\n+                    f\" when creating {global_object_name(self.__class__)}. The settings instance will be ignored\"\n+                    f\" and crawler.settings will be used. The settings argument will be removed in a future Scrapy version.\",\n+                    category=ScrapyDeprecationWarning,\n+                    stacklevel=2,\n+                )\n+            settings = crawler.settings\n+        elif isinstance(settings, dict) or settings is None:\n             settings = Settings(settings)\n \n         resolve = functools.partial(\n@@ -108,32 +120,21 @@ def __init__(\n         )\n \n     @classmethod\n-    def from_settings(cls, settings: Settings) -> Self:\n-        s3store: type[S3FilesStore] = cast(type[S3FilesStore], cls.STORE_SCHEMES[\"s3\"])\n-        s3store.AWS_ACCESS_KEY_ID = settings[\"AWS_ACCESS_KEY_ID\"]\n-        s3store.AWS_SECRET_ACCESS_KEY = settings[\"AWS_SECRET_ACCESS_KEY\"]\n-        s3store.AWS_SESSION_TOKEN = settings[\"AWS_SESSION_TOKEN\"]\n-        s3store.AWS_ENDPOINT_URL = settings[\"AWS_ENDPOINT_URL\"]\n-        s3store.AWS_REGION_NAME = settings[\"AWS_REGION_NAME\"]\n-        s3store.AWS_USE_SSL = settings[\"AWS_USE_SSL\"]\n-        s3store.AWS_VERIFY = settings[\"AWS_VERIFY\"]\n-        s3store.POLICY = settings[\"IMAGES_STORE_S3_ACL\"]\n-\n-        gcs_store: type[GCSFilesStore] = cast(\n-            type[GCSFilesStore], cls.STORE_SCHEMES[\"gs\"]\n-        )\n-        gcs_store.GCS_PROJECT_ID = settings[\"GCS_PROJECT_ID\"]\n-        gcs_store.POLICY = settings[\"IMAGES_STORE_GCS_ACL\"] or None\n-\n-        ftp_store: type[FTPFilesStore] = cast(\n-            type[FTPFilesStore], cls.STORE_SCHEMES[\"ftp\"]\n-        )\n-        ftp_store.FTP_USERNAME = settings[\"FTP_USER\"]\n-        ftp_store.FTP_PASSWORD = settings[\"FTP_PASSWORD\"]\n-        ftp_store.USE_ACTIVE_MODE = settings.getbool(\"FEED_STORAGE_FTP_ACTIVE\")\n-\n+    def _from_settings(cls, settings: Settings, crawler: Crawler | None) -> Self:\n+        cls._update_stores(settings)\n         store_uri = settings[\"IMAGES_STORE\"]\n-        return cls(store_uri, settings=settings)\n+        if \"crawler\" in get_func_args(cls.__init__):\n+            o = cls(store_uri, crawler=crawler)\n+        else:\n+            o = cls(store_uri, settings=settings)\n+            if crawler:\n+                o._finish_init(crawler)\n+            warnings.warn(\n+                f\"{global_object_name(cls)}.__init__() doesn't take a crawler argument.\"\n+                \" This is deprecated and the argument will be required in future Scrapy versions.\",\n+                category=ScrapyDeprecationWarning,\n+            )\n+        return o\n \n     def file_downloaded(\n         self,\ndiff --git a/scrapy/pipelines/media.py b/scrapy/pipelines/media.py\nindex b10ec147b34..6d7808c31b4 100644\n--- a/scrapy/pipelines/media.py\n+++ b/scrapy/pipelines/media.py\n@@ -2,6 +2,7 @@\n \n import functools\n import logging\n+import warnings\n from abc import ABC, abstractmethod\n from collections import defaultdict\n from typing import (\n@@ -20,12 +21,14 @@\n from twisted.python.failure import Failure\n from twisted.python.versions import Version\n \n+from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.http.request import NO_CALLBACK, Request\n from scrapy.settings import Settings\n from scrapy.utils.datatypes import SequenceExclude\n from scrapy.utils.defer import defer_result, mustbe_deferred\n from scrapy.utils.log import failure_to_exc_info\n from scrapy.utils.misc import arg_to_iter\n+from scrapy.utils.python import get_func_args, global_object_name\n \n if TYPE_CHECKING:\n     from collections.abc import Callable\n@@ -38,7 +41,6 @@\n     from scrapy.http import Response\n     from scrapy.utils.request import RequestFingerprinter\n \n-\n _T = TypeVar(\"_T\")\n \n \n@@ -51,13 +53,13 @@ class FileInfo(TypedDict):\n \n FileInfoOrError = Union[tuple[Literal[True], FileInfo], tuple[Literal[False], Failure]]\n \n-\n logger = logging.getLogger(__name__)\n \n \n class MediaPipeline(ABC):\n     crawler: Crawler\n     _fingerprinter: RequestFingerprinter\n+    _modern_init = False\n \n     LOG_FAILED_RESULTS: bool = True\n \n@@ -74,10 +76,22 @@ def __init__(\n         self,\n         download_func: Callable[[Request, Spider], Response] | None = None,\n         settings: Settings | dict[str, Any] | None = None,\n+        *,\n+        crawler: Crawler | None = None,\n     ):\n         self.download_func = download_func\n \n-        if isinstance(settings, dict) or settings is None:\n+        if crawler is not None:\n+            if settings is not None:\n+                warnings.warn(\n+                    f\"MediaPipeline.__init__() was called with a crawler instance and a settings instance\"\n+                    f\" when creating {global_object_name(self.__class__)}. The settings instance will be ignored\"\n+                    f\" and crawler.settings will be used. The settings argument will be removed in a future Scrapy version.\",\n+                    category=ScrapyDeprecationWarning,\n+                    stacklevel=2,\n+                )\n+            settings = crawler.settings\n+        elif isinstance(settings, dict) or settings is None:\n             settings = Settings(settings)\n         resolve = functools.partial(\n             self._key_for_pipe, base_class_name=\"MediaPipeline\", settings=settings\n@@ -87,6 +101,27 @@ def __init__(\n         )\n         self._handle_statuses(self.allow_redirects)\n \n+        if crawler:\n+            self._finish_init(crawler)\n+            self._modern_init = True\n+        else:\n+            warnings.warn(\n+                f\"MediaPipeline.__init__() was called without the crawler argument\"\n+                f\" when creating {global_object_name(self.__class__)}.\"\n+                f\" This is deprecated and the argument will be required in future Scrapy versions.\",\n+                category=ScrapyDeprecationWarning,\n+                stacklevel=2,\n+            )\n+\n+    def _finish_init(self, crawler: Crawler) -> None:\n+        # This was done in from_crawler() before 2.12, now it's done in __init__()\n+        # if the crawler was passed to it and may be needed to be called in other\n+        # deprecated code paths explicitly too. After the crawler argument of __init__()\n+        # becomes mandatory this should be inlined there.\n+        self.crawler = crawler\n+        assert crawler.request_fingerprinter\n+        self._fingerprinter = crawler.request_fingerprinter\n+\n     def _handle_statuses(self, allow_redirects: bool) -> None:\n         self.handle_httpstatus_list = None\n         if allow_redirects:\n@@ -112,13 +147,19 @@ def _key_for_pipe(\n     @classmethod\n     def from_crawler(cls, crawler: Crawler) -> Self:\n         pipe: Self\n-        try:\n+        if hasattr(cls, \"from_settings\"):\n             pipe = cls.from_settings(crawler.settings)  # type: ignore[attr-defined]\n-        except AttributeError:\n+        elif \"crawler\" in get_func_args(cls.__init__):\n+            pipe = cls(crawler=crawler)\n+        else:\n             pipe = cls()\n-        pipe.crawler = crawler\n-        assert crawler.request_fingerprinter\n-        pipe._fingerprinter = crawler.request_fingerprinter\n+            warnings.warn(\n+                f\"{global_object_name(cls)}.__init__() doesn't take a crawler argument.\"\n+                \" This is deprecated and the argument will be required in future Scrapy versions.\",\n+                category=ScrapyDeprecationWarning,\n+            )\n+        if not pipe._modern_init:\n+            pipe._finish_init(crawler)\n         return pipe\n \n     def open_spider(self, spider: Spider) -> None:\ndiff --git a/scrapy/spidermiddlewares/urllength.py b/scrapy/spidermiddlewares/urllength.py\nindex 191adb6cd32..a1cd1bb7cfa 100644\n--- a/scrapy/spidermiddlewares/urllength.py\n+++ b/scrapy/spidermiddlewares/urllength.py\n@@ -7,9 +7,10 @@\n from __future__ import annotations\n \n import logging\n+import warnings\n from typing import TYPE_CHECKING, Any\n \n-from scrapy.exceptions import NotConfigured\n+from scrapy.exceptions import NotConfigured, ScrapyDeprecationWarning\n from scrapy.http import Request, Response\n \n if TYPE_CHECKING:\n@@ -19,6 +20,7 @@\n     from typing_extensions import Self\n \n     from scrapy import Spider\n+    from scrapy.crawler import Crawler\n     from scrapy.settings import BaseSettings\n \n \n@@ -31,6 +33,19 @@ def __init__(self, maxlength: int):\n \n     @classmethod\n     def from_settings(cls, settings: BaseSettings) -> Self:\n+        warnings.warn(\n+            f\"{cls.__name__}.from_settings() is deprecated, use from_crawler() instead.\",\n+            category=ScrapyDeprecationWarning,\n+            stacklevel=2,\n+        )\n+        return cls._from_settings(settings)\n+\n+    @classmethod\n+    def from_crawler(cls, crawler: Crawler) -> Self:\n+        return cls._from_settings(crawler.settings)\n+\n+    @classmethod\n+    def _from_settings(cls, settings: BaseSettings) -> Self:\n         maxlength = settings.getint(\"URLLENGTH_LIMIT\")\n         if not maxlength:\n             raise NotConfigured\ndiff --git a/scrapy/utils/misc.py b/scrapy/utils/misc.py\nindex 12c09839f0f..a408a205dda 100644\n--- a/scrapy/utils/misc.py\n+++ b/scrapy/utils/misc.py\n@@ -26,7 +26,6 @@\n \n     from scrapy import Spider\n     from scrapy.crawler import Crawler\n-    from scrapy.settings import BaseSettings\n \n \n _ITERABLE_SINGLE_VALUES = dict, Item, str, bytes\n@@ -150,7 +149,7 @@ def create_instance(objcls, settings, crawler, *args, **kwargs):\n     \"\"\"\n     warnings.warn(\n         \"The create_instance() function is deprecated. \"\n-        \"Please use build_from_crawler() or build_from_settings() instead.\",\n+        \"Please use build_from_crawler() instead.\",\n         category=ScrapyDeprecationWarning,\n         stacklevel=2,\n     )\n@@ -176,7 +175,7 @@ def create_instance(objcls, settings, crawler, *args, **kwargs):\n def build_from_crawler(\n     objcls: type[T], crawler: Crawler, /, *args: Any, **kwargs: Any\n ) -> T:\n-    \"\"\"Construct a class instance using its ``from_crawler`` constructor.\n+    \"\"\"Construct a class instance using its ``from_crawler`` or ``from_settings`` constructor.\n \n     ``*args`` and ``**kwargs`` are forwarded to the constructor.\n \n@@ -186,6 +185,14 @@ def build_from_crawler(\n         instance = objcls.from_crawler(crawler, *args, **kwargs)  # type: ignore[attr-defined]\n         method_name = \"from_crawler\"\n     elif hasattr(objcls, \"from_settings\"):\n+        warnings.warn(\n+            f\"{objcls.__qualname__} has from_settings() but not from_crawler().\"\n+            \" This is deprecated and calling from_settings() will be removed in a future\"\n+            \" Scrapy version. You can implement a simple from_crawler() that calls\"\n+            \" from_settings() with crawler.settings.\",\n+            category=ScrapyDeprecationWarning,\n+            stacklevel=2,\n+        )\n         instance = objcls.from_settings(crawler.settings, *args, **kwargs)  # type: ignore[attr-defined]\n         method_name = \"from_settings\"\n     else:\n@@ -196,26 +203,6 @@ def build_from_crawler(\n     return cast(T, instance)\n \n \n-def build_from_settings(\n-    objcls: type[T], settings: BaseSettings, /, *args: Any, **kwargs: Any\n-) -> T:\n-    \"\"\"Construct a class instance using its ``from_settings`` constructor.\n-\n-    ``*args`` and ``**kwargs`` are forwarded to the constructor.\n-\n-    Raises ``TypeError`` if the resulting instance is ``None``.\n-    \"\"\"\n-    if hasattr(objcls, \"from_settings\"):\n-        instance = objcls.from_settings(settings, *args, **kwargs)  # type: ignore[attr-defined]\n-        method_name = \"from_settings\"\n-    else:\n-        instance = objcls(*args, **kwargs)\n-        method_name = \"__new__\"\n-    if instance is None:\n-        raise TypeError(f\"{objcls.__qualname__}.{method_name} returned None\")\n-    return cast(T, instance)\n-\n-\n @contextmanager\n def set_environ(**kwargs: str) -> Iterator[None]:\n     \"\"\"Temporarily set environment variables inside the context manager and\n",
    "test_patch": "diff --git a/tests/test_dupefilters.py b/tests/test_dupefilters.py\nindex 9ba8bd64f40..4fd648f4834 100644\n--- a/tests/test_dupefilters.py\n+++ b/tests/test_dupefilters.py\n@@ -33,14 +33,6 @@ def from_crawler(cls, crawler):\n         return df\n \n \n-class FromSettingsRFPDupeFilter(RFPDupeFilter):\n-    @classmethod\n-    def from_settings(cls, settings, *, fingerprinter=None):\n-        df = super().from_settings(settings, fingerprinter=fingerprinter)\n-        df.method = \"from_settings\"\n-        return df\n-\n-\n class DirectDupeFilter:\n     method = \"n/a\"\n \n@@ -56,16 +48,6 @@ def test_df_from_crawler_scheduler(self):\n         self.assertTrue(scheduler.df.debug)\n         self.assertEqual(scheduler.df.method, \"from_crawler\")\n \n-    def test_df_from_settings_scheduler(self):\n-        settings = {\n-            \"DUPEFILTER_DEBUG\": True,\n-            \"DUPEFILTER_CLASS\": FromSettingsRFPDupeFilter,\n-        }\n-        crawler = get_crawler(settings_dict=settings)\n-        scheduler = Scheduler.from_crawler(crawler)\n-        self.assertTrue(scheduler.df.debug)\n-        self.assertEqual(scheduler.df.method, \"from_settings\")\n-\n     def test_df_direct_scheduler(self):\n         settings = {\n             \"DUPEFILTER_CLASS\": DirectDupeFilter,\ndiff --git a/tests/test_middleware.py b/tests/test_middleware.py\nindex a42c7b3d1e2..3a1cf19ad30 100644\n--- a/tests/test_middleware.py\n+++ b/tests/test_middleware.py\n@@ -2,7 +2,7 @@\n \n from scrapy.exceptions import NotConfigured\n from scrapy.middleware import MiddlewareManager\n-from scrapy.settings import Settings\n+from scrapy.utils.test import get_crawler\n \n \n class M1:\n@@ -23,8 +23,6 @@ def open_spider(self, spider):\n     def close_spider(self, spider):\n         pass\n \n-    pass\n-\n \n class M3:\n     def process(self, response, request, spider):\n@@ -83,7 +81,7 @@ def test_enabled(self):\n         self.assertEqual(mwman.middlewares, (m1, m2, m3))\n \n     def test_enabled_from_settings(self):\n-        settings = Settings()\n-        mwman = TestMiddlewareManager.from_settings(settings)\n+        crawler = get_crawler()\n+        mwman = TestMiddlewareManager.from_crawler(crawler)\n         classes = [x.__class__ for x in mwman.middlewares]\n         self.assertEqual(classes, [M1, M3])\ndiff --git a/tests/test_pipeline_files.py b/tests/test_pipeline_files.py\nindex 47840caaa16..9dcb3e4d18d 100644\n--- a/tests/test_pipeline_files.py\n+++ b/tests/test_pipeline_files.py\n@@ -2,6 +2,7 @@\n import os\n import random\n import time\n+import warnings\n from datetime import datetime\n from io import BytesIO\n from pathlib import Path\n@@ -25,7 +26,6 @@\n     GCSFilesStore,\n     S3FilesStore,\n )\n-from scrapy.settings import Settings\n from scrapy.utils.test import (\n     assert_gcs_environ,\n     get_crawler,\n@@ -217,8 +217,8 @@ class CustomFilesPipeline(FilesPipeline):\n             def file_path(self, request, response=None, info=None, item=None):\n                 return f'full/{item.get(\"path\")}'\n \n-        file_path = CustomFilesPipeline.from_settings(\n-            Settings({\"FILES_STORE\": self.tempdir})\n+        file_path = CustomFilesPipeline.from_crawler(\n+            get_crawler(None, {\"FILES_STORE\": self.tempdir})\n         ).file_path\n         item = {\"path\": \"path-to-store-file\"}\n         request = Request(\"http://example.com\")\n@@ -235,7 +235,9 @@ def tearDown(self):\n     def test_item_fields_default(self):\n         url = \"http://www.example.com/files/1.txt\"\n         item = self.item_class(name=\"item1\", file_urls=[url])\n-        pipeline = FilesPipeline.from_settings(Settings({\"FILES_STORE\": self.tempdir}))\n+        pipeline = FilesPipeline.from_crawler(\n+            get_crawler(None, {\"FILES_STORE\": self.tempdir})\n+        )\n         requests = list(pipeline.get_media_requests(item, None))\n         self.assertEqual(requests[0].url, url)\n         results = [(True, {\"url\": url})]\n@@ -247,13 +249,14 @@ def test_item_fields_default(self):\n     def test_item_fields_override_settings(self):\n         url = \"http://www.example.com/files/1.txt\"\n         item = self.item_class(name=\"item1\", custom_file_urls=[url])\n-        pipeline = FilesPipeline.from_settings(\n-            Settings(\n+        pipeline = FilesPipeline.from_crawler(\n+            get_crawler(\n+                None,\n                 {\n                     \"FILES_STORE\": self.tempdir,\n                     \"FILES_URLS_FIELD\": \"custom_file_urls\",\n                     \"FILES_RESULT_FIELD\": \"custom_files\",\n-                }\n+                },\n             )\n         )\n         requests = list(pipeline.get_media_requests(item, None))\n@@ -371,8 +374,10 @@ def test_different_settings_for_different_instances(self):\n         different settings.\n         \"\"\"\n         custom_settings = self._generate_fake_settings()\n-        another_pipeline = FilesPipeline.from_settings(Settings(custom_settings))\n-        one_pipeline = FilesPipeline(self.tempdir)\n+        another_pipeline = FilesPipeline.from_crawler(\n+            get_crawler(None, custom_settings)\n+        )\n+        one_pipeline = FilesPipeline(self.tempdir, crawler=get_crawler(None))\n         for pipe_attr, settings_attr, pipe_ins_attr in self.file_cls_attr_settings_map:\n             default_value = self.default_cls_settings[pipe_attr]\n             self.assertEqual(getattr(one_pipeline, pipe_attr), default_value)\n@@ -385,7 +390,7 @@ def test_subclass_attributes_preserved_if_no_settings(self):\n         If subclasses override class attributes and there are no special settings those values should be kept.\n         \"\"\"\n         pipe_cls = self._generate_fake_pipeline()\n-        pipe = pipe_cls.from_settings(Settings({\"FILES_STORE\": self.tempdir}))\n+        pipe = pipe_cls.from_crawler(get_crawler(None, {\"FILES_STORE\": self.tempdir}))\n         for pipe_attr, settings_attr, pipe_ins_attr in self.file_cls_attr_settings_map:\n             custom_value = getattr(pipe, pipe_ins_attr)\n             self.assertNotEqual(custom_value, self.default_cls_settings[pipe_attr])\n@@ -398,7 +403,7 @@ def test_subclass_attrs_preserved_custom_settings(self):\n         \"\"\"\n         pipeline_cls = self._generate_fake_pipeline()\n         settings = self._generate_fake_settings()\n-        pipeline = pipeline_cls.from_settings(Settings(settings))\n+        pipeline = pipeline_cls.from_crawler(get_crawler(None, settings))\n         for pipe_attr, settings_attr, pipe_ins_attr in self.file_cls_attr_settings_map:\n             value = getattr(pipeline, pipe_ins_attr)\n             setting_value = settings.get(settings_attr)\n@@ -414,8 +419,8 @@ def test_no_custom_settings_for_subclasses(self):\n         class UserDefinedFilesPipeline(FilesPipeline):\n             pass\n \n-        user_pipeline = UserDefinedFilesPipeline.from_settings(\n-            Settings({\"FILES_STORE\": self.tempdir})\n+        user_pipeline = UserDefinedFilesPipeline.from_crawler(\n+            get_crawler(None, {\"FILES_STORE\": self.tempdir})\n         )\n         for pipe_attr, settings_attr, pipe_ins_attr in self.file_cls_attr_settings_map:\n             # Values from settings for custom pipeline should be set on pipeline instance.\n@@ -433,7 +438,9 @@ class UserDefinedFilesPipeline(FilesPipeline):\n \n         prefix = UserDefinedFilesPipeline.__name__.upper()\n         settings = self._generate_fake_settings(prefix=prefix)\n-        user_pipeline = UserDefinedFilesPipeline.from_settings(Settings(settings))\n+        user_pipeline = UserDefinedFilesPipeline.from_crawler(\n+            get_crawler(None, settings)\n+        )\n         for pipe_attr, settings_attr, pipe_inst_attr in self.file_cls_attr_settings_map:\n             # Values from settings for custom pipeline should be set on pipeline instance.\n             custom_value = settings.get(prefix + \"_\" + settings_attr)\n@@ -448,7 +455,7 @@ def test_custom_settings_and_class_attrs_for_subclasses(self):\n         pipeline_cls = self._generate_fake_pipeline()\n         prefix = pipeline_cls.__name__.upper()\n         settings = self._generate_fake_settings(prefix=prefix)\n-        user_pipeline = pipeline_cls.from_settings(Settings(settings))\n+        user_pipeline = pipeline_cls.from_crawler(get_crawler(None, settings))\n         for (\n             pipe_cls_attr,\n             settings_attr,\n@@ -463,8 +470,8 @@ class UserDefinedFilesPipeline(FilesPipeline):\n             DEFAULT_FILES_RESULT_FIELD = \"this\"\n             DEFAULT_FILES_URLS_FIELD = \"that\"\n \n-        pipeline = UserDefinedFilesPipeline.from_settings(\n-            Settings({\"FILES_STORE\": self.tempdir})\n+        pipeline = UserDefinedFilesPipeline.from_crawler(\n+            get_crawler(None, {\"FILES_STORE\": self.tempdir})\n         )\n         self.assertEqual(\n             pipeline.files_result_field,\n@@ -484,7 +491,7 @@ def test_user_defined_subclass_default_key_names(self):\n         class UserPipe(FilesPipeline):\n             pass\n \n-        pipeline_cls = UserPipe.from_settings(Settings(settings))\n+        pipeline_cls = UserPipe.from_crawler(get_crawler(None, settings))\n \n         for pipe_attr, settings_attr, pipe_inst_attr in self.file_cls_attr_settings_map:\n             expected_value = settings.get(settings_attr)\n@@ -495,8 +502,8 @@ class CustomFilesPipelineWithPathLikeDir(FilesPipeline):\n             def file_path(self, request, response=None, info=None, *, item=None):\n                 return Path(\"subdir\") / Path(request.url).name\n \n-        pipeline = CustomFilesPipelineWithPathLikeDir.from_settings(\n-            Settings({\"FILES_STORE\": Path(\"./Temp\")})\n+        pipeline = CustomFilesPipelineWithPathLikeDir.from_crawler(\n+            get_crawler(None, {\"FILES_STORE\": Path(\"./Temp\")})\n         )\n         request = Request(\"http://example.com/image01.jpg\")\n         self.assertEqual(pipeline.file_path(request), Path(\"subdir/image01.jpg\"))\n@@ -687,3 +694,75 @@ def _prepare_request_object(item_url, flags=None):\n         item_url,\n         meta={\"response\": Response(item_url, status=200, body=b\"data\", flags=flags)},\n     )\n+\n+\n+# this is separate from the one in test_pipeline_media.py to specifically test FilesPipeline subclasses\n+class BuildFromCrawlerTestCase(unittest.TestCase):\n+    def setUp(self):\n+        self.tempdir = mkdtemp()\n+        self.crawler = get_crawler(None, {\"FILES_STORE\": self.tempdir})\n+\n+    def tearDown(self):\n+        rmtree(self.tempdir)\n+\n+    def test_simple(self):\n+        class Pipeline(FilesPipeline):\n+            pass\n+\n+        with warnings.catch_warnings(record=True) as w:\n+            pipe = Pipeline.from_crawler(self.crawler)\n+            assert pipe.crawler == self.crawler\n+            assert pipe._fingerprinter\n+            self.assertEqual(len(w), 0)\n+            assert pipe.store\n+\n+    def test_has_old_init(self):\n+        class Pipeline(FilesPipeline):\n+            def __init__(self, store_uri, download_func=None, settings=None):\n+                super().__init__(store_uri, download_func, settings)\n+                self._init_called = True\n+\n+        with warnings.catch_warnings(record=True) as w:\n+            pipe = Pipeline.from_crawler(self.crawler)\n+            assert pipe.crawler == self.crawler\n+            assert pipe._fingerprinter\n+            self.assertEqual(len(w), 2)\n+            assert pipe._init_called\n+\n+    def test_has_from_settings(self):\n+        class Pipeline(FilesPipeline):\n+            _from_settings_called = False\n+\n+            @classmethod\n+            def from_settings(cls, settings):\n+                o = super().from_settings(settings)\n+                o._from_settings_called = True\n+                return o\n+\n+        with warnings.catch_warnings(record=True) as w:\n+            pipe = Pipeline.from_crawler(self.crawler)\n+            assert pipe.crawler == self.crawler\n+            assert pipe._fingerprinter\n+            self.assertEqual(len(w), 3)\n+            assert pipe.store\n+            assert pipe._from_settings_called\n+\n+    def test_has_from_crawler_and_init(self):\n+        class Pipeline(FilesPipeline):\n+            _from_crawler_called = False\n+\n+            @classmethod\n+            def from_crawler(cls, crawler):\n+                settings = crawler.settings\n+                store_uri = settings[\"FILES_STORE\"]\n+                o = cls(store_uri, crawler=crawler)\n+                o._from_crawler_called = True\n+                return o\n+\n+        with warnings.catch_warnings(record=True) as w:\n+            pipe = Pipeline.from_crawler(self.crawler)\n+            assert pipe.crawler == self.crawler\n+            assert pipe._fingerprinter\n+            self.assertEqual(len(w), 0)\n+            assert pipe.store\n+            assert pipe._from_crawler_called\ndiff --git a/tests/test_pipeline_images.py b/tests/test_pipeline_images.py\nindex dfeead999d5..3ffef410249 100644\n--- a/tests/test_pipeline_images.py\n+++ b/tests/test_pipeline_images.py\n@@ -13,7 +13,7 @@\n from scrapy.http import Request, Response\n from scrapy.item import Field, Item\n from scrapy.pipelines.images import ImageException, ImagesPipeline\n-from scrapy.settings import Settings\n+from scrapy.utils.test import get_crawler\n \n skip_pillow: str | None\n try:\n@@ -33,7 +33,8 @@ class ImagesPipelineTestCase(unittest.TestCase):\n \n     def setUp(self):\n         self.tempdir = mkdtemp()\n-        self.pipeline = ImagesPipeline(self.tempdir)\n+        crawler = get_crawler()\n+        self.pipeline = ImagesPipeline(self.tempdir, crawler=crawler)\n \n     def tearDown(self):\n         rmtree(self.tempdir)\n@@ -123,8 +124,8 @@ def thumb_path(\n             ):\n                 return f\"thumb/{thumb_id}/{item.get('path')}\"\n \n-        thumb_path = CustomImagesPipeline.from_settings(\n-            Settings({\"IMAGES_STORE\": self.tempdir})\n+        thumb_path = CustomImagesPipeline.from_crawler(\n+            get_crawler(None, {\"IMAGES_STORE\": self.tempdir})\n         ).thumb_path\n         item = {\"path\": \"path-to-store-file\"}\n         request = Request(\"http://example.com\")\n@@ -218,8 +219,8 @@ class ImagesPipelineTestCaseFieldsMixin:\n     def test_item_fields_default(self):\n         url = \"http://www.example.com/images/1.jpg\"\n         item = self.item_class(name=\"item1\", image_urls=[url])\n-        pipeline = ImagesPipeline.from_settings(\n-            Settings({\"IMAGES_STORE\": \"s3://example/images/\"})\n+        pipeline = ImagesPipeline.from_crawler(\n+            get_crawler(None, {\"IMAGES_STORE\": \"s3://example/images/\"})\n         )\n         requests = list(pipeline.get_media_requests(item, None))\n         self.assertEqual(requests[0].url, url)\n@@ -232,13 +233,14 @@ def test_item_fields_default(self):\n     def test_item_fields_override_settings(self):\n         url = \"http://www.example.com/images/1.jpg\"\n         item = self.item_class(name=\"item1\", custom_image_urls=[url])\n-        pipeline = ImagesPipeline.from_settings(\n-            Settings(\n+        pipeline = ImagesPipeline.from_crawler(\n+            get_crawler(\n+                None,\n                 {\n                     \"IMAGES_STORE\": \"s3://example/images/\",\n                     \"IMAGES_URLS_FIELD\": \"custom_image_urls\",\n                     \"IMAGES_RESULT_FIELD\": \"custom_images\",\n-                }\n+                },\n             )\n         )\n         requests = list(pipeline.get_media_requests(item, None))\n@@ -389,9 +391,8 @@ def test_different_settings_for_different_instances(self):\n         have different settings.\n         \"\"\"\n         custom_settings = self._generate_fake_settings()\n-        default_settings = Settings()\n-        default_sts_pipe = ImagesPipeline(self.tempdir, settings=default_settings)\n-        user_sts_pipe = ImagesPipeline.from_settings(Settings(custom_settings))\n+        default_sts_pipe = ImagesPipeline(self.tempdir, crawler=get_crawler(None))\n+        user_sts_pipe = ImagesPipeline.from_crawler(get_crawler(None, custom_settings))\n         for pipe_attr, settings_attr in self.img_cls_attribute_names:\n             expected_default_value = self.default_pipeline_settings.get(pipe_attr)\n             custom_value = custom_settings.get(settings_attr)\n@@ -407,7 +408,9 @@ def test_subclass_attrs_preserved_default_settings(self):\n         from class attributes.\n         \"\"\"\n         pipeline_cls = self._generate_fake_pipeline_subclass()\n-        pipeline = pipeline_cls.from_settings(Settings({\"IMAGES_STORE\": self.tempdir}))\n+        pipeline = pipeline_cls.from_crawler(\n+            get_crawler(None, {\"IMAGES_STORE\": self.tempdir})\n+        )\n         for pipe_attr, settings_attr in self.img_cls_attribute_names:\n             # Instance attribute (lowercase) must be equal to class attribute (uppercase).\n             attr_value = getattr(pipeline, pipe_attr.lower())\n@@ -421,7 +424,7 @@ def test_subclass_attrs_preserved_custom_settings(self):\n         \"\"\"\n         pipeline_cls = self._generate_fake_pipeline_subclass()\n         settings = self._generate_fake_settings()\n-        pipeline = pipeline_cls.from_settings(Settings(settings))\n+        pipeline = pipeline_cls.from_crawler(get_crawler(None, settings))\n         for pipe_attr, settings_attr in self.img_cls_attribute_names:\n             # Instance attribute (lowercase) must be equal to\n             # value defined in settings.\n@@ -439,8 +442,8 @@ def test_no_custom_settings_for_subclasses(self):\n         class UserDefinedImagePipeline(ImagesPipeline):\n             pass\n \n-        user_pipeline = UserDefinedImagePipeline.from_settings(\n-            Settings({\"IMAGES_STORE\": self.tempdir})\n+        user_pipeline = UserDefinedImagePipeline.from_crawler(\n+            get_crawler(None, {\"IMAGES_STORE\": self.tempdir})\n         )\n         for pipe_attr, settings_attr in self.img_cls_attribute_names:\n             # Values from settings for custom pipeline should be set on pipeline instance.\n@@ -458,7 +461,9 @@ class UserDefinedImagePipeline(ImagesPipeline):\n \n         prefix = UserDefinedImagePipeline.__name__.upper()\n         settings = self._generate_fake_settings(prefix=prefix)\n-        user_pipeline = UserDefinedImagePipeline.from_settings(Settings(settings))\n+        user_pipeline = UserDefinedImagePipeline.from_crawler(\n+            get_crawler(None, settings)\n+        )\n         for pipe_attr, settings_attr in self.img_cls_attribute_names:\n             # Values from settings for custom pipeline should be set on pipeline instance.\n             custom_value = settings.get(prefix + \"_\" + settings_attr)\n@@ -473,7 +478,7 @@ def test_custom_settings_and_class_attrs_for_subclasses(self):\n         pipeline_cls = self._generate_fake_pipeline_subclass()\n         prefix = pipeline_cls.__name__.upper()\n         settings = self._generate_fake_settings(prefix=prefix)\n-        user_pipeline = pipeline_cls.from_settings(Settings(settings))\n+        user_pipeline = pipeline_cls.from_crawler(get_crawler(None, settings))\n         for pipe_attr, settings_attr in self.img_cls_attribute_names:\n             custom_value = settings.get(prefix + \"_\" + settings_attr)\n             self.assertNotEqual(custom_value, self.default_pipeline_settings[pipe_attr])\n@@ -484,8 +489,8 @@ class UserDefinedImagePipeline(ImagesPipeline):\n             DEFAULT_IMAGES_URLS_FIELD = \"something\"\n             DEFAULT_IMAGES_RESULT_FIELD = \"something_else\"\n \n-        pipeline = UserDefinedImagePipeline.from_settings(\n-            Settings({\"IMAGES_STORE\": self.tempdir})\n+        pipeline = UserDefinedImagePipeline.from_crawler(\n+            get_crawler(None, {\"IMAGES_STORE\": self.tempdir})\n         )\n         self.assertEqual(\n             pipeline.images_result_field,\n@@ -506,7 +511,7 @@ def test_user_defined_subclass_default_key_names(self):\n         class UserPipe(ImagesPipeline):\n             pass\n \n-        pipeline_cls = UserPipe.from_settings(Settings(settings))\n+        pipeline_cls = UserPipe.from_crawler(get_crawler(None, settings))\n \n         for pipe_attr, settings_attr in self.img_cls_attribute_names:\n             expected_value = settings.get(settings_attr)\ndiff --git a/tests/test_pipeline_media.py b/tests/test_pipeline_media.py\nindex c979e45d70a..58a2d367825 100644\n--- a/tests/test_pipeline_media.py\n+++ b/tests/test_pipeline_media.py\n@@ -1,5 +1,7 @@\n from __future__ import annotations\n \n+import warnings\n+\n from testfixtures import LogCapture\n from twisted.internet import reactor\n from twisted.internet.defer import Deferred, inlineCallbacks\n@@ -11,7 +13,6 @@\n from scrapy.http.request import NO_CALLBACK\n from scrapy.pipelines.files import FileException\n from scrapy.pipelines.media import MediaPipeline\n-from scrapy.settings import Settings\n from scrapy.spiders import Spider\n from scrapy.utils.log import failure_to_exc_info\n from scrapy.utils.signal import disconnect_all\n@@ -175,8 +176,8 @@ def test_default_process_item(self):\n \n \n class MockedMediaPipeline(UserDefinedPipeline):\n-    def __init__(self, *args, **kwargs):\n-        super().__init__(*args, **kwargs)\n+    def __init__(self, *args, crawler=None, **kwargs):\n+        super().__init__(*args, crawler=crawler, **kwargs)\n         self._mockcalled = []\n \n     def download(self, request, info):\n@@ -377,7 +378,7 @@ def test_key_for_pipe(self):\n class MediaPipelineAllowRedirectSettingsTestCase(unittest.TestCase):\n \n     def _assert_request_no3xx(self, pipeline_class, settings):\n-        pipe = pipeline_class(settings=Settings(settings))\n+        pipe = pipeline_class(crawler=get_crawler(None, settings))\n         request = Request(\"http://url\")\n         pipe._modify_media_request(request)\n \n@@ -410,3 +411,115 @@ def test_subclass_specific_setting(self):\n         self._assert_request_no3xx(\n             UserDefinedPipeline, {\"USERDEFINEDPIPELINE_MEDIA_ALLOW_REDIRECTS\": True}\n         )\n+\n+\n+class BuildFromCrawlerTestCase(unittest.TestCase):\n+    def setUp(self):\n+        self.crawler = get_crawler(None, {\"FILES_STORE\": \"/foo\"})\n+\n+    def test_simple(self):\n+        class Pipeline(UserDefinedPipeline):\n+            pass\n+\n+        with warnings.catch_warnings(record=True) as w:\n+            pipe = Pipeline.from_crawler(self.crawler)\n+            assert pipe.crawler == self.crawler\n+            assert pipe._fingerprinter\n+            self.assertEqual(len(w), 0)\n+\n+    def test_has_old_init(self):\n+        class Pipeline(UserDefinedPipeline):\n+            def __init__(self):\n+                super().__init__()\n+                self._init_called = True\n+\n+        with warnings.catch_warnings(record=True) as w:\n+            pipe = Pipeline.from_crawler(self.crawler)\n+            assert pipe.crawler == self.crawler\n+            assert pipe._fingerprinter\n+            self.assertEqual(len(w), 2)\n+            assert pipe._init_called\n+\n+    def test_has_from_settings(self):\n+        class Pipeline(UserDefinedPipeline):\n+            _from_settings_called = False\n+\n+            @classmethod\n+            def from_settings(cls, settings):\n+                o = cls()\n+                o._from_settings_called = True\n+                return o\n+\n+        with warnings.catch_warnings(record=True) as w:\n+            pipe = Pipeline.from_crawler(self.crawler)\n+            assert pipe.crawler == self.crawler\n+            assert pipe._fingerprinter\n+            self.assertEqual(len(w), 1)\n+            assert pipe._from_settings_called\n+\n+    def test_has_from_settings_and_init(self):\n+        class Pipeline(UserDefinedPipeline):\n+            _from_settings_called = False\n+\n+            def __init__(self, store_uri, settings):\n+                super().__init__()\n+                self._init_called = True\n+\n+            @classmethod\n+            def from_settings(cls, settings):\n+                store_uri = settings[\"FILES_STORE\"]\n+                o = cls(store_uri, settings=settings)\n+                o._from_settings_called = True\n+                return o\n+\n+        with warnings.catch_warnings(record=True) as w:\n+            pipe = Pipeline.from_crawler(self.crawler)\n+            assert pipe.crawler == self.crawler\n+            assert pipe._fingerprinter\n+            self.assertEqual(len(w), 1)\n+            assert pipe._from_settings_called\n+            assert pipe._init_called\n+\n+    def test_has_from_crawler_and_init(self):\n+        class Pipeline(UserDefinedPipeline):\n+            _from_crawler_called = False\n+\n+            def __init__(self, store_uri, settings, *, crawler):\n+                super().__init__(crawler=crawler)\n+                self._init_called = True\n+\n+            @classmethod\n+            def from_crawler(cls, crawler):\n+                settings = crawler.settings\n+                store_uri = settings[\"FILES_STORE\"]\n+                o = cls(store_uri, settings=settings, crawler=crawler)\n+                o._from_crawler_called = True\n+                return o\n+\n+        with warnings.catch_warnings(record=True) as w:\n+            pipe = Pipeline.from_crawler(self.crawler)\n+            assert pipe.crawler == self.crawler\n+            assert pipe._fingerprinter\n+            self.assertEqual(len(w), 0)\n+            assert pipe._from_crawler_called\n+            assert pipe._init_called\n+\n+    def test_has_from_crawler(self):\n+        class Pipeline(UserDefinedPipeline):\n+            _from_crawler_called = False\n+\n+            @classmethod\n+            def from_crawler(cls, crawler):\n+                settings = crawler.settings\n+                o = super().from_crawler(crawler)\n+                o._from_crawler_called = True\n+                o.store_uri = settings[\"FILES_STORE\"]\n+                return o\n+\n+        with warnings.catch_warnings(record=True) as w:\n+            pipe = Pipeline.from_crawler(self.crawler)\n+            # this and the next assert will fail as MediaPipeline.from_crawler() wasn't called\n+            assert pipe.crawler == self.crawler\n+            assert pipe._fingerprinter\n+            self.assertEqual(len(w), 0)\n+            assert pipe._from_crawler_called\ndiff --git a/tests/test_spidermiddleware_urllength.py b/tests/test_spidermiddleware_urllength.py\nindex 9111e4c82ab..1a0f2e223c4 100644\n--- a/tests/test_spidermiddleware_urllength.py\n+++ b/tests/test_spidermiddleware_urllength.py\n@@ -3,7 +3,6 @@\n from testfixtures import LogCapture\n \n from scrapy.http import Request, Response\n-from scrapy.settings import Settings\n from scrapy.spidermiddlewares.urllength import UrlLengthMiddleware\n from scrapy.spiders import Spider\n from scrapy.utils.test import get_crawler\n@@ -12,12 +11,10 @@\n class TestUrlLengthMiddleware(TestCase):\n     def setUp(self):\n         self.maxlength = 25\n-        settings = Settings({\"URLLENGTH_LIMIT\": self.maxlength})\n-\n-        crawler = get_crawler(Spider)\n+        crawler = get_crawler(Spider, {\"URLLENGTH_LIMIT\": self.maxlength})\n         self.spider = crawler._create_spider(\"foo\")\n         self.stats = crawler.stats\n-        self.mw = UrlLengthMiddleware.from_settings(settings)\n+        self.mw = UrlLengthMiddleware.from_crawler(crawler)\n \n         self.response = Response(\"http://scrapytest.org\")\n         self.short_url_req = Request(\"http://scrapytest.org/\")\ndiff --git a/tests/test_utils_misc/__init__.py b/tests/test_utils_misc/__init__.py\nindex 4d8e715210d..f71b2b034a9 100644\n--- a/tests/test_utils_misc/__init__.py\n+++ b/tests/test_utils_misc/__init__.py\n@@ -10,7 +10,6 @@\n from scrapy.utils.misc import (\n     arg_to_iter,\n     build_from_crawler,\n-    build_from_settings,\n     create_instance,\n     load_object,\n     rel_has_nofollow,\n@@ -197,39 +196,6 @@ def _test_with_crawler(mock, settings, crawler):\n         with self.assertRaises(TypeError):\n             build_from_crawler(m, crawler, *args, **kwargs)\n \n-    def test_build_from_settings(self):\n-        settings = mock.MagicMock()\n-        args = (True, 100.0)\n-        kwargs = {\"key\": \"val\"}\n-\n-        def _test_with_settings(mock, settings):\n-            build_from_settings(mock, settings, *args, **kwargs)\n-            if hasattr(mock, \"from_settings\"):\n-                mock.from_settings.assert_called_once_with(settings, *args, **kwargs)\n-                self.assertEqual(mock.call_count, 0)\n-            else:\n-                mock.assert_called_once_with(*args, **kwargs)\n-\n-        # Check usage of correct constructor using three mocks:\n-        #   1. with no alternative constructors\n-        #   2. with from_settings() constructor\n-        #   3. with from_settings() and from_crawler() constructor\n-        spec_sets = (\n-            [\"__qualname__\"],\n-            [\"__qualname__\", \"from_settings\"],\n-            [\"__qualname__\", \"from_settings\", \"from_crawler\"],\n-        )\n-        for specs in spec_sets:\n-            m = mock.MagicMock(spec_set=specs)\n-            _test_with_settings(m, settings)\n-            m.reset_mock()\n-\n-        # Check adoption of crawler settings\n-        m = mock.MagicMock(spec_set=[\"__qualname__\", \"from_settings\"])\n-        m.from_settings.return_value = None\n-        with self.assertRaises(TypeError):\n-            build_from_settings(m, settings, *args, **kwargs)\n-\n     def test_set_environ(self):\n         assert os.environ.get(\"some_test_environ\") is None\n         with set_environ(some_test_environ=\"test_value\"):\ndiff --git a/tests/test_utils_request.py b/tests/test_utils_request.py\nindex 965d050a4da..0a3e3b00be5 100644\n--- a/tests/test_utils_request.py\n+++ b/tests/test_utils_request.py\n@@ -8,6 +8,7 @@\n \n import pytest\n \n+from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.http import Request\n from scrapy.utils.python import to_bytes\n from scrapy.utils.request import (\n@@ -384,7 +385,9 @@ def fingerprint(self, request):\n             \"REQUEST_FINGERPRINTER_CLASS\": RequestFingerprinter,\n             \"FINGERPRINT\": b\"fingerprint\",\n         }\n-        crawler = get_crawler(settings_dict=settings)\n+        with warnings.catch_warnings():\n+            warnings.simplefilter(\"ignore\", ScrapyDeprecationWarning)\n+            crawler = get_crawler(settings_dict=settings)\n \n         request = Request(\"http://www.example.com\")\n         fingerprint = crawler.request_fingerprinter.fingerprint(request)\ndiff --git a/tests/test_webclient.py b/tests/test_webclient.py\nindex cce119001ac..1797d5e1fcb 100644\n--- a/tests/test_webclient.py\n+++ b/tests/test_webclient.py\n@@ -9,25 +9,18 @@\n \n import OpenSSL.SSL\n from twisted.internet import defer, reactor\n-from twisted.trial import unittest\n-from twisted.web import resource, server, static, util\n-\n-try:\n-    from twisted.internet.testing import StringTransport\n-except ImportError:\n-    # deprecated in Twisted 19.7.0\n-    # (remove once we bump our requirement past that version)\n-    from twisted.test.proto_helpers import StringTransport\n-\n from twisted.internet.defer import inlineCallbacks\n+from twisted.internet.testing import StringTransport\n from twisted.protocols.policies import WrappingFactory\n+from twisted.trial import unittest\n+from twisted.web import resource, server, static, util\n \n from scrapy.core.downloader import webclient as client\n from scrapy.core.downloader.contextfactory import ScrapyClientContextFactory\n from scrapy.http import Headers, Request\n-from scrapy.settings import Settings\n-from scrapy.utils.misc import build_from_settings\n+from scrapy.utils.misc import build_from_crawler\n from scrapy.utils.python import to_bytes, to_unicode\n+from scrapy.utils.test import get_crawler\n from tests.mockserver import (\n     BrokenDownloadResource,\n     ErrorResource,\n@@ -469,22 +462,22 @@ class WebClientCustomCiphersSSLTestCase(WebClientSSLTestCase):\n \n     def testPayload(self):\n         s = \"0123456789\" * 10\n-        settings = Settings({\"DOWNLOADER_CLIENT_TLS_CIPHERS\": self.custom_ciphers})\n-        client_context_factory = build_from_settings(\n-            ScrapyClientContextFactory, settings\n+        crawler = get_crawler(\n+            settings_dict={\"DOWNLOADER_CLIENT_TLS_CIPHERS\": self.custom_ciphers}\n         )\n+        client_context_factory = build_from_crawler(ScrapyClientContextFactory, crawler)\n         return getPage(\n             self.getURL(\"payload\"), body=s, contextFactory=client_context_factory\n         ).addCallback(self.assertEqual, to_bytes(s))\n \n     def testPayloadDisabledCipher(self):\n         s = \"0123456789\" * 10\n-        settings = Settings(\n-            {\"DOWNLOADER_CLIENT_TLS_CIPHERS\": \"ECDHE-RSA-AES256-GCM-SHA384\"}\n-        )\n-        client_context_factory = build_from_settings(\n-            ScrapyClientContextFactory, settings\n+        crawler = get_crawler(\n+            settings_dict={\n+                \"DOWNLOADER_CLIENT_TLS_CIPHERS\": \"ECDHE-RSA-AES256-GCM-SHA384\"\n+            }\n         )\n+        client_context_factory = build_from_crawler(ScrapyClientContextFactory, crawler)\n         d = getPage(\n             self.getURL(\"payload\"), body=s, contextFactory=client_context_factory\n         )\n",
    "problem_statement": "Don't ship `build_from_settings()`\nWe discussed this with @kmike and decided that we want to ship `build_from_crawler()` but not `build_from_settings()` as a last-minute follow-up to #5523/#6169. This, to my knowledge, has two consequences:\r\n\r\n1. We need to change `scrapy.middleware.MiddlewareManager` to require a `Crawler` instance for building.\r\n2. Users that use `create_instance()` and pass `settings` but not `crawler` will need to change the logic when migrating to `build_from_crawler()`, but we think they should normally have a Crawler instance there.\r\n\r\n`build_from_crawler()` also has a wrong docstring as it doesn't mention `from_settings()`.\n",
    "hints_text": "`build_from_settings()` is also used in a test where we create a Settings instance to create a component instance with it and don't need a Crawler - this should be easy to fix.\r\n\r\nRegarding `scrapy.middleware.MiddlewareManager`: it currently has a `from_crawler()` that just calls `from_settings()` and `from_settings()` that can optionally take a Crawler instance. If the Crawler instance is passed, it is also passed to created middlewares via `build_from_crawler()` and if it isn't passed, the middlewares are created with `build_from_settings()`. So the ideal state is simple: it has a `from_crawler()` but not `from_settings()` and always passed the Crawler instance to created middleware. But as a temporary backwards-compatible state we want to keep both methods (with appropriate deprecation warnings) and be able to create middlewares without a Crawler instance, for which we should either keep (inlined?) a `build_from_settings()` function or use `create_instance()` there (which will print additional warnings). Thoughts?\r\n\r\nIn Scrapy itself `MiddlewareManager` is not used directly but only as a base class for: `scrapy.extension.ExtensionManager`, `scrapy.core.spidermw.SpiderMiddlewareManager`, `scrapy.core.downloader.middleware.DownloaderMiddlewareManager`, `scrapy.pipelines.ItemPipelineManager`. None of these override either `from_crawler()` or `from_settings()`. Instances of all of these are created via `from_crawler()`. Only `ItemPipelineManager` can be replaced (via `ITEM_PROCESSOR`) but it still needs to implement `from_crawler()` due to the previous statement. So we can safely drop the code path that doesn't take and pass a Crawler instance.\n(Additionally, if we don't have `build_from_settings()` we will be able deprecate the whole `from_settings()` interface later, though I don't know if we should)\n> we should either keep (inlined?) a `build_from_settings()` function or use `create_instance()` there (which will print additional warnings). Thoughts?\r\n\r\nI think it\u2019s best to avoid the `create_instance()` warning, be it by silencing it or by using an in-lined `build_from_settings()`, no opinion on that part. The `from_settings()` class method should emit its own, specific deprecation warning.\r\n\r\n> Additionally, if we don't have `build_from_settings()` we will be able deprecate the whole `from_settings()` interface later, though I don't know if we should\r\n\r\nUnless there is a good reason not to, it sounds consistent with deprecating `create_instance()` without adding `build_from_settings()`, so we should consider doing it for 2.12 already.",
    "created_at": "2024-11-12T16:34:43Z",
    "version": "2.11",
    "PASS_TO_PASS": [],
    "FAIL_TO_PASS": [
      "tests/test_utils_request.py::CustomRequestFingerprinterTestCase::test_from_settings",
      "tests/test_webclient.py::WebClientCustomCiphersSSLTestCase::testPayload",
      "tests/test_webclient.py::WebClientCustomCiphersSSLTestCase::testPayloadDisabledCipher"
    ]
  },
  {
    "repo": "scrapy/scrapy",
    "pull_number": 6469,
    "instance_id": "scrapy__scrapy-6469",
    "issue_numbers": [
      "6468"
    ],
    "base_commit": "6ce0342beb1a5b588f353e52fe03d5e0ec84d938",
    "patch": "diff --git a/docs/topics/contracts.rst b/docs/topics/contracts.rst\nindex 2d61026e9a5..61aef4bbb42 100644\n--- a/docs/topics/contracts.rst\n+++ b/docs/topics/contracts.rst\n@@ -20,13 +20,13 @@ following example:\n         This function parses a sample response. Some contracts are mingled\n         with this docstring.\n \n-        @url http://www.amazon.com/s?field-keywords=selfish+gene\n+        @url http://www.example.com/s?field-keywords=selfish+gene\n         @returns items 1 16\n         @returns requests 0 0\n         @scrapes Title Author Year Price\n         \"\"\"\n \n-This callback is tested using three built-in contracts:\n+You can use the following contracts:\n \n .. module:: scrapy.contracts.default\n \n@@ -46,6 +46,14 @@ This callback is tested using three built-in contracts:\n \n     @cb_kwargs {\"arg1\": \"value1\", \"arg2\": \"value2\", ...}\n \n+.. class:: MetadataContract\n+\n+    This contract (``@meta``) sets the :attr:`meta <scrapy.Request.meta>`\n+    attribute for the sample request. It must be a valid JSON dictionary.\n+    ::\n+\n+    @meta {\"arg1\": \"value1\", \"arg2\": \"value2\", ...}\n+\n .. class:: ReturnsContract\n \n     This contract (``@returns``) sets lower and upper bounds for the items and\ndiff --git a/scrapy/contracts/default.py b/scrapy/contracts/default.py\nindex 71ca4168af9..e7b11d426ff 100644\n--- a/scrapy/contracts/default.py\n+++ b/scrapy/contracts/default.py\n@@ -35,6 +35,20 @@ def adjust_request_args(self, args: Dict[str, Any]) -> Dict[str, Any]:\n         return args\n \n \n+class MetadataContract(Contract):\n+    \"\"\"Contract to set metadata arguments for the request.\n+    The value should be JSON-encoded dictionary, e.g.:\n+\n+    @meta {\"arg1\": \"some value\"}\n+    \"\"\"\n+\n+    name = \"meta\"\n+\n+    def adjust_request_args(self, args: Dict[str, Any]) -> Dict[str, Any]:\n+        args[\"meta\"] = json.loads(\" \".join(self.args))\n+        return args\n+\n+\n class ReturnsContract(Contract):\n     \"\"\"Contract to check the output of a callback\n \ndiff --git a/scrapy/settings/default_settings.py b/scrapy/settings/default_settings.py\nindex 932475fb5ad..7ba0128a597 100644\n--- a/scrapy/settings/default_settings.py\n+++ b/scrapy/settings/default_settings.py\n@@ -333,6 +333,7 @@\n SPIDER_CONTRACTS_BASE = {\n     \"scrapy.contracts.default.UrlContract\": 1,\n     \"scrapy.contracts.default.CallbackKeywordArgumentsContract\": 1,\n+    \"scrapy.contracts.default.MetadataContract\": 1,\n     \"scrapy.contracts.default.ReturnsContract\": 2,\n     \"scrapy.contracts.default.ScrapesContract\": 3,\n }\n",
    "test_patch": "diff --git a/tests/test_contracts.py b/tests/test_contracts.py\nindex c9c12f0d804..d578b3af450 100644\n--- a/tests/test_contracts.py\n+++ b/tests/test_contracts.py\n@@ -8,6 +8,7 @@\n from scrapy.contracts import Contract, ContractsManager\n from scrapy.contracts.default import (\n     CallbackKeywordArgumentsContract,\n+    MetadataContract,\n     ReturnsContract,\n     ScrapesContract,\n     UrlContract,\n@@ -29,6 +30,10 @@ class ResponseMock:\n     url = \"http://scrapy.org\"\n \n \n+class ResponseMetaMock(ResponseMock):\n+    meta = None\n+\n+\n class CustomSuccessContract(Contract):\n     name = \"custom_success_contract\"\n \n@@ -195,6 +200,33 @@ def invalid_regex_with_valid_contract(self, response):\n         \"\"\"\n         pass\n \n+    def returns_request_meta(self, response):\n+        \"\"\"method which returns request\n+        @url https://example.org\n+        @meta {\"cookiejar\": \"session1\"}\n+        @returns requests 1\n+        \"\"\"\n+        return Request(\n+            \"https://example.org\", meta=response.meta, callback=self.returns_item_meta\n+        )\n+\n+    def returns_item_meta(self, response):\n+        \"\"\"method which returns item\n+        @url http://scrapy.org\n+        @meta {\"key\": \"example\"}\n+        @returns items 1 1\n+        \"\"\"\n+        return TestItem(name=\"example\", url=response.url)\n+\n+    def returns_error_missing_meta(self, response):\n+        \"\"\"method which depends of metadata be defined\n+\n+        @url http://scrapy.org\n+        @returns items 1\n+        \"\"\"\n+        key = response.meta[\"key\"]\n+        yield {key: \"value\"}\n+\n \n class CustomContractSuccessSpider(Spider):\n     name = \"custom_contract_success_spider\"\n@@ -224,6 +256,7 @@ class ContractsManagerTest(unittest.TestCase):\n     contracts = [\n         UrlContract,\n         CallbackKeywordArgumentsContract,\n+        MetadataContract,\n         ReturnsContract,\n         ScrapesContract,\n         CustomFormContract,\n@@ -328,6 +361,52 @@ def test_cb_kwargs(self):\n         request.callback(response, **request.cb_kwargs)\n         self.should_error()\n \n+    def test_meta(self):\n+        spider = TestSpider()\n+\n+        # extract contracts correctly\n+        contracts = self.conman.extract_contracts(spider.returns_request_meta)\n+        self.assertEqual(len(contracts), 3)\n+        self.assertEqual(\n+            frozenset(type(x) for x in contracts),\n+            frozenset([UrlContract, MetadataContract, ReturnsContract]),\n+        )\n+\n+        contracts = self.conman.extract_contracts(spider.returns_item_meta)\n+        self.assertEqual(len(contracts), 3)\n+        self.assertEqual(\n+            frozenset(type(x) for x in contracts),\n+            frozenset([UrlContract, MetadataContract, ReturnsContract]),\n+        )\n+\n+        response = ResponseMetaMock()\n+\n+        # returns_request\n+        request = self.conman.from_method(spider.returns_request_meta, self.results)\n+        assert request.meta[\"cookiejar\"] == \"session1\"\n+        response.meta = request.meta\n+        request.callback(response)\n+        assert response.meta[\"cookiejar\"] == \"session1\"\n+        self.should_succeed()\n+\n+        response = ResponseMetaMock()\n+\n+        # returns_item\n+        request = self.conman.from_method(spider.returns_item_meta, self.results)\n+        assert request.meta[\"key\"] == \"example\"\n+        response.meta = request.meta\n+        request.callback(ResponseMetaMock)\n+        assert response.meta[\"key\"] == \"example\"\n+        self.should_succeed()\n+\n+        response = ResponseMetaMock()\n+\n+        request = self.conman.from_method(\n+            spider.returns_error_missing_meta, self.results\n+        )\n+        request.callback(response)\n+        self.should_error()\n+\n     def test_returns(self):\n         spider = TestSpider()\n         response = ResponseMock()\n",
    "problem_statement": "Add support for meta in Spider Contracts\n<!--\r\n\r\nThanks for taking an interest in Scrapy!\r\n\r\nIf you have a question that starts with \"How to...\", please see the Scrapy Community page: https://scrapy.org/community/.\r\nThe GitHub issue tracker's purpose is to deal with bug reports and feature requests for the project itself.\r\n\r\nKeep in mind that by filing an issue, you are expected to comply with Scrapy's Code of Conduct, including treating everyone with respect: https://github.com/scrapy/scrapy/blob/master/CODE_OF_CONDUCT.md\r\n\r\nThe following is a suggested template to structure your pull request, you can find more guidelines at https://doc.scrapy.org/en/latest/contributing.html#writing-patches and https://doc.scrapy.org/en/latest/contributing.html#submitting-patches\r\n\r\n-->\r\n\r\n## Summary\r\n\r\nToday we support `cb_kwargs` but we have scenarios where the data is passed using `meta`.\r\n\r\n## Motivation\r\n\r\nI have some spiders that use `meta` to store information, e.g: `cookiejar`\r\n\r\n## Describe alternatives you've considered\r\n\r\nUse callback args.\r\n\n",
    "hints_text": "Agreed this would be a nice feature to support",
    "created_at": "2024-08-27T02:22:06Z",
    "version": "2.11",
    "PASS_TO_PASS": [],
    "FAIL_TO_PASS": [
      "tests/test_contracts.py",
      "tests/test_contracts.py::ContractsManagerTest::test_meta"
    ]
  },
  {
    "repo": "scrapy/scrapy",
    "pull_number": 6374,
    "instance_id": "scrapy__scrapy-6374",
    "issue_numbers": [
      "6361"
    ],
    "base_commit": "631fc65fadb874629787ae5f7fdd876b9ec96a29",
    "patch": "diff --git a/.flake8 b/.flake8\nindex 62ccad9cf47..cf1a96476c2 100644\n--- a/.flake8\n+++ b/.flake8\n@@ -16,6 +16,7 @@ per-file-ignores =\n     scrapy/linkextractors/__init__.py:E402,F401\n     scrapy/selector/__init__.py:F401\n     scrapy/spiders/__init__.py:E402,F401\n+    tests/CrawlerRunner/change_reactor.py:E402\n \n     # Issues pending a review:\n     scrapy/utils/url.py:F403,F405\ndiff --git a/docs/topics/practices.rst b/docs/topics/practices.rst\nindex cd359b1473e..1500011e7b0 100644\n--- a/docs/topics/practices.rst\n+++ b/docs/topics/practices.rst\n@@ -92,7 +92,6 @@ reactor after ``MySpider`` has finished running.\n \n .. code-block:: python\n \n-    from twisted.internet import reactor\n     import scrapy\n     from scrapy.crawler import CrawlerRunner\n     from scrapy.utils.log import configure_logging\n@@ -107,6 +106,37 @@ reactor after ``MySpider`` has finished running.\n     runner = CrawlerRunner()\n \n     d = runner.crawl(MySpider)\n+\n+    from twisted.internet import reactor\n+\n+    d.addBoth(lambda _: reactor.stop())\n+    reactor.run()  # the script will block here until the crawling is finished\n+\n+Same example but using a non-default reactor, it's only necessary call\n+``install_reactor`` if you are using ``CrawlerRunner`` since ``CrawlerProcess`` already does this automatically.\n+\n+.. code-block:: python\n+\n+    import scrapy\n+    from scrapy.crawler import CrawlerRunner\n+    from scrapy.utils.log import configure_logging\n+\n+\n+    class MySpider(scrapy.Spider):\n+        # Your spider definition\n+        ...\n+\n+\n+    configure_logging({\"LOG_FORMAT\": \"%(levelname)s: %(message)s\"})\n+\n+    from scrapy.utils.reactor import install_reactor\n+\n+    install_reactor(\"twisted.internet.asyncioreactor.AsyncioSelectorReactor\")\n+    runner = CrawlerRunner()\n+    d = runner.crawl(MySpider)\n+\n+    from twisted.internet import reactor\n+\n     d.addBoth(lambda _: reactor.stop())\n     reactor.run()  # the script will block here until the crawling is finished\n \n@@ -151,7 +181,6 @@ Same example using :class:`~scrapy.crawler.CrawlerRunner`:\n .. code-block:: python\n \n     import scrapy\n-    from twisted.internet import reactor\n     from scrapy.crawler import CrawlerRunner\n     from scrapy.utils.log import configure_logging\n     from scrapy.utils.project import get_project_settings\n@@ -173,6 +202,9 @@ Same example using :class:`~scrapy.crawler.CrawlerRunner`:\n     runner.crawl(MySpider1)\n     runner.crawl(MySpider2)\n     d = runner.join()\n+\n+    from twisted.internet import reactor\n+\n     d.addBoth(lambda _: reactor.stop())\n \n     reactor.run()  # the script will block here until all crawling jobs are finished\n@@ -181,7 +213,7 @@ Same example but running the spiders sequentially by chaining the deferreds:\n \n .. code-block:: python\n \n-    from twisted.internet import reactor, defer\n+    from twisted.internet import defer\n     from scrapy.crawler import CrawlerRunner\n     from scrapy.utils.log import configure_logging\n     from scrapy.utils.project import get_project_settings\n@@ -209,6 +241,8 @@ Same example but running the spiders sequentially by chaining the deferreds:\n         reactor.stop()\n \n \n+    from twisted.internet import reactor\n+\n     crawl()\n     reactor.run()  # the script will block here until the last crawl call is finished\n \ndiff --git a/scrapy/crawler.py b/scrapy/crawler.py\nindex ccfe788913a..4fe5987a783 100644\n--- a/scrapy/crawler.py\n+++ b/scrapy/crawler.py\n@@ -129,6 +129,8 @@ def _apply_settings(self) -> None:\n             if is_asyncio_reactor_installed() and event_loop:\n                 verify_installed_asyncio_event_loop(event_loop)\n \n+            log_reactor_info()\n+\n         self.extensions = ExtensionManager.from_crawler(self)\n         self.settings.freeze()\n \n",
    "test_patch": "diff --git a/tests/CrawlerRunner/change_reactor.py b/tests/CrawlerRunner/change_reactor.py\nnew file mode 100644\nindex 00000000000..b20aa0c7cbf\n--- /dev/null\n+++ b/tests/CrawlerRunner/change_reactor.py\n@@ -0,0 +1,31 @@\n+from scrapy import Spider\n+from scrapy.crawler import CrawlerRunner\n+from scrapy.utils.log import configure_logging\n+\n+\n+class NoRequestsSpider(Spider):\n+    name = \"no_request\"\n+\n+    custom_settings = {\n+        \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n+    }\n+\n+    def start_requests(self):\n+        return []\n+\n+\n+configure_logging({\"LOG_FORMAT\": \"%(levelname)s: %(message)s\", \"LOG_LEVEL\": \"DEBUG\"})\n+\n+\n+from scrapy.utils.reactor import install_reactor\n+\n+install_reactor(\"twisted.internet.asyncioreactor.AsyncioSelectorReactor\")\n+\n+runner = CrawlerRunner()\n+\n+d = runner.crawl(NoRequestsSpider)\n+\n+from twisted.internet import reactor\n+\n+d.addBoth(callback=lambda _: reactor.stop())\n+reactor.run()\ndiff --git a/tests/test_crawler.py b/tests/test_crawler.py\nindex 989208694cb..791ea1faa66 100644\n--- a/tests/test_crawler.py\n+++ b/tests/test_crawler.py\n@@ -926,3 +926,11 @@ def test_response_ip_address(self):\n         self.assertIn(\"INFO: Host: not.a.real.domain\", log)\n         self.assertIn(\"INFO: Type: <class 'ipaddress.IPv4Address'>\", log)\n         self.assertIn(\"INFO: IP address: 127.0.0.1\", log)\n+\n+    def test_change_default_reactor(self):\n+        log = self.run_script(\"change_reactor.py\")\n+        self.assertIn(\n+            \"DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n+            log,\n+        )\n+        self.assertIn(\"DEBUG: Using asyncio event loop\", log)\n",
    "problem_statement": "Remove top-level reactor imports from CrawlerProces/CrawlerRunner examples \nThere are several code examples on https://docs.scrapy.org/en/latest/topics/practices.html that have a top-level `from twisted.internet import reactor`, which is problematic (breaks when the settings specify a non-default reactor) and needs to be fixed.\n",
    "hints_text": "For this we should check if we have `TWISTED_REACTOR` setting defined (`get_project_settings`) and if is we call `install_reactor` before importing `reactor`? \nI think it's enough to move the imports inside blocks so that they only run after the setting is applied (i.e. after `Crawler.crawl()`, so after `CrawlerRunner.crawl()`). The changed examples should be tested with a non-default reactor setting value in any case.\r\n\r\nIf/when that's not possible to do it makes sense to add `install_reactor()` to examples I think.\n@wRAR I was testing this and noticed that if we have `TWISTED_REACTOR` in `custom_settings` but we don't call `install_reactor` we always get an exception when Scrapy runs `_apply_settings` method (using `CrawlerRunner`):\r\n\r\n```\r\nException: The installed reactor (twisted.internet.selectreactor.SelectReactor) does not match the requested one (twisted.internet.asyncioreactor.AsyncioSelectorReactor)\r\n```\r\n\r\nBecause `init_reactor`  is `False`:\r\n\r\nhttps://github.com/scrapy/scrapy/blob/631fc65fadb874629787ae5f7fdd876b9ec96a29/scrapy/crawler.py#L119\r\n\r\nI see `init_reactor` parameter https://github.com/scrapy/scrapy/blob/631fc65fadb874629787ae5f7fdd876b9ec96a29/scrapy/crawler.py#L79 but when we use `runner.crawl` from `CrawlerRunner` theres no way to override this parameter, when is created:\r\n\r\nhttps://github.com/scrapy/scrapy/blob/631fc65fadb874629787ae5f7fdd876b9ec96a29/scrapy/crawler.py#L330-L334\r\n\r\nP.S: If I switch to `CrawlerProcess` works (even without calling `install_reactor`), just to confirm if this is expected.\r\n\r\nHere my snippet:\r\n\r\n```python\r\nfrom scrapy import Spider\r\nfrom scrapy.http import Request\r\nfrom scrapy.crawler import CrawlerRunner\r\nfrom scrapy.utils.log import configure_logging\r\nfrom scrapy.utils.project import get_project_settings\r\n\r\n\r\nclass MySpider1(Spider):\r\n    name = \"my_spider\"\r\n\r\n    custom_settings = {\r\n        \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\"\r\n    }\r\n\r\n    def start_requests(self):\r\n        yield Request(url=\"https://httpbin.org/anything\")\r\n\r\n    def parse(self, response):\r\n        yield response.json()\r\n\r\n\r\nclass MySpider2(Spider):\r\n    name = \"my_spider2\"\r\n\r\n    custom_settings = {\r\n        \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\"\r\n    }\r\n\r\n    def start_requests(self):\r\n        yield Request(url=\"https://httpbin.org/anything\")\r\n\r\n    def parse(self, response):\r\n        yield response.json()\r\n\r\n\r\nconfigure_logging()\r\nsettings = get_project_settings()\r\nrunner = CrawlerRunner(settings)\r\n# from scrapy.utils.reactor import install_reactor\r\n# install_reactor(\"twisted.internet.asyncioreactor.AsyncioSelectorReactor\")\r\nrunner.crawl(MySpider1)\r\nrunner.crawl(MySpider2)\r\nfrom twisted.internet import reactor\r\nd = runner.join()\r\nd.addBoth(lambda _: reactor.stop())\r\nreactor.run()\r\n```\n`CrawlerRunner` indeed requires you to install (and start) the reactor in your code, so it makes sense that `CrawlerRunner` examples show installing a non-default reactor manually.\n> `CrawlerRunner` indeed requires you to install (and start) the reactor in your code, so it makes sense that `CrawlerRunner` examples show installing a non-default reactor manually.\r\n\r\nGot it, thanks!",
    "created_at": "2024-05-22T10:51:11Z",
    "version": "2.11",
    "PASS_TO_PASS": [],
    "FAIL_TO_PASS": [
      "tests/test_crawler.py::CrawlerRunnerSubprocess::test_change_default_reactor",
      "tests/test_crawler.py"
    ]
  },
  {
    "repo": "scrapy/scrapy",
    "pull_number": 6368,
    "instance_id": "scrapy__scrapy-6368",
    "issue_numbers": [
      "6365"
    ],
    "base_commit": "631fc65fadb874629787ae5f7fdd876b9ec96a29",
    "patch": "diff --git a/scrapy/pipelines/media.py b/scrapy/pipelines/media.py\nindex fd5e70cb903..5b03731a42f 100644\n--- a/scrapy/pipelines/media.py\n+++ b/scrapy/pipelines/media.py\n@@ -2,6 +2,7 @@\n \n import functools\n import logging\n+from abc import ABC, abstractmethod\n from collections import defaultdict\n from typing import TYPE_CHECKING\n \n@@ -27,7 +28,7 @@ def _DUMMY_CALLBACK(response):\n     return response\n \n \n-class MediaPipeline:\n+class MediaPipeline(ABC):\n     LOG_FAILED_RESULTS = True\n \n     class SpiderInfo:\n@@ -55,14 +56,6 @@ def _handle_statuses(self, allow_redirects):\n             self.handle_httpstatus_list = SequenceExclude(range(300, 400))\n \n     def _key_for_pipe(self, key, base_class_name=None, settings=None):\n-        \"\"\"\n-        >>> MediaPipeline()._key_for_pipe(\"IMAGES\")\n-        'IMAGES'\n-        >>> class MyPipe(MediaPipeline):\n-        ...     pass\n-        >>> MyPipe()._key_for_pipe(\"IMAGES\", base_class_name=\"MediaPipeline\")\n-        'MYPIPE_IMAGES'\n-        \"\"\"\n         class_name = self.__class__.__name__\n         formatted_key = f\"{class_name.upper()}_{key}\"\n         if (\n@@ -197,21 +190,25 @@ def _cache_result_and_execute_waiters(self, result, fp, info):\n             defer_result(result).chainDeferred(wad)\n \n     # Overridable Interface\n+    @abstractmethod\n     def media_to_download(self, request, info, *, item=None):\n         \"\"\"Check request before starting download\"\"\"\n-        pass\n+        raise NotImplementedError()\n \n+    @abstractmethod\n     def get_media_requests(self, item, info):\n         \"\"\"Returns the media requests to download\"\"\"\n-        pass\n+        raise NotImplementedError()\n \n+    @abstractmethod\n     def media_downloaded(self, response, request, info, *, item=None):\n         \"\"\"Handler for success downloads\"\"\"\n-        return response\n+        raise NotImplementedError()\n \n+    @abstractmethod\n     def media_failed(self, failure, request, info):\n         \"\"\"Handler for failed downloads\"\"\"\n-        return failure\n+        raise NotImplementedError()\n \n     def item_completed(self, results, item, info):\n         \"\"\"Called per item when all media requests has been processed\"\"\"\n@@ -226,6 +223,7 @@ def item_completed(self, results, item, info):\n                     )\n         return item\n \n+    @abstractmethod\n     def file_path(self, request, response=None, info=None, *, item=None):\n         \"\"\"Returns the path where downloaded media should be stored\"\"\"\n-        pass\n+        raise NotImplementedError()\n",
    "test_patch": "diff --git a/tests/test_pipeline_media.py b/tests/test_pipeline_media.py\nindex d4dde4a4036..76345355169 100644\n--- a/tests/test_pipeline_media.py\n+++ b/tests/test_pipeline_media.py\n@@ -1,4 +1,3 @@\n-import io\n from typing import Optional\n \n from testfixtures import LogCapture\n@@ -11,7 +10,6 @@\n from scrapy.http import Request, Response\n from scrapy.http.request import NO_CALLBACK\n from scrapy.pipelines.files import FileException\n-from scrapy.pipelines.images import ImagesPipeline\n from scrapy.pipelines.media import MediaPipeline\n from scrapy.settings import Settings\n from scrapy.spiders import Spider\n@@ -35,8 +33,26 @@ def _mocked_download_func(request, info):\n     return response() if callable(response) else response\n \n \n+class UserDefinedPipeline(MediaPipeline):\n+\n+    def media_to_download(self, request, info, *, item=None):\n+        pass\n+\n+    def get_media_requests(self, item, info):\n+        pass\n+\n+    def media_downloaded(self, response, request, info, *, item=None):\n+        return {}\n+\n+    def media_failed(self, failure, request, info):\n+        return failure\n+\n+    def file_path(self, request, response=None, info=None, *, item=None):\n+        return \"\"\n+\n+\n class BaseMediaPipelineTestCase(unittest.TestCase):\n-    pipeline_class = MediaPipeline\n+    pipeline_class = UserDefinedPipeline\n     settings = None\n \n     def setUp(self):\n@@ -54,54 +70,6 @@ def tearDown(self):\n             if not name.startswith(\"_\"):\n                 disconnect_all(signal)\n \n-    def test_default_media_to_download(self):\n-        request = Request(\"http://url\")\n-        assert self.pipe.media_to_download(request, self.info) is None\n-\n-    def test_default_get_media_requests(self):\n-        item = {\"name\": \"name\"}\n-        assert self.pipe.get_media_requests(item, self.info) is None\n-\n-    def test_default_media_downloaded(self):\n-        request = Request(\"http://url\")\n-        response = Response(\"http://url\", body=b\"\")\n-        assert self.pipe.media_downloaded(response, request, self.info) is response\n-\n-    def test_default_media_failed(self):\n-        request = Request(\"http://url\")\n-        fail = Failure(Exception())\n-        assert self.pipe.media_failed(fail, request, self.info) is fail\n-\n-    def test_default_item_completed(self):\n-        item = {\"name\": \"name\"}\n-        assert self.pipe.item_completed([], item, self.info) is item\n-\n-        # Check that failures are logged by default\n-        fail = Failure(Exception())\n-        results = [(True, 1), (False, fail)]\n-\n-        with LogCapture() as log:\n-            new_item = self.pipe.item_completed(results, item, self.info)\n-\n-        assert new_item is item\n-        assert len(log.records) == 1\n-        record = log.records[0]\n-        assert record.levelname == \"ERROR\"\n-        self.assertTupleEqual(record.exc_info, failure_to_exc_info(fail))\n-\n-        # disable failure logging and check again\n-        self.pipe.LOG_FAILED_RESULTS = False\n-        with LogCapture() as log:\n-            new_item = self.pipe.item_completed(results, item, self.info)\n-        assert new_item is item\n-        assert len(log.records) == 0\n-\n-    @inlineCallbacks\n-    def test_default_process_item(self):\n-        item = {\"name\": \"name\"}\n-        new_item = yield self.pipe.process_item(item, self.spider)\n-        assert new_item is item\n-\n     def test_modify_media_request(self):\n         request = Request(\"http://url\")\n         self.pipe._modify_media_request(request)\n@@ -175,8 +143,38 @@ def test_should_remove_req_res_references_before_caching_the_results(self):\n         context = getattr(info.downloaded[fp].value, \"__context__\", None)\n         self.assertIsNone(context)\n \n+    def test_default_item_completed(self):\n+        item = {\"name\": \"name\"}\n+        assert self.pipe.item_completed([], item, self.info) is item\n+\n+        # Check that failures are logged by default\n+        fail = Failure(Exception())\n+        results = [(True, 1), (False, fail)]\n+\n+        with LogCapture() as log:\n+            new_item = self.pipe.item_completed(results, item, self.info)\n+\n+        assert new_item is item\n+        assert len(log.records) == 1\n+        record = log.records[0]\n+        assert record.levelname == \"ERROR\"\n+        self.assertTupleEqual(record.exc_info, failure_to_exc_info(fail))\n+\n+        # disable failure logging and check again\n+        self.pipe.LOG_FAILED_RESULTS = False\n+        with LogCapture() as log:\n+            new_item = self.pipe.item_completed(results, item, self.info)\n+        assert new_item is item\n+        assert len(log.records) == 0\n+\n+    @inlineCallbacks\n+    def test_default_process_item(self):\n+        item = {\"name\": \"name\"}\n+        new_item = yield self.pipe.process_item(item, self.spider)\n+        assert new_item is item\n+\n \n-class MockedMediaPipeline(MediaPipeline):\n+class MockedMediaPipeline(UserDefinedPipeline):\n     def __init__(self, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n         self._mockcalled = []\n@@ -232,7 +230,7 @@ def test_result_succeed(self):\n         )\n         item = {\"requests\": req}\n         new_item = yield self.pipe.process_item(item, self.spider)\n-        self.assertEqual(new_item[\"results\"], [(True, rsp)])\n+        self.assertEqual(new_item[\"results\"], [(True, {})])\n         self.assertEqual(\n             self.pipe._mockcalled,\n             [\n@@ -277,7 +275,7 @@ def test_mix_of_success_and_failure(self):\n         req2 = Request(\"http://url2\", meta={\"response\": fail})\n         item = {\"requests\": [req1, req2]}\n         new_item = yield self.pipe.process_item(item, self.spider)\n-        self.assertEqual(new_item[\"results\"], [(True, rsp1), (False, fail)])\n+        self.assertEqual(new_item[\"results\"], [(True, {}), (False, fail)])\n         m = self.pipe._mockcalled\n         # only once\n         self.assertEqual(m[0], \"get_media_requests\")  # first hook called\n@@ -315,7 +313,7 @@ def test_results_are_cached_across_multiple_items(self):\n         item = {\"requests\": req1}\n         new_item = yield self.pipe.process_item(item, self.spider)\n         self.assertTrue(new_item is item)\n-        self.assertEqual(new_item[\"results\"], [(True, rsp1)])\n+        self.assertEqual(new_item[\"results\"], [(True, {})])\n \n         # rsp2 is ignored, rsp1 must be in results because request fingerprints are the same\n         req2 = Request(\n@@ -325,7 +323,7 @@ def test_results_are_cached_across_multiple_items(self):\n         new_item = yield self.pipe.process_item(item, self.spider)\n         self.assertTrue(new_item is item)\n         self.assertEqual(self.fingerprint(req1), self.fingerprint(req2))\n-        self.assertEqual(new_item[\"results\"], [(True, rsp1)])\n+        self.assertEqual(new_item[\"results\"], [(True, {})])\n \n     @inlineCallbacks\n     def test_results_are_cached_for_requests_of_single_item(self):\n@@ -337,7 +335,7 @@ def test_results_are_cached_for_requests_of_single_item(self):\n         item = {\"requests\": [req1, req2]}\n         new_item = yield self.pipe.process_item(item, self.spider)\n         self.assertTrue(new_item is item)\n-        self.assertEqual(new_item[\"results\"], [(True, rsp1), (True, rsp1)])\n+        self.assertEqual(new_item[\"results\"], [(True, {}), (True, {})])\n \n     @inlineCallbacks\n     def test_wait_if_request_is_downloading(self):\n@@ -363,7 +361,7 @@ def rsp2_func():\n         req2 = Request(req1.url, meta={\"response\": rsp2_func})\n         item = {\"requests\": [req1, req2]}\n         new_item = yield self.pipe.process_item(item, self.spider)\n-        self.assertEqual(new_item[\"results\"], [(True, rsp1), (True, rsp1)])\n+        self.assertEqual(new_item[\"results\"], [(True, {}), (True, {})])\n \n     @inlineCallbacks\n     def test_use_media_to_download_result(self):\n@@ -376,57 +374,15 @@ def test_use_media_to_download_result(self):\n             [\"get_media_requests\", \"media_to_download\", \"item_completed\"],\n         )\n \n-\n-class MockedMediaPipelineDeprecatedMethods(ImagesPipeline):\n-    def __init__(self, *args, **kwargs):\n-        super().__init__(*args, **kwargs)\n-        self._mockcalled = []\n-\n-    def get_media_requests(self, item, info):\n-        item_url = item[\"image_urls\"][0]\n-        output_img = io.BytesIO()\n-        img = Image.new(\"RGB\", (60, 30), color=\"red\")\n-        img.save(output_img, format=\"JPEG\")\n-        return Request(\n-            item_url,\n-            meta={\n-                \"response\": Response(item_url, status=200, body=output_img.getvalue())\n-            },\n+    def test_key_for_pipe(self):\n+        self.assertEqual(\n+            self.pipe._key_for_pipe(\"IMAGES\", base_class_name=\"MediaPipeline\"),\n+            \"MOCKEDMEDIAPIPELINE_IMAGES\",\n         )\n \n-    def inc_stats(self, *args, **kwargs):\n-        return True\n-\n-    def media_to_download(self, request, info):\n-        self._mockcalled.append(\"media_to_download\")\n-        return super().media_to_download(request, info)\n-\n-    def media_downloaded(self, response, request, info):\n-        self._mockcalled.append(\"media_downloaded\")\n-        return super().media_downloaded(response, request, info)\n-\n-    def file_downloaded(self, response, request, info):\n-        self._mockcalled.append(\"file_downloaded\")\n-        return super().file_downloaded(response, request, info)\n-\n-    def file_path(self, request, response=None, info=None):\n-        self._mockcalled.append(\"file_path\")\n-        return super().file_path(request, response, info)\n-\n-    def thumb_path(self, request, thumb_id, response=None, info=None):\n-        self._mockcalled.append(\"thumb_path\")\n-        return super().thumb_path(request, thumb_id, response, info)\n-\n-    def get_images(self, response, request, info):\n-        self._mockcalled.append(\"get_images\")\n-        return super().get_images(response, request, info)\n-\n-    def image_downloaded(self, response, request, info):\n-        self._mockcalled.append(\"image_downloaded\")\n-        return super().image_downloaded(response, request, info)\n-\n \n class MediaPipelineAllowRedirectSettingsTestCase(unittest.TestCase):\n+\n     def _assert_request_no3xx(self, pipeline_class, settings):\n         pipe = pipeline_class(settings=Settings(settings))\n         request = Request(\"http://url\")\n@@ -452,18 +408,11 @@ def _assert_request_no3xx(self, pipeline_class, settings):\n             else:\n                 self.assertNotIn(status, request.meta[\"handle_httpstatus_list\"])\n \n-    def test_standard_setting(self):\n-        self._assert_request_no3xx(MediaPipeline, {\"MEDIA_ALLOW_REDIRECTS\": True})\n-\n     def test_subclass_standard_setting(self):\n-        class UserDefinedPipeline(MediaPipeline):\n-            pass\n \n         self._assert_request_no3xx(UserDefinedPipeline, {\"MEDIA_ALLOW_REDIRECTS\": True})\n \n     def test_subclass_specific_setting(self):\n-        class UserDefinedPipeline(MediaPipeline):\n-            pass\n \n         self._assert_request_no3xx(\n             UserDefinedPipeline, {\"USERDEFINEDPIPELINE_MEDIA_ALLOW_REDIRECTS\": True}\n",
    "problem_statement": "Fix overridable methods in MediaPipeline\n`MediaPipeline` defines several empty or almost empty \"overridable\" methods, which return things inconsistent with their overrides. I propose making all of them raise `NotImplementedError`. Alternatively `MediaPipeline` should just be made an abstract class and all those methods made abstract methods, but I have no idea if that will break anything (e.g. do all children always override all of those methods?).\r\n\r\nAnother problem is existing tests, that test specifically that e.g. `MediaPipeline.media_downloaded()` returns a response, which makes no sense to me (normally `media_downloaded()` returns a file info dict), so all those need to be changed or removed.\r\n\r\nAnd another problem, indirectly related to this, is that this interface is very poorly documented, most of these functions are not mentioned in the docs at all, so it's not always clear what should they take and return (and the code uses many of them as callbacks in long callback chains so it's not clear even from the code).\n",
    "hints_text": "",
    "created_at": "2024-05-18T19:20:40Z",
    "version": "2.11",
    "PASS_TO_PASS": [],
    "FAIL_TO_PASS": [
      "tests/test_pipeline_media.py"
    ]
  },
  {
    "repo": "scrapy/scrapy",
    "pull_number": 6352,
    "instance_id": "scrapy__scrapy-6352",
    "issue_numbers": [
      "6340",
      "6340"
    ],
    "base_commit": "ae7bb849f50af0b91eea4f022d93ad201e545c06",
    "patch": "diff --git a/docs/news.rst b/docs/news.rst\nindex fafea0bf8aa..7db4e59a10e 100644\n--- a/docs/news.rst\n+++ b/docs/news.rst\n@@ -3,6 +3,20 @@\n Release notes\n =============\n \n+\n+.. _release-VERSION:\n+\n+Scrapy VERSION (YYYY-MM-DD)\n+---------------------------\n+\n+Deprecations\n+~~~~~~~~~~~~\n+\n+-   :func:`scrapy.core.downloader.Downloader._get_slot_key` is now deprecated.\n+    Consider using its corresponding public method get_slot_key() instead.\n+    (:issue:`6340`)\n+\n+\n .. _release-2.11.1:\n \n Scrapy 2.11.1 (2024-02-14)\ndiff --git a/scrapy/core/downloader/__init__.py b/scrapy/core/downloader/__init__.py\nindex 98e1af6fb1c..0ab3bdb779b 100644\n--- a/scrapy/core/downloader/__init__.py\n+++ b/scrapy/core/downloader/__init__.py\n@@ -1,4 +1,5 @@\n import random\n+import warnings\n from collections import deque\n from datetime import datetime\n from time import time\n@@ -10,6 +11,7 @@\n from scrapy import Request, Spider, signals\n from scrapy.core.downloader.handlers import DownloadHandlers\n from scrapy.core.downloader.middleware import DownloaderMiddlewareManager\n+from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.http import Response\n from scrapy.resolver import dnscache\n from scrapy.settings import BaseSettings\n@@ -125,7 +127,7 @@ def needs_backout(self) -> bool:\n         return len(self.active) >= self.total_concurrency\n \n     def _get_slot(self, request: Request, spider: Spider) -> Tuple[str, Slot]:\n-        key = self._get_slot_key(request, spider)\n+        key = self.get_slot_key(request)\n         if key not in self.slots:\n             slot_settings = self.per_slot_settings.get(key, {})\n             conc = (\n@@ -143,7 +145,7 @@ def _get_slot(self, request: Request, spider: Spider) -> Tuple[str, Slot]:\n \n         return key, self.slots[key]\n \n-    def _get_slot_key(self, request: Request, spider: Optional[Spider]) -> str:\n+    def get_slot_key(self, request: Request) -> str:\n         if self.DOWNLOAD_SLOT in request.meta:\n             return cast(str, request.meta[self.DOWNLOAD_SLOT])\n \n@@ -153,6 +155,14 @@ def _get_slot_key(self, request: Request, spider: Optional[Spider]) -> str:\n \n         return key\n \n+    def _get_slot_key(self, request: Request, spider: Optional[Spider]) -> str:\n+        warnings.warn(\n+            \"Use of this protected method is deprecated. Consider using its corresponding public method get_slot_key() instead.\",\n+            ScrapyDeprecationWarning,\n+            stacklevel=2,\n+        )\n+        return self.get_slot_key(request)\n+\n     def _enqueue_request(self, request: Request, spider: Spider) -> Deferred:\n         key, slot = self._get_slot(request, spider)\n         request.meta[self.DOWNLOAD_SLOT] = key\ndiff --git a/scrapy/pqueues.py b/scrapy/pqueues.py\nindex 773825c5e41..58a47ef0ff0 100644\n--- a/scrapy/pqueues.py\n+++ b/scrapy/pqueues.py\n@@ -180,7 +180,7 @@ def stats(self, possible_slots: Iterable[str]) -> List[Tuple[int, str]]:\n         return [(self._active_downloads(slot), slot) for slot in possible_slots]\n \n     def get_slot_key(self, request: Request) -> str:\n-        return self.downloader._get_slot_key(request, None)\n+        return self.downloader.get_slot_key(request)\n \n     def _active_downloads(self, slot: str) -> int:\n         \"\"\"Return a number of requests in a Downloader for a given slot\"\"\"\n",
    "test_patch": "diff --git a/tests/test_scheduler.py b/tests/test_scheduler.py\nindex 37099dae676..02b50baa3a6 100644\n--- a/tests/test_scheduler.py\n+++ b/tests/test_scheduler.py\n@@ -25,7 +25,7 @@ class MockDownloader:\n     def __init__(self):\n         self.slots = {}\n \n-    def _get_slot_key(self, request, spider):\n+    def get_slot_key(self, request):\n         if Downloader.DOWNLOAD_SLOT in request.meta:\n             return request.meta[Downloader.DOWNLOAD_SLOT]\n \n@@ -273,14 +273,14 @@ def test_logic(self):\n         while self.scheduler.has_pending_requests():\n             request = self.scheduler.next_request()\n             # pylint: disable=protected-access\n-            slot = downloader._get_slot_key(request, None)\n+            slot = downloader.get_slot_key(request)\n             dequeued_slots.append(slot)\n             downloader.increment(slot)\n             requests.append(request)\n \n         for request in requests:\n             # pylint: disable=protected-access\n-            slot = downloader._get_slot_key(request, None)\n+            slot = downloader.get_slot_key(request)\n             downloader.decrement(slot)\n \n         self.assertTrue(\n",
    "problem_statement": "Deprecate the `spider` argument to `Downloader._get_slot_key()`\nThe `spider` argument is not used inside the method since 2012, but we can't remove it as external code calls it (either because it subclasses `Downloader` or because it wants the slot name).\r\n\r\nActually maybe we want to promote it to a public method for the second reason? Not sure how often it's needed but see e.g. https://github.com/scrapy-plugins/scrapy-zyte-api/blob/a2284c8cdf157ef6d36c1cc413933761c5ed792b/scrapy_zyte_api/_middlewares.py#L32\nDeprecate the `spider` argument to `Downloader._get_slot_key()`\nThe `spider` argument is not used inside the method since 2012, but we can't remove it as external code calls it (either because it subclasses `Downloader` or because it wants the slot name).\r\n\r\nActually maybe we want to promote it to a public method for the second reason? Not sure how often it's needed but see e.g. https://github.com/scrapy-plugins/scrapy-zyte-api/blob/a2284c8cdf157ef6d36c1cc413933761c5ed792b/scrapy_zyte_api/_middlewares.py#L32\n",
    "hints_text": "+1 to make it public.\r\n\r\nAlthough I wonder if, for examples like the linked one, the right approach could be to allow customizing how slots IDs are generated instead.\n@wRAR do you want to promote this method to public and keep the old method with some Deprecate message ?\nYeah, let's make it public and without the extra argument and keep the old one with a deprecation warning.\n+1 to make it public.\r\n\r\nAlthough I wonder if, for examples like the linked one, the right approach could be to allow customizing how slots IDs are generated instead.\n@wRAR do you want to promote this method to public and keep the old method with some Deprecate message ?\nYeah, let's make it public and without the extra argument and keep the old one with a deprecation warning.",
    "created_at": "2024-05-09T18:52:31Z",
    "version": "2.11",
    "PASS_TO_PASS": [],
    "FAIL_TO_PASS": [
      "tests/test_scheduler.py"
    ]
  },
  {
    "repo": "scrapy/scrapy",
    "pull_number": 6347,
    "instance_id": "scrapy__scrapy-6347",
    "issue_numbers": [
      "6342",
      "6342"
    ],
    "base_commit": "4300a1d240c7c2c21a4ef0c1c60c3d844493e516",
    "patch": "diff --git a/docs/topics/downloader-middleware.rst b/docs/topics/downloader-middleware.rst\nindex 1abbc49684f..d4cd062fe38 100644\n--- a/docs/topics/downloader-middleware.rst\n+++ b/docs/topics/downloader-middleware.rst\n@@ -884,6 +884,10 @@ Meta tags within these tags are ignored.\n    The default value of :setting:`METAREFRESH_IGNORE_TAGS` changed from\n    ``['script', 'noscript']`` to ``[]``.\n \n+.. versionchanged:: VERSION\n+   The default value of :setting:`METAREFRESH_IGNORE_TAGS` changed from\n+   ``[]`` to ``['noscript']``.\n+\n .. setting:: METAREFRESH_MAXDELAY\n \n METAREFRESH_MAXDELAY\ndiff --git a/scrapy/settings/default_settings.py b/scrapy/settings/default_settings.py\nindex 2b3d95a0e14..d7ac7ec350f 100644\n--- a/scrapy/settings/default_settings.py\n+++ b/scrapy/settings/default_settings.py\n@@ -239,7 +239,7 @@\n MEMUSAGE_WARNING_MB = 0\n \n METAREFRESH_ENABLED = True\n-METAREFRESH_IGNORE_TAGS = []\n+METAREFRESH_IGNORE_TAGS = [\"noscript\"]\n METAREFRESH_MAXDELAY = 100\n \n NEWSPIDER_MODULE = \"\"\n",
    "test_patch": "diff --git a/tests/test_downloadermiddleware_redirect.py b/tests/test_downloadermiddleware_redirect.py\nindex 10b8ca9afb9..83ff259823a 100644\n--- a/tests/test_downloadermiddleware_redirect.py\n+++ b/tests/test_downloadermiddleware_redirect.py\n@@ -395,9 +395,8 @@ def test_ignore_tags_default(self):\n             \"\"\"content=\"0;URL='http://example.org/newpage'\"></noscript>\"\"\"\n         )\n         rsp = HtmlResponse(req.url, body=body.encode())\n-        req2 = self.mw.process_response(req, rsp, self.spider)\n-        assert isinstance(req2, Request)\n-        self.assertEqual(req2.url, \"http://example.org/newpage\")\n+        response = self.mw.process_response(req, rsp, self.spider)\n+        assert isinstance(response, Response)\n \n     def test_ignore_tags_1_x_list(self):\n         \"\"\"Test that Scrapy 1.x behavior remains possible\"\"\"\n",
    "problem_statement": "Set METAREFRESH_IGNORE_TAGS to [\"noscript\"] by default\nI was wrong in https://github.com/scrapy/scrapy/issues/3844. The default value should be `[\"noscript\"]`, to deal with [antibot behaviors](https://github.com/scrapy/scrapy/commit/ec1ef0235f9deee0c263c9b31652d3e74a754acc).\r\n\r\nFound by @mukthy.\nSet METAREFRESH_IGNORE_TAGS to [\"noscript\"] by default\nI was wrong in https://github.com/scrapy/scrapy/issues/3844. The default value should be `[\"noscript\"]`, to deal with [antibot behaviors](https://github.com/scrapy/scrapy/commit/ec1ef0235f9deee0c263c9b31652d3e74a754acc).\r\n\r\nFound by @mukthy.\n",
    "hints_text": "on it\non it",
    "created_at": "2024-05-08T16:39:49Z",
    "version": "2.11",
    "PASS_TO_PASS": [],
    "FAIL_TO_PASS": [
      "tests/test_downloadermiddleware_redirect.py"
    ]
  },
  {
    "repo": "scrapy/scrapy",
    "pull_number": 6269,
    "instance_id": "scrapy__scrapy-6269",
    "issue_numbers": [
      "6263"
    ],
    "base_commit": "bf149356fc6e519e92fb55150a60b40b14e45ae8",
    "patch": "diff --git a/scrapy/downloadermiddlewares/httpcompression.py b/scrapy/downloadermiddlewares/httpcompression.py\nindex aa3abe85379..0e5e215ac8e 100644\n--- a/scrapy/downloadermiddlewares/httpcompression.py\n+++ b/scrapy/downloadermiddlewares/httpcompression.py\n@@ -29,7 +29,10 @@\n ACCEPTED_ENCODINGS: List[bytes] = [b\"gzip\", b\"deflate\"]\n \n try:\n-    import brotli  # noqa: F401\n+    try:\n+        import brotli  # noqa: F401\n+    except ImportError:\n+        import brotlicffi  # noqa: F401\n except ImportError:\n     pass\n else:\ndiff --git a/scrapy/utils/_compression.py b/scrapy/utils/_compression.py\nindex 7c40d0a02d1..84c255c28f9 100644\n--- a/scrapy/utils/_compression.py\n+++ b/scrapy/utils/_compression.py\n@@ -5,7 +5,10 @@\n from scrapy.exceptions import ScrapyDeprecationWarning\n \n try:\n-    import brotli\n+    try:\n+        import brotli\n+    except ImportError:\n+        import brotlicffi as brotli\n except ImportError:\n     pass\n else:\n@@ -17,9 +20,9 @@\n                 \"You have brotlipy installed, and Scrapy will use it, but \"\n                 \"Scrapy support for brotlipy is deprecated and will stop \"\n                 \"working in a future version of Scrapy. brotlipy itself is \"\n-                \"deprecated, it has been superseded by brotlicffi (not \"\n-                \"currently supported by Scrapy). Please, uninstall brotlipy \"\n-                \"and install brotli instead. brotlipy has the same import \"\n+                \"deprecated, it has been superseded by brotlicffi. \"\n+                \"Please, uninstall brotlipy \"\n+                \"and install brotli or brotlicffi instead. brotlipy has the same import \"\n                 \"name as brotli, so keeping both installed is strongly \"\n                 \"discouraged.\"\n             ),\n",
    "test_patch": "diff --git a/tests/requirements.txt b/tests/requirements.txt\nindex 5b75674f513..ca5f6ddbd93 100644\n--- a/tests/requirements.txt\n+++ b/tests/requirements.txt\n@@ -11,8 +11,7 @@ uvloop; platform_system != \"Windows\"\n \n bpython  # optional for shell wrapper tests\n brotli; implementation_name != 'pypy'  # optional for HTTP compress downloader middleware tests\n-# 1.1.0 is broken on PyPy: https://github.com/google/brotli/issues/1072\n-brotli==1.0.9; implementation_name == 'pypy'  # optional for HTTP compress downloader middleware tests\n+brotlicffi; implementation_name == 'pypy'  # optional for HTTP compress downloader middleware tests\n zstandard; implementation_name != 'pypy'  # optional for HTTP compress downloader middleware tests\n ipython\n pywin32; sys_platform == \"win32\"\ndiff --git a/tests/test_downloadermiddleware_httpcompression.py b/tests/test_downloadermiddleware_httpcompression.py\nindex ae5569d0a8a..7c36f748e35 100644\n--- a/tests/test_downloadermiddleware_httpcompression.py\n+++ b/tests/test_downloadermiddleware_httpcompression.py\n@@ -130,7 +130,10 @@ def test_process_response_gzip(self):\n \n     def test_process_response_br(self):\n         try:\n-            import brotli  # noqa: F401\n+            try:\n+                import brotli  # noqa: F401\n+            except ImportError:\n+                import brotlicffi  # noqa: F401\n         except ImportError:\n             raise SkipTest(\"no brotli\")\n         response = self._getresponse(\"br\")\n@@ -448,7 +451,10 @@ def _test_compression_bomb_setting(self, compression_id):\n \n     def test_compression_bomb_setting_br(self):\n         try:\n-            import brotli  # noqa: F401\n+            try:\n+                import brotli  # noqa: F401\n+            except ImportError:\n+                import brotlicffi  # noqa: F401\n         except ImportError:\n             raise SkipTest(\"no brotli\")\n         self._test_compression_bomb_setting(\"br\")\n@@ -486,7 +492,10 @@ class DownloadMaxSizeSpider(Spider):\n \n     def test_compression_bomb_spider_attr_br(self):\n         try:\n-            import brotli  # noqa: F401\n+            try:\n+                import brotli  # noqa: F401\n+            except ImportError:\n+                import brotlicffi  # noqa: F401\n         except ImportError:\n             raise SkipTest(\"no brotli\")\n         self._test_compression_bomb_spider_attr(\"br\")\n@@ -522,7 +531,10 @@ def _test_compression_bomb_request_meta(self, compression_id):\n \n     def test_compression_bomb_request_meta_br(self):\n         try:\n-            import brotli  # noqa: F401\n+            try:\n+                import brotli  # noqa: F401\n+            except ImportError:\n+                import brotlicffi  # noqa: F401\n         except ImportError:\n             raise SkipTest(\"no brotli\")\n         self._test_compression_bomb_request_meta(\"br\")\n@@ -568,7 +580,10 @@ def _test_download_warnsize_setting(self, compression_id):\n \n     def test_download_warnsize_setting_br(self):\n         try:\n-            import brotli  # noqa: F401\n+            try:\n+                import brotli  # noqa: F401\n+            except ImportError:\n+                import brotlicffi  # noqa: F401\n         except ImportError:\n             raise SkipTest(\"no brotli\")\n         self._test_download_warnsize_setting(\"br\")\n@@ -616,7 +631,10 @@ class DownloadWarnSizeSpider(Spider):\n \n     def test_download_warnsize_spider_attr_br(self):\n         try:\n-            import brotli  # noqa: F401\n+            try:\n+                import brotli  # noqa: F401\n+            except ImportError:\n+                import brotlicffi  # noqa: F401\n         except ImportError:\n             raise SkipTest(\"no brotli\")\n         self._test_download_warnsize_spider_attr(\"br\")\n@@ -662,7 +680,10 @@ def _test_download_warnsize_request_meta(self, compression_id):\n \n     def test_download_warnsize_request_meta_br(self):\n         try:\n-            import brotli  # noqa: F401\n+            try:\n+                import brotli  # noqa: F401\n+            except ImportError:\n+                import brotlicffi  # noqa: F401\n         except ImportError:\n             raise SkipTest(\"no brotli\")\n         self._test_download_warnsize_request_meta(\"br\")\n",
    "problem_statement": "Add brotlicffi support\nCurrently, brotli compression is supported when using `brotli` or `brotlipy` (deprecated). We should also support it thorugh `brotlicffi`, the new name of `brotlipy`, which performs worse than `brotli` but works on PyPy.\n",
    "hints_text": "",
    "created_at": "2024-03-06T01:20:27Z",
    "version": "2.11",
    "PASS_TO_PASS": [],
    "FAIL_TO_PASS": [
      "tests/requirements.txt",
      "tests/test_downloadermiddleware_httpcompression.py"
    ]
  },
  {
    "repo": "scrapy/scrapy",
    "pull_number": 6257,
    "instance_id": "scrapy__scrapy-6257",
    "issue_numbers": [
      "6254"
    ],
    "base_commit": "532cc8a517b31dca4ca28d0a35d25d1a790c9801",
    "patch": "diff --git a/pylintrc b/pylintrc\nindex 78004e78ac8..c60e4e16a33 100644\n--- a/pylintrc\n+++ b/pylintrc\n@@ -68,7 +68,6 @@ disable=abstract-method,\n         too-many-public-methods,\n         too-many-return-statements,\n         unbalanced-tuple-unpacking,\n-        unnecessary-comprehension,\n         unnecessary-dunder-call,\n         unnecessary-pass,\n         unreachable,\n@@ -77,7 +76,6 @@ disable=abstract-method,\n         unused-private-member,\n         unused-variable,\n         unused-wildcard-import,\n-        use-dict-literal,\n         used-before-assignment,\n         useless-return,\n         wildcard-import,\ndiff --git a/scrapy/downloadermiddlewares/httpcompression.py b/scrapy/downloadermiddlewares/httpcompression.py\nindex f0ad24f72a6..aa3abe85379 100644\n--- a/scrapy/downloadermiddlewares/httpcompression.py\n+++ b/scrapy/downloadermiddlewares/httpcompression.py\n@@ -135,7 +135,7 @@ def process_response(\n                 respcls = responsetypes.from_args(\n                     headers=response.headers, url=response.url, body=decoded_body\n                 )\n-                kwargs = dict(cls=respcls, body=decoded_body)\n+                kwargs = {\"cls\": respcls, \"body\": decoded_body}\n                 if issubclass(respcls, TextResponse):\n                     # force recalculating the encoding until we make sure the\n                     # responsetypes guessing is reliable\ndiff --git a/scrapy/spiders/crawl.py b/scrapy/spiders/crawl.py\nindex ebb4f598456..2a3913da582 100644\n--- a/scrapy/spiders/crawl.py\n+++ b/scrapy/spiders/crawl.py\n@@ -85,7 +85,7 @@ def _build_request(self, rule_index, link):\n             url=link.url,\n             callback=self._callback,\n             errback=self._errback,\n-            meta=dict(rule=rule_index, link_text=link.text),\n+            meta={\"rule\": rule_index, \"link_text\": link.text},\n         )\n \n     def _requests_to_follow(self, response):\ndiff --git a/scrapy/utils/python.py b/scrapy/utils/python.py\nindex 7b408c49cf4..1e7364e494d 100644\n--- a/scrapy/utils/python.py\n+++ b/scrapy/utils/python.py\n@@ -162,7 +162,7 @@ def _chunk_iter() -> Generator[Tuple[str, int], Any, None]:\n         pattern = re.compile(pattern)\n \n     for chunk, offset in _chunk_iter():\n-        matches = [match for match in pattern.finditer(chunk)]\n+        matches = list(pattern.finditer(chunk))\n         if matches:\n             start, end = matches[-1].span()\n             return offset + start, offset + end\n",
    "test_patch": "diff --git a/tests/test_crawl.py b/tests/test_crawl.py\nindex 96d43b2b96d..6cde4ed8c50 100644\n--- a/tests/test_crawl.py\n+++ b/tests/test_crawl.py\n@@ -76,11 +76,11 @@ def test_randomized_delay(self):\n \n     @defer.inlineCallbacks\n     def _test_delay(self, total, delay, randomize=False):\n-        crawl_kwargs = dict(\n-            maxlatency=delay * 2,\n-            mockserver=self.mockserver,\n-            total=total,\n-        )\n+        crawl_kwargs = {\n+            \"maxlatency\": delay * 2,\n+            \"mockserver\": self.mockserver,\n+            \"total\": total,\n+        }\n         tolerance = 1 - (0.6 if randomize else 0.2)\n \n         settings = {\"DOWNLOAD_DELAY\": delay, \"RANDOMIZE_DOWNLOAD_DELAY\": randomize}\ndiff --git a/tests/test_downloadermiddleware_cookies.py b/tests/test_downloadermiddleware_cookies.py\nindex 4a81a638ee1..425fabcc7a8 100644\n--- a/tests/test_downloadermiddleware_cookies.py\n+++ b/tests/test_downloadermiddleware_cookies.py\n@@ -320,7 +320,7 @@ def test_local_domain(self):\n \n     @pytest.mark.xfail(reason=\"Cookie header is not currently being processed\")\n     def test_keep_cookie_from_default_request_headers_middleware(self):\n-        DEFAULT_REQUEST_HEADERS = dict(Cookie=\"default=value; asdf=qwerty\")\n+        DEFAULT_REQUEST_HEADERS = {\"Cookie\": \"default=value; asdf=qwerty\"}\n         mw_default_headers = DefaultHeadersMiddleware(DEFAULT_REQUEST_HEADERS.items())\n         # overwrite with values from 'cookies' request argument\n         req1 = Request(\"http://example.org\", cookies={\"default\": \"something\"})\ndiff --git a/tests/test_downloadermiddleware_httpauth.py b/tests/test_downloadermiddleware_httpauth.py\nindex fc110e6cc2c..500af65364a 100644\n--- a/tests/test_downloadermiddleware_httpauth.py\n+++ b/tests/test_downloadermiddleware_httpauth.py\n@@ -59,7 +59,7 @@ def test_auth_subdomain(self):\n         self.assertEqual(req.headers[\"Authorization\"], basic_auth_header(\"foo\", \"bar\"))\n \n     def test_auth_already_set(self):\n-        req = Request(\"http://example.com/\", headers=dict(Authorization=\"Digest 123\"))\n+        req = Request(\"http://example.com/\", headers={\"Authorization\": \"Digest 123\"})\n         assert self.mw.process_request(req, self.spider) is None\n         self.assertEqual(req.headers[\"Authorization\"], b\"Digest 123\")\n \n@@ -79,6 +79,6 @@ def test_auth(self):\n         self.assertEqual(req.headers[\"Authorization\"], basic_auth_header(\"foo\", \"bar\"))\n \n     def test_auth_already_set(self):\n-        req = Request(\"http://example.com/\", headers=dict(Authorization=\"Digest 123\"))\n+        req = Request(\"http://example.com/\", headers={\"Authorization\": \"Digest 123\"})\n         assert self.mw.process_request(req, self.spider) is None\n         self.assertEqual(req.headers[\"Authorization\"], b\"Digest 123\")\ndiff --git a/tests/test_exporters.py b/tests/test_exporters.py\nindex 59b724495d1..fa938904412 100644\n--- a/tests/test_exporters.py\n+++ b/tests/test_exporters.py\n@@ -152,7 +152,7 @@ def test_invalid_option(self):\n \n     def test_nested_item(self):\n         i1 = self.item_class(name=\"Joseph\", age=\"22\")\n-        i2 = dict(name=\"Maria\", age=i1)\n+        i2 = {\"name\": \"Maria\", \"age\": i1}\n         i3 = self.item_class(name=\"Jesus\", age=i2)\n         ie = self._get_exporter()\n         exported = ie.export_item(i3)\n@@ -185,7 +185,7 @@ def test_export_list(self):\n \n     def test_export_item_dict_list(self):\n         i1 = self.item_class(name=\"Joseph\", age=\"22\")\n-        i2 = dict(name=\"Maria\", age=[i1])\n+        i2 = {\"name\": \"Maria\", \"age\": [i1]}\n         i3 = self.item_class(name=\"Jesus\", age=[i2])\n         ie = self._get_exporter()\n         exported = ie.export_item(i3)\n@@ -373,7 +373,7 @@ class TestItem2(Item):\n \n     def test_join_multivalue_not_strings(self):\n         self.assertExportResult(\n-            item=dict(name=\"John\", friends=[4, 8]),\n+            item={\"name\": \"John\", \"friends\": [4, 8]},\n             include_headers_line=False,\n             expected='\"[4, 8]\",John\\r\\n',\n         )\n@@ -388,14 +388,14 @@ def test_nonstring_types_item(self):\n     def test_errors_default(self):\n         with self.assertRaises(UnicodeEncodeError):\n             self.assertExportResult(\n-                item=dict(text=\"W\\u0275\\u200Brd\"),\n+                item={\"text\": \"W\\u0275\\u200Brd\"},\n                 expected=None,\n                 encoding=\"windows-1251\",\n             )\n \n     def test_errors_xmlcharrefreplace(self):\n         self.assertExportResult(\n-            item=dict(text=\"W\\u0275\\u200Brd\"),\n+            item={\"text\": \"W\\u0275\\u200Brd\"},\n             include_headers_line=False,\n             expected=\"W&#629;&#8203;rd\\r\\n\",\n             encoding=\"windows-1251\",\n@@ -455,8 +455,8 @@ def test_multivalued_fields(self):\n         )\n \n     def test_nested_item(self):\n-        i1 = dict(name=\"foo\\xa3hoo\", age=\"22\")\n-        i2 = dict(name=\"bar\", age=i1)\n+        i1 = {\"name\": \"foo\\xa3hoo\", \"age\": \"22\"}\n+        i2 = {\"name\": \"bar\", \"age\": i1}\n         i3 = self.item_class(name=\"buz\", age=i2)\n \n         self.assertExportResult(\n@@ -478,8 +478,8 @@ def test_nested_item(self):\n         )\n \n     def test_nested_list_item(self):\n-        i1 = dict(name=\"foo\")\n-        i2 = dict(name=\"bar\", v2={\"egg\": [\"spam\"]})\n+        i1 = {\"name\": \"foo\"}\n+        i2 = {\"name\": \"bar\", \"v2\": {\"egg\": [\"spam\"]}}\n         i3 = self.item_class(name=\"buz\", age=[i1, i2])\n \n         self.assertExportResult(\n@@ -534,7 +534,7 @@ def _check_output(self):\n \n     def test_nested_item(self):\n         i1 = self.item_class(name=\"Joseph\", age=\"22\")\n-        i2 = dict(name=\"Maria\", age=i1)\n+        i2 = {\"name\": \"Maria\", \"age\": i1}\n         i3 = self.item_class(name=\"Jesus\", age=i2)\n         self.ie.start_exporting()\n         self.ie.export_item(i3)\n@@ -622,9 +622,9 @@ def test_nested_item(self):\n         self.assertEqual(exported, [expected])\n \n     def test_nested_dict_item(self):\n-        i1 = dict(name=\"Joseph\\xa3\", age=\"22\")\n+        i1 = {\"name\": \"Joseph\\xa3\", \"age\": \"22\"}\n         i2 = self.item_class(name=\"Maria\", age=i1)\n-        i3 = dict(name=\"Jesus\", age=i2)\n+        i3 = {\"name\": \"Jesus\", \"age\": i2}\n         self.ie.start_exporting()\n         self.ie.export_item(i3)\n         self.ie.finish_exporting()\ndiff --git a/tests/test_linkextractors.py b/tests/test_linkextractors.py\nindex 6b4df90d888..217c7a29904 100644\n--- a/tests/test_linkextractors.py\n+++ b/tests/test_linkextractors.py\n@@ -37,7 +37,7 @@ def test_extract_all_links(self):\n             page4_url = \"http://example.com/page%204.html\"\n \n             self.assertEqual(\n-                [link for link in lx.extract_links(self.response)],\n+                list(lx.extract_links(self.response)),\n                 [\n                     Link(url=\"http://example.com/sample1.html\", text=\"\"),\n                     Link(url=\"http://example.com/sample2.html\", text=\"sample 2\"),\n@@ -55,7 +55,7 @@ def test_extract_all_links(self):\n         def test_extract_filter_allow(self):\n             lx = self.extractor_cls(allow=(\"sample\",))\n             self.assertEqual(\n-                [link for link in lx.extract_links(self.response)],\n+                list(lx.extract_links(self.response)),\n                 [\n                     Link(url=\"http://example.com/sample1.html\", text=\"\"),\n                     Link(url=\"http://example.com/sample2.html\", text=\"sample 2\"),\n@@ -70,7 +70,7 @@ def test_extract_filter_allow(self):\n         def test_extract_filter_allow_with_duplicates(self):\n             lx = self.extractor_cls(allow=(\"sample\",), unique=False)\n             self.assertEqual(\n-                [link for link in lx.extract_links(self.response)],\n+                list(lx.extract_links(self.response)),\n                 [\n                     Link(url=\"http://example.com/sample1.html\", text=\"\"),\n                     Link(url=\"http://example.com/sample2.html\", text=\"sample 2\"),\n@@ -93,7 +93,7 @@ def test_extract_filter_allow_with_duplicates(self):\n         def test_extract_filter_allow_with_duplicates_canonicalize(self):\n             lx = self.extractor_cls(allow=(\"sample\",), unique=False, canonicalize=True)\n             self.assertEqual(\n-                [link for link in lx.extract_links(self.response)],\n+                list(lx.extract_links(self.response)),\n                 [\n                     Link(url=\"http://example.com/sample1.html\", text=\"\"),\n                     Link(url=\"http://example.com/sample2.html\", text=\"sample 2\"),\n@@ -116,7 +116,7 @@ def test_extract_filter_allow_with_duplicates_canonicalize(self):\n         def test_extract_filter_allow_no_duplicates_canonicalize(self):\n             lx = self.extractor_cls(allow=(\"sample\",), unique=True, canonicalize=True)\n             self.assertEqual(\n-                [link for link in lx.extract_links(self.response)],\n+                list(lx.extract_links(self.response)),\n                 [\n                     Link(url=\"http://example.com/sample1.html\", text=\"\"),\n                     Link(url=\"http://example.com/sample2.html\", text=\"sample 2\"),\n@@ -127,7 +127,7 @@ def test_extract_filter_allow_no_duplicates_canonicalize(self):\n         def test_extract_filter_allow_and_deny(self):\n             lx = self.extractor_cls(allow=(\"sample\",), deny=(\"3\",))\n             self.assertEqual(\n-                [link for link in lx.extract_links(self.response)],\n+                list(lx.extract_links(self.response)),\n                 [\n                     Link(url=\"http://example.com/sample1.html\", text=\"\"),\n                     Link(url=\"http://example.com/sample2.html\", text=\"sample 2\"),\n@@ -137,7 +137,7 @@ def test_extract_filter_allow_and_deny(self):\n         def test_extract_filter_allowed_domains(self):\n             lx = self.extractor_cls(allow_domains=(\"google.com\",))\n             self.assertEqual(\n-                [link for link in lx.extract_links(self.response)],\n+                list(lx.extract_links(self.response)),\n                 [\n                     Link(url=\"http://www.google.com/something\", text=\"\"),\n                 ],\n@@ -148,7 +148,7 @@ def test_extraction_using_single_values(self):\n \n             lx = self.extractor_cls(allow=\"sample\")\n             self.assertEqual(\n-                [link for link in lx.extract_links(self.response)],\n+                list(lx.extract_links(self.response)),\n                 [\n                     Link(url=\"http://example.com/sample1.html\", text=\"\"),\n                     Link(url=\"http://example.com/sample2.html\", text=\"sample 2\"),\n@@ -162,7 +162,7 @@ def test_extraction_using_single_values(self):\n \n             lx = self.extractor_cls(allow=\"sample\", deny=\"3\")\n             self.assertEqual(\n-                [link for link in lx.extract_links(self.response)],\n+                list(lx.extract_links(self.response)),\n                 [\n                     Link(url=\"http://example.com/sample1.html\", text=\"\"),\n                     Link(url=\"http://example.com/sample2.html\", text=\"sample 2\"),\n@@ -171,7 +171,7 @@ def test_extraction_using_single_values(self):\n \n             lx = self.extractor_cls(allow_domains=\"google.com\")\n             self.assertEqual(\n-                [link for link in lx.extract_links(self.response)],\n+                list(lx.extract_links(self.response)),\n                 [\n                     Link(url=\"http://www.google.com/something\", text=\"\"),\n                 ],\n@@ -179,7 +179,7 @@ def test_extraction_using_single_values(self):\n \n             lx = self.extractor_cls(deny_domains=\"example.com\")\n             self.assertEqual(\n-                [link for link in lx.extract_links(self.response)],\n+                list(lx.extract_links(self.response)),\n                 [\n                     Link(url=\"http://www.google.com/something\", text=\"\"),\n                 ],\n@@ -265,7 +265,7 @@ def test_matches(self):\n         def test_restrict_xpaths(self):\n             lx = self.extractor_cls(restrict_xpaths=('//div[@id=\"subwrapper\"]',))\n             self.assertEqual(\n-                [link for link in lx.extract_links(self.response)],\n+                list(lx.extract_links(self.response)),\n                 [\n                     Link(url=\"http://example.com/sample1.html\", text=\"\"),\n                     Link(url=\"http://example.com/sample2.html\", text=\"sample 2\"),\n@@ -337,7 +337,7 @@ def test_restrict_css_and_restrict_xpaths_together(self):\n                 restrict_css=(\"#subwrapper + a\",),\n             )\n             self.assertEqual(\n-                [link for link in lx.extract_links(self.response)],\n+                list(lx.extract_links(self.response)),\n                 [\n                     Link(url=\"http://example.com/sample1.html\", text=\"\"),\n                     Link(url=\"http://example.com/sample2.html\", text=\"sample 2\"),\n@@ -705,7 +705,7 @@ def test_link_wrong_href(self):\n             response = HtmlResponse(\"http://example.org/index.html\", body=html)\n             lx = self.extractor_cls()\n             self.assertEqual(\n-                [link for link in lx.extract_links(response)],\n+                list(lx.extract_links(response)),\n                 [\n                     Link(\n                         url=\"http://example.org/item1.html\",\n@@ -758,7 +758,7 @@ def test_link_wrong_href(self):\n         response = HtmlResponse(\"http://example.org/index.html\", body=html)\n         lx = self.extractor_cls()\n         self.assertEqual(\n-            [link for link in lx.extract_links(response)],\n+            list(lx.extract_links(response)),\n             [\n                 Link(\n                     url=\"http://example.org/item1.html\", text=\"Item 1\", nofollow=False\n@@ -779,7 +779,7 @@ def test_link_restrict_text(self):\n         # Simple text inclusion test\n         lx = self.extractor_cls(restrict_text=\"dog\")\n         self.assertEqual(\n-            [link for link in lx.extract_links(response)],\n+            list(lx.extract_links(response)),\n             [\n                 Link(\n                     url=\"http://example.org/item2.html\",\n@@ -791,7 +791,7 @@ def test_link_restrict_text(self):\n         # Unique regex test\n         lx = self.extractor_cls(restrict_text=r\"of.*dog\")\n         self.assertEqual(\n-            [link for link in lx.extract_links(response)],\n+            list(lx.extract_links(response)),\n             [\n                 Link(\n                     url=\"http://example.org/item2.html\",\n@@ -803,7 +803,7 @@ def test_link_restrict_text(self):\n         # Multiple regex test\n         lx = self.extractor_cls(restrict_text=[r\"of.*dog\", r\"of.*cat\"])\n         self.assertEqual(\n-            [link for link in lx.extract_links(response)],\n+            list(lx.extract_links(response)),\n             [\n                 Link(\n                     url=\"http://example.org/item1.html\",\n@@ -834,7 +834,7 @@ def test_skip_bad_links(self):\n         response = HtmlResponse(\"http://example.org/index.html\", body=html)\n         lx = self.extractor_cls()\n         self.assertEqual(\n-            [link for link in lx.extract_links(response)],\n+            list(lx.extract_links(response)),\n             [\n                 Link(\n                     url=\"http://example.org/item2.html\",\ndiff --git a/tests/test_loader_deprecated.py b/tests/test_loader_deprecated.py\nindex 99cdf88d96f..528efa142a7 100644\n--- a/tests/test_loader_deprecated.py\n+++ b/tests/test_loader_deprecated.py\n@@ -565,37 +565,37 @@ class NoInputReprocessingFromDictTest(unittest.TestCase):\n     \"\"\"\n \n     def test_avoid_reprocessing_with_initial_values_single(self):\n-        il = NoInputReprocessingDictLoader(item=dict(title=\"foo\"))\n+        il = NoInputReprocessingDictLoader(item={\"title\": \"foo\"})\n         il_loaded = il.load_item()\n-        self.assertEqual(il_loaded, dict(title=\"foo\"))\n+        self.assertEqual(il_loaded, {\"title\": \"foo\"})\n         self.assertEqual(\n-            NoInputReprocessingDictLoader(item=il_loaded).load_item(), dict(title=\"foo\")\n+            NoInputReprocessingDictLoader(item=il_loaded).load_item(), {\"title\": \"foo\"}\n         )\n \n     def test_avoid_reprocessing_with_initial_values_list(self):\n-        il = NoInputReprocessingDictLoader(item=dict(title=[\"foo\", \"bar\"]))\n+        il = NoInputReprocessingDictLoader(item={\"title\": [\"foo\", \"bar\"]})\n         il_loaded = il.load_item()\n-        self.assertEqual(il_loaded, dict(title=\"foo\"))\n+        self.assertEqual(il_loaded, {\"title\": \"foo\"})\n         self.assertEqual(\n-            NoInputReprocessingDictLoader(item=il_loaded).load_item(), dict(title=\"foo\")\n+            NoInputReprocessingDictLoader(item=il_loaded).load_item(), {\"title\": \"foo\"}\n         )\n \n     def test_avoid_reprocessing_without_initial_values_single(self):\n         il = NoInputReprocessingDictLoader()\n         il.add_value(\"title\", \"foo\")\n         il_loaded = il.load_item()\n-        self.assertEqual(il_loaded, dict(title=\"FOO\"))\n+        self.assertEqual(il_loaded, {\"title\": \"FOO\"})\n         self.assertEqual(\n-            NoInputReprocessingDictLoader(item=il_loaded).load_item(), dict(title=\"FOO\")\n+            NoInputReprocessingDictLoader(item=il_loaded).load_item(), {\"title\": \"FOO\"}\n         )\n \n     def test_avoid_reprocessing_without_initial_values_list(self):\n         il = NoInputReprocessingDictLoader()\n         il.add_value(\"title\", [\"foo\", \"bar\"])\n         il_loaded = il.load_item()\n-        self.assertEqual(il_loaded, dict(title=\"FOO\"))\n+        self.assertEqual(il_loaded, {\"title\": \"FOO\"})\n         self.assertEqual(\n-            NoInputReprocessingDictLoader(item=il_loaded).load_item(), dict(title=\"FOO\")\n+            NoInputReprocessingDictLoader(item=il_loaded).load_item(), {\"title\": \"FOO\"}\n         )\n \n \ndiff --git a/tests/test_mail.py b/tests/test_mail.py\nindex 2535e58db26..ff15053978a 100644\n--- a/tests/test_mail.py\n+++ b/tests/test_mail.py\n@@ -91,7 +91,7 @@ def test_send_attach(self):\n         self.assertEqual(attach.get_payload(decode=True), b\"content\")\n \n     def _catch_mail_sent(self, **kwargs):\n-        self.catched_msg = dict(**kwargs)\n+        self.catched_msg = {**kwargs}\n \n     def test_send_utf8(self):\n         subject = \"s\u00fcbj\u00e8\u00e7t\"\ndiff --git a/tests/test_pipeline_crawl.py b/tests/test_pipeline_crawl.py\nindex be9811980df..5a9a217cee3 100644\n--- a/tests/test_pipeline_crawl.py\n+++ b/tests/test_pipeline_crawl.py\n@@ -140,7 +140,7 @@ def _assert_files_download_failure(self, crawler, items, code, logs):\n         self.assertEqual(logs.count(file_dl_failure), 3)\n \n         # check that no files were written to the media store\n-        self.assertEqual([x for x in self.tmpmediastore.iterdir()], [])\n+        self.assertEqual(list(self.tmpmediastore.iterdir()), [])\n \n     @defer.inlineCallbacks\n     def test_download_media(self):\ndiff --git a/tests/test_pipeline_files.py b/tests/test_pipeline_files.py\nindex e7000e3140c..0babde4d90f 100644\n--- a/tests/test_pipeline_files.py\n+++ b/tests/test_pipeline_files.py\n@@ -221,7 +221,7 @@ def file_path(self, request, response=None, info=None, item=None):\n         file_path = CustomFilesPipeline.from_settings(\n             Settings({\"FILES_STORE\": self.tempdir})\n         ).file_path\n-        item = dict(path=\"path-to-store-file\")\n+        item = {\"path\": \"path-to-store-file\"}\n         request = Request(\"http://example.com\")\n         self.assertEqual(file_path(request, item=item), \"full/path-to-store-file\")\n \ndiff --git a/tests/test_pipeline_images.py b/tests/test_pipeline_images.py\nindex 2e2e06b89a9..18a2454b3db 100644\n--- a/tests/test_pipeline_images.py\n+++ b/tests/test_pipeline_images.py\n@@ -132,7 +132,7 @@ def thumb_path(\n         thumb_path = CustomImagesPipeline.from_settings(\n             Settings({\"IMAGES_STORE\": self.tempdir})\n         ).thumb_path\n-        item = dict(path=\"path-to-store-file\")\n+        item = {\"path\": \"path-to-store-file\"}\n         request = Request(\"http://example.com\")\n         self.assertEqual(\n             thumb_path(request, \"small\", item=item), \"thumb/small/path-to-store-file\"\n@@ -433,14 +433,14 @@ class ImagesPipelineTestCaseCustomSettings(unittest.TestCase):\n     ]\n \n     # This should match what is defined in ImagesPipeline.\n-    default_pipeline_settings = dict(\n-        MIN_WIDTH=0,\n-        MIN_HEIGHT=0,\n-        EXPIRES=90,\n-        THUMBS={},\n-        IMAGES_URLS_FIELD=\"image_urls\",\n-        IMAGES_RESULT_FIELD=\"images\",\n-    )\n+    default_pipeline_settings = {\n+        \"MIN_WIDTH\": 0,\n+        \"MIN_HEIGHT\": 0,\n+        \"EXPIRES\": 90,\n+        \"THUMBS\": {},\n+        \"IMAGES_URLS_FIELD\": \"image_urls\",\n+        \"IMAGES_RESULT_FIELD\": \"images\",\n+    }\n \n     def setUp(self):\n         self.tempdir = mkdtemp()\ndiff --git a/tests/test_pipeline_media.py b/tests/test_pipeline_media.py\nindex d477b59be40..d4dde4a4036 100644\n--- a/tests/test_pipeline_media.py\n+++ b/tests/test_pipeline_media.py\n@@ -59,7 +59,7 @@ def test_default_media_to_download(self):\n         assert self.pipe.media_to_download(request, self.info) is None\n \n     def test_default_get_media_requests(self):\n-        item = dict(name=\"name\")\n+        item = {\"name\": \"name\"}\n         assert self.pipe.get_media_requests(item, self.info) is None\n \n     def test_default_media_downloaded(self):\n@@ -73,7 +73,7 @@ def test_default_media_failed(self):\n         assert self.pipe.media_failed(fail, request, self.info) is fail\n \n     def test_default_item_completed(self):\n-        item = dict(name=\"name\")\n+        item = {\"name\": \"name\"}\n         assert self.pipe.item_completed([], item, self.info) is item\n \n         # Check that failures are logged by default\n@@ -98,7 +98,7 @@ def test_default_item_completed(self):\n \n     @inlineCallbacks\n     def test_default_process_item(self):\n-        item = dict(name=\"name\")\n+        item = {\"name\": \"name\"}\n         new_item = yield self.pipe.process_item(item, self.spider)\n         assert new_item is item\n \n@@ -226,11 +226,11 @@ def test_result_succeed(self):\n         rsp = Response(\"http://url1\")\n         req = Request(\n             \"http://url1\",\n-            meta=dict(response=rsp),\n+            meta={\"response\": rsp},\n             callback=self._callback,\n             errback=self._errback,\n         )\n-        item = dict(requests=req)\n+        item = {\"requests\": req}\n         new_item = yield self.pipe.process_item(item, self.spider)\n         self.assertEqual(new_item[\"results\"], [(True, rsp)])\n         self.assertEqual(\n@@ -250,11 +250,11 @@ def test_result_failure(self):\n         fail = Failure(Exception())\n         req = Request(\n             \"http://url1\",\n-            meta=dict(response=fail),\n+            meta={\"response\": fail},\n             callback=self._callback,\n             errback=self._errback,\n         )\n-        item = dict(requests=req)\n+        item = {\"requests\": req}\n         new_item = yield self.pipe.process_item(item, self.spider)\n         self.assertEqual(new_item[\"results\"], [(False, fail)])\n         self.assertEqual(\n@@ -272,10 +272,10 @@ def test_result_failure(self):\n     def test_mix_of_success_and_failure(self):\n         self.pipe.LOG_FAILED_RESULTS = False\n         rsp1 = Response(\"http://url1\")\n-        req1 = Request(\"http://url1\", meta=dict(response=rsp1))\n+        req1 = Request(\"http://url1\", meta={\"response\": rsp1})\n         fail = Failure(Exception())\n-        req2 = Request(\"http://url2\", meta=dict(response=fail))\n-        item = dict(requests=[req1, req2])\n+        req2 = Request(\"http://url2\", meta={\"response\": fail})\n+        item = {\"requests\": [req1, req2]}\n         new_item = yield self.pipe.process_item(item, self.spider)\n         self.assertEqual(new_item[\"results\"], [(True, rsp1), (False, fail)])\n         m = self.pipe._mockcalled\n@@ -294,7 +294,7 @@ def test_mix_of_success_and_failure(self):\n     def test_get_media_requests(self):\n         # returns single Request (without callback)\n         req = Request(\"http://url\")\n-        item = dict(requests=req)  # pass a single item\n+        item = {\"requests\": req}  # pass a single item\n         new_item = yield self.pipe.process_item(item, self.spider)\n         assert new_item is item\n         self.assertIn(self.fingerprint(req), self.info.downloaded)\n@@ -302,7 +302,7 @@ def test_get_media_requests(self):\n         # returns iterable of Requests\n         req1 = Request(\"http://url1\")\n         req2 = Request(\"http://url2\")\n-        item = dict(requests=iter([req1, req2]))\n+        item = {\"requests\": iter([req1, req2])}\n         new_item = yield self.pipe.process_item(item, self.spider)\n         assert new_item is item\n         assert self.fingerprint(req1) in self.info.downloaded\n@@ -311,17 +311,17 @@ def test_get_media_requests(self):\n     @inlineCallbacks\n     def test_results_are_cached_across_multiple_items(self):\n         rsp1 = Response(\"http://url1\")\n-        req1 = Request(\"http://url1\", meta=dict(response=rsp1))\n-        item = dict(requests=req1)\n+        req1 = Request(\"http://url1\", meta={\"response\": rsp1})\n+        item = {\"requests\": req1}\n         new_item = yield self.pipe.process_item(item, self.spider)\n         self.assertTrue(new_item is item)\n         self.assertEqual(new_item[\"results\"], [(True, rsp1)])\n \n         # rsp2 is ignored, rsp1 must be in results because request fingerprints are the same\n         req2 = Request(\n-            req1.url, meta=dict(response=Response(\"http://donot.download.me\"))\n+            req1.url, meta={\"response\": Response(\"http://donot.download.me\")}\n         )\n-        item = dict(requests=req2)\n+        item = {\"requests\": req2}\n         new_item = yield self.pipe.process_item(item, self.spider)\n         self.assertTrue(new_item is item)\n         self.assertEqual(self.fingerprint(req1), self.fingerprint(req2))\n@@ -330,11 +330,11 @@ def test_results_are_cached_across_multiple_items(self):\n     @inlineCallbacks\n     def test_results_are_cached_for_requests_of_single_item(self):\n         rsp1 = Response(\"http://url1\")\n-        req1 = Request(\"http://url1\", meta=dict(response=rsp1))\n+        req1 = Request(\"http://url1\", meta={\"response\": rsp1})\n         req2 = Request(\n-            req1.url, meta=dict(response=Response(\"http://donot.download.me\"))\n+            req1.url, meta={\"response\": Response(\"http://donot.download.me\")}\n         )\n-        item = dict(requests=[req1, req2])\n+        item = {\"requests\": [req1, req2]}\n         new_item = yield self.pipe.process_item(item, self.spider)\n         self.assertTrue(new_item is item)\n         self.assertEqual(new_item[\"results\"], [(True, rsp1), (True, rsp1)])\n@@ -359,16 +359,16 @@ def rsp1_func():\n         def rsp2_func():\n             self.fail(\"it must cache rsp1 result and must not try to redownload\")\n \n-        req1 = Request(\"http://url\", meta=dict(response=rsp1_func))\n-        req2 = Request(req1.url, meta=dict(response=rsp2_func))\n-        item = dict(requests=[req1, req2])\n+        req1 = Request(\"http://url\", meta={\"response\": rsp1_func})\n+        req2 = Request(req1.url, meta={\"response\": rsp2_func})\n+        item = {\"requests\": [req1, req2]}\n         new_item = yield self.pipe.process_item(item, self.spider)\n         self.assertEqual(new_item[\"results\"], [(True, rsp1), (True, rsp1)])\n \n     @inlineCallbacks\n     def test_use_media_to_download_result(self):\n-        req = Request(\"http://url\", meta=dict(result=\"ITSME\", response=self.fail))\n-        item = dict(requests=req)\n+        req = Request(\"http://url\", meta={\"result\": \"ITSME\", \"response\": self.fail})\n+        item = {\"requests\": req}\n         new_item = yield self.pipe.process_item(item, self.spider)\n         self.assertEqual(new_item[\"results\"], [(True, \"ITSME\")])\n         self.assertEqual(\ndiff --git a/tests/test_scheduler.py b/tests/test_scheduler.py\nindex f8465a5ffed..37099dae676 100644\n--- a/tests/test_scheduler.py\n+++ b/tests/test_scheduler.py\n@@ -45,15 +45,15 @@ def close(self):\n \n class MockCrawler(Crawler):\n     def __init__(self, priority_queue_cls, jobdir):\n-        settings = dict(\n-            SCHEDULER_DEBUG=False,\n-            SCHEDULER_DISK_QUEUE=\"scrapy.squeues.PickleLifoDiskQueue\",\n-            SCHEDULER_MEMORY_QUEUE=\"scrapy.squeues.LifoMemoryQueue\",\n-            SCHEDULER_PRIORITY_QUEUE=priority_queue_cls,\n-            JOBDIR=jobdir,\n-            DUPEFILTER_CLASS=\"scrapy.dupefilters.BaseDupeFilter\",\n-            REQUEST_FINGERPRINTER_IMPLEMENTATION=\"2.7\",\n-        )\n+        settings = {\n+            \"SCHEDULER_DEBUG\": False,\n+            \"SCHEDULER_DISK_QUEUE\": \"scrapy.squeues.PickleLifoDiskQueue\",\n+            \"SCHEDULER_MEMORY_QUEUE\": \"scrapy.squeues.LifoMemoryQueue\",\n+            \"SCHEDULER_PRIORITY_QUEUE\": priority_queue_cls,\n+            \"JOBDIR\": jobdir,\n+            \"DUPEFILTER_CLASS\": \"scrapy.dupefilters.BaseDupeFilter\",\n+            \"REQUEST_FINGERPRINTER_IMPLEMENTATION\": \"2.7\",\n+        }\n         super().__init__(Spider, settings)\n         self.engine = MockEngine(downloader=MockDownloader())\n         self.stats = load_object(self.settings[\"STATS_CLASS\"])(self)\n@@ -338,10 +338,10 @@ def test_integration_downloader_aware_priority_queue(self):\n \n class TestIncompatibility(unittest.TestCase):\n     def _incompatible(self):\n-        settings = dict(\n-            SCHEDULER_PRIORITY_QUEUE=\"scrapy.pqueues.DownloaderAwarePriorityQueue\",\n-            CONCURRENT_REQUESTS_PER_IP=1,\n-        )\n+        settings = {\n+            \"SCHEDULER_PRIORITY_QUEUE\": \"scrapy.pqueues.DownloaderAwarePriorityQueue\",\n+            \"CONCURRENT_REQUESTS_PER_IP\": 1,\n+        }\n         crawler = get_crawler(Spider, settings)\n         scheduler = Scheduler.from_crawler(crawler)\n         spider = Spider(name=\"spider\")\ndiff --git a/tests/test_spidermiddleware_offsite.py b/tests/test_spidermiddleware_offsite.py\nindex ea45b769869..837f1c2c8f5 100644\n--- a/tests/test_spidermiddleware_offsite.py\n+++ b/tests/test_spidermiddleware_offsite.py\n@@ -16,10 +16,10 @@ def setUp(self):\n         self.mw.spider_opened(self.spider)\n \n     def _get_spiderargs(self):\n-        return dict(\n-            name=\"foo\",\n-            allowed_domains=[\"scrapytest.org\", \"scrapy.org\", \"scrapy.test.org\"],\n-        )\n+        return {\n+            \"name\": \"foo\",\n+            \"allowed_domains\": [\"scrapytest.org\", \"scrapy.org\", \"scrapy.test.org\"],\n+        }\n \n     def test_process_spider_output(self):\n         res = Response(\"http://scrapytest.org\")\n@@ -50,7 +50,7 @@ def test_process_spider_output(self):\n \n class TestOffsiteMiddleware2(TestOffsiteMiddleware):\n     def _get_spiderargs(self):\n-        return dict(name=\"foo\", allowed_domains=None)\n+        return {\"name\": \"foo\", \"allowed_domains\": None}\n \n     def test_process_spider_output(self):\n         res = Response(\"http://scrapytest.org\")\n@@ -61,13 +61,16 @@ def test_process_spider_output(self):\n \n class TestOffsiteMiddleware3(TestOffsiteMiddleware2):\n     def _get_spiderargs(self):\n-        return dict(name=\"foo\")\n+        return {\"name\": \"foo\"}\n \n \n class TestOffsiteMiddleware4(TestOffsiteMiddleware3):\n     def _get_spiderargs(self):\n         bad_hostname = urlparse(\"http:////scrapytest.org\").hostname\n-        return dict(name=\"foo\", allowed_domains=[\"scrapytest.org\", None, bad_hostname])\n+        return {\n+            \"name\": \"foo\",\n+            \"allowed_domains\": [\"scrapytest.org\", None, bad_hostname],\n+        }\n \n     def test_process_spider_output(self):\n         res = Response(\"http://scrapytest.org\")\ndiff --git a/tests/test_utils_iterators.py b/tests/test_utils_iterators.py\nindex ee22e6675d3..ec377bb19ad 100644\n--- a/tests/test_utils_iterators.py\n+++ b/tests/test_utils_iterators.py\n@@ -355,7 +355,7 @@ def test_csviter_defaults(self):\n         response = TextResponse(url=\"http://example.com/\", body=body)\n         csv = csviter(response)\n \n-        result = [row for row in csv]\n+        result = list(csv)\n         self.assertEqual(\n             result,\n             [\n@@ -377,7 +377,7 @@ def test_csviter_delimiter(self):\n         csv = csviter(response, delimiter=\"\\t\")\n \n         self.assertEqual(\n-            [row for row in csv],\n+            list(csv),\n             [\n                 {\"id\": \"1\", \"name\": \"alpha\", \"value\": \"foobar\"},\n                 {\"id\": \"2\", \"name\": \"unicode\", \"value\": \"\\xfan\\xedc\\xf3d\\xe9\\u203d\"},\n@@ -394,7 +394,7 @@ def test_csviter_quotechar(self):\n         csv1 = csviter(response1, quotechar=\"'\")\n \n         self.assertEqual(\n-            [row for row in csv1],\n+            list(csv1),\n             [\n                 {\"id\": \"1\", \"name\": \"alpha\", \"value\": \"foobar\"},\n                 {\"id\": \"2\", \"name\": \"unicode\", \"value\": \"\\xfan\\xedc\\xf3d\\xe9\\u203d\"},\n@@ -407,7 +407,7 @@ def test_csviter_quotechar(self):\n         csv2 = csviter(response2, delimiter=\"|\", quotechar=\"'\")\n \n         self.assertEqual(\n-            [row for row in csv2],\n+            list(csv2),\n             [\n                 {\"id\": \"1\", \"name\": \"alpha\", \"value\": \"foobar\"},\n                 {\"id\": \"2\", \"name\": \"unicode\", \"value\": \"\\xfan\\xedc\\xf3d\\xe9\\u203d\"},\n@@ -422,7 +422,7 @@ def test_csviter_wrong_quotechar(self):\n         csv = csviter(response)\n \n         self.assertEqual(\n-            [row for row in csv],\n+            list(csv),\n             [\n                 {\"'id'\": \"1\", \"'name'\": \"'alpha'\", \"'value'\": \"'foobar'\"},\n                 {\n@@ -441,7 +441,7 @@ def test_csviter_delimiter_binary_response_assume_utf8_encoding(self):\n         csv = csviter(response, delimiter=\"\\t\")\n \n         self.assertEqual(\n-            [row for row in csv],\n+            list(csv),\n             [\n                 {\"id\": \"1\", \"name\": \"alpha\", \"value\": \"foobar\"},\n                 {\"id\": \"2\", \"name\": \"unicode\", \"value\": \"\\xfan\\xedc\\xf3d\\xe9\\u203d\"},\n@@ -458,7 +458,7 @@ def test_csviter_headers(self):\n         csv = csviter(response, headers=[h.decode(\"utf-8\") for h in headers])\n \n         self.assertEqual(\n-            [row for row in csv],\n+            list(csv),\n             [\n                 {\"id\": \"1\", \"name\": \"alpha\", \"value\": \"foobar\"},\n                 {\"id\": \"2\", \"name\": \"unicode\", \"value\": \"\\xfan\\xedc\\xf3d\\xe9\\u203d\"},\n@@ -475,7 +475,7 @@ def test_csviter_falserow(self):\n         csv = csviter(response)\n \n         self.assertEqual(\n-            [row for row in csv],\n+            list(csv),\n             [\n                 {\"id\": \"1\", \"name\": \"alpha\", \"value\": \"foobar\"},\n                 {\"id\": \"2\", \"name\": \"unicode\", \"value\": \"\\xfan\\xedc\\xf3d\\xe9\\u203d\"},\ndiff --git a/tests/test_utils_template.py b/tests/test_utils_template.py\nindex cbe80e157d1..fc42c0d2f4d 100644\n--- a/tests/test_utils_template.py\n+++ b/tests/test_utils_template.py\n@@ -16,7 +16,7 @@ def tearDown(self):\n         rmtree(self.tmp_path)\n \n     def test_simple_render(self):\n-        context = dict(project_name=\"proj\", name=\"spi\", classname=\"TheSpider\")\n+        context = {\"project_name\": \"proj\", \"name\": \"spi\", \"classname\": \"TheSpider\"}\n         template = \"from ${project_name}.spiders.${name} import ${classname}\"\n         rendered = \"from proj.spiders.spi import TheSpider\"\n \n",
    "problem_statement": "Fix and re-enable `unnecessary-comprehension` and `use-dict-literal` pylint tags\nBoth are valid simplification hints.\n",
    "hints_text": "",
    "created_at": "2024-02-28T19:40:38Z",
    "version": "2.11",
    "PASS_TO_PASS": [],
    "FAIL_TO_PASS": [
      "tests/test_crawl.py",
      "tests/test_downloadermiddleware_cookies.py",
      "tests/test_downloadermiddleware_httpauth.py",
      "tests/test_exporters.py",
      "tests/test_linkextractors.py",
      "tests/test_loader_deprecated.py",
      "tests/test_mail.py",
      "tests/test_pipeline_crawl.py",
      "tests/test_pipeline_files.py",
      "tests/test_pipeline_images.py",
      "tests/test_pipeline_media.py",
      "tests/test_scheduler.py",
      "tests/test_spidermiddleware_offsite.py",
      "tests/test_utils_iterators.py",
      "tests/test_utils_template.py"
    ]
  },
  {
    "repo": "scrapy/scrapy",
    "pull_number": 6239,
    "instance_id": "scrapy__scrapy-6239",
    "issue_numbers": [
      "5932",
      "6178"
    ],
    "base_commit": "ee1189512f652fae72f013c9d4759976b8b69994",
    "patch": "diff --git a/docs/topics/feed-exports.rst b/docs/topics/feed-exports.rst\nindex f64bbac06a0..922b765db7e 100644\n--- a/docs/topics/feed-exports.rst\n+++ b/docs/topics/feed-exports.rst\n@@ -390,7 +390,13 @@ Each plugin is a class that must implement the following methods:\n \n .. method:: close(self)\n \n-    Close the target file object.\n+    Clean up the plugin.\n+\n+    For example, you might want to close a file wrapper that you might have\n+    used to compress data written into the file received in the ``__init__``\n+    method.\n+\n+    .. warning:: Do not close the file from the ``__init__`` method.\n \n To pass a parameter to your plugin, use :ref:`feed options <feed-options>`. You\n can then access those parameters from the ``__init__`` method of your plugin.\ndiff --git a/scrapy/extensions/postprocessing.py b/scrapy/extensions/postprocessing.py\nindex 79e3b1656ea..17969c5b0d9 100644\n--- a/scrapy/extensions/postprocessing.py\n+++ b/scrapy/extensions/postprocessing.py\n@@ -42,7 +42,6 @@ def write(self, data: bytes) -> int:\n \n     def close(self) -> None:\n         self.gzipfile.close()\n-        self.file.close()\n \n \n class Bz2Plugin:\n@@ -69,7 +68,6 @@ def write(self, data: bytes) -> int:\n \n     def close(self) -> None:\n         self.bz2file.close()\n-        self.file.close()\n \n \n class LZMAPlugin:\n@@ -111,7 +109,6 @@ def write(self, data: bytes) -> int:\n \n     def close(self) -> None:\n         self.lzmafile.close()\n-        self.file.close()\n \n \n # io.IOBase is subclassed here, so that exporters can use the PostProcessingManager\n",
    "test_patch": "diff --git a/tests/test_feedexport.py b/tests/test_feedexport.py\nindex 277555608e6..d7560b5ff58 100644\n--- a/tests/test_feedexport.py\n+++ b/tests/test_feedexport.py\n@@ -1731,6 +1731,7 @@ def open(self, spider):\n \n             def store(self, file):\n                 Storage.store_file = file\n+                Storage.file_was_closed = file.closed\n                 file.close()\n \n         settings = {\n@@ -1746,6 +1747,7 @@ def store(self, file):\n         }\n         yield self.exported_no_data(settings)\n         self.assertIs(Storage.open_file, Storage.store_file)\n+        self.assertFalse(Storage.file_was_closed)\n \n \n class FeedPostProcessedExportsTest(FeedExportTestBase):\n",
    "problem_statement": "_store_in_thread must not receive a closed file when using postprocessing plugins\nWhen using post-processing plugins, the FeedExporter closes the temporary file. This behavior has been introduced in commit 500aaa258fc72e943d7bbd258900aeb5af6c8b0d.\r\n\r\nSome storage backends, such as GCSFeedStorage, expect an open file in the _store_in_thread method.\r\n\r\nTo solve the problem we should only close the e.g. GzipFile instance, not the temporary file. Removing the line self.file.close() in GzipPlugin, Bz2Plugin and LZMAPlugin should resolve the error. I think this change would be a clean solution because usually the component that created a resource should be responsible to close that resource - so closing the BinaryIO object in the plugins is a bit unexpected.\r\n\r\nFurthermore, GCSFeedExporter should be modified to close the file in its _store_in_thread method after the upload. FTPFeedStorage and S3FeedStorage do this as well.\nresolve issue 5932\nFixes #5932 \r\n\r\nRemoved the lines closing the temporary file in GzipPlugin, Bz2Plugin, and LMZAPlugin in postprocessing.py.\r\n\r\nPlaced file closes in GCSFeedExporter, FTPFeedStorage, and S3FeedStorage in feedexport.py.\n",
    "hints_text": "I'm interested in working on this issue. It seems like a valuable improvement and aligns with my interests. I hope this issue still open and available for contributions?\nPlease, feel free to give it a try.\n## [Codecov](https://app.codecov.io/gh/scrapy/scrapy/pull/6178?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scrapy) Report\n> Merging [#6178](https://app.codecov.io/gh/scrapy/scrapy/pull/6178?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scrapy) (329175c) into [master](https://app.codecov.io/gh/scrapy/scrapy/commit/04024f1e796f99e77dda544396722fb9203f1d49?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scrapy) (04024f1) will **decrease** coverage by `45.57%`.\n> Report is 21 commits behind head on master.\n> The diff coverage is `0.00%`.\n\n<details><summary>Additional details and impacted files</summary>\n\n\n```diff\n@@             Coverage Diff             @@\n##           master    #6178       +/-   ##\n===========================================\n- Coverage   88.75%   43.18%   -45.57%     \n===========================================\n  Files         160      159        -1     \n  Lines       11414    11581      +167     \n  Branches     1860     1885       +25     \n===========================================\n- Hits        10130     5001     -5129     \n- Misses        976     6188     +5212     \n- Partials      308      392       +84     \n```\n\n| [Files](https://app.codecov.io/gh/scrapy/scrapy/pull/6178?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scrapy) | Coverage \u0394 | |\n|---|---|---|\n| [scrapy/extensions/postprocessing.py](https://app.codecov.io/gh/scrapy/scrapy/pull/6178?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scrapy#diff-c2NyYXB5L2V4dGVuc2lvbnMvcG9zdHByb2Nlc3NpbmcucHk=) | `41.26% <\u00f8> (-57.22%)` | :arrow_down: |\n| [scrapy/extensions/feedexport.py](https://app.codecov.io/gh/scrapy/scrapy/pull/6178?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scrapy#diff-c2NyYXB5L2V4dGVuc2lvbnMvZmVlZGV4cG9ydC5weQ==) | `67.93% <0.00%> (-24.74%)` | :arrow_down: |\n\n... and [141 files with indirect coverage changes](https://app.codecov.io/gh/scrapy/scrapy/pull/6178/indirect-changes?src=pr&el=tree-more&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scrapy)\n\n</details>\nIt seems this approach breaks some feedexport scenarios.",
    "created_at": "2024-02-21T10:13:15Z",
    "version": "2.11",
    "PASS_TO_PASS": [],
    "FAIL_TO_PASS": [
      "tests/test_feedexport.py"
    ]
  },
  {
    "repo": "scrapy/scrapy",
    "pull_number": 6229,
    "instance_id": "scrapy__scrapy-6229",
    "issue_numbers": [
      "6228"
    ],
    "base_commit": "c4e4b9b56e7fe10c5e7472b152dd47253a97af5b",
    "patch": "diff --git a/docs/topics/media-pipeline.rst b/docs/topics/media-pipeline.rst\nindex da0587aa465..c96dd0f991b 100644\n--- a/docs/topics/media-pipeline.rst\n+++ b/docs/topics/media-pipeline.rst\n@@ -532,14 +532,14 @@ See here the methods that you can override in your custom Files Pipeline:\n       .. code-block:: python\n \n         from pathlib import PurePosixPath\n-        from urllib.parse import urlparse\n+        from scrapy.utils.httpobj import urlparse_cached\n \n         from scrapy.pipelines.files import FilesPipeline\n \n \n         class MyFilesPipeline(FilesPipeline):\n             def file_path(self, request, response=None, info=None, *, item=None):\n-                return \"files/\" + PurePosixPath(urlparse(request.url).path).name\n+                return \"files/\" + PurePosixPath(urlparse_cached(request).path).name\n \n       Similarly, you can use the ``item`` to determine the file path based on some item \n       property.\n@@ -690,14 +690,14 @@ See here the methods that you can override in your custom Images Pipeline:\n       .. code-block:: python\n \n         from pathlib import PurePosixPath\n-        from urllib.parse import urlparse\n+        from scrapy.utils.httpobj import urlparse_cached\n \n         from scrapy.pipelines.images import ImagesPipeline\n \n \n         class MyImagesPipeline(ImagesPipeline):\n             def file_path(self, request, response=None, info=None, *, item=None):\n-                return \"files/\" + PurePosixPath(urlparse(request.url).path).name\n+                return \"files/\" + PurePosixPath(urlparse_cached(request).path).name\n \n       Similarly, you can use the ``item`` to determine the file path based on some item \n       property.\ndiff --git a/scrapy/core/http2/stream.py b/scrapy/core/http2/stream.py\nindex 39d5921f4ec..0f282d83d38 100644\n--- a/scrapy/core/http2/stream.py\n+++ b/scrapy/core/http2/stream.py\n@@ -2,7 +2,6 @@\n from enum import Enum\n from io import BytesIO\n from typing import TYPE_CHECKING, Dict, List, Optional, Tuple\n-from urllib.parse import urlparse\n \n from h2.errors import ErrorCodes\n from h2.exceptions import H2Error, ProtocolError, StreamClosedError\n@@ -15,6 +14,7 @@\n from scrapy.http import Request\n from scrapy.http.headers import Headers\n from scrapy.responsetypes import responsetypes\n+from scrapy.utils.httpobj import urlparse_cached\n \n if TYPE_CHECKING:\n     from scrapy.core.http2.protocol import H2ClientProtocol\n@@ -185,7 +185,7 @@ def get_response(self) -> Deferred:\n \n     def check_request_url(self) -> bool:\n         # Make sure that we are sending the request to the correct URL\n-        url = urlparse(self._request.url)\n+        url = urlparse_cached(self._request)\n         return (\n             url.netloc == str(self._protocol.metadata[\"uri\"].host, \"utf-8\")\n             or url.netloc == str(self._protocol.metadata[\"uri\"].netloc, \"utf-8\")\n@@ -194,7 +194,7 @@ def check_request_url(self) -> bool:\n         )\n \n     def _get_request_headers(self) -> List[Tuple[str, str]]:\n-        url = urlparse(self._request.url)\n+        url = urlparse_cached(self._request)\n \n         path = url.path\n         if url.query:\ndiff --git a/scrapy/downloadermiddlewares/redirect.py b/scrapy/downloadermiddlewares/redirect.py\nindex 83afdf7d7dc..24089afea88 100644\n--- a/scrapy/downloadermiddlewares/redirect.py\n+++ b/scrapy/downloadermiddlewares/redirect.py\n@@ -2,7 +2,7 @@\n \n import logging\n from typing import TYPE_CHECKING, Any, List, Union, cast\n-from urllib.parse import urljoin, urlparse\n+from urllib.parse import urljoin\n \n from w3lib.url import safe_url_string\n \n@@ -125,7 +125,7 @@ def process_response(\n         assert response.headers[\"Location\"] is not None\n         location = safe_url_string(response.headers[\"Location\"])\n         if response.headers[\"Location\"].startswith(b\"//\"):\n-            request_scheme = urlparse(request.url).scheme\n+            request_scheme = urlparse_cached(request).scheme\n             location = request_scheme + \"://\" + location.lstrip(\"/\")\n \n         redirected_url = urljoin(request.url, location)\n",
    "test_patch": "diff --git a/tests/CrawlerRunner/ip_address.py b/tests/CrawlerRunner/ip_address.py\nindex 23260ab0d10..5bf7512bc7e 100644\n--- a/tests/CrawlerRunner/ip_address.py\n+++ b/tests/CrawlerRunner/ip_address.py\n@@ -9,6 +9,7 @@\n \n from scrapy import Request, Spider\n from scrapy.crawler import CrawlerRunner\n+from scrapy.utils.httpobj import urlparse_cached\n from scrapy.utils.log import configure_logging\n from tests.mockserver import MockDNSServer, MockServer\n \n@@ -30,7 +31,7 @@ def start_requests(self):\n         yield Request(self.url)\n \n     def parse(self, response):\n-        netloc = urlparse(response.url).netloc\n+        netloc = urlparse_cached(response).netloc\n         host = netloc.split(\":\")[0]\n         self.logger.info(f\"Host: {host}\")\n         self.logger.info(f\"Type: {type(response.ip_address)}\")\ndiff --git a/tests/test_http_cookies.py b/tests/test_http_cookies.py\nindex 9e43b72b056..8b555491496 100644\n--- a/tests/test_http_cookies.py\n+++ b/tests/test_http_cookies.py\n@@ -1,8 +1,8 @@\n from unittest import TestCase\n-from urllib.parse import urlparse\n \n from scrapy.http import Request, Response\n from scrapy.http.cookies import WrappedRequest, WrappedResponse\n+from scrapy.utils.httpobj import urlparse_cached\n \n \n class WrappedRequestTest(TestCase):\n@@ -17,12 +17,12 @@ def test_get_full_url(self):\n         self.assertEqual(self.wrapped.full_url, self.request.url)\n \n     def test_get_host(self):\n-        self.assertEqual(self.wrapped.get_host(), urlparse(self.request.url).netloc)\n-        self.assertEqual(self.wrapped.host, urlparse(self.request.url).netloc)\n+        self.assertEqual(self.wrapped.get_host(), urlparse_cached(self.request).netloc)\n+        self.assertEqual(self.wrapped.host, urlparse_cached(self.request).netloc)\n \n     def test_get_type(self):\n-        self.assertEqual(self.wrapped.get_type(), urlparse(self.request.url).scheme)\n-        self.assertEqual(self.wrapped.type, urlparse(self.request.url).scheme)\n+        self.assertEqual(self.wrapped.get_type(), urlparse_cached(self.request).scheme)\n+        self.assertEqual(self.wrapped.type, urlparse_cached(self.request).scheme)\n \n     def test_is_unverifiable(self):\n         self.assertFalse(self.wrapped.is_unverifiable())\ndiff --git a/tests/test_http_request.py b/tests/test_http_request.py\nindex 6dc9ec8b7fb..04fcaa2315d 100644\n--- a/tests/test_http_request.py\n+++ b/tests/test_http_request.py\n@@ -5,7 +5,7 @@\n import xmlrpc.client\n from typing import Any, Dict, List\n from unittest import mock\n-from urllib.parse import parse_qs, unquote_to_bytes, urlparse\n+from urllib.parse import parse_qs, unquote_to_bytes\n \n from scrapy.http import (\n     FormRequest,\n@@ -16,6 +16,7 @@\n     XmlRpcRequest,\n )\n from scrapy.http.request import NO_CALLBACK\n+from scrapy.utils.httpobj import urlparse_cached\n from scrapy.utils.python import to_bytes, to_unicode\n \n \n@@ -617,8 +618,8 @@ def test_from_response_duplicate_form_key(self):\n             method=\"GET\",\n             formdata=((\"foo\", \"bar\"), (\"foo\", \"baz\")),\n         )\n-        self.assertEqual(urlparse(req.url).hostname, \"www.example.com\")\n-        self.assertEqual(urlparse(req.url).query, \"foo=bar&foo=baz\")\n+        self.assertEqual(urlparse_cached(req).hostname, \"www.example.com\")\n+        self.assertEqual(urlparse_cached(req).query, \"foo=bar&foo=baz\")\n \n     def test_from_response_override_duplicate_form_key(self):\n         response = _buildresponse(\n@@ -666,8 +667,8 @@ def test_from_response_get(self):\n             response, formdata={\"one\": [\"two\", \"three\"], \"six\": \"seven\"}\n         )\n         self.assertEqual(r1.method, \"GET\")\n-        self.assertEqual(urlparse(r1.url).hostname, \"www.example.com\")\n-        self.assertEqual(urlparse(r1.url).path, \"/this/get.php\")\n+        self.assertEqual(urlparse_cached(r1).hostname, \"www.example.com\")\n+        self.assertEqual(urlparse_cached(r1).path, \"/this/get.php\")\n         fs = _qs(r1)\n         self.assertEqual(set(fs[b\"test\"]), {b\"val1\", b\"val2\"})\n         self.assertEqual(set(fs[b\"one\"]), {b\"two\", b\"three\"})\ndiff --git a/tests/test_scheduler_base.py b/tests/test_scheduler_base.py\nindex 76ca777a87a..5db2e4e509b 100644\n--- a/tests/test_scheduler_base.py\n+++ b/tests/test_scheduler_base.py\n@@ -1,6 +1,6 @@\n from typing import Dict, Optional\n from unittest import TestCase\n-from urllib.parse import urljoin, urlparse\n+from urllib.parse import urljoin\n \n from testfixtures import LogCapture\n from twisted.internet import defer\n@@ -9,6 +9,7 @@\n from scrapy.core.scheduler import BaseScheduler\n from scrapy.http import Request\n from scrapy.spiders import Spider\n+from scrapy.utils.httpobj import urlparse_cached\n from scrapy.utils.request import fingerprint\n from scrapy.utils.test import get_crawler\n from tests.mockserver import MockServer\n@@ -57,7 +58,7 @@ def __init__(self, mockserver, *args, **kwargs):\n         self.start_urls = map(mockserver.url, PATHS)\n \n     def parse(self, response):\n-        return {\"path\": urlparse(response.url).path}\n+        return {\"path\": urlparse_cached(response).path}\n \n \n class InterfaceCheckMixin:\n",
    "problem_statement": "Replace urlparse with urlparse_cached where possible\nLook for regular expression `urllib.*urlparse` in the code base (docs included), and see if replacing the use of `urllib.parse.urlparse` with `scrapy.utils.httpobj.urlparse_cached` is feasible (I think it should as long as there is a `request` object involved).\n",
    "hints_text": "",
    "created_at": "2024-02-20T10:13:05Z",
    "version": "2.11",
    "PASS_TO_PASS": [],
    "FAIL_TO_PASS": [
      "tests/CrawlerRunner/ip_address.py",
      "tests/test_http_cookies.py",
      "tests/test_http_request.py",
      "tests/test_scheduler_base.py"
    ]
  },
  {
    "repo": "scrapy/scrapy",
    "pull_number": 6151,
    "instance_id": "scrapy__scrapy-6151",
    "issue_numbers": [
      "3690",
      "3691"
    ],
    "base_commit": "4d31277bc67169460dc2d8bca80946df8b355b8f",
    "patch": "diff --git a/docs/topics/downloader-middleware.rst b/docs/topics/downloader-middleware.rst\nindex 11a3fcb94f4..5acc6daa999 100644\n--- a/docs/topics/downloader-middleware.rst\n+++ b/docs/topics/downloader-middleware.rst\n@@ -797,9 +797,12 @@ OffsiteMiddleware\n    :attr:`~scrapy.Spider.allowed_domains` attribute, or the\n    attribute is empty, the offsite middleware will allow all requests.\n \n-   If the request has the :attr:`~scrapy.Request.dont_filter` attribute\n-   set, the offsite middleware will allow the request even if its domain is not\n-   listed in allowed domains.\n+   .. reqmeta:: allow_offsite\n+\n+   If the request has the :attr:`~scrapy.Request.dont_filter` attribute set to\n+   ``True`` or :attr:`Request.meta` has ``allow_offsite`` set to ``True``, then\n+   the OffsiteMiddleware will allow the request even if its domain is not listed\n+   in allowed domains.\n \n RedirectMiddleware\n ------------------\ndiff --git a/docs/topics/request-response.rst b/docs/topics/request-response.rst\nindex 710e2e1314e..056e2ed383a 100644\n--- a/docs/topics/request-response.rst\n+++ b/docs/topics/request-response.rst\n@@ -145,9 +145,9 @@ Request objects\n     :type priority: int\n \n     :param dont_filter: indicates that this request should not be filtered by\n-       the scheduler. This is used when you want to perform an identical\n-       request multiple times, to ignore the duplicates filter. Use it with\n-       care, or you will get into crawling loops. Default to ``False``.\n+       the scheduler or some middlewares. This is used when you want to perform\n+       an identical request multiple times, to ignore the duplicates filter.\n+       Use it with care, or you will get into crawling loops. Default to ``False``.\n     :type dont_filter: bool\n \n     :param errback: a function that will be called if any exception was\n@@ -661,6 +661,7 @@ are some special keys recognized by Scrapy and its built-in extensions.\n \n Those are:\n \n+* :reqmeta:`allow_offsite`\n * :reqmeta:`autothrottle_dont_adjust_delay`\n * :reqmeta:`bindaddress`\n * :reqmeta:`cookiejar`\ndiff --git a/scrapy/downloadermiddlewares/offsite.py b/scrapy/downloadermiddlewares/offsite.py\nindex a69f531a75a..a2cff65e7ef 100644\n--- a/scrapy/downloadermiddlewares/offsite.py\n+++ b/scrapy/downloadermiddlewares/offsite.py\n@@ -40,7 +40,11 @@ def request_scheduled(self, request: Request, spider: Spider) -> None:\n         self.process_request(request, spider)\n \n     def process_request(self, request: Request, spider: Spider) -> None:\n-        if request.dont_filter or self.should_follow(request, spider):\n+        if (\n+            request.dont_filter\n+            or request.meta.get(\"allow_offsite\")\n+            or self.should_follow(request, spider)\n+        ):\n             return\n         domain = urlparse_cached(request).hostname\n         if domain and domain not in self.domains_seen:\ndiff --git a/scrapy/spidermiddlewares/offsite.py b/scrapy/spidermiddlewares/offsite.py\nindex d3ed64ef546..95e753830be 100644\n--- a/scrapy/spidermiddlewares/offsite.py\n+++ b/scrapy/spidermiddlewares/offsite.py\n@@ -61,7 +61,11 @@ async def process_spider_output_async(\n     def _filter(self, request: Any, spider: Spider) -> bool:\n         if not isinstance(request, Request):\n             return True\n-        if request.dont_filter or self.should_follow(request, spider):\n+        if (\n+            request.dont_filter\n+            or request.meta.get(\"allow_offsite\")\n+            or self.should_follow(request, spider)\n+        ):\n             return True\n         domain = urlparse_cached(request).hostname\n         if domain and domain not in self.domains_seen:\n",
    "test_patch": "diff --git a/tests/test_downloadermiddleware_offsite.py b/tests/test_downloadermiddleware_offsite.py\nindex fec56a39f23..23a1d06dac0 100644\n--- a/tests/test_downloadermiddleware_offsite.py\n+++ b/tests/test_downloadermiddleware_offsite.py\n@@ -64,6 +64,37 @@ def test_process_request_dont_filter(value, filtered):\n         assert mw.process_request(request, spider) is None\n \n \n+@pytest.mark.parametrize(\n+    (\"allow_offsite\", \"dont_filter\", \"filtered\"),\n+    (\n+        (True, UNSET, False),\n+        (True, None, False),\n+        (True, False, False),\n+        (True, True, False),\n+        (False, UNSET, True),\n+        (False, None, True),\n+        (False, False, True),\n+        (False, True, False),\n+    ),\n+)\n+def test_process_request_allow_offsite(allow_offsite, dont_filter, filtered):\n+    crawler = get_crawler(Spider)\n+    spider = crawler._create_spider(name=\"a\", allowed_domains=[\"a.example\"])\n+    mw = OffsiteMiddleware.from_crawler(crawler)\n+    mw.spider_opened(spider)\n+    kwargs = {\"meta\": {}}\n+    if allow_offsite is not UNSET:\n+        kwargs[\"meta\"][\"allow_offsite\"] = allow_offsite\n+    if dont_filter is not UNSET:\n+        kwargs[\"dont_filter\"] = dont_filter\n+    request = Request(\"https://b.example\", **kwargs)\n+    if filtered:\n+        with pytest.raises(IgnoreRequest):\n+            mw.process_request(request, spider)\n+    else:\n+        assert mw.process_request(request, spider) is None\n+\n+\n @pytest.mark.parametrize(\n     \"value\",\n     (\ndiff --git a/tests/test_spidermiddleware_offsite.py b/tests/test_spidermiddleware_offsite.py\nindex 837f1c2c8f5..906928e0126 100644\n--- a/tests/test_spidermiddleware_offsite.py\n+++ b/tests/test_spidermiddleware_offsite.py\n@@ -29,6 +29,7 @@ def test_process_spider_output(self):\n             Request(\"http://scrapy.org/1\"),\n             Request(\"http://sub.scrapy.org/1\"),\n             Request(\"http://offsite.tld/letmepass\", dont_filter=True),\n+            Request(\"http://offsite-2.tld/allow\", meta={\"allow_offsite\": True}),\n             Request(\"http://scrapy.test.org/\"),\n             Request(\"http://scrapy.test.org:8000/\"),\n         ]\n",
    "problem_statement": "Separate Attribute to allow offsite requests\nRight now Request.dont_filter attribute is used for two intentions. To allow duplicated requests and to allow offsite requests. This behavior is not desirable since, you could want to allow a duplicate request but still want to filter out offsite requests and vice versa.\nAttribute to control offsite filtering\nThis is a fix for #3690\n",
    "hints_text": "\n# [Codecov](https://codecov.io/gh/scrapy/scrapy/pull/3691?src=pr&el=h1) Report\n> Merging [#3691](https://codecov.io/gh/scrapy/scrapy/pull/3691?src=pr&el=desc) into [master](https://codecov.io/gh/scrapy/scrapy/commit/d346b8cb0f7e206f9a878cfadcd62ef981e85310?src=pr&el=desc) will **increase** coverage by `0.17%`.\n> The diff coverage is `100%`.\n\n\n```diff\n@@            Coverage Diff             @@\n##           master    #3691      +/-   ##\n==========================================\n+ Coverage   84.54%   84.71%   +0.17%     \n==========================================\n  Files         167      168       +1     \n  Lines        9420     9460      +40     \n  Branches     1402     1407       +5     \n==========================================\n+ Hits         7964     8014      +50     \n+ Misses       1199     1188      -11     \n- Partials      257      258       +1\n```\n\n| [Impacted Files](https://codecov.io/gh/scrapy/scrapy/pull/3691?src=pr&el=tree) | Coverage \u0394 | |\n|---|---|---|\n| [scrapy/spidermiddlewares/offsite.py](https://codecov.io/gh/scrapy/scrapy/pull/3691/diff?src=pr&el=tree#diff-c2NyYXB5L3NwaWRlcm1pZGRsZXdhcmVzL29mZnNpdGUucHk=) | `100% <100%> (\u00f8)` | :arrow_up: |\n| [scrapy/utils/trackref.py](https://codecov.io/gh/scrapy/scrapy/pull/3691/diff?src=pr&el=tree#diff-c2NyYXB5L3V0aWxzL3RyYWNrcmVmLnB5) | `83.78% <0%> (-2.71%)` | :arrow_down: |\n| [scrapy/http/\\_\\_init\\_\\_.py](https://codecov.io/gh/scrapy/scrapy/pull/3691/diff?src=pr&el=tree#diff-c2NyYXB5L2h0dHAvX19pbml0X18ucHk=) | `100% <0%> (\u00f8)` | :arrow_up: |\n| [scrapy/http/request/json\\_request.py](https://codecov.io/gh/scrapy/scrapy/pull/3691/diff?src=pr&el=tree#diff-c2NyYXB5L2h0dHAvcmVxdWVzdC9qc29uX3JlcXVlc3QucHk=) | `93.75% <0%> (\u00f8)` | |\n| [scrapy/settings/default\\_settings.py](https://codecov.io/gh/scrapy/scrapy/pull/3691/diff?src=pr&el=tree#diff-c2NyYXB5L3NldHRpbmdzL2RlZmF1bHRfc2V0dGluZ3MucHk=) | `98.64% <0%> (\u00f8)` | :arrow_up: |\n| [scrapy/item.py](https://codecov.io/gh/scrapy/scrapy/pull/3691/diff?src=pr&el=tree#diff-c2NyYXB5L2l0ZW0ucHk=) | `98.48% <0%> (+0.07%)` | :arrow_up: |\n| [scrapy/extensions/feedexport.py](https://codecov.io/gh/scrapy/scrapy/pull/3691/diff?src=pr&el=tree#diff-c2NyYXB5L2V4dGVuc2lvbnMvZmVlZGV4cG9ydC5weQ==) | `84.9% <0%> (+6.43%)` | :arrow_up: |\n\nLGTM",
    "created_at": "2023-11-22T05:32:53Z",
    "version": "2.12",
    "PASS_TO_PASS": [],
    "FAIL_TO_PASS": [
      "tests/test_downloadermiddleware_offsite.py",
      "tests/test_spidermiddleware_offsite.py"
    ]
  },
  {
    "repo": "scrapy/scrapy",
    "pull_number": 6064,
    "instance_id": "scrapy__scrapy-6064",
    "issue_numbers": [
      "6024",
      "6030"
    ],
    "base_commit": "c65567988da2f6dd8ad894cf0cf57f2c074be10f",
    "patch": "diff --git a/scrapy/crawler.py b/scrapy/crawler.py\nindex 22fd65be7af..6f54e62e990 100644\n--- a/scrapy/crawler.py\n+++ b/scrapy/crawler.py\n@@ -404,8 +404,8 @@ def start(\n         :param bool stop_after_crawl: stop or not the reactor when all\n             crawlers have finished\n \n-        :param bool install_signal_handlers: whether to install the shutdown\n-            handlers (default: True)\n+        :param bool install_signal_handlers: whether to install the OS signal\n+            handlers from Twisted and Scrapy (default: True)\n         \"\"\"\n         from twisted.internet import reactor\n \n@@ -416,15 +416,17 @@ def start(\n                 return\n             d.addBoth(self._stop_reactor)\n \n-        if install_signal_handlers:\n-            install_shutdown_handlers(self._signal_shutdown)\n         resolver_class = load_object(self.settings[\"DNS_RESOLVER\"])\n         resolver = create_instance(resolver_class, self.settings, self, reactor=reactor)\n         resolver.install_on_reactor()\n         tp = reactor.getThreadPool()\n         tp.adjustPoolsize(maxthreads=self.settings.getint(\"REACTOR_THREADPOOL_MAXSIZE\"))\n         reactor.addSystemEventTrigger(\"before\", \"shutdown\", self.stop)\n-        reactor.run(installSignalHandlers=False)  # blocking call\n+        if install_signal_handlers:\n+            reactor.addSystemEventTrigger(\n+                \"after\", \"startup\", install_shutdown_handlers, self._signal_shutdown\n+            )\n+        reactor.run(installSignalHandlers=install_signal_handlers)  # blocking call\n \n     def _graceful_stop_reactor(self) -> Deferred:\n         d = self.stop()\ndiff --git a/scrapy/utils/ossignal.py b/scrapy/utils/ossignal.py\nindex 2334ea79242..db9a7127372 100644\n--- a/scrapy/utils/ossignal.py\n+++ b/scrapy/utils/ossignal.py\n@@ -19,13 +19,10 @@ def install_shutdown_handlers(\n     function: SignalHandlerT, override_sigint: bool = True\n ) -> None:\n     \"\"\"Install the given function as a signal handler for all common shutdown\n-    signals (such as SIGINT, SIGTERM, etc). If override_sigint is ``False`` the\n-    SIGINT handler won't be install if there is already a handler in place\n-    (e.g.  Pdb)\n+    signals (such as SIGINT, SIGTERM, etc). If ``override_sigint`` is ``False`` the\n+    SIGINT handler won't be installed if there is already a handler in place\n+    (e.g. Pdb)\n     \"\"\"\n-    from twisted.internet import reactor\n-\n-    reactor._handleSignals()\n     signal.signal(signal.SIGTERM, function)\n     if signal.getsignal(signal.SIGINT) == signal.default_int_handler or override_sigint:\n         signal.signal(signal.SIGINT, function)\ndiff --git a/setup.py b/setup.py\nindex 47c0af0b045..405633f5552 100644\n--- a/setup.py\n+++ b/setup.py\n@@ -6,8 +6,7 @@\n \n \n install_requires = [\n-    # 23.8.0 incompatibility: https://github.com/scrapy/scrapy/issues/6024\n-    \"Twisted>=18.9.0,<23.8.0\",\n+    \"Twisted>=18.9.0\",\n     \"cryptography>=36.0.0\",\n     \"cssselect>=0.9.1\",\n     \"itemloaders>=1.0.1\",\n",
    "test_patch": "diff --git a/scrapy/utils/testproc.py b/scrapy/utils/testproc.py\nindex 5f7a7db14b2..0688e014be0 100644\n--- a/scrapy/utils/testproc.py\n+++ b/scrapy/utils/testproc.py\n@@ -2,7 +2,7 @@\n \n import os\n import sys\n-from typing import Iterable, Optional, Tuple, cast\n+from typing import Iterable, List, Optional, Tuple, cast\n \n from twisted.internet.defer import Deferred\n from twisted.internet.error import ProcessTerminated\n@@ -26,14 +26,15 @@ def execute(\n         env = os.environ.copy()\n         if settings is not None:\n             env[\"SCRAPY_SETTINGS_MODULE\"] = settings\n+        assert self.command\n         cmd = self.prefix + [self.command] + list(args)\n         pp = TestProcessProtocol()\n-        pp.deferred.addBoth(self._process_finished, cmd, check_code)\n+        pp.deferred.addCallback(self._process_finished, cmd, check_code)\n         reactor.spawnProcess(pp, cmd[0], cmd, env=env, path=self.cwd)\n         return pp.deferred\n \n     def _process_finished(\n-        self, pp: TestProcessProtocol, cmd: str, check_code: bool\n+        self, pp: TestProcessProtocol, cmd: List[str], check_code: bool\n     ) -> Tuple[int, bytes, bytes]:\n         if pp.exitcode and check_code:\n             msg = f\"process {cmd} exit with code {pp.exitcode}\"\ndiff --git a/tests/CrawlerProcess/sleeping.py b/tests/CrawlerProcess/sleeping.py\nnew file mode 100644\nindex 00000000000..420d9d328ff\n--- /dev/null\n+++ b/tests/CrawlerProcess/sleeping.py\n@@ -0,0 +1,24 @@\n+from twisted.internet.defer import Deferred\n+\n+import scrapy\n+from scrapy.crawler import CrawlerProcess\n+from scrapy.utils.defer import maybe_deferred_to_future\n+\n+\n+class SleepingSpider(scrapy.Spider):\n+    name = \"sleeping\"\n+\n+    start_urls = [\"data:,;\"]\n+\n+    async def parse(self, response):\n+        from twisted.internet import reactor\n+\n+        d = Deferred()\n+        reactor.callLater(3, d.callback, None)\n+        await maybe_deferred_to_future(d)\n+\n+\n+process = CrawlerProcess(settings={})\n+\n+process.crawl(SleepingSpider)\n+process.start()\ndiff --git a/tests/requirements.txt b/tests/requirements.txt\nindex c07fda2d688..d4bfead40cf 100644\n--- a/tests/requirements.txt\n+++ b/tests/requirements.txt\n@@ -1,5 +1,6 @@\n # Tests requirements\n attrs\n+pexpect >= 4.8.0\n pyftpdlib >= 1.5.8\n pytest\n pytest-cov==4.0.0\ndiff --git a/tests/test_command_shell.py b/tests/test_command_shell.py\nindex 6589381f3b8..7d87eb62c4a 100644\n--- a/tests/test_command_shell.py\n+++ b/tests/test_command_shell.py\n@@ -1,11 +1,15 @@\n+import sys\n+from io import BytesIO\n from pathlib import Path\n \n+from pexpect.popen_spawn import PopenSpawn\n from twisted.internet import defer\n from twisted.trial import unittest\n \n from scrapy.utils.testproc import ProcessTest\n from scrapy.utils.testsite import SiteTest\n from tests import NON_EXISTING_RESOLVABLE, tests_datadir\n+from tests.mockserver import MockServer\n \n \n class ShellTest(ProcessTest, SiteTest, unittest.TestCase):\n@@ -133,3 +137,25 @@ def test_shell_fetch_async(self):\n         args = [\"-c\", code, \"--set\", f\"TWISTED_REACTOR={reactor_path}\"]\n         _, _, err = yield self.execute(args, check_code=True)\n         self.assertNotIn(b\"RuntimeError: There is no current event loop in thread\", err)\n+\n+\n+class InteractiveShellTest(unittest.TestCase):\n+    def test_fetch(self):\n+        args = (\n+            sys.executable,\n+            \"-m\",\n+            \"scrapy.cmdline\",\n+            \"shell\",\n+        )\n+        logfile = BytesIO()\n+        p = PopenSpawn(args, timeout=5)\n+        p.logfile_read = logfile\n+        p.expect_exact(\"Available Scrapy objects\")\n+        with MockServer() as mockserver:\n+            p.sendline(f\"fetch('{mockserver.url('/')}')\")\n+            p.sendline(\"type(response)\")\n+            p.expect_exact(\"HtmlResponse\")\n+        p.sendeof()\n+        p.wait()\n+        logfile.seek(0)\n+        self.assertNotIn(\"Traceback\", logfile.read().decode())\ndiff --git a/tests/test_crawler.py b/tests/test_crawler.py\nindex 2b141e89454..60b92377dd6 100644\n--- a/tests/test_crawler.py\n+++ b/tests/test_crawler.py\n@@ -1,13 +1,16 @@\n import logging\n import os\n import platform\n+import signal\n import subprocess\n import sys\n import warnings\n from pathlib import Path\n+from typing import List\n \n import pytest\n from packaging.version import parse as parse_version\n+from pexpect.popen_spawn import PopenSpawn\n from pytest import mark, raises\n from twisted.internet import defer\n from twisted.trial import unittest\n@@ -289,9 +292,12 @@ class ScriptRunnerMixin:\n     script_dir: Path\n     cwd = os.getcwd()\n \n-    def run_script(self, script_name: str, *script_args):\n+    def get_script_args(self, script_name: str, *script_args: str) -> List[str]:\n         script_path = self.script_dir / script_name\n-        args = [sys.executable, str(script_path)] + list(script_args)\n+        return [sys.executable, str(script_path)] + list(script_args)\n+\n+    def run_script(self, script_name: str, *script_args: str) -> str:\n+        args = self.get_script_args(script_name, *script_args)\n         p = subprocess.Popen(\n             args,\n             env=get_mockserver_env(),\n@@ -517,6 +523,29 @@ def test_args_change_settings(self):\n         self.assertIn(\"Spider closed (finished)\", log)\n         self.assertIn(\"The value of FOO is 42\", log)\n \n+    def test_shutdown_graceful(self):\n+        sig = signal.SIGINT if sys.platform != \"win32\" else signal.SIGBREAK\n+        args = self.get_script_args(\"sleeping.py\")\n+        p = PopenSpawn(args, timeout=5)\n+        p.expect_exact(\"Spider opened\")\n+        p.expect_exact(\"Crawled (200)\")\n+        p.kill(sig)\n+        p.expect_exact(\"shutting down gracefully\")\n+        p.expect_exact(\"Spider closed (shutdown)\")\n+        p.wait()\n+\n+    def test_shutdown_forced(self):\n+        sig = signal.SIGINT if sys.platform != \"win32\" else signal.SIGBREAK\n+        args = self.get_script_args(\"sleeping.py\")\n+        p = PopenSpawn(args, timeout=5)\n+        p.expect_exact(\"Spider opened\")\n+        p.expect_exact(\"Crawled (200)\")\n+        p.kill(sig)\n+        p.expect_exact(\"shutting down gracefully\")\n+        p.kill(sig)\n+        p.expect_exact(\"forcing unclean shutdown\")\n+        p.wait()\n+\n \n class CrawlerRunnerSubprocess(ScriptRunnerMixin, unittest.TestCase):\n     script_dir = Path(__file__).parent.resolve() / \"CrawlerRunner\"\n",
    "problem_statement": "Problem with Twisted AttributeError: 'EPollReactor' object has no attribute '_handleSignals'\n### Description\r\n\r\nSince the release of Twisted 23.8.0 recently, I've had problems running Scrapy. I had to fix to the previous version, Twisted==22.10.0\r\n\r\n### Steps to Reproduce\r\n\r\n1. Install Scrapy in a new environment\r\n2. Run .start() from an CrawlerProcess object\r\n\r\n**Expected behavior:** Scrapy to crawl\r\n\r\n**Actual behavior:** \r\n![image](https://github.com/scrapy/scrapy/assets/14026017/4d769673-52ee-4039-887f-c810017d48e2)\r\n\r\n**Reproduces how often:** Always in the new version\r\n\r\n### Versions\r\n\r\nScrapy>=2.9.0\r\n\r\n### Additional context\r\n\r\nI've seen this fix on [stackoverflow](https://stackoverflow.com/questions/76995567/error-when-crawl-data-epollreactor-object-has-no-attribute-handlesignals)\r\n\n[wip] reactor handlesignals\nFix to https://github.com/scrapy/scrapy/issues/6024\r\n\r\nFrom my observations of scrapy code and related twisted pull request I conclude that call of `reactor._handleSignals()` or it's equivalent from new twisted version - are not needed inside `install_shutdown_handlers` to run scrapy.\r\n\r\nAt least on my local environment with both new/old twisted versions I didn't noticed anything unexpected.\n",
    "hints_text": "I'm having the same issue, but if you revert back to `pip install Twisted==22.10.0 ` it's working.\nI assume that changes from twisted side that caused this issue came from this issue https://github.com/twisted/twisted/issues/11751 and this pull request https://github.com/twisted/twisted/pull/11752\nThis affects both `scrapy crawl` and direct usage of `CrawlerProcess`. \nLooks like we are calling `reactor._handleSignals()` in `scrapy.utils.ossignal.install_shutdown_handlers()` so that we can override the Twisted handlers. Twisted itself calls `reactor._handleSignals()` in `_SignalReactorMixin._reallyStartRunning()`, I assume this happens later.\r\n\r\nIt seems that the equivalent code in new Twisted is in `twisted.internet._signals._WithSignalHandling.install()` but this is even more private and even more tied to internal logic so I hesitate to use it. It may be possible to redo our logic instead.\r\n\r\nUnfortunately, this isn't covered by tests and deleting `reactor._handleSignals()` doesn't fail them :)\r\n\r\nOverall, this looks like releasing a version with the Twisted version restriction is much more viable than quickly fixing this.\nAfter looking at the code for some more I think we need to check the order of handler installation in Scrapy with older Twisted in order to understand how it works. For example, I'm not sure how do other handlers work if Twisted resets them on reactor.run() (as the comments in the new logic suggest).\nJust popping my head in here to say that this might be something to discuss on the Twisted mailing list, if there are requirements that Scrapy has for signal handling that we've missed!\r\n\r\nhttps://mail.python.org/mailman3/lists/twisted.python.org/\nPrepared pull request with removed failing call of `_handleSignals` - with this change scrapy works on my local environment as expected (both earlier 22.10.0 and new 23.8.0 version of twisted checked locally). At least it looks like expected from my first view.\r\n\r\nAlso it is noteable fact that the most of scrapy code parts related to handling these signals([/scrapy/utils/ossignal.py](https://github.com/scrapy/scrapy/blob/2.10.0/scrapy/utils/ossignal.py), [/scrapy/crawler.py](https://github.com/scrapy/scrapy/blob/2.10.0/scrapy/crawler.py#L337-L356)) looks nearly the same as 10+ years ago when It appeared. \r\n\r\nSo I assume that this call of  `_handleSignals` was needed for.. much earlier versions of twisted however I am not aware how twisted looked like 10+ years ago (I am not aware of deep internals of recent versions of twisted as well)\nWe just released 2.10.1 with the Twisted version restricted as a workaround for this.\n## [Codecov](https://app.codecov.io/gh/scrapy/scrapy/pull/6030?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scrapy) Report\n> Merging [#6030](https://app.codecov.io/gh/scrapy/scrapy/pull/6030?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scrapy) (8c99f44) into [master](https://app.codecov.io/gh/scrapy/scrapy/commit/0e7fc60b89b0ba5cdf72709ff2e70bfe6ccecbdc?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scrapy) (0e7fc60) will **decrease** coverage by `0.01%`.\n> The diff coverage is `n/a`.\n\n> :exclamation: Current head 8c99f44 differs from pull request most recent head 47bcf1a. Consider uploading reports for the commit 47bcf1a to get more accurate results\n\n```diff\n@@            Coverage Diff             @@\n##           master    #6030      +/-   ##\n==========================================\n- Coverage   88.92%   88.92%   -0.01%     \n==========================================\n  Files         162      162              \n  Lines       11445    11443       -2     \n  Branches     1861     1861              \n==========================================\n- Hits        10178    10176       -2     \n  Misses        962      962              \n  Partials      305      305              \n```\n\n| [Files Changed](https://app.codecov.io/gh/scrapy/scrapy/pull/6030?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scrapy) | Coverage \u0394 | |\n|---|---|---|\n| [scrapy/utils/ossignal.py](https://app.codecov.io/gh/scrapy/scrapy/pull/6030?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scrapy#diff-c2NyYXB5L3V0aWxzL29zc2lnbmFsLnB5) | `75.00% <\u00f8> (-2.78%)` | :arrow_down: |\n\nI think Twisted sets handlers for INT and BREAK conditionally but for TERM unconditionally, so I would expect our TERM handler to be overridden in at least some cases if this line is removed.\nAfter thinking about it for some more I think it makes sense that calling or not calling this doesn't make a difference, whatever the call order. But can you please check if with old Twisted a simple `scrapy crawl` has the TERM handler last set by Scrapy or by Twisted?\nIn `CrawlerProcess.start`.  `reactor.run` - called with `installSignalHandlers=False` which means that reactor will not apply it's default handlers. And as far as I understand in this case handlers from scrapy `install_shutdown_handlers` will be the only applied signal handlers\r\nhttps://github.com/scrapy/scrapy/blob/a320e5f6a421ea3bae06d2f63d29bae9d327f580/scrapy/crawler.py#L389-L397\r\n\r\nWhile if we will run scrapy with `CrawlerRunner` with simply `reactor.run()` as documented on https://docs.scrapy.org/en/latest/topics/practices.html#running-multiple-spiders-in-the-same-process - reactor will run with reactor default signal handlers (and no scrapy `install_shutdown_handlers` as it called in `CrawlerProcess` only) \n> reactor.run - called with installSignalHandlers=False\r\n\r\nOh!\r\n\r\nThen yeah, it probably doesn't make sense. OTOH `twisted.internet.posixbase.PosixReactorBase._handleSignals()` also installs a SIGCHLD handler, but I don't know how useful is it for Scrapy.\nAfter discussing this with @kmike we want this:\r\n\r\n1. An integration test that makes sure that spawning a spider process and sending it a Ctrl-C (or a similar signal) once leads to a graceful shutdown while sending it twice leads to an immediate exit. We may already have such test, I haven't checked.\r\n2. Making sure that spawning a process with `subprocess` and with `reactor.spawnProcess()` works on a POSIX OS. It will also be interesting to check if this process becomes a zombie or is reaped correctly, both with old and new code. It will also be interesting to check `twisted.internet.utils.getProcessValue()` as it may depend on SIGCHLD handling.\r\n3. Making sure that the new code works on Scrapy Cloud as expected.\n> We may already have such test, I haven't checked.\r\n\r\nI don\u2019t think we do. It may be useful for https://github.com/scrapy/scrapy/issues/4749 as well.\nFor (2): \r\n\r\n- `subprocess.run()` doesn't leave a zombie\r\n- `getProcessValue()` returns the exit code and doesn't leave a zombie\r\n- `reactor.spawnProcess` leaves a zombie and doesn't fire the protocol's `processEnded` handler\r\n\r\nThe last point is an important difference, I don't want to break this. So we want to either call the SIGCHLD-related Twisted code directly in `install_shutdown_handlers`, or remove `installSignalHandlers=False` and try to work with that.\r\n\r\nFor the first option it's again calling some private code which I don't want to do. For the second option it means Twisted will overwrite our SIGTERM and SIGBREAK handlers but leave our SIGINT handler. We can try finding a later point at which to install our handlers.\nhttps://stackoverflow.com/questions/74929947/twisted-application-ignoring-a-certain-unix-signal-is-it-possible suggests `reactor.addSystemEventTrigger('after', 'startup', install_your_handlers)` ",
    "created_at": "2023-09-24T14:17:06Z",
    "version": "2.11",
    "PASS_TO_PASS": [],
    "FAIL_TO_PASS": [
      "scrapy/utils/testproc.py",
      "tests/requirements.txt",
      "tests/test_command_shell.py",
      "tests/test_crawler.py"
    ]
  },
  {
    "repo": "scrapy/scrapy",
    "pull_number": 5508,
    "instance_id": "scrapy__scrapy-5508",
    "issue_numbers": [
      "5504",
      "5504"
    ],
    "base_commit": "078622cfb0ee364acba5d91a20244f9c1ee87d30",
    "patch": "diff --git a/docs/topics/media-pipeline.rst b/docs/topics/media-pipeline.rst\nindex 7dff78390f5..2513faae268 100644\n--- a/docs/topics/media-pipeline.rst\n+++ b/docs/topics/media-pipeline.rst\n@@ -656,6 +656,26 @@ See here the methods that you can override in your custom Images Pipeline:\n       .. versionadded:: 2.4\n          The *item* parameter.\n \n+   .. method:: ImagesPipeline.thumb_path(self, request, thumb_id, response=None, info=None, *, item=None)\n+\n+      This method is called for every item of  :setting:`IMAGES_THUMBS` per downloaded item. It returns the\n+      thumbnail download path of the image originating from the specified\n+      :class:`response <scrapy.http.Response>`.\n+\n+      In addition to ``response``, this method receives the original\n+      :class:`request <scrapy.Request>`,\n+      ``thumb_id``,\n+      :class:`info <scrapy.pipelines.media.MediaPipeline.SpiderInfo>` and\n+      :class:`item <scrapy.Item>`.\n+\n+      You can override this method to customize the thumbnail download path of each image.\n+      You can use the ``item`` to determine the file path based on some item\n+      property.\n+\n+      By default the :meth:`thumb_path` method returns\n+      ``thumbs/<size name>/<request URL hash>.<extension>``.\n+\n+\n    .. method:: ImagesPipeline.get_media_requests(item, info)\n \n       Works the same way as :meth:`FilesPipeline.get_media_requests` method,\ndiff --git a/scrapy/pipelines/images.py b/scrapy/pipelines/images.py\nindex 9c99dc69ee8..6b97190ee58 100644\n--- a/scrapy/pipelines/images.py\n+++ b/scrapy/pipelines/images.py\n@@ -141,7 +141,7 @@ def get_images(self, response, request, info, *, item=None):\n         yield path, image, buf\n \n         for thumb_id, size in self.thumbs.items():\n-            thumb_path = self.thumb_path(request, thumb_id, response=response, info=info)\n+            thumb_path = self.thumb_path(request, thumb_id, response=response, info=info, item=item)\n             thumb_image, thumb_buf = self.convert_image(image, size)\n             yield thumb_path, thumb_image, thumb_buf\n \n@@ -179,6 +179,6 @@ def file_path(self, request, response=None, info=None, *, item=None):\n         image_guid = hashlib.sha1(to_bytes(request.url)).hexdigest()\n         return f'full/{image_guid}.jpg'\n \n-    def thumb_path(self, request, thumb_id, response=None, info=None):\n+    def thumb_path(self, request, thumb_id, response=None, info=None, *, item=None):\n         thumb_guid = hashlib.sha1(to_bytes(request.url)).hexdigest()\n         return f'thumbs/{thumb_id}/{thumb_guid}.jpg'\ndiff --git a/scrapy/pipelines/media.py b/scrapy/pipelines/media.py\nindex d1bccf32355..430c37227de 100644\n--- a/scrapy/pipelines/media.py\n+++ b/scrapy/pipelines/media.py\n@@ -121,7 +121,7 @@ def _process_request(self, request, info, item):\n     def _make_compatible(self):\n         \"\"\"Make overridable methods of MediaPipeline and subclasses backwards compatible\"\"\"\n         methods = [\n-            \"file_path\", \"media_to_download\", \"media_downloaded\",\n+            \"file_path\", \"thumb_path\", \"media_to_download\", \"media_downloaded\",\n             \"file_downloaded\", \"image_downloaded\", \"get_images\"\n         ]\n \n",
    "test_patch": "diff --git a/tests/test_pipeline_images.py b/tests/test_pipeline_images.py\nindex c69cd0e4a7e..dd94d296b33 100644\n--- a/tests/test_pipeline_images.py\n+++ b/tests/test_pipeline_images.py\n@@ -93,6 +93,22 @@ def test_thumbnail_name(self):\n                                     info=object()),\n                          'thumbs/50/850233df65a5b83361798f532f1fc549cd13cbe9.jpg')\n \n+    def test_thumbnail_name_from_item(self):\n+        \"\"\"\n+        Custom thumbnail name based on item data, overriding default implementation\n+        \"\"\"\n+\n+        class CustomImagesPipeline(ImagesPipeline):\n+            def thumb_path(self, request, thumb_id, response=None, info=None, item=None):\n+                return f\"thumb/{thumb_id}/{item.get('path')}\"\n+\n+        thumb_path = CustomImagesPipeline.from_settings(Settings(\n+            {'IMAGES_STORE': self.tempdir}\n+        )).thumb_path\n+        item = dict(path='path-to-store-file')\n+        request = Request(\"http://example.com\")\n+        self.assertEqual(thumb_path(request, 'small', item=item), 'thumb/small/path-to-store-file')\n+\n     def test_convert_image(self):\n         SIZE = (100, 100)\n         # straigh forward case: RGB and JPEG\ndiff --git a/tests/test_pipeline_media.py b/tests/test_pipeline_media.py\nindex 893d4305200..a802c7cf114 100644\n--- a/tests/test_pipeline_media.py\n+++ b/tests/test_pipeline_media.py\n@@ -1,4 +1,5 @@\n from typing import Optional\n+import io\n \n from testfixtures import LogCapture\n from twisted.trial import unittest\n@@ -355,9 +356,12 @@ def __init__(self, *args, **kwargs):\n \n     def get_media_requests(self, item, info):\n         item_url = item['image_urls'][0]\n+        output_img = io.BytesIO()\n+        img = Image.new('RGB', (60, 30), color='red')\n+        img.save(output_img, format='JPEG')\n         return Request(\n             item_url,\n-            meta={'response': Response(item_url, status=200, body=b'data')}\n+            meta={'response': Response(item_url, status=200, body=output_img.getvalue())}\n         )\n \n     def inc_stats(self, *args, **kwargs):\n@@ -379,9 +383,13 @@ def file_path(self, request, response=None, info=None):\n         self._mockcalled.append('file_path')\n         return super(MockedMediaPipelineDeprecatedMethods, self).file_path(request, response, info)\n \n+    def thumb_path(self, request, thumb_id, response=None, info=None):\n+        self._mockcalled.append('thumb_path')\n+        return super(MockedMediaPipelineDeprecatedMethods, self).thumb_path(request, thumb_id, response, info)\n+\n     def get_images(self, response, request, info):\n         self._mockcalled.append('get_images')\n-        return []\n+        return super(MockedMediaPipelineDeprecatedMethods, self).get_images(response, request, info)\n \n     def image_downloaded(self, response, request, info):\n         self._mockcalled.append('image_downloaded')\n@@ -392,7 +400,11 @@ class MediaPipelineDeprecatedMethodsTestCase(unittest.TestCase):\n     skip = skip_pillow\n \n     def setUp(self):\n-        self.pipe = MockedMediaPipelineDeprecatedMethods(store_uri='store-uri', download_func=_mocked_download_func)\n+        self.pipe = MockedMediaPipelineDeprecatedMethods(\n+            store_uri='store-uri',\n+            download_func=_mocked_download_func,\n+            settings=Settings({\"IMAGES_THUMBS\": {'small': (50, 50)}})\n+        )\n         self.pipe.open_spider(None)\n         self.item = dict(image_urls=['http://picsum.photos/id/1014/200/300'], images=[])\n \n@@ -444,6 +456,16 @@ def test_file_path_called(self):\n         )\n         self._assert_method_called_with_warnings('file_path', message, warnings)\n \n+    @inlineCallbacks\n+    def test_thumb_path_called(self):\n+        yield self.pipe.process_item(self.item, None)\n+        warnings = self.flushWarnings([MediaPipeline._compatible])\n+        message = (\n+            'thumb_path(self, request, thumb_id, response=None, info=None) is deprecated, '\n+            'please use thumb_path(self, request, thumb_id, response=None, info=None, *, item=None)'\n+        )\n+        self._assert_method_called_with_warnings('thumb_path', message, warnings)\n+\n     @inlineCallbacks\n     def test_get_images_called(self):\n         yield self.pipe.process_item(self.item, None)\n",
    "problem_statement": "ImagesPipeline.thumb_path should allow access to item\n## Summary\r\n\r\nImagesPipeline should pass on item to thumb_path function as additional argument.\r\n\r\n## Motivation\r\n\r\nThe ImagesPipeline.file_path function allows access to item. So when an user overwrites this function and makes the file name\r\ndepended on the item itself, it makes sense to have the thumb path to be depended on the item as well.\r\n\r\nThis way main image file and thumbnail image file can have the same name and it is easily possible to jump between different sizes of the image.\r\n\r\nExample of how a file and thumbnail could be build:\r\n```python\r\nfile_path = f\"full/{item['ID']}.jpg\"\r\nthumb_path = f\"thumbs/small/{item['ID']}.jpg\"\r\n```\r\n\r\nSo I suggest to change the signature of ImagesPipeline.thumb_path to the following:\r\n```python\r\ndef thumb_path(self, request, thumb_id, response=None, info=None, *, item=None):\r\n```\r\nwhich makes the functions thumb_path and file_path more consistent.\r\n\r\n## Describe alternatives you've considered\r\n\r\nAs a workaround when I overwrite `file_path` I set `request._item = item` which I can then access in `thumb_path` function.\r\n\r\n## Additional context\r\n\r\nIn case this is of interest I am happy to work on a pull request.\r\n\nImagesPipeline.thumb_path should allow access to item\n## Summary\r\n\r\nImagesPipeline should pass on item to thumb_path function as additional argument.\r\n\r\n## Motivation\r\n\r\nThe ImagesPipeline.file_path function allows access to item. So when an user overwrites this function and makes the file name\r\ndepended on the item itself, it makes sense to have the thumb path to be depended on the item as well.\r\n\r\nThis way main image file and thumbnail image file can have the same name and it is easily possible to jump between different sizes of the image.\r\n\r\nExample of how a file and thumbnail could be build:\r\n```python\r\nfile_path = f\"full/{item['ID']}.jpg\"\r\nthumb_path = f\"thumbs/small/{item['ID']}.jpg\"\r\n```\r\n\r\nSo I suggest to change the signature of ImagesPipeline.thumb_path to the following:\r\n```python\r\ndef thumb_path(self, request, thumb_id, response=None, info=None, *, item=None):\r\n```\r\nwhich makes the functions thumb_path and file_path more consistent.\r\n\r\n## Describe alternatives you've considered\r\n\r\nAs a workaround when I overwrite `file_path` I set `request._item = item` which I can then access in `thumb_path` function.\r\n\r\n## Additional context\r\n\r\nIn case this is of interest I am happy to work on a pull request.\r\n\n",
    "hints_text": "\n",
    "created_at": "2022-05-24T10:33:12Z",
    "version": "2.6",
    "PASS_TO_PASS": [],
    "FAIL_TO_PASS": [
      "tests/test_pipeline_images.py",
      "tests/test_pipeline_media.py"
    ]
  },
  {
    "repo": "scrapy/scrapy",
    "pull_number": 5514,
    "instance_id": "scrapy__scrapy-5514",
    "issue_numbers": [
      "5512"
    ],
    "base_commit": "bafe874ecca8f2ab86611739202806ca0cdba844",
    "patch": "diff --git a/.github/workflows/checks.yml b/.github/workflows/checks.yml\nindex 98fa44c7ff6..b26f344ffb0 100644\n--- a/.github/workflows/checks.yml\n+++ b/.github/workflows/checks.yml\n@@ -19,7 +19,7 @@ jobs:\n         - python-version: 3.8\n           env:\n             TOXENV: pylint\n-        - python-version: 3.6\n+        - python-version: 3.7\n           env:\n             TOXENV: typing\n         - python-version: \"3.10\"  # Keep in sync with .readthedocs.yml\ndiff --git a/README.rst b/README.rst\nindex 6b563d638f9..b543a30f49c 100644\n--- a/README.rst\n+++ b/README.rst\n@@ -57,7 +57,7 @@ including a list of features.\n Requirements\n ============\n \n-* Python 3.6+\n+* Python 3.7+\n * Works on Linux, Windows, macOS, BSD\n \n Install\ndiff --git a/docs/contributing.rst b/docs/contributing.rst\nindex 4d2580a6c8b..946bdc23e10 100644\n--- a/docs/contributing.rst\n+++ b/docs/contributing.rst\n@@ -232,15 +232,15 @@ To run a specific test (say ``tests/test_loader.py``) use:\n \n To run the tests on a specific :doc:`tox <tox:index>` environment, use\n ``-e <name>`` with an environment name from ``tox.ini``. For example, to run\n-the tests with Python 3.6 use::\n+the tests with Python 3.7 use::\n \n-    tox -e py36\n+    tox -e py37\n \n You can also specify a comma-separated list of environments, and use :ref:`tox\u2019s\n parallel mode <tox:parallel_mode>` to run the tests on multiple environments in\n parallel::\n \n-    tox -e py36,py38 -p auto\n+    tox -e py37,py38 -p auto\n \n To pass command-line options to :doc:`pytest <pytest:index>`, add them after\n ``--`` in your call to :doc:`tox <tox:index>`. Using ``--`` overrides the\n@@ -250,9 +250,9 @@ default positional arguments (``scrapy tests``) after ``--`` as well::\n     tox -- scrapy tests -x  # stop after first failure\n \n You can also use the `pytest-xdist`_ plugin. For example, to run all tests on\n-the Python 3.6 :doc:`tox <tox:index>` environment using all your CPU cores::\n+the Python 3.7 :doc:`tox <tox:index>` environment using all your CPU cores::\n \n-    tox -e py36 -- scrapy tests -n auto\n+    tox -e py37 -- scrapy tests -n auto\n \n To see coverage report install :doc:`coverage <coverage:index>`\n (``pip install coverage``) and run:\ndiff --git a/docs/intro/install.rst b/docs/intro/install.rst\nindex b8d3a16bccd..1f01c068dfc 100644\n--- a/docs/intro/install.rst\n+++ b/docs/intro/install.rst\n@@ -9,8 +9,8 @@ Installation guide\n Supported Python versions\n =========================\n \n-Scrapy requires Python 3.6+, either the CPython implementation (default) or\n-the PyPy 7.2.0+ implementation (see :ref:`python:implementations`).\n+Scrapy requires Python 3.7+, either the CPython implementation (default) or\n+the PyPy 7.3.5+ implementation (see :ref:`python:implementations`).\n \n .. _intro-install-scrapy:\n \ndiff --git a/docs/topics/items.rst b/docs/topics/items.rst\nindex 7cd482d0746..16701438179 100644\n--- a/docs/topics/items.rst\n+++ b/docs/topics/items.rst\n@@ -102,11 +102,6 @@ Additionally, ``dataclass`` items also allow to:\n * define custom field metadata through :func:`dataclasses.field`, which can be used to\n   :ref:`customize serialization <topics-exporters-field-serialization>`.\n \n-They work natively in Python 3.7 or later, or using the `dataclasses\n-backport`_ in Python 3.6.\n-\n-.. _dataclasses backport: https://pypi.org/project/dataclasses/\n-\n Example::\n \n     from dataclasses import dataclass\ndiff --git a/docs/topics/media-pipeline.rst b/docs/topics/media-pipeline.rst\nindex 7dff78390f5..7457d09cdad 100644\n--- a/docs/topics/media-pipeline.rst\n+++ b/docs/topics/media-pipeline.rst\n@@ -70,7 +70,7 @@ The advantage of using the :class:`ImagesPipeline` for image files is that you\n can configure some extra functions like generating thumbnails and filtering\n the images based on their size.\n \n-The Images Pipeline requires Pillow_ 4.0.0 or greater. It is used for\n+The Images Pipeline requires Pillow_ 7.1.0 or greater. It is used for\n thumbnailing and normalizing images to JPEG/RGB format.\n \n .. _Pillow: https://github.com/python-pillow/Pillow\ndiff --git a/scrapy/__init__.py b/scrapy/__init__.py\nindex 396f98219f8..86e5843963c 100644\n--- a/scrapy/__init__.py\n+++ b/scrapy/__init__.py\n@@ -28,8 +28,8 @@\n \n \n # Check minimum required Python version\n-if sys.version_info < (3, 6):\n-    print(f\"Scrapy {__version__} requires Python 3.6+\")\n+if sys.version_info < (3, 7):\n+    print(f\"Scrapy {__version__} requires Python 3.7+\")\n     sys.exit(1)\n \n \ndiff --git a/scrapy/utils/py36.py b/scrapy/utils/py36.py\ndeleted file mode 100644\nindex 653e2bbbb49..00000000000\n--- a/scrapy/utils/py36.py\n+++ /dev/null\n@@ -1,11 +0,0 @@\n-import warnings\n-\n-from scrapy.exceptions import ScrapyDeprecationWarning\n-from scrapy.utils.asyncgen import collect_asyncgen  # noqa: F401\n-\n-\n-warnings.warn(\n-    \"Module `scrapy.utils.py36` is deprecated, please import from `scrapy.utils.asyncgen` instead.\",\n-    category=ScrapyDeprecationWarning,\n-    stacklevel=2,\n-)\ndiff --git a/setup.py b/setup.py\nindex d86c0f285d0..ed197273fe5 100644\n--- a/setup.py\n+++ b/setup.py\n@@ -19,35 +19,29 @@ def has_environment_marker_platform_impl_support():\n \n \n install_requires = [\n-    'Twisted>=17.9.0',\n-    'cryptography>=2.0',\n+    'Twisted>=18.9.0',\n+    'cryptography>=2.8',\n     'cssselect>=0.9.1',\n     'itemloaders>=1.0.1',\n     'parsel>=1.5.0',\n-    'pyOpenSSL>=16.2.0',\n+    'pyOpenSSL>=19.1.0',\n     'queuelib>=1.4.2',\n     'service_identity>=16.0.0',\n     'w3lib>=1.17.0',\n-    'zope.interface>=4.1.3',\n+    'zope.interface>=5.1.0',\n     'protego>=0.1.15',\n     'itemadapter>=0.1.0',\n     'setuptools',\n     'tldextract',\n+    'lxml>=4.3.0',\n ]\n extras_require = {}\n cpython_dependencies = [\n-    'lxml>=3.5.0',\n     'PyDispatcher>=2.0.5',\n ]\n if has_environment_marker_platform_impl_support():\n     extras_require[':platform_python_implementation == \"CPython\"'] = cpython_dependencies\n     extras_require[':platform_python_implementation == \"PyPy\"'] = [\n-        # Earlier lxml versions are affected by\n-        # https://foss.heptapod.net/pypy/pypy/-/issues/2498,\n-        # which was fixed in Cython 0.26, released on 2017-06-19, and used to\n-        # generate the C headers of lxml release tarballs published since then, the\n-        # first of which was:\n-        'lxml>=4.0.0',\n         'PyPyDispatcher>=2.1.0',\n     ]\n else:\n@@ -84,7 +78,6 @@ def has_environment_marker_platform_impl_support():\n         'Operating System :: OS Independent',\n         'Programming Language :: Python',\n         'Programming Language :: Python :: 3',\n-        'Programming Language :: Python :: 3.6',\n         'Programming Language :: Python :: 3.7',\n         'Programming Language :: Python :: 3.8',\n         'Programming Language :: Python :: 3.9',\n@@ -95,7 +88,7 @@ def has_environment_marker_platform_impl_support():\n         'Topic :: Software Development :: Libraries :: Application Frameworks',\n         'Topic :: Software Development :: Libraries :: Python Modules',\n     ],\n-    python_requires='>=3.6',\n+    python_requires='>=3.7',\n     install_requires=install_requires,\n     extras_require=extras_require,\n )\ndiff --git a/tox.ini b/tox.ini\nindex 6951b6d1632..ab8a715c2c9 100644\n--- a/tox.ini\n+++ b/tox.ini\n@@ -11,15 +11,13 @@ minversion = 1.7.0\n deps =\n     -rtests/requirements.txt\n     # mitmproxy does not support PyPy\n-    # mitmproxy does not support Windows when running Python < 3.7\n     # Python 3.9+ requires mitmproxy >= 5.3.0\n     # mitmproxy >= 5.3.0 requires h2 >= 4.0, Twisted 21.2 requires h2 < 4.0\n     #mitmproxy >= 5.3.0; python_version >= '3.9' and implementation_name != 'pypy'\n     # The tests hang with mitmproxy 8.0.0: https://github.com/scrapy/scrapy/issues/5454\n-    mitmproxy >= 4.0.4, < 8; python_version >= '3.7' and python_version < '3.9' and implementation_name != 'pypy'\n-    mitmproxy >= 4.0.4, < 5; python_version >= '3.6' and python_version < '3.7' and platform_system != 'Windows' and implementation_name != 'pypy'\n+    mitmproxy >= 4.0.4, < 8; python_version < '3.9' and implementation_name != 'pypy'\n     # newer markupsafe is incompatible with deps of old mitmproxy (which we get on Python 3.7 and lower)\n-    markupsafe < 2.1.0; python_version >= '3.6' and python_version < '3.8' and implementation_name != 'pypy'\n+    markupsafe < 2.1.0; python_version < '3.8' and implementation_name != 'pypy'\n     # Extras\n     botocore>=1.4.87\n passenv =\n@@ -44,7 +42,6 @@ deps =\n     types-pyOpenSSL==20.0.3\n     types-setuptools==57.0.0\n commands =\n-    pip install types-dataclasses  # remove once py36 support is dropped\n     mypy --show-error-codes {posargs: scrapy tests}\n \n [testenv:security]\n@@ -75,18 +72,19 @@ commands =\n \n [pinned]\n deps =\n-    cryptography==2.0\n+    cryptography==2.8\n     cssselect==0.9.1\n     h2==3.0\n     itemadapter==0.1.0\n     parsel==1.5.0\n     Protego==0.1.15\n-    pyOpenSSL==16.2.0\n+    pyOpenSSL==19.1.0\n     queuelib==1.4.2\n     service_identity==16.0.0\n-    Twisted[http2]==17.9.0\n+    Twisted[http2]==18.9.0\n     w3lib==1.17.0\n-    zope.interface==4.1.3\n+    zope.interface==5.1.0\n+    lxml==4.3.0\n     -rtests/requirements.txt\n \n     # mitmproxy 4.0.4+ requires upgrading some of the pinned dependencies\n@@ -95,7 +93,7 @@ deps =\n     # Extras\n     botocore==1.4.87\n     google-cloud-storage==1.29.0\n-    Pillow==4.0.0\n+    Pillow==7.1.0\n setenv =\n     _SCRAPY_PINNED=true\n install_command =\n@@ -104,7 +102,6 @@ install_command =\n [testenv:pinned]\n deps =\n     {[pinned]deps}\n-    lxml==3.5.0\n     PyDispatcher==2.0.5\n install_command = {[pinned]install_command}\n setenv =\n@@ -114,9 +111,6 @@ setenv =\n basepython = python3\n deps =\n     {[pinned]deps}\n-    # First lxml version that includes a Windows wheel for Python 3.6, so we do\n-    # not need to build lxml from sources in a CI Windows job:\n-    lxml==3.8.0\n     PyDispatcher==2.0.5\n install_command = {[pinned]install_command}\n setenv =\n@@ -155,7 +149,6 @@ commands =\n basepython = {[testenv:pypy3]basepython}\n deps =\n     {[pinned]deps}\n-    lxml==4.0.0\n     PyPyDispatcher==2.1.0\n commands = {[testenv:pypy3]commands}\n install_command = {[pinned]install_command}\n",
    "test_patch": "diff --git a/.github/workflows/tests-macos.yml b/.github/workflows/tests-macos.yml\nindex 3aaf688c712..7819a4e12f9 100644\n--- a/.github/workflows/tests-macos.yml\n+++ b/.github/workflows/tests-macos.yml\n@@ -7,7 +7,7 @@ jobs:\n     strategy:\n       fail-fast: false\n       matrix:\n-        python-version: [\"3.6\", \"3.7\", \"3.8\", \"3.9\", \"3.10\"]\n+        python-version: [\"3.7\", \"3.8\", \"3.9\", \"3.10\"]\n \n     steps:\n     - uses: actions/checkout@v2\ndiff --git a/.github/workflows/tests-ubuntu.yml b/.github/workflows/tests-ubuntu.yml\nindex 1fc8d914b88..be40c7c7111 100644\n--- a/.github/workflows/tests-ubuntu.yml\n+++ b/.github/workflows/tests-ubuntu.yml\n@@ -8,9 +8,6 @@ jobs:\n       fail-fast: false\n       matrix:\n         include:\n-        - python-version: 3.7\n-          env:\n-            TOXENV: py\n         - python-version: 3.8\n           env:\n             TOXENV: py\n@@ -26,19 +23,19 @@ jobs:\n         - python-version: pypy3\n           env:\n             TOXENV: pypy3\n-            PYPY_VERSION: 3.6-v7.3.3\n+            PYPY_VERSION: 3.9-v7.3.9\n \n         # pinned deps\n-        - python-version: 3.6.12\n+        - python-version: 3.7.13\n           env:\n             TOXENV: pinned\n-        - python-version: 3.6.12\n+        - python-version: 3.7.13\n           env:\n             TOXENV: asyncio-pinned\n         - python-version: pypy3\n           env:\n             TOXENV: pypy3-pinned\n-            PYPY_VERSION: 3.6-v7.2.0\n+            PYPY_VERSION: 3.7-v7.3.5\n \n         # extras\n         # extra-deps includes reppy, which does not support Python 3.9\ndiff --git a/.github/workflows/tests-windows.yml b/.github/workflows/tests-windows.yml\nindex ab738511897..955b9b44909 100644\n--- a/.github/workflows/tests-windows.yml\n+++ b/.github/workflows/tests-windows.yml\n@@ -8,12 +8,9 @@ jobs:\n       fail-fast: false\n       matrix:\n         include:\n-        - python-version: 3.6\n-          env:\n-            TOXENV: windows-pinned\n         - python-version: 3.7\n           env:\n-            TOXENV: py\n+            TOXENV: windows-pinned\n         - python-version: 3.8\n           env:\n             TOXENV: py\ndiff --git a/tests/requirements.txt b/tests/requirements.txt\nindex d2a8aae1b57..d9373dfa808 100644\n--- a/tests/requirements.txt\n+++ b/tests/requirements.txt\n@@ -1,14 +1,12 @@\n # Tests requirements\n attrs\n-dataclasses; python_version == '3.6'\n pyftpdlib\n pytest\n pytest-cov==3.0.0\n pytest-xdist\n sybil >= 1.3.0  # https://github.com/cjw296/sybil/issues/20#issuecomment-605433422\n testfixtures\n-uvloop < 0.15.0; platform_system != \"Windows\" and python_version == '3.6'\n-uvloop; platform_system != \"Windows\" and python_version > '3.6'\n+uvloop; platform_system != \"Windows\"\n \n # optional for shell wrapper tests\n bpython\ndiff --git a/tests/test_utils_python.py b/tests/test_utils_python.py\nindex 4b3964154b7..7dec5624a20 100644\n--- a/tests/test_utils_python.py\n+++ b/tests/test_utils_python.py\n@@ -3,7 +3,6 @@\n import operator\n import platform\n import unittest\n-from datetime import datetime\n from itertools import count\n from warnings import catch_warnings, filterwarnings\n \n@@ -224,12 +223,7 @@ def __call__(self, a, b, c):\n         elif platform.python_implementation() == 'PyPy':\n             self.assertEqual(get_func_args(str.split, stripself=True), ['sep', 'maxsplit'])\n             self.assertEqual(get_func_args(operator.itemgetter(2), stripself=True), ['obj'])\n-\n-            build_date = datetime.strptime(platform.python_build()[1], '%b %d %Y')\n-            if build_date >= datetime(2020, 4, 7):  # PyPy 3.6-v7.3.1\n-                self.assertEqual(get_func_args(\" \".join, stripself=True), ['iterable'])\n-            else:\n-                self.assertEqual(get_func_args(\" \".join, stripself=True), ['list'])\n+            self.assertEqual(get_func_args(\" \".join, stripself=True), ['iterable'])\n \n     def test_without_none_values(self):\n         self.assertEqual(without_none_values([1, None, 3, 4]), [1, 3, 4])\n",
    "problem_statement": "Drop Python 3.6 support\n[It went end-of-life on December 2021](https://endoflife.date/python).\n",
    "hints_text": "",
    "created_at": "2022-06-03T10:20:47Z",
    "version": "2.6",
    "PASS_TO_PASS": [],
    "FAIL_TO_PASS": [
      ".github/workflows/tests-macos.yml",
      ".github/workflows/tests-ubuntu.yml",
      ".github/workflows/tests-windows.yml",
      "tests/requirements.txt",
      "tests/test_utils_python.py"
    ]
  },
  {
    "repo": "scrapy/scrapy",
    "pull_number": 5497,
    "instance_id": "scrapy__scrapy-5497",
    "issue_numbers": [
      "3264",
      "5376"
    ],
    "base_commit": "b2afcbfe2bf090827540d072866bef0d1ab3a3e8",
    "patch": "diff --git a/scrapy/commands/parse.py b/scrapy/commands/parse.py\nindex a3f6b96f420..99fc8f955ba 100644\n--- a/scrapy/commands/parse.py\n+++ b/scrapy/commands/parse.py\n@@ -146,7 +146,8 @@ def set_spidercls(self, url, opts):\n \n         def _start_requests(spider):\n             yield self.prepare_request(spider, Request(url), opts)\n-        self.spidercls.start_requests = _start_requests\n+        if self.spidercls:\n+            self.spidercls.start_requests = _start_requests\n \n     def start_parsing(self, url, opts):\n         self.crawler_process.crawl(self.spidercls, **opts.spargs)\n",
    "test_patch": "diff --git a/tests/test_command_parse.py b/tests/test_command_parse.py\nindex f21ee971d38..0d992be5657 100644\n--- a/tests/test_command_parse.py\n+++ b/tests/test_command_parse.py\n@@ -1,6 +1,7 @@\n import os\n import argparse\n from os.path import join, abspath, isfile, exists\n+\n from twisted.internet import defer\n from scrapy.commands import parse\n from scrapy.settings import Settings\n@@ -222,6 +223,11 @@ def test_crawlspider_no_matching_rule(self):\n         self.assertRegex(_textmode(out), r\"\"\"# Scraped Items  -+\\n\\[\\]\"\"\")\n         self.assertIn(\"\"\"Cannot find a rule that matches\"\"\", _textmode(stderr))\n \n+    @defer.inlineCallbacks\n+    def test_crawlspider_not_exists_with_not_matched_url(self):\n+        status, out, stderr = yield self.execute([self.url('/invalid_url')])\n+        self.assertEqual(status, 0)\n+\n     @defer.inlineCallbacks\n     def test_output_flag(self):\n         \"\"\"Checks if a file was created successfully having\n",
    "problem_statement": "Command parse unhandled error :AttributeError: 'NoneType' object has no attribute 'start_requests'\nScrapy version :1.5.0\r\nWhen i run the command **scrapy parse http://www.baidu.com**, and the url www.baidu.com dosn't  have spider matched , then i got the error:\r\n\r\n> 2018-03-11 16:23:35 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: DouTu)\r\n> 2018-03-11 16:23:35 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.9.0, Python 2.7.12 (default, Dec  4 2017, 14:50:18) - [GCC 5.4.0 20160609], pyOpenSSL 17.5.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Linux-4.13.0-38-generic-x86_64-with-Ubuntu-16.04-xenial\r\n> 2018-05-18 16:23:35 [scrapy.commands.parse] ERROR: Unable to find spider for: http://www.baidu.com\r\n> Traceback (most recent call last):\r\n>   File \"/home/wangsir/code/sourceWorkSpace/scrapy/cmdline.py\", line 239, in <module>\r\n>     execute(['scrapy','parse','http://www.baidu.com'])\r\n>   File \"/home/wangsir/code/sourceWorkSpace/scrapy/cmdline.py\", line 168, in execute\r\n>     _run_print_help(parser, _run_command, cmd, args, opts)\r\n>   File \"/home/wangsir/code/sourceWorkSpace/scrapy/cmdline.py\", line 98, in _run_print_help\r\n>     func(*a, **kw)\r\n>   File \"/home/wangsir/code/sourceWorkSpace/scrapy/cmdline.py\", line 176, in _run_command\r\n>     cmd.run(args, opts)\r\n>   File \"/home/wangsir/code/sourceWorkSpace/scrapy/commands/parse.py\", line 250, in run\r\n>     self.set_spidercls(url, opts)\r\n>   File \"/home/wangsir/code/sourceWorkSpace/scrapy/commands/parse.py\", line 151, in set_spidercls\r\n>     self.spidercls.start_requests = _start_requests\r\n> AttributeError: 'NoneType' object has no attribute 'start_requests'.\r\n\r\nThe failed reason should be follwing code(scrapy/commands/parse.py line 151):\r\n         **`self.spidercls.start_requests = _start_requests`**\r\nbecause the url www.baidu.com dosn't  have spider matched,so self.spidercls is none,so self.spidercls.start_requests throw the error.\nFix command parse unhandled error :AttributeError: 'NoneType' object has no attribute 'start_requests'(#3264)\nReopening @wangrenlei's PR \r\nFixes #3264\n",
    "hints_text": "For those who need a quick-and-dirty workaround, specify the spider `--spider=NAME_OF_MY_SPIDER`\nThe original PR, #3265, looks correct to me, someone just needs to create a new one, import the commit from that one and publish it.\nHello , I think i can help with this issue if someone could give me a little insight .\nhi @PushanAgrawal , as you can see above your comment there is already a PR open for it.\n# [Codecov](https://codecov.io/gh/scrapy/scrapy/pull/5376?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scrapy) Report\n> Merging [#5376](https://codecov.io/gh/scrapy/scrapy/pull/5376?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scrapy) (837038d) into [master](https://codecov.io/gh/scrapy/scrapy/commit/92764d68e272079b6004c194bf237f0e1c8ee95d?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scrapy) (92764d6) will **decrease** coverage by `0.46%`.\n> The diff coverage is `91.89%`.\n\n> :exclamation: Current head 837038d differs from pull request most recent head e7ccec1. Consider uploading reports for the commit e7ccec1 to get more accurate results\n\n\n```diff\n@@            Coverage Diff             @@\n##           master    #5376      +/-   ##\n==========================================\n- Coverage   88.58%   88.11%   -0.47%     \n==========================================\n  Files         163      163              \n  Lines       10635    10628       -7     \n  Branches     1812     1810       -2     \n==========================================\n- Hits         9421     9365      -56     \n- Misses        938      985      +47     \n- Partials      276      278       +2     \n```\n\n| [Impacted Files](https://codecov.io/gh/scrapy/scrapy/pull/5376?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scrapy) | Coverage \u0394 | |\n|---|---|---|\n| [scrapy/commands/parse.py](https://codecov.io/gh/scrapy/scrapy/pull/5376/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scrapy#diff-c2NyYXB5L2NvbW1hbmRzL3BhcnNlLnB5) | `20.11% <0.00%> (-0.12%)` | :arrow_down: |\n| [scrapy/http/response/text.py](https://codecov.io/gh/scrapy/scrapy/pull/5376/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scrapy#diff-c2NyYXB5L2h0dHAvcmVzcG9uc2UvdGV4dC5weQ==) | `100.00% <\u00f8> (\u00f8)` | |\n| [scrapy/downloadermiddlewares/stats.py](https://codecov.io/gh/scrapy/scrapy/pull/5376/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scrapy#diff-c2NyYXB5L2Rvd25sb2FkZXJtaWRkbGV3YXJlcy9zdGF0cy5weQ==) | `91.66% <91.66%> (-0.93%)` | :arrow_down: |\n| [scrapy/exporters.py](https://codecov.io/gh/scrapy/scrapy/pull/5376/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scrapy#diff-c2NyYXB5L2V4cG9ydGVycy5weQ==) | `100.00% <100.00%> (\u00f8)` | |\n| [scrapy/extensions/feedexport.py](https://codecov.io/gh/scrapy/scrapy/pull/5376/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scrapy#diff-c2NyYXB5L2V4dGVuc2lvbnMvZmVlZGV4cG9ydC5weQ==) | `91.23% <100.00%> (-3.85%)` | :arrow_down: |\n| [scrapy/item.py](https://codecov.io/gh/scrapy/scrapy/pull/5376/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scrapy#diff-c2NyYXB5L2l0ZW0ucHk=) | `98.36% <100.00%> (-0.39%)` | :arrow_down: |\n| [scrapy/settings/\\_\\_init\\_\\_.py](https://codecov.io/gh/scrapy/scrapy/pull/5376/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scrapy#diff-c2NyYXB5L3NldHRpbmdzL19faW5pdF9fLnB5) | `93.06% <100.00%> (+0.08%)` | :arrow_up: |\n| [scrapy/spiders/feed.py](https://codecov.io/gh/scrapy/scrapy/pull/5376/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scrapy#diff-c2NyYXB5L3NwaWRlcnMvZmVlZC5weQ==) | `73.84% <100.00%> (+7.69%)` | :arrow_up: |\n| [scrapy/utils/misc.py](https://codecov.io/gh/scrapy/scrapy/pull/5376/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scrapy#diff-c2NyYXB5L3V0aWxzL21pc2MucHk=) | `97.72% <100.00%> (\u00f8)` | |\n| [scrapy/utils/response.py](https://codecov.io/gh/scrapy/scrapy/pull/5376/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scrapy#diff-c2NyYXB5L3V0aWxzL3Jlc3BvbnNlLnB5) | `90.19% <100.00%> (+0.40%)` | :arrow_up: |\n| ... and [13 more](https://codecov.io/gh/scrapy/scrapy/pull/5376/diff?src=pr&el=tree-more&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=scrapy) | |\n\nHey @wRAR, I am new to this, in fact this is my first PR. Is there something else I need to do here? Thanks.\nIdeally, the pull requests should pass all tests, and if not you should find out why and address any issue.\r\n\r\nIt is possible that some of the broken tests are unrelated to your changes though. Let me close and reopen the pull request to re-trigger the tests, and see if they pass.\nThe tests indeed fail for different reasons but this code is not covered by tests. I think it shouldn't be hard to make a test that runs the failing command (even though all existing tests in test_command_parse.py pass --spider explicitly).",
    "created_at": "2022-05-06T22:13:45Z",
    "version": "2.6",
    "PASS_TO_PASS": [],
    "FAIL_TO_PASS": [
      "tests/test_command_parse.py"
    ]
  },
  {
    "repo": "scrapy/scrapy",
    "pull_number": 5482,
    "instance_id": "scrapy__scrapy-5482",
    "issue_numbers": [
      "5481"
    ],
    "base_commit": "aead27bcbdf7c2a4d959dcb357c7f12cc8411739",
    "patch": "diff --git a/scrapy/commands/parse.py b/scrapy/commands/parse.py\nindex a3f6b96f420..8e52d0d7661 100644\n--- a/scrapy/commands/parse.py\n+++ b/scrapy/commands/parse.py\n@@ -51,7 +51,7 @@ def add_options(self, parser):\n         parser.add_argument(\"--cbkwargs\", dest=\"cbkwargs\",\n                             help=\"inject extra callback kwargs into the Request, it must be a valid raw json string\")\n         parser.add_argument(\"-d\", \"--depth\", dest=\"depth\", type=int, default=1,\n-                            help=\"maximum depth for parsing requests [default: %default]\")\n+                            help=\"maximum depth for parsing requests [default: %(default)s]\")\n         parser.add_argument(\"-v\", \"--verbose\", dest=\"verbose\", action=\"store_true\",\n                             help=\"print each depth level one by one\")\n \n",
    "test_patch": "diff --git a/tests/test_commands.py b/tests/test_commands.py\nindex b5e6c2b8bc4..76d5f3935b4 100644\n--- a/tests/test_commands.py\n+++ b/tests/test_commands.py\n@@ -930,3 +930,17 @@ def start_requests(self):\n         args = ['-o', 'example1.json', '-O', 'example2.json']\n         log = self.get_log(spider_code, args=args)\n         self.assertIn(\"error: Please use only one of -o/--output and -O/--overwrite-output\", log)\n+\n+\n+class HelpMessageTest(CommandTest):\n+\n+    def setUp(self):\n+        super().setUp()\n+        self.commands = [\"parse\", \"startproject\", \"view\", \"crawl\", \"edit\",\n+                         \"list\", \"fetch\", \"settings\", \"shell\", \"runspider\",\n+                         \"version\", \"genspider\", \"check\", \"bench\"]\n+\n+    def test_help_messages(self):\n+        for command in self.commands:\n+            _, out, _ = self.proc(command, \"-h\")\n+            self.assertIn(\"Usage\", out)\n",
    "problem_statement": "scrapy parse -h throws error\n### Description\r\nrunning `scrapy parse -h` from inside of a project throws an error.\r\n\r\n### Steps to Reproduce\r\n1.  `scrapy startproject example`\r\n2.  `cd example`\r\n3.  `scrapy parse -h`\r\n\r\n**Expected behavior:**  It should show the help message output for the `parse` command\r\n**Actual behavior:**  Throws error and displays traceback information\r\n**Reproduces how often:**  Every time I tried it.\r\n\r\n### Versions\r\nv2.6.1\r\n\r\n### Additional context\r\n\r\nI identified the issue and will submit a PR shortly. \n",
    "hints_text": "I can reproduce. For completeness, here's the full output I'm getting:\r\n\r\n```\r\n$ scrapy parse -h\r\nTraceback (most recent call last):\r\n  File \"/home/eugenio/zyte/scrapy/venv-scrapy/bin/scrapy\", line 33, in <module>\r\n    sys.exit(load_entry_point('Scrapy', 'console_scripts', 'scrapy')())\r\n  File \"/home/eugenio/zyte/scrapy/scrapy/cmdline.py\", line 141, in execute\r\n    opts, args = parser.parse_known_args(args=argv[1:])\r\n  File \"/usr/local/lib/python3.9/argparse.py\", line 1851, in parse_known_args\r\n    namespace, args = self._parse_known_args(args, namespace)\r\n  File \"/usr/local/lib/python3.9/argparse.py\", line 2060, in _parse_known_args\r\n    start_index = consume_optional(start_index)\r\n  File \"/usr/local/lib/python3.9/argparse.py\", line 2000, in consume_optional\r\n    take_action(action, args, option_string)\r\n  File \"/usr/local/lib/python3.9/argparse.py\", line 1928, in take_action\r\n    action(self, namespace, argument_values, option_string)\r\n  File \"/usr/local/lib/python3.9/argparse.py\", line 1092, in __call__\r\n    parser.print_help()\r\n  File \"/usr/local/lib/python3.9/argparse.py\", line 2548, in print_help\r\n    self._print_message(self.format_help(), file)\r\n  File \"/usr/local/lib/python3.9/argparse.py\", line 2532, in format_help\r\n    return formatter.format_help()\r\n  File \"/usr/local/lib/python3.9/argparse.py\", line 283, in format_help\r\n    help = self._root_section.format_help()\r\n  File \"/usr/local/lib/python3.9/argparse.py\", line 214, in format_help\r\n    item_help = join([func(*args) for func, args in self.items])\r\n  File \"/usr/local/lib/python3.9/argparse.py\", line 214, in <listcomp>\r\n    item_help = join([func(*args) for func, args in self.items])\r\n  File \"/usr/local/lib/python3.9/argparse.py\", line 214, in format_help\r\n    item_help = join([func(*args) for func, args in self.items])\r\n  File \"/usr/local/lib/python3.9/argparse.py\", line 214, in <listcomp>\r\n    item_help = join([func(*args) for func, args in self.items])\r\n  File \"/usr/local/lib/python3.9/argparse.py\", line 530, in _format_action\r\n    help_text = self._expand_help(action)\r\n  File \"/usr/local/lib/python3.9/argparse.py\", line 626, in _expand_help\r\n    return self._get_help_string(action) % params\r\nTypeError: %d format: a number is required, not dict\r\n```\r\n\r\nI suspect this was valid with `optparse` and broke on the move to `argparse` (#5374)",
    "created_at": "2022-04-17T01:20:10Z",
    "version": "2.6",
    "PASS_TO_PASS": [],
    "FAIL_TO_PASS": [
      "tests/test_commands.py"
    ]
  },
  {
    "repo": "scrapy/scrapy",
    "pull_number": 5406,
    "instance_id": "scrapy__scrapy-5406",
    "issue_numbers": [
      "5386"
    ],
    "base_commit": "e2e2ffd0d162cfed5a2e82e9fb9472dbf233c919",
    "patch": "diff --git a/scrapy/mail.py b/scrapy/mail.py\nindex 2a25ccd4499..b8cc28335fa 100644\n--- a/scrapy/mail.py\n+++ b/scrapy/mail.py\n@@ -12,7 +12,9 @@\n from email.utils import formatdate\n from io import BytesIO\n \n+from twisted.python.versions import Version\n from twisted.internet import defer, ssl\n+from twisted import version as twisted_version\n \n from scrapy.utils.misc import arg_to_iter\n from scrapy.utils.python import to_bytes\n@@ -126,16 +128,11 @@ def _sent_failed(self, failure, to, cc, subject, nattachs):\n                       'mailattachs': nattachs, 'mailerr': errstr})\n \n     def _sendmail(self, to_addrs, msg):\n-        # Import twisted.mail here because it is not available in python3\n         from twisted.internet import reactor\n-        from twisted.mail.smtp import ESMTPSenderFactory\n         msg = BytesIO(msg)\n         d = defer.Deferred()\n-        factory = ESMTPSenderFactory(\n-            self.smtpuser, self.smtppass, self.mailfrom, to_addrs, msg, d,\n-            heloFallback=True, requireAuthentication=False, requireTransportSecurity=self.smtptls,\n-        )\n-        factory.noisy = False\n+\n+        factory = self._create_sender_factory(to_addrs, msg, d)\n \n         if self.smtpssl:\n             reactor.connectSSL(self.smtphost, self.smtpport, factory, ssl.ClientContextFactory())\n@@ -143,3 +140,20 @@ def _sendmail(self, to_addrs, msg):\n             reactor.connectTCP(self.smtphost, self.smtpport, factory)\n \n         return d\n+\n+    def _create_sender_factory(self, to_addrs, msg, d):\n+        from twisted.mail.smtp import ESMTPSenderFactory\n+\n+        factory_keywords = {\n+            'heloFallback': True,\n+            'requireAuthentication': False,\n+            'requireTransportSecurity': self.smtptls\n+        }\n+\n+        # Newer versions of twisted require the hostname to use STARTTLS\n+        if twisted_version >= Version('twisted', 21, 2, 0):\n+            factory_keywords['hostname'] = self.smtphost\n+\n+        factory = ESMTPSenderFactory(self.smtpuser, self.smtppass, self.mailfrom, to_addrs, msg, d, **factory_keywords)\n+        factory.noisy = False\n+        return factory\n",
    "test_patch": "diff --git a/tests/test_mail.py b/tests/test_mail.py\nindex 9b248fbfadc..fd02020ee59 100644\n--- a/tests/test_mail.py\n+++ b/tests/test_mail.py\n@@ -4,6 +4,11 @@\n from io import BytesIO\n from email.charset import Charset\n \n+from twisted.internet._sslverify import ClientTLSOptions\n+from twisted.internet.ssl import ClientContextFactory\n+from twisted.python.versions import Version\n+from twisted.internet import defer\n+from twisted import version as twisted_version\n from scrapy.mail import MailSender\n \n \n@@ -121,6 +126,17 @@ def test_send_attach_utf8(self):\n         self.assertEqual(text.get_charset(), Charset('utf-8'))\n         self.assertEqual(attach.get_payload(decode=True).decode('utf-8'), body)\n \n+    def test_create_sender_factory_with_host(self):\n+        mailsender = MailSender(debug=False, smtphost='smtp.testhost.com')\n+\n+        factory = mailsender._create_sender_factory(to_addrs=['test@scrapy.org'], msg='test', d=defer.Deferred())\n+\n+        context = factory.buildProtocol('test@scrapy.org').context\n+        if twisted_version >= Version('twisted', 21, 2, 0):\n+            self.assertIsInstance(context, ClientTLSOptions)\n+        else:\n+            self.assertIsInstance(context, ClientContextFactory)\n+\n \n if __name__ == \"__main__\":\n     unittest.main()\n",
    "problem_statement": "Fix SMTP STARTTLS for Twisted >= 21.2.0\n## Summary\r\n\r\nThe [Mail settings](https://docs.scrapy.org/en/latest/topics/email.html#topics-email-settings) don't have an option to choose a TLS version. Only to enforce upgrading connections to use SSL/TLS.\r\nMail servers like smtp.office365.com dropped support for TLS1.0 and TLS1.1 and now require TLS1.2: https://techcommunity.microsoft.com/t5/exchange-team-blog/new-opt-in-endpoint-available-for-smtp-auth-clients-still/ba-p/2659652 \r\n\r\nIt seems that scrapy mail doesn't support TLS1.2. The error message (with `MAIL_TLS = True`):\r\n\r\n`[scrapy.mail] Unable to send mail: To=['user@gmail.com'] Cc=[] Subject=\"Test\" Attachs=0- 421 b'4.7.66 TLS 1.0 and 1.1 are not supported. Please upgrade/update your client to support TLS 1.2. Visit https://aka.ms/smtp_auth_tls. [AM6P194CA0047.EURP194.PROD.OUTLOOK.COM]'` \r\n\r\n## Motivation\r\n\r\nWithout TLS1.2 it's not possible anymore to send mails via smtp.office365.com. An option to use TLS1.2 would fix this issue\r\n\n",
    "hints_text": "Scrapy doesn't configure protocol versions explicitly when using `ESMTPSenderFactory` and neither does that class, so I would expect it to use global OpenSSL settings, including TLS 1.2 or 1.3 if supported. See https://twistedmatrix.com/documents/current/core/howto/ssl.html\r\n\r\nCan you show the library versions printed by Scrapy?\nIt depends on the version of scrapy and which library version it uses for OpenSSL?\r\nI use scrapy 2.0.1.\r\nScrapy prints the following when starting a job:\r\n`[scrapy.utils.log] Versions: lxml 4.5.0.0, libxml2 2.9.10, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.10.0, Python 3.8.2 (default, Feb 26 2020, 15:09:34) - [GCC 8.3.0], pyOpenSSL 19.1.0 (OpenSSL 1.1.1d 10 Sep 2019), cryptography 2.8, Platform Linux-4.15.0-66-generic-x86_64-with-glibc2.2.5`\r\nThanks for your help!\nI suggest trying newer pyOpenSSL/cryptography/Twisted versions first.\nI upgraded the versions to the newest:\r\n`cryptography==36.0.1\r\npyOpenSSL==22.0.0\r\ntwisted==21.7.0`\r\n\r\nNow I get a new error:\r\n`502 Server does not support secure communication via TLS / SSL`\r\n\r\nSo it definitely looks like the issue is my mail server now. I will close this feature request.\r\nThank you for your quick help\nHi @wRAR I might still need your help.\r\n\r\nThe smtp server supports TLS1.2. I also tested this by creating a script with `smtplib`.\r\nThis script successfully sends my email using TLS1.2:\r\n\r\n```\r\nimport smtplib, ssl\r\n\r\nport = 587\r\nsmtp_server = 'smtp.office365.com'\r\nsender_email = 'test1@test.com'\r\nreceiver_email = 'test2@test.com'\r\nuser = 'test3@test.de'\r\npassword = input('Type your password and press enter:')\r\nmessage = 'test'\r\n\r\ncontext = ssl.create_default_context()\r\ncontext.minimum_version = ssl.TLSVersion.TLSv1_2\r\nwith smtplib.SMTP(smtp_server, port) as server:\r\n    server.ehlo()  # Can be omitted\r\n    server.starttls(context=context)\r\n    server.ehlo()  # Can be omitted\r\n    server.login(user, password)\r\n    server.sendmail(sender_email, receiver_email, message)\r\n```\r\n\r\nI can't seem to get it to work with `scrapy`. I created a new scrapy project with a newer version(`2.5.1`) as well.\r\nThese are all my versions:\r\n`[scrapy.utils.log] INFO: Versions: lxml 4.7.1.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 21.7.0, Python 3.9.7 (default, Sep  3 2021, 12:37:55) - [Clang 12.0.5 (clang-1205.0.22.9)], pyOpenSSL 22.0.0 (OpenSSL 1.1.1m  14 Dec 2021), cryptography 36.0.1, Platform macOS-11.6.2-x86_64-i386-64bit`\r\n\r\nI still get the error:\r\n`[scrapy.mail] ERROR: Unable to send mail: To=['test@test.com'] Cc=[] Subject=\"tls-test\" Attachs=0- 502 Server does not support secure communication via TLS / SSL`\r\n\r\nDo you have an idea what I could try next?\nAre you using MAIL_TLS and port 587?\nBoth yes\nI'll try to check this later\nHi @wRAR, sorry to bother you. \r\nDo you have some more ideas of what I could try? \r\n\nActually \"Server does not support secure communication via TLS / SSL\" is a client error from Twisted and it's misleading because in fact it's the client that doesn't support STARTTLS.\r\n\r\nSince Twisted 21.2.0 (https://github.com/twisted/twisted/commit/abef1218a9013223ee237d8179b4705cbfd716c0), `ESMTPSenderFactory` needs to be passed `hostname` to use STARTTLS. Unfortunately this isn't documented, but the whole of `ESMTPSenderFactory` isn't, the public interface seems to be `sendmail()` which we don't use.\r\n\r\nThe fix to this seems to be just passing `hostname=self.smtphost` to `ESMTPSenderFactory()` in `MailSender._sendmail()`, but only for Twisted 21.2.0 and above.",
    "created_at": "2022-02-09T20:24:48Z",
    "version": "2.5",
    "PASS_TO_PASS": [],
    "FAIL_TO_PASS": [
      "tests/test_mail.py"
    ]
  },
  {
    "repo": "scrapy/scrapy",
    "pull_number": 5394,
    "instance_id": "scrapy__scrapy-5394",
    "issue_numbers": [
      "5391"
    ],
    "base_commit": "b282a7af012a4804eb91bdd850df3b86065b3fd6",
    "patch": "diff --git a/scrapy/spiders/feed.py b/scrapy/spiders/feed.py\nindex bef2d6b2478..79e12e030a1 100644\n--- a/scrapy/spiders/feed.py\n+++ b/scrapy/spiders/feed.py\n@@ -123,7 +123,7 @@ def parse_rows(self, response):\n         process_results methods for pre and post-processing purposes.\n         \"\"\"\n \n-        for row in csviter(response, self.delimiter, self.headers, self.quotechar):\n+        for row in csviter(response, self.delimiter, self.headers, quotechar=self.quotechar):\n             ret = iterate_spider_output(self.parse_row(response, row))\n             for result_item in self.process_results(response, ret):\n                 yield result_item\n",
    "test_patch": "diff --git a/tests/test_spider.py b/tests/test_spider.py\nindex a7c3ee04871..68934999995 100644\n--- a/tests/test_spider.py\n+++ b/tests/test_spider.py\n@@ -21,6 +21,7 @@\n )\n from scrapy.linkextractors import LinkExtractor\n from scrapy.utils.test import get_crawler\n+from tests import get_testdata\n \n \n class SpiderTest(unittest.TestCase):\n@@ -167,6 +168,23 @@ class CSVFeedSpiderTest(SpiderTest):\n \n     spider_class = CSVFeedSpider\n \n+    def test_parse_rows(self):\n+        body = get_testdata('feeds', 'feed-sample6.csv')\n+        response = Response(\"http://example.org/dummy.csv\", body=body)\n+\n+        class _CrawlSpider(self.spider_class):\n+            name = \"test\"\n+            delimiter = \",\"\n+            quotechar = \"'\"\n+\n+            def parse_row(self, response, row):\n+                return row\n+\n+        spider = _CrawlSpider()\n+        rows = list(spider.parse_rows(response))\n+        assert rows[0] == {'id': '1', 'name': 'alpha', 'value': 'foobar'}\n+        assert len(rows) == 4\n+\n \n class CrawlSpiderTest(SpiderTest):\n \n",
    "problem_statement": "Incorrect initialization of `csviter` in `scrapy.spiders.feed.CSVFeedSpider`\n### Description\r\n\r\nAccording to the master branch, `scrapy.spiders.feed.CSVFeedSpider` initializes csviter like this:\r\n```python\r\nfor row in csviter(response, self.delimiter, self.headers, self.quotechar):\r\n```\r\nhttps://github.com/scrapy/scrapy/blob/master/scrapy/spiders/feed.py#L126\r\n\r\nBut `csviter` definition says:\r\n`def csviter(obj, delimiter=None, headers=None, encoding=None, quotechar=None):`\r\nhttps://github.com/scrapy/scrapy/blob/master/scrapy/utils/iterators.py#L96\r\n\r\nSo effectively it passed quotechar instead of encoding\r\n\r\n### Steps to Reproduce\r\n\r\n1. Check the first link\r\n2. Check the second link\r\n3. [and so on...]\r\n\r\n**Expected behavior:** CSVFeedSpider has a separate setting for the encoding or specify param names when calling csviter\r\n\r\n**Actual behavior:** Mess\r\n\r\n**Reproduces how often:** 100%\r\n\r\n### Versions\r\n\r\n```\r\nScrapy       : 2.5.0\r\nlxml         : 4.6.3.0\r\nlibxml2      : 2.9.10\r\ncssselect    : 1.1.0\r\nparsel       : 1.6.0\r\nw3lib        : 1.22.0\r\nTwisted      : 21.2.0\r\nPython       : 3.9.8 (main, Nov 10 2021, 09:21:22) - [Clang 13.0.0 (clang-1300.0.29.3)]\r\npyOpenSSL    : 20.0.1 (OpenSSL 1.1.1k  25 Mar 2021)\r\ncryptography : 3.4.7\r\nPlatform     : macOS-11.6-x86_64-i386-64bit\r\n```\r\n\r\n### Additional context\r\n\r\nPlease fix it :)\n",
    "hints_text": "",
    "created_at": "2022-02-05T18:27:07Z",
    "version": "2.5",
    "PASS_TO_PASS": [],
    "FAIL_TO_PASS": [
      "tests/test_spider.py::CrawlerProcessSubprocess::test_multi"
    ]
  },
  {
    "repo": "scrapy/scrapy",
    "pull_number": 5320,
    "instance_id": "scrapy__scrapy-5320",
    "issue_numbers": [
      "5319"
    ],
    "base_commit": "28eba610e22c0d2a42e830b4e64746edf44598f9",
    "patch": "diff --git a/scrapy/utils/response.py b/scrapy/utils/response.py\nindex b3ef7b4637f..8b109dced2a 100644\n--- a/scrapy/utils/response.py\n+++ b/scrapy/utils/response.py\n@@ -3,8 +3,9 @@\n scrapy.http.Response objects\n \"\"\"\n import os\n-import webbrowser\n+import re\n import tempfile\n+import webbrowser\n from typing import Any, Callable, Iterable, Optional, Tuple, Union\n from weakref import WeakKeyDictionary\n \n@@ -80,8 +81,9 @@ def open_in_browser(\n     body = response.body\n     if isinstance(response, HtmlResponse):\n         if b'<base' not in body:\n-            repl = f'<head><base href=\"{response.url}\">'\n-            body = body.replace(b'<head>', to_bytes(repl))\n+            repl = fr'\\1<base href=\"{response.url}\">'\n+            body = re.sub(b\"<!--.*?-->\", b\"\", body, flags=re.DOTALL)\n+            body = re.sub(rb\"(<head(?:>|\\s.*?>))\", to_bytes(repl), body)\n         ext = '.html'\n     elif isinstance(response, TextResponse):\n         ext = '.txt'\n",
    "test_patch": "diff --git a/tests/test_utils_response.py b/tests/test_utils_response.py\nindex d6f4c0bb59e..0a09f610927 100644\n--- a/tests/test_utils_response.py\n+++ b/tests/test_utils_response.py\n@@ -83,3 +83,56 @@ def test_response_status_message(self):\n         self.assertEqual(response_status_message(200), '200 OK')\n         self.assertEqual(response_status_message(404), '404 Not Found')\n         self.assertEqual(response_status_message(573), \"573 Unknown Status\")\n+\n+    def test_inject_base_url(self):\n+        url = \"http://www.example.com\"\n+\n+        def check_base_url(burl):\n+            path = urlparse(burl).path\n+            if not os.path.exists(path):\n+                path = burl.replace('file://', '')\n+            with open(path, \"rb\") as f:\n+                bbody = f.read()\n+            self.assertEqual(bbody.count(b'<base href=\"' + to_bytes(url) + b'\">'), 1)\n+            return True\n+\n+        r1 = HtmlResponse(url, body=b\"\"\"\n+        <html>\n+            <head><title>Dummy</title></head>\n+            <body><p>Hello world.</p></body>\n+        </html>\"\"\")\n+        r2 = HtmlResponse(url, body=b\"\"\"\n+        <html>\n+            <head id=\"foo\"><title>Dummy</title></head>\n+            <body>Hello world.</body>\n+        </html>\"\"\")\n+        r3 = HtmlResponse(url, body=b\"\"\"\n+        <html>\n+            <head><title>Dummy</title></head>\n+            <body>\n+                <header>Hello header</header>\n+                <p>Hello world.</p>\n+            </body>\n+        </html>\"\"\")\n+        r4 = HtmlResponse(url, body=b\"\"\"\n+        <html>\n+            <!-- <head>Dummy comment</head> -->\n+            <head><title>Dummy</title></head>\n+            <body><p>Hello world.</p></body>\n+        </html>\"\"\")\n+        r5 = HtmlResponse(url, body=b\"\"\"\n+        <html>\n+            <!--[if IE]>\n+            <head><title>IE head</title></head>\n+            <![endif]-->\n+            <!--[if !IE]>-->\n+            <head><title>Standard head</title></head>\n+            <!--<![endif]-->\n+            <body><p>Hello world.</p></body>\n+        </html>\"\"\")\n+\n+        assert open_in_browser(r1, _openfunc=check_base_url), \"Inject base url\"\n+        assert open_in_browser(r2, _openfunc=check_base_url), \"Inject base url with argumented head\"\n+        assert open_in_browser(r3, _openfunc=check_base_url), \"Inject unique base url with misleading tag\"\n+        assert open_in_browser(r4, _openfunc=check_base_url), \"Inject unique base url with misleading comment\"\n+        assert open_in_browser(r5, _openfunc=check_base_url), \"Inject unique base url with conditional comment\"\n",
    "problem_statement": "Open in Browser `<base>` replacement will fail if `<head>` has attributes\n### Description\r\n\r\nWhen using `open_in_browser()` feature, Scrapy will try to add a `<base>` tag to ensure remote resources are loaded, and to make external links to work in our local browser. This feature rely on the following code:\r\n\r\nhttps://github.com/scrapy/scrapy/blob/06f3d12c1208c380f9f1a16cb36ba2dfa3c244c5/scrapy/utils/response.py#L81-L84\r\n\r\nSome website are using attributes on the `<head>` tag, which will prevent the `<base>` tag to be injected, and therefore external resources to be loaded.\r\n\r\n### How to reproduce the issue \r\n\r\nSimply create a basic spider [following Scrapy tutorial](https://docs.scrapy.org/en/latest/intro/tutorial.html) and use the following code:\r\n\r\n```py\r\nimport scrapy\r\nfrom scrapy.utils.response import open_in_browser\r\n\r\nclass ExampleSpider(scrapy.Spider):\r\n    name = 'example'\r\n    allowed_domains = ['example.com']\r\n    start_urls = [\r\n        'https://example.com/head-without-argument.html', \r\n        'https://example.com/head-with-argument.html']\r\n\r\n    def parse(self, response):\r\n        open_in_browser(response)\r\n        pass\r\n```\r\n\r\nFor the scrapped pages itselves, use the simplest code possible (I've not been able to quickly find a public page using arguments on `<head>`, sorry):\r\n\r\n```html\r\n<!DOCTYPE html>\r\n<html>\r\n  <!-- head-without-argument.html -->\r\n  <head>\r\n    <title>Title</title>\r\n  </head>\r\n  <body>\r\n    <p>Foo</p>\r\n    <img src=\"./assets/image.jpg\">\r\n  </body>\r\n</html>\r\n```\r\n\r\n<!-- -->\r\n\r\n```html\r\n<!DOCTYPE html>\r\n<html>\r\n  <!-- head-with-argument.html -->\r\n  <head id=\"example\">\r\n    <title>Title</title>\r\n  </head>\r\n  <body>\r\n    <p>Foo</p>\r\n    <img src=\"./assets/image.jpg\">\r\n  </body>\r\n</html>\r\n```\r\n\r\nThen run the spider with `scrapy crawl example` and you'll see that:\r\n1. `head-without-argument.html` output renders resource correctly\r\n2. `head-with-argument.html` output doesn't render resource\r\n\r\n### How to fix the issue\r\n\r\nAt the very least, the literal `replace()` function should be replace by a regex replacement:\r\n```py\r\n if isinstance(response, HtmlResponse): \r\n     if b'<base' not in body: \r\n         repl = f'\\\\1<base href=\"{response.url}\">' \r\n         body = re.sub(b\"(<head.*?>)\", to_bytes(repl), body)\r\n```\r\n\r\n### Environment \r\n```\r\nScrapy       : 2.5.1\r\nlxml         : 4.6.3.0\r\nlibxml2      : 2.9.4\r\ncssselect    : 1.1.0\r\nparsel       : 1.6.0\r\nw3lib        : 1.22.0\r\nTwisted      : 21.7.0\r\nPython       : 3.9.7 (default, Sep  3 2021, 04:31:11) - [Clang 12.0.5 (clang-1205.0.22.9)]\r\npyOpenSSL    : 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021)\r\ncryptography : 35.0.0\r\nPlatform     : macOS-11.6-arm64-arm-64bit\r\n```\n",
    "hints_text": "",
    "created_at": "2021-11-15T10:15:57Z",
    "version": "2.5",
    "PASS_TO_PASS": [],
    "FAIL_TO_PASS": [
      "tests/test_utils_response.py"
    ]
  }
]
