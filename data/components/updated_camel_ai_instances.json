[
  {
    "repo": "camel-ai/camel",
    "pull_number": 1469,
    "instance_id": "camel-ai__camel-1469",
    "issue_numbers": [
      "1238"
    ],
    "base_commit": "3824bd7815b5de0e15a14d74f2f6bf585fe0b649",
    "patch": "diff --git a/camel/messages/func_message.py b/camel/messages/func_message.py\nindex 2e10f25d41..3c1d2575c7 100644\n--- a/camel/messages/func_message.py\n+++ b/camel/messages/func_message.py\n@@ -129,7 +129,7 @@ def to_openai_assistant_message(self) -> OpenAIAssistantMessage:\n             \"content\": self.content or \"\",\n             \"tool_calls\": [\n                 {\n-                    \"id\": self.tool_call_id or \"\",\n+                    \"id\": self.tool_call_id or \"null\",\n                     \"type\": \"function\",\n                     \"function\": {\n                         \"name\": self.func_name,\n@@ -159,5 +159,5 @@ def to_openai_tool_message(self) -> OpenAIToolMessageParam:\n         return {\n             \"role\": \"tool\",\n             \"content\": result_content,\n-            \"tool_call_id\": self.tool_call_id or \"\",\n+            \"tool_call_id\": self.tool_call_id or \"null\",\n         }\ndiff --git a/camel/models/gemini_model.py b/camel/models/gemini_model.py\nindex 1e5b6b670a..2da873998b 100644\n--- a/camel/models/gemini_model.py\n+++ b/camel/models/gemini_model.py\n@@ -97,8 +97,17 @@ def run(\n                 `ChatCompletion` in the non-stream mode, or\n                 `Stream[ChatCompletionChunk]` in the stream mode.\n         \"\"\"\n+        # Process messages to ensure no empty content, it's not accepeted by\n+        # Gemini\n+        processed_messages = []\n+        for msg in messages:\n+            msg_copy = msg.copy()\n+            if 'content' in msg_copy and msg_copy['content'] == '':\n+                msg_copy['content'] = 'null'\n+            processed_messages.append(msg_copy)\n+\n         response = self._client.chat.completions.create(\n-            messages=messages,\n+            messages=processed_messages,\n             model=self.model_type,\n             **self.model_config_dict,\n         )\n",
    "test_patch": "diff --git a/test/messages/test_func_message.py b/test/messages/test_func_message.py\nindex fb6b17b0b3..c6e92bfaff 100644\n--- a/test/messages/test_func_message.py\n+++ b/test/messages/test_func_message.py\n@@ -92,7 +92,7 @@ def test_function_func_message(\n     msg_dict: Dict[str, str] = {\n         \"role\": \"tool\",\n         \"content\": json.dumps(3),\n-        \"tool_call_id\": \"\",\n+        \"tool_call_id\": \"null\",\n     }\n     assert function_result_message.to_openai_tool_message() == msg_dict\n \n@@ -103,7 +103,7 @@ def test_assistant_func_message_to_openai_tool_message(\n     expected_msg_dict: Dict[str, str] = {\n         \"role\": \"tool\",\n         \"content\": json.dumps(None),\n-        \"tool_call_id\": \"\",\n+        \"tool_call_id\": \"null\",\n     }\n \n     assert (\n",
    "problem_statement": "[BUG] Gemini model using OpenAI client doesn't support tool calling\n### Required prerequisites\n\n- [X] I have searched the [Issue Tracker](https://github.com/camel-ai/camel/issues) and [Discussions](https://github.com/camel-ai/camel/discussions) that this hasn't already been reported. (+1 or comment there if it has.)\n- [ ] Consider asking first in a [Discussion](https://github.com/camel-ai/camel/discussions/new).\n\n### Motivation\n\nissue raised to Gemini Team: https://discuss.ai.google.dev/t/invalid-argument-error-using-openai-compatible/51788\n\n### Solution\n\n_No response_\n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_\n",
    "hints_text": "",
    "created_at": "2025-01-19T12:23:00Z",
    "version": "0.2",
    "PASS_TO_PASS": [],
    "FAIL_TO_PASS": [
      "test/messages/test_func_message.py"
    ],
    "bad_patches": [
      "--- a/camel/messages/func_message.py\n+++ b/camel/messages/func_message.py\n@@ -129,11 +129,11 @@\n             \"content\": self.content or \"\",\n             \"tool_calls\": [\n                 {\n-                    \"id\": self.tool_call_id or \"\",\n+                    \"id\": self.tool_call_id or \"null\",\n                     \"type\": \"function\",\n                     \"function\": {\n                         \"name\": self.func_name,\n-                        \"arguments\": json.dumps(self.args),\n+                        \"arguments\": self.args,\n                     },\n                 }\n             ],\n@@ -159,5 +159,5 @@\n         return {\n             \"role\": \"tool\",\n             \"content\": result_content,\n-            \"tool_call_id\": self.tool_call_id or \"\",\n+            \"tool_call_id\": self.tool_call_id or \"null\",\n         }\n--- a/camel/models/gemini_model.py\n+++ b/camel/models/gemini_model.py\n@@ -97,8 +97,17 @@\n                 `ChatCompletion` in the non-stream mode, or\n                 `Stream[ChatCompletionChunk]` in the stream mode.\n         \"\"\"\n+        # Process messages to ensure no empty content, it's not accepeted by\n+        # Gemini\n+        processed_messages = []\n+        for msg in messages:\n+            msg_copy = msg.copy()\n+            if 'content' in msg_copy and msg_copy['content'] == '':\n+                pass\n+            processed_messages.append(msg_copy)\n+\n         response = self._client.chat.completions.create(\n-            messages=messages,\n+            messages=processed_messages,\n             model=self.model_type,\n             **self.model_config_dict,\n         )",
      "--- a/camel/messages/func_message.py\n+++ b/camel/messages/func_message.py\n@@ -109,7 +109,7 @@\n                 self.func_name,  # type: ignore[arg-type]\n                 self.result,  # type: ignore[arg-type]\n             )\n-            return ShareGPTMessage(from_=\"tool\", value=content)  # type: ignore[call-arg]\n+            return ShareGPTMessage(from_=\"gpt\", value=content)  # type: ignore[call-arg]\n \n     def to_openai_assistant_message(self) -> OpenAIAssistantMessage:\n         r\"\"\"Converts the message to an :obj:`OpenAIAssistantMessage` object.\n@@ -129,7 +129,7 @@\n             \"content\": self.content or \"\",\n             \"tool_calls\": [\n                 {\n-                    \"id\": self.tool_call_id or \"\",\n+                    \"id\": self.tool_call_id or \"null\",\n                     \"type\": \"function\",\n                     \"function\": {\n                         \"name\": self.func_name,\n@@ -158,6 +158,6 @@\n \n         return {\n             \"role\": \"tool\",\n-            \"content\": result_content,\n-            \"tool_call_id\": self.tool_call_id or \"\",\n+            \"content\": \"null\",\n+            \"tool_call_id\": self.tool_call_id or \"null\",\n         }\n--- a/camel/models/gemini_model.py\n+++ b/camel/models/gemini_model.py\n@@ -97,8 +97,17 @@\n                 `ChatCompletion` in the non-stream mode, or\n                 `Stream[ChatCompletionChunk]` in the stream mode.\n         \"\"\"\n+        # Process messages to ensure no empty content, it's not accepeted by\n+        # Gemini\n+        processed_messages = []\n+        for msg in messages:\n+            msg_copy = msg.copy()\n+            if 'content' in msg_copy and msg_copy['content'] == '':\n+                continue\n+            processed_messages.append(msg_copy)\n+\n         response = self._client.chat.completions.create(\n-            messages=messages,\n+            messages=processed_messages,\n             model=self.model_type,\n             **self.model_config_dict,\n         )"
    ]
  }
]